{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b46fe41",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - Course Code: 25737</h1>\n",
    "<h4 align=\"center\">Instructor: Dr. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "<h4 align=\"center\">Computer Assignment 3</h4>\n",
    "<h4 align=\"center\">\n",
    "\n",
    "Question 1\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0fc13",
   "metadata": {
    "id": "a6c2fd6d"
   },
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44babb65",
   "metadata": {
    "id": "4e90a030"
   },
   "outputs": [],
   "source": [
    "# Set your student number\n",
    "student_number = 400101078\n",
    "Name = 'Tina'\n",
    "Last_Name = 'Halimi'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a337a",
   "metadata": {
    "id": "339da203"
   },
   "source": [
    "# Rules\n",
    "- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  \n",
    "\n",
    "- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.\n",
    "\n",
    "- You are not allowed to use `torch.nn`, `torch.optim` and any activation function and loss function implemented in torch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b76789",
   "metadata": {
    "id": "881f2e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tinahalimi/Library/Python/3.11/lib/python/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/tinahalimi/Library/Python/3.11/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tinahalimi/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch==2.2.0->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch==2.2.0->torchvision) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install torchvision\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886188c7",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a0adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18510868",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "\n",
    "Here, we download and load the train and test `FashionMNIST` dataset with the desired transforms. Then, we define the dataloaders for `train` and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8759e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FashionMNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_set = FashionMNIST(root='.', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df47fcb",
   "metadata": {},
   "source": [
    "\n",
    "Here you have to calculate the number of classes amd input dimention of the first layer (how many pixels does each image have?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6763e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimentian: torch.Size([1, 28, 28])\n",
      "Number of Pixels: 784 pixels\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "## FILL HERE\n",
    "\n",
    "image_shape = train_set[0][0].shape\n",
    "print(f\"Dimentian: {image_shape}\")\n",
    "input_dim = image_shape[1] * image_shape[2]\n",
    "print(f\"Number of Pixels: {input_dim} pixels\")\n",
    "num_classes = len(train_set.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c695ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, 64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dac6c2",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize 1 random image from each class by using `plt.subplots`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53a764b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "classes = train_set.classes\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 3))\n",
    "\n",
    "class_indices = {i: [] for i in range(10)}\n",
    "\n",
    "for idx, (image, label) in enumerate(train_set):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# Display one random image from each class\n",
    "for class_idx, img_indices in class_indices.items():\n",
    "    img_idx = random.choice(img_indices)\n",
    "    image, label = train_set[img_idx]\n",
    "    axes[class_idx].imshow(image.squeeze(), cmap='gray')\n",
    "    axes[class_idx].set_title(classes[label])\n",
    "    axes[class_idx].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c5aba",
   "metadata": {},
   "source": [
    "## Initializing model's parameters\n",
    "\n",
    "In this part, we create the model and initialize its parameters and store the values of these parameters in the variable `parameters` which is a dictionary including the weigths and biases of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d40952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linear_layer(parameters: dict, shape, device, i=None):\n",
    "    \"\"\"\n",
    "    This function adds parameters of a linear unit of shape `shape` to the `parameters` dictionary.\n",
    "    \"\"\"\n",
    "    n_in, n_out = shape\n",
    "    with torch.no_grad():\n",
    "        w = torch.zeros(*shape, device=device)\n",
    "        # kaiming initialization for ReLU activations:\n",
    "        bound = 1 / np.sqrt(n_in).item()\n",
    "        w.uniform_(-bound, bound)\n",
    "        b = torch.zeros(n_out, device=device)\n",
    "    w.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    # `i` is used to give numbers to parameter names\n",
    "    parameters.update({f'w{i}': w, f'b{i}': b})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce914706",
   "metadata": {},
   "source": [
    "Now we define our neural network with the given layers and add the weights and biases to the dictionary `parameters`. **You are allowed to modify the values of the layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3867d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['w0', 'b0', 'w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "layers = [\n",
    "    (input_dim, 512),\n",
    "    (512, 256),\n",
    "    (256, 128),\n",
    "    (128, 64),\n",
    "    (64, num_classes)\n",
    "]\n",
    "num_layers = len(layers)\n",
    "parameters = {}\n",
    "\n",
    "# setting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i, shape in enumerate(layers):\n",
    "    add_linear_layer(parameters, shape, device, i)\n",
    "\n",
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd2c8e",
   "metadata": {},
   "source": [
    "## Defining the required functions\n",
    "\n",
    "In this section, we should define the required functions. For each of these functions, the inputs and the desired outputs are given and you should write all or part of the function. **You are not allowed to use the activation functions and the loss functions implemented in torch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b413d8",
   "metadata": {},
   "source": [
    "Computing affine and relu outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bebeeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    return torch.matmul(x,w) + b\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    # x.clamp(min=0)\n",
    "    return torch.max(x,torch.zeros(x.shape,device=x.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9baa5e",
   "metadata": {},
   "source": [
    "Function `model` returns output of the whole model for the input `x` using the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2562962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x: torch.Tensor, parameters, num_layers=num_layers):\n",
    "    # number of batches\n",
    "    B = x.shape[0]\n",
    "    x = x.view(B, -1)\n",
    "    \n",
    "    x_temp = x\n",
    "    for i in range(num_layers):\n",
    "        w = parameters[f'w{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "\n",
    "        x_temp = affine_forward(x_temp, w, b)\n",
    "        \n",
    "        if i < num_layers:\n",
    "            x_temp = relu(x_temp)\n",
    "    \n",
    "    output = x_temp\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a9b4c",
   "metadata": {},
   "source": [
    "Implementing cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6959621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(scores, y):\n",
    "    n = len(y)\n",
    "\n",
    "    exp_scores = torch.exp(scores - torch.max(scores, dim=1, keepdim=True).values)\n",
    "    probs = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
    "    # print(len(probs))\n",
    "    correct_logprobs = -torch.log(probs[torch.arange(n), y])\n",
    "    # loss = torch.sum(correct_logprobs) / n\n",
    "    loss = correct_logprobs.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a589af",
   "metadata": {},
   "source": [
    "Implementing a function for optimizing paramters and a function to zeroing out their gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3121c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_optimizer(parameters: Dict[str, torch.Tensor], learning_rate=0.001):\n",
    "    '''This function gets the parameters and a learning rate. Then updates the parameters using their\n",
    "    gradient. Finally, you should zero the gradients of the parameters after updating\n",
    "    the parameter value.'''\n",
    "\n",
    "    for param_name, param in parameters.items():\n",
    "        param.data -= learning_rate * param.grad\n",
    "        param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b4cf8",
   "metadata": {},
   "source": [
    "Training functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76c0f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    return acc\n",
    "\n",
    "def train(train_loader, learning_rate=0.001, epoch=None):\n",
    "    '''This function implements the training loop for a single epoch. For each batch you should do the following:\n",
    "        1- Calculate the output of the model to the given input batch\n",
    "        2- Calculate the loss based on the model output\n",
    "        3- Update the gradients using backward method\n",
    "        4- Optimize the model parameters using the sgd_optimizer function defined previously\n",
    "        5- Print the train loss (Show the epoch and batch as well)\n",
    "        '''\n",
    "    train_loss = 0\n",
    "    N_train = len(train_loader.dataset)\n",
    "    # print(N_train)\n",
    "    \n",
    "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
    "    # for calculateing the accuracy later\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    \n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        p = model(x, parameters)\n",
    "\n",
    "        loss = cross_entropy_loss(p , y)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        sgd_optimizer(parameters, learning_rate)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        print(f\"Train: Epoch [{epoch}], Batch [{i+1}/{len(train_loader)}], Loss: {loss}\")\n",
    "        \n",
    "        y_pred = p.argmax(dim=-1)\n",
    "        Y.append(y.cpu().numpy())\n",
    "        Y_pred.append(y_pred.cpu().numpy())\n",
    "\n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    acc = accuracy(Y_pred, Y)\n",
    "    print(f'Accuracy of train set: {acc}')\n",
    "    train_loss /= len(Y_pred)\n",
    "    # print('epoch loss')\n",
    "    # print(train_loss)\n",
    "    return train_loss, acc\n",
    "\n",
    "\n",
    "def validate(loader, epoch=None, set_name=None):\n",
    "    '''This function validates the model on the test dataloader. The function goes through each batch and does\n",
    "    the following on each batch:\n",
    "        1- Calculate the model output\n",
    "        2- Calculate the loss using the model output\n",
    "        3- Print the loss for each batch and epoch\n",
    "    \n",
    "    Finally the function calculates the model accuracy.'''\n",
    "    total_loss = 0\n",
    "    N = len(loader.dataset)\n",
    "    # print(N)\n",
    "    \n",
    "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
    "    # for calculateing the accuracy later\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        p = model(x, parameters)\n",
    "\n",
    "        loss = cross_entropy_loss(p, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        print(f\"Validation: Epoch [{epoch}], Batch [{i+1}/{len(train_loader)}], Loss: {loss}\")\n",
    "\n",
    "        y_pred = p.argmax(dim=-1)\n",
    "        Y.append(y.cpu().numpy())\n",
    "        Y_pred.append(y_pred.cpu().numpy())\n",
    "    \n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    total_loss /= len(Y_pred)\n",
    "    acc = accuracy(Y_pred, Y)\n",
    "    print(f'Accuracy of {set_name} set: {acc}')\n",
    "    # print('epoch loss valid')\n",
    "    # print(total_loss)\n",
    "\n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ebb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28d4eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, num_epochs, learning_rate=0.001, model_name='pytorch_model'):\n",
    "    '''This function trains the model for the number of epochs given and stores, calculates and prints the train\n",
    "    and test losses and accuracies. Finally, it plots the accuracy and loss history for training and test sets'''\n",
    "    train_loader, test_loader = dataloaders\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss, train_acc = train(train_loader, learning_rate, epoch + 1)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        test_loss, test_acc = validate(test_loader, epoch + 1, set_name='test')\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train')\n",
    "    plt.plot(range(1, num_epochs + 1), test_losses, label='Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss History')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train')\n",
    "    plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy History')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22befeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(train_losses))\n",
    "print(len(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ec4bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch [1], Batch [1/938], Loss: 2.3027215003967285\n",
      "Train: Epoch [1], Batch [2/938], Loss: 2.30234694480896\n",
      "Train: Epoch [1], Batch [3/938], Loss: 2.3020036220550537\n",
      "Train: Epoch [1], Batch [4/938], Loss: 2.3022048473358154\n",
      "Train: Epoch [1], Batch [5/938], Loss: 2.3019611835479736\n",
      "Train: Epoch [1], Batch [6/938], Loss: 2.301673412322998\n",
      "Train: Epoch [1], Batch [7/938], Loss: 2.301635503768921\n",
      "Train: Epoch [1], Batch [8/938], Loss: 2.3019447326660156\n",
      "Train: Epoch [1], Batch [9/938], Loss: 2.3024179935455322\n",
      "Train: Epoch [1], Batch [10/938], Loss: 2.3021247386932373\n",
      "Train: Epoch [1], Batch [11/938], Loss: 2.3019871711730957\n",
      "Train: Epoch [1], Batch [12/938], Loss: 2.301964282989502\n",
      "Train: Epoch [1], Batch [13/938], Loss: 2.301469326019287\n",
      "Train: Epoch [1], Batch [14/938], Loss: 2.302267551422119\n",
      "Train: Epoch [1], Batch [15/938], Loss: 2.3021249771118164\n",
      "Train: Epoch [1], Batch [16/938], Loss: 2.3024168014526367\n",
      "Train: Epoch [1], Batch [17/938], Loss: 2.302306652069092\n",
      "Train: Epoch [1], Batch [18/938], Loss: 2.3008389472961426\n",
      "Train: Epoch [1], Batch [19/938], Loss: 2.302345037460327\n",
      "Train: Epoch [1], Batch [20/938], Loss: 2.30338716506958\n",
      "Train: Epoch [1], Batch [21/938], Loss: 2.3025102615356445\n",
      "Train: Epoch [1], Batch [22/938], Loss: 2.302072048187256\n",
      "Train: Epoch [1], Batch [23/938], Loss: 2.3023157119750977\n",
      "Train: Epoch [1], Batch [24/938], Loss: 2.302353858947754\n",
      "Train: Epoch [1], Batch [25/938], Loss: 2.3016197681427\n",
      "Train: Epoch [1], Batch [26/938], Loss: 2.3017899990081787\n",
      "Train: Epoch [1], Batch [27/938], Loss: 2.3020408153533936\n",
      "Train: Epoch [1], Batch [28/938], Loss: 2.3021771907806396\n",
      "Train: Epoch [1], Batch [29/938], Loss: 2.301215648651123\n",
      "Train: Epoch [1], Batch [30/938], Loss: 2.3010377883911133\n",
      "Train: Epoch [1], Batch [31/938], Loss: 2.302765369415283\n",
      "Train: Epoch [1], Batch [32/938], Loss: 2.301906108856201\n",
      "Train: Epoch [1], Batch [33/938], Loss: 2.301818370819092\n",
      "Train: Epoch [1], Batch [34/938], Loss: 2.3018789291381836\n",
      "Train: Epoch [1], Batch [35/938], Loss: 2.303088426589966\n",
      "Train: Epoch [1], Batch [36/938], Loss: 2.3019862174987793\n",
      "Train: Epoch [1], Batch [37/938], Loss: 2.300445079803467\n",
      "Train: Epoch [1], Batch [38/938], Loss: 2.3015248775482178\n",
      "Train: Epoch [1], Batch [39/938], Loss: 2.3024020195007324\n",
      "Train: Epoch [1], Batch [40/938], Loss: 2.30145263671875\n",
      "Train: Epoch [1], Batch [41/938], Loss: 2.30199933052063\n",
      "Train: Epoch [1], Batch [42/938], Loss: 2.3012447357177734\n",
      "Train: Epoch [1], Batch [43/938], Loss: 2.301927089691162\n",
      "Train: Epoch [1], Batch [44/938], Loss: 2.3011770248413086\n",
      "Train: Epoch [1], Batch [45/938], Loss: 2.3014981746673584\n",
      "Train: Epoch [1], Batch [46/938], Loss: 2.3013968467712402\n",
      "Train: Epoch [1], Batch [47/938], Loss: 2.3011603355407715\n",
      "Train: Epoch [1], Batch [48/938], Loss: 2.3018174171447754\n",
      "Train: Epoch [1], Batch [49/938], Loss: 2.3014636039733887\n",
      "Train: Epoch [1], Batch [50/938], Loss: 2.301424980163574\n",
      "Train: Epoch [1], Batch [51/938], Loss: 2.301142930984497\n",
      "Train: Epoch [1], Batch [52/938], Loss: 2.30163311958313\n",
      "Train: Epoch [1], Batch [53/938], Loss: 2.3011579513549805\n",
      "Train: Epoch [1], Batch [54/938], Loss: 2.3023107051849365\n",
      "Train: Epoch [1], Batch [55/938], Loss: 2.3024911880493164\n",
      "Train: Epoch [1], Batch [56/938], Loss: 2.3027334213256836\n",
      "Train: Epoch [1], Batch [57/938], Loss: 2.3005940914154053\n",
      "Train: Epoch [1], Batch [58/938], Loss: 2.3007569313049316\n",
      "Train: Epoch [1], Batch [59/938], Loss: 2.3010528087615967\n",
      "Train: Epoch [1], Batch [60/938], Loss: 2.30137300491333\n",
      "Train: Epoch [1], Batch [61/938], Loss: 2.302476644515991\n",
      "Train: Epoch [1], Batch [62/938], Loss: 2.300846576690674\n",
      "Train: Epoch [1], Batch [63/938], Loss: 2.302377700805664\n",
      "Train: Epoch [1], Batch [64/938], Loss: 2.301938056945801\n",
      "Train: Epoch [1], Batch [65/938], Loss: 2.300431489944458\n",
      "Train: Epoch [1], Batch [66/938], Loss: 2.3011534214019775\n",
      "Train: Epoch [1], Batch [67/938], Loss: 2.3012051582336426\n",
      "Train: Epoch [1], Batch [68/938], Loss: 2.299933910369873\n",
      "Train: Epoch [1], Batch [69/938], Loss: 2.3021717071533203\n",
      "Train: Epoch [1], Batch [70/938], Loss: 2.3018927574157715\n",
      "Train: Epoch [1], Batch [71/938], Loss: 2.2999675273895264\n",
      "Train: Epoch [1], Batch [72/938], Loss: 2.3008720874786377\n",
      "Train: Epoch [1], Batch [73/938], Loss: 2.3006091117858887\n",
      "Train: Epoch [1], Batch [74/938], Loss: 2.3012866973876953\n",
      "Train: Epoch [1], Batch [75/938], Loss: 2.3005778789520264\n",
      "Train: Epoch [1], Batch [76/938], Loss: 2.3025243282318115\n",
      "Train: Epoch [1], Batch [77/938], Loss: 2.3003039360046387\n",
      "Train: Epoch [1], Batch [78/938], Loss: 2.3012304306030273\n",
      "Train: Epoch [1], Batch [79/938], Loss: 2.301814556121826\n",
      "Train: Epoch [1], Batch [80/938], Loss: 2.3018860816955566\n",
      "Train: Epoch [1], Batch [81/938], Loss: 2.300736665725708\n",
      "Train: Epoch [1], Batch [82/938], Loss: 2.302530288696289\n",
      "Train: Epoch [1], Batch [83/938], Loss: 2.302137851715088\n",
      "Train: Epoch [1], Batch [84/938], Loss: 2.3009231090545654\n",
      "Train: Epoch [1], Batch [85/938], Loss: 2.3004586696624756\n",
      "Train: Epoch [1], Batch [86/938], Loss: 2.3008534908294678\n",
      "Train: Epoch [1], Batch [87/938], Loss: 2.3013412952423096\n",
      "Train: Epoch [1], Batch [88/938], Loss: 2.3008909225463867\n",
      "Train: Epoch [1], Batch [89/938], Loss: 2.301492214202881\n",
      "Train: Epoch [1], Batch [90/938], Loss: 2.3005006313323975\n",
      "Train: Epoch [1], Batch [91/938], Loss: 2.3016273975372314\n",
      "Train: Epoch [1], Batch [92/938], Loss: 2.300490379333496\n",
      "Train: Epoch [1], Batch [93/938], Loss: 2.3005154132843018\n",
      "Train: Epoch [1], Batch [94/938], Loss: 2.3005783557891846\n",
      "Train: Epoch [1], Batch [95/938], Loss: 2.3028037548065186\n",
      "Train: Epoch [1], Batch [96/938], Loss: 2.3028857707977295\n",
      "Train: Epoch [1], Batch [97/938], Loss: 2.301365613937378\n",
      "Train: Epoch [1], Batch [98/938], Loss: 2.3004097938537598\n",
      "Train: Epoch [1], Batch [99/938], Loss: 2.2998902797698975\n",
      "Train: Epoch [1], Batch [100/938], Loss: 2.300652503967285\n",
      "Train: Epoch [1], Batch [101/938], Loss: 2.300429582595825\n",
      "Train: Epoch [1], Batch [102/938], Loss: 2.3006691932678223\n",
      "Train: Epoch [1], Batch [103/938], Loss: 2.301442861557007\n",
      "Train: Epoch [1], Batch [104/938], Loss: 2.301990509033203\n",
      "Train: Epoch [1], Batch [105/938], Loss: 2.300417423248291\n",
      "Train: Epoch [1], Batch [106/938], Loss: 2.301619529724121\n",
      "Train: Epoch [1], Batch [107/938], Loss: 2.3012893199920654\n",
      "Train: Epoch [1], Batch [108/938], Loss: 2.300445079803467\n",
      "Train: Epoch [1], Batch [109/938], Loss: 2.3005664348602295\n",
      "Train: Epoch [1], Batch [110/938], Loss: 2.2989883422851562\n",
      "Train: Epoch [1], Batch [111/938], Loss: 2.3019206523895264\n",
      "Train: Epoch [1], Batch [112/938], Loss: 2.300051689147949\n",
      "Train: Epoch [1], Batch [113/938], Loss: 2.300229787826538\n",
      "Train: Epoch [1], Batch [114/938], Loss: 2.301138401031494\n",
      "Train: Epoch [1], Batch [115/938], Loss: 2.3005495071411133\n",
      "Train: Epoch [1], Batch [116/938], Loss: 2.299738883972168\n",
      "Train: Epoch [1], Batch [117/938], Loss: 2.300898551940918\n",
      "Train: Epoch [1], Batch [118/938], Loss: 2.300968647003174\n",
      "Train: Epoch [1], Batch [119/938], Loss: 2.3009679317474365\n",
      "Train: Epoch [1], Batch [120/938], Loss: 2.299694061279297\n",
      "Train: Epoch [1], Batch [121/938], Loss: 2.300525188446045\n",
      "Train: Epoch [1], Batch [122/938], Loss: 2.3008034229278564\n",
      "Train: Epoch [1], Batch [123/938], Loss: 2.299947738647461\n",
      "Train: Epoch [1], Batch [124/938], Loss: 2.301393508911133\n",
      "Train: Epoch [1], Batch [125/938], Loss: 2.30090069770813\n",
      "Train: Epoch [1], Batch [126/938], Loss: 2.2989585399627686\n",
      "Train: Epoch [1], Batch [127/938], Loss: 2.299945592880249\n",
      "Train: Epoch [1], Batch [128/938], Loss: 2.299617290496826\n",
      "Train: Epoch [1], Batch [129/938], Loss: 2.300255537033081\n",
      "Train: Epoch [1], Batch [130/938], Loss: 2.299463987350464\n",
      "Train: Epoch [1], Batch [131/938], Loss: 2.3014025688171387\n",
      "Train: Epoch [1], Batch [132/938], Loss: 2.300893783569336\n",
      "Train: Epoch [1], Batch [133/938], Loss: 2.3003089427948\n",
      "Train: Epoch [1], Batch [134/938], Loss: 2.300811767578125\n",
      "Train: Epoch [1], Batch [135/938], Loss: 2.300182342529297\n",
      "Train: Epoch [1], Batch [136/938], Loss: 2.302077054977417\n",
      "Train: Epoch [1], Batch [137/938], Loss: 2.300532817840576\n",
      "Train: Epoch [1], Batch [138/938], Loss: 2.301912546157837\n",
      "Train: Epoch [1], Batch [139/938], Loss: 2.300537586212158\n",
      "Train: Epoch [1], Batch [140/938], Loss: 2.3009748458862305\n",
      "Train: Epoch [1], Batch [141/938], Loss: 2.300929307937622\n",
      "Train: Epoch [1], Batch [142/938], Loss: 2.299978494644165\n",
      "Train: Epoch [1], Batch [143/938], Loss: 2.301151752471924\n",
      "Train: Epoch [1], Batch [144/938], Loss: 2.300494432449341\n",
      "Train: Epoch [1], Batch [145/938], Loss: 2.299043655395508\n",
      "Train: Epoch [1], Batch [146/938], Loss: 2.2996950149536133\n",
      "Train: Epoch [1], Batch [147/938], Loss: 2.3013157844543457\n",
      "Train: Epoch [1], Batch [148/938], Loss: 2.301927089691162\n",
      "Train: Epoch [1], Batch [149/938], Loss: 2.3003180027008057\n",
      "Train: Epoch [1], Batch [150/938], Loss: 2.300705909729004\n",
      "Train: Epoch [1], Batch [151/938], Loss: 2.3008410930633545\n",
      "Train: Epoch [1], Batch [152/938], Loss: 2.2993879318237305\n",
      "Train: Epoch [1], Batch [153/938], Loss: 2.2982373237609863\n",
      "Train: Epoch [1], Batch [154/938], Loss: 2.3010919094085693\n",
      "Train: Epoch [1], Batch [155/938], Loss: 2.3011245727539062\n",
      "Train: Epoch [1], Batch [156/938], Loss: 2.3009190559387207\n",
      "Train: Epoch [1], Batch [157/938], Loss: 2.299924373626709\n",
      "Train: Epoch [1], Batch [158/938], Loss: 2.3004801273345947\n",
      "Train: Epoch [1], Batch [159/938], Loss: 2.301515579223633\n",
      "Train: Epoch [1], Batch [160/938], Loss: 2.300244092941284\n",
      "Train: Epoch [1], Batch [161/938], Loss: 2.3010928630828857\n",
      "Train: Epoch [1], Batch [162/938], Loss: 2.300895929336548\n",
      "Train: Epoch [1], Batch [163/938], Loss: 2.300787925720215\n",
      "Train: Epoch [1], Batch [164/938], Loss: 2.2995200157165527\n",
      "Train: Epoch [1], Batch [165/938], Loss: 2.3001861572265625\n",
      "Train: Epoch [1], Batch [166/938], Loss: 2.299428939819336\n",
      "Train: Epoch [1], Batch [167/938], Loss: 2.3001863956451416\n",
      "Train: Epoch [1], Batch [168/938], Loss: 2.298549175262451\n",
      "Train: Epoch [1], Batch [169/938], Loss: 2.300529956817627\n",
      "Train: Epoch [1], Batch [170/938], Loss: 2.3017544746398926\n",
      "Train: Epoch [1], Batch [171/938], Loss: 2.3000752925872803\n",
      "Train: Epoch [1], Batch [172/938], Loss: 2.2998204231262207\n",
      "Train: Epoch [1], Batch [173/938], Loss: 2.2998762130737305\n",
      "Train: Epoch [1], Batch [174/938], Loss: 2.2994186878204346\n",
      "Train: Epoch [1], Batch [175/938], Loss: 2.300337314605713\n",
      "Train: Epoch [1], Batch [176/938], Loss: 2.2992310523986816\n",
      "Train: Epoch [1], Batch [177/938], Loss: 2.300384998321533\n",
      "Train: Epoch [1], Batch [178/938], Loss: 2.3007726669311523\n",
      "Train: Epoch [1], Batch [179/938], Loss: 2.2988524436950684\n",
      "Train: Epoch [1], Batch [180/938], Loss: 2.3005073070526123\n",
      "Train: Epoch [1], Batch [181/938], Loss: 2.3006601333618164\n",
      "Train: Epoch [1], Batch [182/938], Loss: 2.299888849258423\n",
      "Train: Epoch [1], Batch [183/938], Loss: 2.301499366760254\n",
      "Train: Epoch [1], Batch [184/938], Loss: 2.2991552352905273\n",
      "Train: Epoch [1], Batch [185/938], Loss: 2.3002777099609375\n",
      "Train: Epoch [1], Batch [186/938], Loss: 2.2991199493408203\n",
      "Train: Epoch [1], Batch [187/938], Loss: 2.300442695617676\n",
      "Train: Epoch [1], Batch [188/938], Loss: 2.298872709274292\n",
      "Train: Epoch [1], Batch [189/938], Loss: 2.299637794494629\n",
      "Train: Epoch [1], Batch [190/938], Loss: 2.300436019897461\n",
      "Train: Epoch [1], Batch [191/938], Loss: 2.30068039894104\n",
      "Train: Epoch [1], Batch [192/938], Loss: 2.297823429107666\n",
      "Train: Epoch [1], Batch [193/938], Loss: 2.2997536659240723\n",
      "Train: Epoch [1], Batch [194/938], Loss: 2.301004648208618\n",
      "Train: Epoch [1], Batch [195/938], Loss: 2.3003158569335938\n",
      "Train: Epoch [1], Batch [196/938], Loss: 2.2993533611297607\n",
      "Train: Epoch [1], Batch [197/938], Loss: 2.30108904838562\n",
      "Train: Epoch [1], Batch [198/938], Loss: 2.299377918243408\n",
      "Train: Epoch [1], Batch [199/938], Loss: 2.3003902435302734\n",
      "Train: Epoch [1], Batch [200/938], Loss: 2.2991442680358887\n",
      "Train: Epoch [1], Batch [201/938], Loss: 2.2993831634521484\n",
      "Train: Epoch [1], Batch [202/938], Loss: 2.2983219623565674\n",
      "Train: Epoch [1], Batch [203/938], Loss: 2.2996926307678223\n",
      "Train: Epoch [1], Batch [204/938], Loss: 2.300215005874634\n",
      "Train: Epoch [1], Batch [205/938], Loss: 2.299868106842041\n",
      "Train: Epoch [1], Batch [206/938], Loss: 2.299297571182251\n",
      "Train: Epoch [1], Batch [207/938], Loss: 2.2984957695007324\n",
      "Train: Epoch [1], Batch [208/938], Loss: 2.2998409271240234\n",
      "Train: Epoch [1], Batch [209/938], Loss: 2.2979447841644287\n",
      "Train: Epoch [1], Batch [210/938], Loss: 2.2990963459014893\n",
      "Train: Epoch [1], Batch [211/938], Loss: 2.299734115600586\n",
      "Train: Epoch [1], Batch [212/938], Loss: 2.2986011505126953\n",
      "Train: Epoch [1], Batch [213/938], Loss: 2.300001859664917\n",
      "Train: Epoch [1], Batch [214/938], Loss: 2.3013663291931152\n",
      "Train: Epoch [1], Batch [215/938], Loss: 2.301255226135254\n",
      "Train: Epoch [1], Batch [216/938], Loss: 2.299454927444458\n",
      "Train: Epoch [1], Batch [217/938], Loss: 2.3016982078552246\n",
      "Train: Epoch [1], Batch [218/938], Loss: 2.298992156982422\n",
      "Train: Epoch [1], Batch [219/938], Loss: 2.3005034923553467\n",
      "Train: Epoch [1], Batch [220/938], Loss: 2.299705743789673\n",
      "Train: Epoch [1], Batch [221/938], Loss: 2.3001067638397217\n",
      "Train: Epoch [1], Batch [222/938], Loss: 2.2996292114257812\n",
      "Train: Epoch [1], Batch [223/938], Loss: 2.2986903190612793\n",
      "Train: Epoch [1], Batch [224/938], Loss: 2.299069881439209\n",
      "Train: Epoch [1], Batch [225/938], Loss: 2.298649311065674\n",
      "Train: Epoch [1], Batch [226/938], Loss: 2.2997169494628906\n",
      "Train: Epoch [1], Batch [227/938], Loss: 2.3006575107574463\n",
      "Train: Epoch [1], Batch [228/938], Loss: 2.2986695766448975\n",
      "Train: Epoch [1], Batch [229/938], Loss: 2.3017501831054688\n",
      "Train: Epoch [1], Batch [230/938], Loss: 2.298224925994873\n",
      "Train: Epoch [1], Batch [231/938], Loss: 2.2996909618377686\n",
      "Train: Epoch [1], Batch [232/938], Loss: 2.3000106811523438\n",
      "Train: Epoch [1], Batch [233/938], Loss: 2.299197196960449\n",
      "Train: Epoch [1], Batch [234/938], Loss: 2.3001527786254883\n",
      "Train: Epoch [1], Batch [235/938], Loss: 2.299844264984131\n",
      "Train: Epoch [1], Batch [236/938], Loss: 2.299726963043213\n",
      "Train: Epoch [1], Batch [237/938], Loss: 2.298724889755249\n",
      "Train: Epoch [1], Batch [238/938], Loss: 2.299668788909912\n",
      "Train: Epoch [1], Batch [239/938], Loss: 2.2976253032684326\n",
      "Train: Epoch [1], Batch [240/938], Loss: 2.300299644470215\n",
      "Train: Epoch [1], Batch [241/938], Loss: 2.2997045516967773\n",
      "Train: Epoch [1], Batch [242/938], Loss: 2.29938006401062\n",
      "Train: Epoch [1], Batch [243/938], Loss: 2.2999587059020996\n",
      "Train: Epoch [1], Batch [244/938], Loss: 2.2987892627716064\n",
      "Train: Epoch [1], Batch [245/938], Loss: 2.298422336578369\n",
      "Train: Epoch [1], Batch [246/938], Loss: 2.3015224933624268\n",
      "Train: Epoch [1], Batch [247/938], Loss: 2.2980942726135254\n",
      "Train: Epoch [1], Batch [248/938], Loss: 2.298527956008911\n",
      "Train: Epoch [1], Batch [249/938], Loss: 2.29915714263916\n",
      "Train: Epoch [1], Batch [250/938], Loss: 2.299006938934326\n",
      "Train: Epoch [1], Batch [251/938], Loss: 2.2979209423065186\n",
      "Train: Epoch [1], Batch [252/938], Loss: 2.2996671199798584\n",
      "Train: Epoch [1], Batch [253/938], Loss: 2.299834966659546\n",
      "Train: Epoch [1], Batch [254/938], Loss: 2.2992303371429443\n",
      "Train: Epoch [1], Batch [255/938], Loss: 2.2986955642700195\n",
      "Train: Epoch [1], Batch [256/938], Loss: 2.2971181869506836\n",
      "Train: Epoch [1], Batch [257/938], Loss: 2.2981841564178467\n",
      "Train: Epoch [1], Batch [258/938], Loss: 2.299640417098999\n",
      "Train: Epoch [1], Batch [259/938], Loss: 2.2989742755889893\n",
      "Train: Epoch [1], Batch [260/938], Loss: 2.2999660968780518\n",
      "Train: Epoch [1], Batch [261/938], Loss: 2.3015496730804443\n",
      "Train: Epoch [1], Batch [262/938], Loss: 2.3003244400024414\n",
      "Train: Epoch [1], Batch [263/938], Loss: 2.2998735904693604\n",
      "Train: Epoch [1], Batch [264/938], Loss: 2.298921585083008\n",
      "Train: Epoch [1], Batch [265/938], Loss: 2.2981622219085693\n",
      "Train: Epoch [1], Batch [266/938], Loss: 2.3001906871795654\n",
      "Train: Epoch [1], Batch [267/938], Loss: 2.298017740249634\n",
      "Train: Epoch [1], Batch [268/938], Loss: 2.2998952865600586\n",
      "Train: Epoch [1], Batch [269/938], Loss: 2.299323558807373\n",
      "Train: Epoch [1], Batch [270/938], Loss: 2.300111770629883\n",
      "Train: Epoch [1], Batch [271/938], Loss: 2.2977728843688965\n",
      "Train: Epoch [1], Batch [272/938], Loss: 2.299039840698242\n",
      "Train: Epoch [1], Batch [273/938], Loss: 2.2989065647125244\n",
      "Train: Epoch [1], Batch [274/938], Loss: 2.299053430557251\n",
      "Train: Epoch [1], Batch [275/938], Loss: 2.2999072074890137\n",
      "Train: Epoch [1], Batch [276/938], Loss: 2.298102855682373\n",
      "Train: Epoch [1], Batch [277/938], Loss: 2.299569606781006\n",
      "Train: Epoch [1], Batch [278/938], Loss: 2.2988929748535156\n",
      "Train: Epoch [1], Batch [279/938], Loss: 2.2975692749023438\n",
      "Train: Epoch [1], Batch [280/938], Loss: 2.2978615760803223\n",
      "Train: Epoch [1], Batch [281/938], Loss: 2.295762062072754\n",
      "Train: Epoch [1], Batch [282/938], Loss: 2.300442695617676\n",
      "Train: Epoch [1], Batch [283/938], Loss: 2.2981343269348145\n",
      "Train: Epoch [1], Batch [284/938], Loss: 2.2980051040649414\n",
      "Train: Epoch [1], Batch [285/938], Loss: 2.2994444370269775\n",
      "Train: Epoch [1], Batch [286/938], Loss: 2.2987983226776123\n",
      "Train: Epoch [1], Batch [287/938], Loss: 2.299386501312256\n",
      "Train: Epoch [1], Batch [288/938], Loss: 2.2991108894348145\n",
      "Train: Epoch [1], Batch [289/938], Loss: 2.2999603748321533\n",
      "Train: Epoch [1], Batch [290/938], Loss: 2.2988736629486084\n",
      "Train: Epoch [1], Batch [291/938], Loss: 2.297621011734009\n",
      "Train: Epoch [1], Batch [292/938], Loss: 2.29765248298645\n",
      "Train: Epoch [1], Batch [293/938], Loss: 2.2993323802948\n",
      "Train: Epoch [1], Batch [294/938], Loss: 2.298804998397827\n",
      "Train: Epoch [1], Batch [295/938], Loss: 2.2981467247009277\n",
      "Train: Epoch [1], Batch [296/938], Loss: 2.2975378036499023\n",
      "Train: Epoch [1], Batch [297/938], Loss: 2.297022581100464\n",
      "Train: Epoch [1], Batch [298/938], Loss: 2.298943042755127\n",
      "Train: Epoch [1], Batch [299/938], Loss: 2.2982065677642822\n",
      "Train: Epoch [1], Batch [300/938], Loss: 2.2997231483459473\n",
      "Train: Epoch [1], Batch [301/938], Loss: 2.29948091506958\n",
      "Train: Epoch [1], Batch [302/938], Loss: 2.301140785217285\n",
      "Train: Epoch [1], Batch [303/938], Loss: 2.297758102416992\n",
      "Train: Epoch [1], Batch [304/938], Loss: 2.299304485321045\n",
      "Train: Epoch [1], Batch [305/938], Loss: 2.298185348510742\n",
      "Train: Epoch [1], Batch [306/938], Loss: 2.2999749183654785\n",
      "Train: Epoch [1], Batch [307/938], Loss: 2.298750162124634\n",
      "Train: Epoch [1], Batch [308/938], Loss: 2.2989003658294678\n",
      "Train: Epoch [1], Batch [309/938], Loss: 2.2985758781433105\n",
      "Train: Epoch [1], Batch [310/938], Loss: 2.2966620922088623\n",
      "Train: Epoch [1], Batch [311/938], Loss: 2.2985117435455322\n",
      "Train: Epoch [1], Batch [312/938], Loss: 2.299021005630493\n",
      "Train: Epoch [1], Batch [313/938], Loss: 2.2991979122161865\n",
      "Train: Epoch [1], Batch [314/938], Loss: 2.2979319095611572\n",
      "Train: Epoch [1], Batch [315/938], Loss: 2.2991247177124023\n",
      "Train: Epoch [1], Batch [316/938], Loss: 2.29809308052063\n",
      "Train: Epoch [1], Batch [317/938], Loss: 2.2966408729553223\n",
      "Train: Epoch [1], Batch [318/938], Loss: 2.296111583709717\n",
      "Train: Epoch [1], Batch [319/938], Loss: 2.2978084087371826\n",
      "Train: Epoch [1], Batch [320/938], Loss: 2.297840118408203\n",
      "Train: Epoch [1], Batch [321/938], Loss: 2.296461820602417\n",
      "Train: Epoch [1], Batch [322/938], Loss: 2.2981679439544678\n",
      "Train: Epoch [1], Batch [323/938], Loss: 2.299132823944092\n",
      "Train: Epoch [1], Batch [324/938], Loss: 2.2993011474609375\n",
      "Train: Epoch [1], Batch [325/938], Loss: 2.3007075786590576\n",
      "Train: Epoch [1], Batch [326/938], Loss: 2.2990267276763916\n",
      "Train: Epoch [1], Batch [327/938], Loss: 2.299929618835449\n",
      "Train: Epoch [1], Batch [328/938], Loss: 2.300200939178467\n",
      "Train: Epoch [1], Batch [329/938], Loss: 2.298820734024048\n",
      "Train: Epoch [1], Batch [330/938], Loss: 2.2991883754730225\n",
      "Train: Epoch [1], Batch [331/938], Loss: 2.2993433475494385\n",
      "Train: Epoch [1], Batch [332/938], Loss: 2.296797513961792\n",
      "Train: Epoch [1], Batch [333/938], Loss: 2.297341823577881\n",
      "Train: Epoch [1], Batch [334/938], Loss: 2.3002877235412598\n",
      "Train: Epoch [1], Batch [335/938], Loss: 2.298466920852661\n",
      "Train: Epoch [1], Batch [336/938], Loss: 2.2975449562072754\n",
      "Train: Epoch [1], Batch [337/938], Loss: 2.296674966812134\n",
      "Train: Epoch [1], Batch [338/938], Loss: 2.2989108562469482\n",
      "Train: Epoch [1], Batch [339/938], Loss: 2.297736644744873\n",
      "Train: Epoch [1], Batch [340/938], Loss: 2.2972660064697266\n",
      "Train: Epoch [1], Batch [341/938], Loss: 2.3001437187194824\n",
      "Train: Epoch [1], Batch [342/938], Loss: 2.298097610473633\n",
      "Train: Epoch [1], Batch [343/938], Loss: 2.2978720664978027\n",
      "Train: Epoch [1], Batch [344/938], Loss: 2.300381660461426\n",
      "Train: Epoch [1], Batch [345/938], Loss: 2.297312021255493\n",
      "Train: Epoch [1], Batch [346/938], Loss: 2.298983097076416\n",
      "Train: Epoch [1], Batch [347/938], Loss: 2.298508405685425\n",
      "Train: Epoch [1], Batch [348/938], Loss: 2.2968266010284424\n",
      "Train: Epoch [1], Batch [349/938], Loss: 2.2983388900756836\n",
      "Train: Epoch [1], Batch [350/938], Loss: 2.2988617420196533\n",
      "Train: Epoch [1], Batch [351/938], Loss: 2.2974982261657715\n",
      "Train: Epoch [1], Batch [352/938], Loss: 2.2982337474823\n",
      "Train: Epoch [1], Batch [353/938], Loss: 2.2983646392822266\n",
      "Train: Epoch [1], Batch [354/938], Loss: 2.299396514892578\n",
      "Train: Epoch [1], Batch [355/938], Loss: 2.298624038696289\n",
      "Train: Epoch [1], Batch [356/938], Loss: 2.299853563308716\n",
      "Train: Epoch [1], Batch [357/938], Loss: 2.2968368530273438\n",
      "Train: Epoch [1], Batch [358/938], Loss: 2.2979135513305664\n",
      "Train: Epoch [1], Batch [359/938], Loss: 2.2988977432250977\n",
      "Train: Epoch [1], Batch [360/938], Loss: 2.2975058555603027\n",
      "Train: Epoch [1], Batch [361/938], Loss: 2.2974672317504883\n",
      "Train: Epoch [1], Batch [362/938], Loss: 2.2979736328125\n",
      "Train: Epoch [1], Batch [363/938], Loss: 2.297769546508789\n",
      "Train: Epoch [1], Batch [364/938], Loss: 2.2961134910583496\n",
      "Train: Epoch [1], Batch [365/938], Loss: 2.2987821102142334\n",
      "Train: Epoch [1], Batch [366/938], Loss: 2.2975471019744873\n",
      "Train: Epoch [1], Batch [367/938], Loss: 2.300722360610962\n",
      "Train: Epoch [1], Batch [368/938], Loss: 2.2975239753723145\n",
      "Train: Epoch [1], Batch [369/938], Loss: 2.298156976699829\n",
      "Train: Epoch [1], Batch [370/938], Loss: 2.297379493713379\n",
      "Train: Epoch [1], Batch [371/938], Loss: 2.2956738471984863\n",
      "Train: Epoch [1], Batch [372/938], Loss: 2.295086622238159\n",
      "Train: Epoch [1], Batch [373/938], Loss: 2.2985682487487793\n",
      "Train: Epoch [1], Batch [374/938], Loss: 2.297567367553711\n",
      "Train: Epoch [1], Batch [375/938], Loss: 2.298278331756592\n",
      "Train: Epoch [1], Batch [376/938], Loss: 2.297834634780884\n",
      "Train: Epoch [1], Batch [377/938], Loss: 2.2980213165283203\n",
      "Train: Epoch [1], Batch [378/938], Loss: 2.299571990966797\n",
      "Train: Epoch [1], Batch [379/938], Loss: 2.298250675201416\n",
      "Train: Epoch [1], Batch [380/938], Loss: 2.296052932739258\n",
      "Train: Epoch [1], Batch [381/938], Loss: 2.296222686767578\n",
      "Train: Epoch [1], Batch [382/938], Loss: 2.297887086868286\n",
      "Train: Epoch [1], Batch [383/938], Loss: 2.3001270294189453\n",
      "Train: Epoch [1], Batch [384/938], Loss: 2.298492670059204\n",
      "Train: Epoch [1], Batch [385/938], Loss: 2.2991418838500977\n",
      "Train: Epoch [1], Batch [386/938], Loss: 2.2965078353881836\n",
      "Train: Epoch [1], Batch [387/938], Loss: 2.2976207733154297\n",
      "Train: Epoch [1], Batch [388/938], Loss: 2.2980542182922363\n",
      "Train: Epoch [1], Batch [389/938], Loss: 2.297269821166992\n",
      "Train: Epoch [1], Batch [390/938], Loss: 2.2985177040100098\n",
      "Train: Epoch [1], Batch [391/938], Loss: 2.2993733882904053\n",
      "Train: Epoch [1], Batch [392/938], Loss: 2.298304557800293\n",
      "Train: Epoch [1], Batch [393/938], Loss: 2.2974984645843506\n",
      "Train: Epoch [1], Batch [394/938], Loss: 2.297869920730591\n",
      "Train: Epoch [1], Batch [395/938], Loss: 2.296912431716919\n",
      "Train: Epoch [1], Batch [396/938], Loss: 2.297926902770996\n",
      "Train: Epoch [1], Batch [397/938], Loss: 2.2986884117126465\n",
      "Train: Epoch [1], Batch [398/938], Loss: 2.2976996898651123\n",
      "Train: Epoch [1], Batch [399/938], Loss: 2.2981629371643066\n",
      "Train: Epoch [1], Batch [400/938], Loss: 2.2984843254089355\n",
      "Train: Epoch [1], Batch [401/938], Loss: 2.29911470413208\n",
      "Train: Epoch [1], Batch [402/938], Loss: 2.298370122909546\n",
      "Train: Epoch [1], Batch [403/938], Loss: 2.2989342212677\n",
      "Train: Epoch [1], Batch [404/938], Loss: 2.298344373703003\n",
      "Train: Epoch [1], Batch [405/938], Loss: 2.2965946197509766\n",
      "Train: Epoch [1], Batch [406/938], Loss: 2.2998080253601074\n",
      "Train: Epoch [1], Batch [407/938], Loss: 2.296081304550171\n",
      "Train: Epoch [1], Batch [408/938], Loss: 2.297546148300171\n",
      "Train: Epoch [1], Batch [409/938], Loss: 2.2946741580963135\n",
      "Train: Epoch [1], Batch [410/938], Loss: 2.294302463531494\n",
      "Train: Epoch [1], Batch [411/938], Loss: 2.296311855316162\n",
      "Train: Epoch [1], Batch [412/938], Loss: 2.297049045562744\n",
      "Train: Epoch [1], Batch [413/938], Loss: 2.2980997562408447\n",
      "Train: Epoch [1], Batch [414/938], Loss: 2.2994771003723145\n",
      "Train: Epoch [1], Batch [415/938], Loss: 2.2965874671936035\n",
      "Train: Epoch [1], Batch [416/938], Loss: 2.297636032104492\n",
      "Train: Epoch [1], Batch [417/938], Loss: 2.2957589626312256\n",
      "Train: Epoch [1], Batch [418/938], Loss: 2.2988297939300537\n",
      "Train: Epoch [1], Batch [419/938], Loss: 2.2987616062164307\n",
      "Train: Epoch [1], Batch [420/938], Loss: 2.298994302749634\n",
      "Train: Epoch [1], Batch [421/938], Loss: 2.298018455505371\n",
      "Train: Epoch [1], Batch [422/938], Loss: 2.2988533973693848\n",
      "Train: Epoch [1], Batch [423/938], Loss: 2.2990384101867676\n",
      "Train: Epoch [1], Batch [424/938], Loss: 2.296856164932251\n",
      "Train: Epoch [1], Batch [425/938], Loss: 2.2975735664367676\n",
      "Train: Epoch [1], Batch [426/938], Loss: 2.298656702041626\n",
      "Train: Epoch [1], Batch [427/938], Loss: 2.2966058254241943\n",
      "Train: Epoch [1], Batch [428/938], Loss: 2.2955682277679443\n",
      "Train: Epoch [1], Batch [429/938], Loss: 2.2979471683502197\n",
      "Train: Epoch [1], Batch [430/938], Loss: 2.296692371368408\n",
      "Train: Epoch [1], Batch [431/938], Loss: 2.299462080001831\n",
      "Train: Epoch [1], Batch [432/938], Loss: 2.2993781566619873\n",
      "Train: Epoch [1], Batch [433/938], Loss: 2.295389413833618\n",
      "Train: Epoch [1], Batch [434/938], Loss: 2.2996551990509033\n",
      "Train: Epoch [1], Batch [435/938], Loss: 2.2960774898529053\n",
      "Train: Epoch [1], Batch [436/938], Loss: 2.2950217723846436\n",
      "Train: Epoch [1], Batch [437/938], Loss: 2.296571731567383\n",
      "Train: Epoch [1], Batch [438/938], Loss: 2.294837474822998\n",
      "Train: Epoch [1], Batch [439/938], Loss: 2.2992475032806396\n",
      "Train: Epoch [1], Batch [440/938], Loss: 2.2972981929779053\n",
      "Train: Epoch [1], Batch [441/938], Loss: 2.2990825176239014\n",
      "Train: Epoch [1], Batch [442/938], Loss: 2.297100067138672\n",
      "Train: Epoch [1], Batch [443/938], Loss: 2.29887056350708\n",
      "Train: Epoch [1], Batch [444/938], Loss: 2.2962679862976074\n",
      "Train: Epoch [1], Batch [445/938], Loss: 2.29880690574646\n",
      "Train: Epoch [1], Batch [446/938], Loss: 2.2975525856018066\n",
      "Train: Epoch [1], Batch [447/938], Loss: 2.297023057937622\n",
      "Train: Epoch [1], Batch [448/938], Loss: 2.296280860900879\n",
      "Train: Epoch [1], Batch [449/938], Loss: 2.2990095615386963\n",
      "Train: Epoch [1], Batch [450/938], Loss: 2.2956597805023193\n",
      "Train: Epoch [1], Batch [451/938], Loss: 2.2978694438934326\n",
      "Train: Epoch [1], Batch [452/938], Loss: 2.300558567047119\n",
      "Train: Epoch [1], Batch [453/938], Loss: 2.297060012817383\n",
      "Train: Epoch [1], Batch [454/938], Loss: 2.2968382835388184\n",
      "Train: Epoch [1], Batch [455/938], Loss: 2.2986948490142822\n",
      "Train: Epoch [1], Batch [456/938], Loss: 2.2963039875030518\n",
      "Train: Epoch [1], Batch [457/938], Loss: 2.2969090938568115\n",
      "Train: Epoch [1], Batch [458/938], Loss: 2.2963480949401855\n",
      "Train: Epoch [1], Batch [459/938], Loss: 2.2946536540985107\n",
      "Train: Epoch [1], Batch [460/938], Loss: 2.2962558269500732\n",
      "Train: Epoch [1], Batch [461/938], Loss: 2.2974953651428223\n",
      "Train: Epoch [1], Batch [462/938], Loss: 2.2976415157318115\n",
      "Train: Epoch [1], Batch [463/938], Loss: 2.2957186698913574\n",
      "Train: Epoch [1], Batch [464/938], Loss: 2.2951090335845947\n",
      "Train: Epoch [1], Batch [465/938], Loss: 2.2968578338623047\n",
      "Train: Epoch [1], Batch [466/938], Loss: 2.2978477478027344\n",
      "Train: Epoch [1], Batch [467/938], Loss: 2.297883987426758\n",
      "Train: Epoch [1], Batch [468/938], Loss: 2.295621395111084\n",
      "Train: Epoch [1], Batch [469/938], Loss: 2.296937942504883\n",
      "Train: Epoch [1], Batch [470/938], Loss: 2.296647071838379\n",
      "Train: Epoch [1], Batch [471/938], Loss: 2.2982678413391113\n",
      "Train: Epoch [1], Batch [472/938], Loss: 2.2970192432403564\n",
      "Train: Epoch [1], Batch [473/938], Loss: 2.298321485519409\n",
      "Train: Epoch [1], Batch [474/938], Loss: 2.297901153564453\n",
      "Train: Epoch [1], Batch [475/938], Loss: 2.2983107566833496\n",
      "Train: Epoch [1], Batch [476/938], Loss: 2.2950849533081055\n",
      "Train: Epoch [1], Batch [477/938], Loss: 2.2943100929260254\n",
      "Train: Epoch [1], Batch [478/938], Loss: 2.2953290939331055\n",
      "Train: Epoch [1], Batch [479/938], Loss: 2.2984161376953125\n",
      "Train: Epoch [1], Batch [480/938], Loss: 2.2971153259277344\n",
      "Train: Epoch [1], Batch [481/938], Loss: 2.2967848777770996\n",
      "Train: Epoch [1], Batch [482/938], Loss: 2.296126127243042\n",
      "Train: Epoch [1], Batch [483/938], Loss: 2.2959702014923096\n",
      "Train: Epoch [1], Batch [484/938], Loss: 2.295686721801758\n",
      "Train: Epoch [1], Batch [485/938], Loss: 2.2973015308380127\n",
      "Train: Epoch [1], Batch [486/938], Loss: 2.2966790199279785\n",
      "Train: Epoch [1], Batch [487/938], Loss: 2.2963645458221436\n",
      "Train: Epoch [1], Batch [488/938], Loss: 2.294635534286499\n",
      "Train: Epoch [1], Batch [489/938], Loss: 2.2975077629089355\n",
      "Train: Epoch [1], Batch [490/938], Loss: 2.295492649078369\n",
      "Train: Epoch [1], Batch [491/938], Loss: 2.297670841217041\n",
      "Train: Epoch [1], Batch [492/938], Loss: 2.2965245246887207\n",
      "Train: Epoch [1], Batch [493/938], Loss: 2.297355890274048\n",
      "Train: Epoch [1], Batch [494/938], Loss: 2.2977914810180664\n",
      "Train: Epoch [1], Batch [495/938], Loss: 2.2953169345855713\n",
      "Train: Epoch [1], Batch [496/938], Loss: 2.2971508502960205\n",
      "Train: Epoch [1], Batch [497/938], Loss: 2.295823335647583\n",
      "Train: Epoch [1], Batch [498/938], Loss: 2.29714035987854\n",
      "Train: Epoch [1], Batch [499/938], Loss: 2.2973294258117676\n",
      "Train: Epoch [1], Batch [500/938], Loss: 2.2974002361297607\n",
      "Train: Epoch [1], Batch [501/938], Loss: 2.297207832336426\n",
      "Train: Epoch [1], Batch [502/938], Loss: 2.2939059734344482\n",
      "Train: Epoch [1], Batch [503/938], Loss: 2.2968645095825195\n",
      "Train: Epoch [1], Batch [504/938], Loss: 2.294560432434082\n",
      "Train: Epoch [1], Batch [505/938], Loss: 2.2957940101623535\n",
      "Train: Epoch [1], Batch [506/938], Loss: 2.296923875808716\n",
      "Train: Epoch [1], Batch [507/938], Loss: 2.296916961669922\n",
      "Train: Epoch [1], Batch [508/938], Loss: 2.297022819519043\n",
      "Train: Epoch [1], Batch [509/938], Loss: 2.2972030639648438\n",
      "Train: Epoch [1], Batch [510/938], Loss: 2.296719551086426\n",
      "Train: Epoch [1], Batch [511/938], Loss: 2.2964367866516113\n",
      "Train: Epoch [1], Batch [512/938], Loss: 2.299407958984375\n",
      "Train: Epoch [1], Batch [513/938], Loss: 2.29834246635437\n",
      "Train: Epoch [1], Batch [514/938], Loss: 2.294191360473633\n",
      "Train: Epoch [1], Batch [515/938], Loss: 2.29426646232605\n",
      "Train: Epoch [1], Batch [516/938], Loss: 2.2968029975891113\n",
      "Train: Epoch [1], Batch [517/938], Loss: 2.2955081462860107\n",
      "Train: Epoch [1], Batch [518/938], Loss: 2.294719934463501\n",
      "Train: Epoch [1], Batch [519/938], Loss: 2.2973246574401855\n",
      "Train: Epoch [1], Batch [520/938], Loss: 2.2976417541503906\n",
      "Train: Epoch [1], Batch [521/938], Loss: 2.2946927547454834\n",
      "Train: Epoch [1], Batch [522/938], Loss: 2.292015790939331\n",
      "Train: Epoch [1], Batch [523/938], Loss: 2.296030282974243\n",
      "Train: Epoch [1], Batch [524/938], Loss: 2.294670581817627\n",
      "Train: Epoch [1], Batch [525/938], Loss: 2.2977638244628906\n",
      "Train: Epoch [1], Batch [526/938], Loss: 2.2955527305603027\n",
      "Train: Epoch [1], Batch [527/938], Loss: 2.296464204788208\n",
      "Train: Epoch [1], Batch [528/938], Loss: 2.297813653945923\n",
      "Train: Epoch [1], Batch [529/938], Loss: 2.2961597442626953\n",
      "Train: Epoch [1], Batch [530/938], Loss: 2.2955377101898193\n",
      "Train: Epoch [1], Batch [531/938], Loss: 2.2971882820129395\n",
      "Train: Epoch [1], Batch [532/938], Loss: 2.297238349914551\n",
      "Train: Epoch [1], Batch [533/938], Loss: 2.297025680541992\n",
      "Train: Epoch [1], Batch [534/938], Loss: 2.2954492568969727\n",
      "Train: Epoch [1], Batch [535/938], Loss: 2.2975029945373535\n",
      "Train: Epoch [1], Batch [536/938], Loss: 2.296506881713867\n",
      "Train: Epoch [1], Batch [537/938], Loss: 2.2971503734588623\n",
      "Train: Epoch [1], Batch [538/938], Loss: 2.293701171875\n",
      "Train: Epoch [1], Batch [539/938], Loss: 2.296008348464966\n",
      "Train: Epoch [1], Batch [540/938], Loss: 2.294773578643799\n",
      "Train: Epoch [1], Batch [541/938], Loss: 2.298274040222168\n",
      "Train: Epoch [1], Batch [542/938], Loss: 2.2956881523132324\n",
      "Train: Epoch [1], Batch [543/938], Loss: 2.294220209121704\n",
      "Train: Epoch [1], Batch [544/938], Loss: 2.296142101287842\n",
      "Train: Epoch [1], Batch [545/938], Loss: 2.300443172454834\n",
      "Train: Epoch [1], Batch [546/938], Loss: 2.2936038970947266\n",
      "Train: Epoch [1], Batch [547/938], Loss: 2.2969536781311035\n",
      "Train: Epoch [1], Batch [548/938], Loss: 2.297372579574585\n",
      "Train: Epoch [1], Batch [549/938], Loss: 2.2962355613708496\n",
      "Train: Epoch [1], Batch [550/938], Loss: 2.29738712310791\n",
      "Train: Epoch [1], Batch [551/938], Loss: 2.297921657562256\n",
      "Train: Epoch [1], Batch [552/938], Loss: 2.293390989303589\n",
      "Train: Epoch [1], Batch [553/938], Loss: 2.298269748687744\n",
      "Train: Epoch [1], Batch [554/938], Loss: 2.296339988708496\n",
      "Train: Epoch [1], Batch [555/938], Loss: 2.2948827743530273\n",
      "Train: Epoch [1], Batch [556/938], Loss: 2.2971134185791016\n",
      "Train: Epoch [1], Batch [557/938], Loss: 2.29508900642395\n",
      "Train: Epoch [1], Batch [558/938], Loss: 2.2921295166015625\n",
      "Train: Epoch [1], Batch [559/938], Loss: 2.2939836978912354\n",
      "Train: Epoch [1], Batch [560/938], Loss: 2.295062303543091\n",
      "Train: Epoch [1], Batch [561/938], Loss: 2.2933313846588135\n",
      "Train: Epoch [1], Batch [562/938], Loss: 2.2964541912078857\n",
      "Train: Epoch [1], Batch [563/938], Loss: 2.293383836746216\n",
      "Train: Epoch [1], Batch [564/938], Loss: 2.2969985008239746\n",
      "Train: Epoch [1], Batch [565/938], Loss: 2.2944014072418213\n",
      "Train: Epoch [1], Batch [566/938], Loss: 2.2930803298950195\n",
      "Train: Epoch [1], Batch [567/938], Loss: 2.294677257537842\n",
      "Train: Epoch [1], Batch [568/938], Loss: 2.2976834774017334\n",
      "Train: Epoch [1], Batch [569/938], Loss: 2.295837879180908\n",
      "Train: Epoch [1], Batch [570/938], Loss: 2.2950685024261475\n",
      "Train: Epoch [1], Batch [571/938], Loss: 2.2938954830169678\n",
      "Train: Epoch [1], Batch [572/938], Loss: 2.2924704551696777\n",
      "Train: Epoch [1], Batch [573/938], Loss: 2.2939538955688477\n",
      "Train: Epoch [1], Batch [574/938], Loss: 2.295159339904785\n",
      "Train: Epoch [1], Batch [575/938], Loss: 2.2943482398986816\n",
      "Train: Epoch [1], Batch [576/938], Loss: 2.295811176300049\n",
      "Train: Epoch [1], Batch [577/938], Loss: 2.296398162841797\n",
      "Train: Epoch [1], Batch [578/938], Loss: 2.2950377464294434\n",
      "Train: Epoch [1], Batch [579/938], Loss: 2.2949271202087402\n",
      "Train: Epoch [1], Batch [580/938], Loss: 2.2997262477874756\n",
      "Train: Epoch [1], Batch [581/938], Loss: 2.296530246734619\n",
      "Train: Epoch [1], Batch [582/938], Loss: 2.2969555854797363\n",
      "Train: Epoch [1], Batch [583/938], Loss: 2.2945024967193604\n",
      "Train: Epoch [1], Batch [584/938], Loss: 2.2967288494110107\n",
      "Train: Epoch [1], Batch [585/938], Loss: 2.293192148208618\n",
      "Train: Epoch [1], Batch [586/938], Loss: 2.2937378883361816\n",
      "Train: Epoch [1], Batch [587/938], Loss: 2.296578884124756\n",
      "Train: Epoch [1], Batch [588/938], Loss: 2.2925777435302734\n",
      "Train: Epoch [1], Batch [589/938], Loss: 2.2922492027282715\n",
      "Train: Epoch [1], Batch [590/938], Loss: 2.2960805892944336\n",
      "Train: Epoch [1], Batch [591/938], Loss: 2.296903610229492\n",
      "Train: Epoch [1], Batch [592/938], Loss: 2.2936620712280273\n",
      "Train: Epoch [1], Batch [593/938], Loss: 2.297978162765503\n",
      "Train: Epoch [1], Batch [594/938], Loss: 2.296447992324829\n",
      "Train: Epoch [1], Batch [595/938], Loss: 2.294348955154419\n",
      "Train: Epoch [1], Batch [596/938], Loss: 2.2959187030792236\n",
      "Train: Epoch [1], Batch [597/938], Loss: 2.2938058376312256\n",
      "Train: Epoch [1], Batch [598/938], Loss: 2.294304847717285\n",
      "Train: Epoch [1], Batch [599/938], Loss: 2.2955121994018555\n",
      "Train: Epoch [1], Batch [600/938], Loss: 2.2935545444488525\n",
      "Train: Epoch [1], Batch [601/938], Loss: 2.29491925239563\n",
      "Train: Epoch [1], Batch [602/938], Loss: 2.2957024574279785\n",
      "Train: Epoch [1], Batch [603/938], Loss: 2.2948288917541504\n",
      "Train: Epoch [1], Batch [604/938], Loss: 2.2962985038757324\n",
      "Train: Epoch [1], Batch [605/938], Loss: 2.2935938835144043\n",
      "Train: Epoch [1], Batch [606/938], Loss: 2.2916464805603027\n",
      "Train: Epoch [1], Batch [607/938], Loss: 2.2979376316070557\n",
      "Train: Epoch [1], Batch [608/938], Loss: 2.2949256896972656\n",
      "Train: Epoch [1], Batch [609/938], Loss: 2.2931723594665527\n",
      "Train: Epoch [1], Batch [610/938], Loss: 2.2950246334075928\n",
      "Train: Epoch [1], Batch [611/938], Loss: 2.2966389656066895\n",
      "Train: Epoch [1], Batch [612/938], Loss: 2.293809652328491\n",
      "Train: Epoch [1], Batch [613/938], Loss: 2.2921347618103027\n",
      "Train: Epoch [1], Batch [614/938], Loss: 2.2942862510681152\n",
      "Train: Epoch [1], Batch [615/938], Loss: 2.2963123321533203\n",
      "Train: Epoch [1], Batch [616/938], Loss: 2.2930550575256348\n",
      "Train: Epoch [1], Batch [617/938], Loss: 2.292910575866699\n",
      "Train: Epoch [1], Batch [618/938], Loss: 2.2929880619049072\n",
      "Train: Epoch [1], Batch [619/938], Loss: 2.2943315505981445\n",
      "Train: Epoch [1], Batch [620/938], Loss: 2.2906382083892822\n",
      "Train: Epoch [1], Batch [621/938], Loss: 2.2926440238952637\n",
      "Train: Epoch [1], Batch [622/938], Loss: 2.294475555419922\n",
      "Train: Epoch [1], Batch [623/938], Loss: 2.2933268547058105\n",
      "Train: Epoch [1], Batch [624/938], Loss: 2.291682004928589\n",
      "Train: Epoch [1], Batch [625/938], Loss: 2.2937443256378174\n",
      "Train: Epoch [1], Batch [626/938], Loss: 2.295438289642334\n",
      "Train: Epoch [1], Batch [627/938], Loss: 2.291978359222412\n",
      "Train: Epoch [1], Batch [628/938], Loss: 2.293006420135498\n",
      "Train: Epoch [1], Batch [629/938], Loss: 2.2914412021636963\n",
      "Train: Epoch [1], Batch [630/938], Loss: 2.294328451156616\n",
      "Train: Epoch [1], Batch [631/938], Loss: 2.295419931411743\n",
      "Train: Epoch [1], Batch [632/938], Loss: 2.2931084632873535\n",
      "Train: Epoch [1], Batch [633/938], Loss: 2.2950053215026855\n",
      "Train: Epoch [1], Batch [634/938], Loss: 2.2931458950042725\n",
      "Train: Epoch [1], Batch [635/938], Loss: 2.29400372505188\n",
      "Train: Epoch [1], Batch [636/938], Loss: 2.2938756942749023\n",
      "Train: Epoch [1], Batch [637/938], Loss: 2.29701566696167\n",
      "Train: Epoch [1], Batch [638/938], Loss: 2.2939515113830566\n",
      "Train: Epoch [1], Batch [639/938], Loss: 2.2937963008880615\n",
      "Train: Epoch [1], Batch [640/938], Loss: 2.2927823066711426\n",
      "Train: Epoch [1], Batch [641/938], Loss: 2.2944998741149902\n",
      "Train: Epoch [1], Batch [642/938], Loss: 2.294398307800293\n",
      "Train: Epoch [1], Batch [643/938], Loss: 2.293912410736084\n",
      "Train: Epoch [1], Batch [644/938], Loss: 2.2970709800720215\n",
      "Train: Epoch [1], Batch [645/938], Loss: 2.2914576530456543\n",
      "Train: Epoch [1], Batch [646/938], Loss: 2.291621446609497\n",
      "Train: Epoch [1], Batch [647/938], Loss: 2.292954683303833\n",
      "Train: Epoch [1], Batch [648/938], Loss: 2.2963054180145264\n",
      "Train: Epoch [1], Batch [649/938], Loss: 2.2967286109924316\n",
      "Train: Epoch [1], Batch [650/938], Loss: 2.294689655303955\n",
      "Train: Epoch [1], Batch [651/938], Loss: 2.2925400733947754\n",
      "Train: Epoch [1], Batch [652/938], Loss: 2.2932288646698\n",
      "Train: Epoch [1], Batch [653/938], Loss: 2.296835422515869\n",
      "Train: Epoch [1], Batch [654/938], Loss: 2.293623685836792\n",
      "Train: Epoch [1], Batch [655/938], Loss: 2.2912912368774414\n",
      "Train: Epoch [1], Batch [656/938], Loss: 2.292506456375122\n",
      "Train: Epoch [1], Batch [657/938], Loss: 2.2937159538269043\n",
      "Train: Epoch [1], Batch [658/938], Loss: 2.293583869934082\n",
      "Train: Epoch [1], Batch [659/938], Loss: 2.292771339416504\n",
      "Train: Epoch [1], Batch [660/938], Loss: 2.2937417030334473\n",
      "Train: Epoch [1], Batch [661/938], Loss: 2.2957019805908203\n",
      "Train: Epoch [1], Batch [662/938], Loss: 2.2944884300231934\n",
      "Train: Epoch [1], Batch [663/938], Loss: 2.2882745265960693\n",
      "Train: Epoch [1], Batch [664/938], Loss: 2.294266939163208\n",
      "Train: Epoch [1], Batch [665/938], Loss: 2.29325532913208\n",
      "Train: Epoch [1], Batch [666/938], Loss: 2.2937071323394775\n",
      "Train: Epoch [1], Batch [667/938], Loss: 2.291818141937256\n",
      "Train: Epoch [1], Batch [668/938], Loss: 2.2950053215026855\n",
      "Train: Epoch [1], Batch [669/938], Loss: 2.295342445373535\n",
      "Train: Epoch [1], Batch [670/938], Loss: 2.293801784515381\n",
      "Train: Epoch [1], Batch [671/938], Loss: 2.294261932373047\n",
      "Train: Epoch [1], Batch [672/938], Loss: 2.29592227935791\n",
      "Train: Epoch [1], Batch [673/938], Loss: 2.2946712970733643\n",
      "Train: Epoch [1], Batch [674/938], Loss: 2.295403003692627\n",
      "Train: Epoch [1], Batch [675/938], Loss: 2.2907540798187256\n",
      "Train: Epoch [1], Batch [676/938], Loss: 2.293879985809326\n",
      "Train: Epoch [1], Batch [677/938], Loss: 2.2919845581054688\n",
      "Train: Epoch [1], Batch [678/938], Loss: 2.292853832244873\n",
      "Train: Epoch [1], Batch [679/938], Loss: 2.291771650314331\n",
      "Train: Epoch [1], Batch [680/938], Loss: 2.2950079441070557\n",
      "Train: Epoch [1], Batch [681/938], Loss: 2.293469190597534\n",
      "Train: Epoch [1], Batch [682/938], Loss: 2.2884180545806885\n",
      "Train: Epoch [1], Batch [683/938], Loss: 2.291735887527466\n",
      "Train: Epoch [1], Batch [684/938], Loss: 2.292797803878784\n",
      "Train: Epoch [1], Batch [685/938], Loss: 2.2914183139801025\n",
      "Train: Epoch [1], Batch [686/938], Loss: 2.2913217544555664\n",
      "Train: Epoch [1], Batch [687/938], Loss: 2.2895565032958984\n",
      "Train: Epoch [1], Batch [688/938], Loss: 2.2909398078918457\n",
      "Train: Epoch [1], Batch [689/938], Loss: 2.2927284240722656\n",
      "Train: Epoch [1], Batch [690/938], Loss: 2.2962868213653564\n",
      "Train: Epoch [1], Batch [691/938], Loss: 2.2901711463928223\n",
      "Train: Epoch [1], Batch [692/938], Loss: 2.288726329803467\n",
      "Train: Epoch [1], Batch [693/938], Loss: 2.2964653968811035\n",
      "Train: Epoch [1], Batch [694/938], Loss: 2.292955160140991\n",
      "Train: Epoch [1], Batch [695/938], Loss: 2.293915271759033\n",
      "Train: Epoch [1], Batch [696/938], Loss: 2.2915449142456055\n",
      "Train: Epoch [1], Batch [697/938], Loss: 2.290998935699463\n",
      "Train: Epoch [1], Batch [698/938], Loss: 2.292036294937134\n",
      "Train: Epoch [1], Batch [699/938], Loss: 2.2940516471862793\n",
      "Train: Epoch [1], Batch [700/938], Loss: 2.2911202907562256\n",
      "Train: Epoch [1], Batch [701/938], Loss: 2.291032314300537\n",
      "Train: Epoch [1], Batch [702/938], Loss: 2.293530225753784\n",
      "Train: Epoch [1], Batch [703/938], Loss: 2.293109655380249\n",
      "Train: Epoch [1], Batch [704/938], Loss: 2.2972309589385986\n",
      "Train: Epoch [1], Batch [705/938], Loss: 2.2910287380218506\n",
      "Train: Epoch [1], Batch [706/938], Loss: 2.2921745777130127\n",
      "Train: Epoch [1], Batch [707/938], Loss: 2.2921483516693115\n",
      "Train: Epoch [1], Batch [708/938], Loss: 2.296527624130249\n",
      "Train: Epoch [1], Batch [709/938], Loss: 2.295078754425049\n",
      "Train: Epoch [1], Batch [710/938], Loss: 2.2908682823181152\n",
      "Train: Epoch [1], Batch [711/938], Loss: 2.2953908443450928\n",
      "Train: Epoch [1], Batch [712/938], Loss: 2.2948825359344482\n",
      "Train: Epoch [1], Batch [713/938], Loss: 2.2901744842529297\n",
      "Train: Epoch [1], Batch [714/938], Loss: 2.29164457321167\n",
      "Train: Epoch [1], Batch [715/938], Loss: 2.2893731594085693\n",
      "Train: Epoch [1], Batch [716/938], Loss: 2.2947580814361572\n",
      "Train: Epoch [1], Batch [717/938], Loss: 2.296349287033081\n",
      "Train: Epoch [1], Batch [718/938], Loss: 2.296980381011963\n",
      "Train: Epoch [1], Batch [719/938], Loss: 2.295473098754883\n",
      "Train: Epoch [1], Batch [720/938], Loss: 2.291311740875244\n",
      "Train: Epoch [1], Batch [721/938], Loss: 2.2962441444396973\n",
      "Train: Epoch [1], Batch [722/938], Loss: 2.295851230621338\n",
      "Train: Epoch [1], Batch [723/938], Loss: 2.292017698287964\n",
      "Train: Epoch [1], Batch [724/938], Loss: 2.2905116081237793\n",
      "Train: Epoch [1], Batch [725/938], Loss: 2.2896158695220947\n",
      "Train: Epoch [1], Batch [726/938], Loss: 2.2928104400634766\n",
      "Train: Epoch [1], Batch [727/938], Loss: 2.2935733795166016\n",
      "Train: Epoch [1], Batch [728/938], Loss: 2.2900125980377197\n",
      "Train: Epoch [1], Batch [729/938], Loss: 2.2925894260406494\n",
      "Train: Epoch [1], Batch [730/938], Loss: 2.289836883544922\n",
      "Train: Epoch [1], Batch [731/938], Loss: 2.2947895526885986\n",
      "Train: Epoch [1], Batch [732/938], Loss: 2.289287567138672\n",
      "Train: Epoch [1], Batch [733/938], Loss: 2.2936131954193115\n",
      "Train: Epoch [1], Batch [734/938], Loss: 2.294785976409912\n",
      "Train: Epoch [1], Batch [735/938], Loss: 2.294722080230713\n",
      "Train: Epoch [1], Batch [736/938], Loss: 2.290452003479004\n",
      "Train: Epoch [1], Batch [737/938], Loss: 2.2906875610351562\n",
      "Train: Epoch [1], Batch [738/938], Loss: 2.2929630279541016\n",
      "Train: Epoch [1], Batch [739/938], Loss: 2.2882139682769775\n",
      "Train: Epoch [1], Batch [740/938], Loss: 2.2887959480285645\n",
      "Train: Epoch [1], Batch [741/938], Loss: 2.2894186973571777\n",
      "Train: Epoch [1], Batch [742/938], Loss: 2.2967844009399414\n",
      "Train: Epoch [1], Batch [743/938], Loss: 2.2930965423583984\n",
      "Train: Epoch [1], Batch [744/938], Loss: 2.29176664352417\n",
      "Train: Epoch [1], Batch [745/938], Loss: 2.2945992946624756\n",
      "Train: Epoch [1], Batch [746/938], Loss: 2.2933731079101562\n",
      "Train: Epoch [1], Batch [747/938], Loss: 2.2924413681030273\n",
      "Train: Epoch [1], Batch [748/938], Loss: 2.289445161819458\n",
      "Train: Epoch [1], Batch [749/938], Loss: 2.2903382778167725\n",
      "Train: Epoch [1], Batch [750/938], Loss: 2.2885634899139404\n",
      "Train: Epoch [1], Batch [751/938], Loss: 2.291217803955078\n",
      "Train: Epoch [1], Batch [752/938], Loss: 2.290693998336792\n",
      "Train: Epoch [1], Batch [753/938], Loss: 2.2920897006988525\n",
      "Train: Epoch [1], Batch [754/938], Loss: 2.288590908050537\n",
      "Train: Epoch [1], Batch [755/938], Loss: 2.2908451557159424\n",
      "Train: Epoch [1], Batch [756/938], Loss: 2.2935781478881836\n",
      "Train: Epoch [1], Batch [757/938], Loss: 2.2955334186553955\n",
      "Train: Epoch [1], Batch [758/938], Loss: 2.291146993637085\n",
      "Train: Epoch [1], Batch [759/938], Loss: 2.2897727489471436\n",
      "Train: Epoch [1], Batch [760/938], Loss: 2.2928848266601562\n",
      "Train: Epoch [1], Batch [761/938], Loss: 2.2904253005981445\n",
      "Train: Epoch [1], Batch [762/938], Loss: 2.2926881313323975\n",
      "Train: Epoch [1], Batch [763/938], Loss: 2.2894203662872314\n",
      "Train: Epoch [1], Batch [764/938], Loss: 2.2917845249176025\n",
      "Train: Epoch [1], Batch [765/938], Loss: 2.2948129177093506\n",
      "Train: Epoch [1], Batch [766/938], Loss: 2.288060188293457\n",
      "Train: Epoch [1], Batch [767/938], Loss: 2.2885913848876953\n",
      "Train: Epoch [1], Batch [768/938], Loss: 2.2930214405059814\n",
      "Train: Epoch [1], Batch [769/938], Loss: 2.2900288105010986\n",
      "Train: Epoch [1], Batch [770/938], Loss: 2.292149305343628\n",
      "Train: Epoch [1], Batch [771/938], Loss: 2.2886996269226074\n",
      "Train: Epoch [1], Batch [772/938], Loss: 2.2935471534729004\n",
      "Train: Epoch [1], Batch [773/938], Loss: 2.2837347984313965\n",
      "Train: Epoch [1], Batch [774/938], Loss: 2.2934703826904297\n",
      "Train: Epoch [1], Batch [775/938], Loss: 2.2924418449401855\n",
      "Train: Epoch [1], Batch [776/938], Loss: 2.28912091255188\n",
      "Train: Epoch [1], Batch [777/938], Loss: 2.294081211090088\n",
      "Train: Epoch [1], Batch [778/938], Loss: 2.292346477508545\n",
      "Train: Epoch [1], Batch [779/938], Loss: 2.2917752265930176\n",
      "Train: Epoch [1], Batch [780/938], Loss: 2.2922592163085938\n",
      "Train: Epoch [1], Batch [781/938], Loss: 2.2950382232666016\n",
      "Train: Epoch [1], Batch [782/938], Loss: 2.291447639465332\n",
      "Train: Epoch [1], Batch [783/938], Loss: 2.2949202060699463\n",
      "Train: Epoch [1], Batch [784/938], Loss: 2.2927350997924805\n",
      "Train: Epoch [1], Batch [785/938], Loss: 2.2887394428253174\n",
      "Train: Epoch [1], Batch [786/938], Loss: 2.2951223850250244\n",
      "Train: Epoch [1], Batch [787/938], Loss: 2.289679527282715\n",
      "Train: Epoch [1], Batch [788/938], Loss: 2.289531946182251\n",
      "Train: Epoch [1], Batch [789/938], Loss: 2.2952046394348145\n",
      "Train: Epoch [1], Batch [790/938], Loss: 2.290052890777588\n",
      "Train: Epoch [1], Batch [791/938], Loss: 2.2906625270843506\n",
      "Train: Epoch [1], Batch [792/938], Loss: 2.288252115249634\n",
      "Train: Epoch [1], Batch [793/938], Loss: 2.2902894020080566\n",
      "Train: Epoch [1], Batch [794/938], Loss: 2.296030282974243\n",
      "Train: Epoch [1], Batch [795/938], Loss: 2.291285753250122\n",
      "Train: Epoch [1], Batch [796/938], Loss: 2.2861058712005615\n",
      "Train: Epoch [1], Batch [797/938], Loss: 2.2916228771209717\n",
      "Train: Epoch [1], Batch [798/938], Loss: 2.294865608215332\n",
      "Train: Epoch [1], Batch [799/938], Loss: 2.291316509246826\n",
      "Train: Epoch [1], Batch [800/938], Loss: 2.2859244346618652\n",
      "Train: Epoch [1], Batch [801/938], Loss: 2.2947821617126465\n",
      "Train: Epoch [1], Batch [802/938], Loss: 2.2887589931488037\n",
      "Train: Epoch [1], Batch [803/938], Loss: 2.293562650680542\n",
      "Train: Epoch [1], Batch [804/938], Loss: 2.291449546813965\n",
      "Train: Epoch [1], Batch [805/938], Loss: 2.287442684173584\n",
      "Train: Epoch [1], Batch [806/938], Loss: 2.28969407081604\n",
      "Train: Epoch [1], Batch [807/938], Loss: 2.2884159088134766\n",
      "Train: Epoch [1], Batch [808/938], Loss: 2.2940073013305664\n",
      "Train: Epoch [1], Batch [809/938], Loss: 2.2962241172790527\n",
      "Train: Epoch [1], Batch [810/938], Loss: 2.285928726196289\n",
      "Train: Epoch [1], Batch [811/938], Loss: 2.286104202270508\n",
      "Train: Epoch [1], Batch [812/938], Loss: 2.2870724201202393\n",
      "Train: Epoch [1], Batch [813/938], Loss: 2.2906360626220703\n",
      "Train: Epoch [1], Batch [814/938], Loss: 2.2873482704162598\n",
      "Train: Epoch [1], Batch [815/938], Loss: 2.2917957305908203\n",
      "Train: Epoch [1], Batch [816/938], Loss: 2.289623260498047\n",
      "Train: Epoch [1], Batch [817/938], Loss: 2.2894718647003174\n",
      "Train: Epoch [1], Batch [818/938], Loss: 2.2900094985961914\n",
      "Train: Epoch [1], Batch [819/938], Loss: 2.28570556640625\n",
      "Train: Epoch [1], Batch [820/938], Loss: 2.2850847244262695\n",
      "Train: Epoch [1], Batch [821/938], Loss: 2.2936434745788574\n",
      "Train: Epoch [1], Batch [822/938], Loss: 2.2917165756225586\n",
      "Train: Epoch [1], Batch [823/938], Loss: 2.2936596870422363\n",
      "Train: Epoch [1], Batch [824/938], Loss: 2.2879676818847656\n",
      "Train: Epoch [1], Batch [825/938], Loss: 2.2939469814300537\n",
      "Train: Epoch [1], Batch [826/938], Loss: 2.287151336669922\n",
      "Train: Epoch [1], Batch [827/938], Loss: 2.288909435272217\n",
      "Train: Epoch [1], Batch [828/938], Loss: 2.290651321411133\n",
      "Train: Epoch [1], Batch [829/938], Loss: 2.291539192199707\n",
      "Train: Epoch [1], Batch [830/938], Loss: 2.2883410453796387\n",
      "Train: Epoch [1], Batch [831/938], Loss: 2.2906992435455322\n",
      "Train: Epoch [1], Batch [832/938], Loss: 2.288292407989502\n",
      "Train: Epoch [1], Batch [833/938], Loss: 2.2919180393218994\n",
      "Train: Epoch [1], Batch [834/938], Loss: 2.2905304431915283\n",
      "Train: Epoch [1], Batch [835/938], Loss: 2.28977108001709\n",
      "Train: Epoch [1], Batch [836/938], Loss: 2.290526866912842\n",
      "Train: Epoch [1], Batch [837/938], Loss: 2.2849841117858887\n",
      "Train: Epoch [1], Batch [838/938], Loss: 2.289879083633423\n",
      "Train: Epoch [1], Batch [839/938], Loss: 2.290167808532715\n",
      "Train: Epoch [1], Batch [840/938], Loss: 2.293776750564575\n",
      "Train: Epoch [1], Batch [841/938], Loss: 2.29071307182312\n",
      "Train: Epoch [1], Batch [842/938], Loss: 2.2875771522521973\n",
      "Train: Epoch [1], Batch [843/938], Loss: 2.2902896404266357\n",
      "Train: Epoch [1], Batch [844/938], Loss: 2.2848026752471924\n",
      "Train: Epoch [1], Batch [845/938], Loss: 2.291469097137451\n",
      "Train: Epoch [1], Batch [846/938], Loss: 2.2896924018859863\n",
      "Train: Epoch [1], Batch [847/938], Loss: 2.2906854152679443\n",
      "Train: Epoch [1], Batch [848/938], Loss: 2.286957263946533\n",
      "Train: Epoch [1], Batch [849/938], Loss: 2.2864456176757812\n",
      "Train: Epoch [1], Batch [850/938], Loss: 2.2867789268493652\n",
      "Train: Epoch [1], Batch [851/938], Loss: 2.288656711578369\n",
      "Train: Epoch [1], Batch [852/938], Loss: 2.286329984664917\n",
      "Train: Epoch [1], Batch [853/938], Loss: 2.297978162765503\n",
      "Train: Epoch [1], Batch [854/938], Loss: 2.2907629013061523\n",
      "Train: Epoch [1], Batch [855/938], Loss: 2.286710739135742\n",
      "Train: Epoch [1], Batch [856/938], Loss: 2.289832592010498\n",
      "Train: Epoch [1], Batch [857/938], Loss: 2.294667959213257\n",
      "Train: Epoch [1], Batch [858/938], Loss: 2.2875564098358154\n",
      "Train: Epoch [1], Batch [859/938], Loss: 2.293336868286133\n",
      "Train: Epoch [1], Batch [860/938], Loss: 2.293875217437744\n",
      "Train: Epoch [1], Batch [861/938], Loss: 2.2889881134033203\n",
      "Train: Epoch [1], Batch [862/938], Loss: 2.2924835681915283\n",
      "Train: Epoch [1], Batch [863/938], Loss: 2.2888190746307373\n",
      "Train: Epoch [1], Batch [864/938], Loss: 2.286470651626587\n",
      "Train: Epoch [1], Batch [865/938], Loss: 2.2888920307159424\n",
      "Train: Epoch [1], Batch [866/938], Loss: 2.2890710830688477\n",
      "Train: Epoch [1], Batch [867/938], Loss: 2.2878530025482178\n",
      "Train: Epoch [1], Batch [868/938], Loss: 2.2904720306396484\n",
      "Train: Epoch [1], Batch [869/938], Loss: 2.2847466468811035\n",
      "Train: Epoch [1], Batch [870/938], Loss: 2.2906341552734375\n",
      "Train: Epoch [1], Batch [871/938], Loss: 2.2880194187164307\n",
      "Train: Epoch [1], Batch [872/938], Loss: 2.2888121604919434\n",
      "Train: Epoch [1], Batch [873/938], Loss: 2.287637948989868\n",
      "Train: Epoch [1], Batch [874/938], Loss: 2.2897799015045166\n",
      "Train: Epoch [1], Batch [875/938], Loss: 2.288961410522461\n",
      "Train: Epoch [1], Batch [876/938], Loss: 2.2929091453552246\n",
      "Train: Epoch [1], Batch [877/938], Loss: 2.291921854019165\n",
      "Train: Epoch [1], Batch [878/938], Loss: 2.2847955226898193\n",
      "Train: Epoch [1], Batch [879/938], Loss: 2.2833197116851807\n",
      "Train: Epoch [1], Batch [880/938], Loss: 2.284536361694336\n",
      "Train: Epoch [1], Batch [881/938], Loss: 2.2875490188598633\n",
      "Train: Epoch [1], Batch [882/938], Loss: 2.288825273513794\n",
      "Train: Epoch [1], Batch [883/938], Loss: 2.2917985916137695\n",
      "Train: Epoch [1], Batch [884/938], Loss: 2.2889952659606934\n",
      "Train: Epoch [1], Batch [885/938], Loss: 2.2883148193359375\n",
      "Train: Epoch [1], Batch [886/938], Loss: 2.286734104156494\n",
      "Train: Epoch [1], Batch [887/938], Loss: 2.295628309249878\n",
      "Train: Epoch [1], Batch [888/938], Loss: 2.293517827987671\n",
      "Train: Epoch [1], Batch [889/938], Loss: 2.2921619415283203\n",
      "Train: Epoch [1], Batch [890/938], Loss: 2.2863657474517822\n",
      "Train: Epoch [1], Batch [891/938], Loss: 2.2924845218658447\n",
      "Train: Epoch [1], Batch [892/938], Loss: 2.2929484844207764\n",
      "Train: Epoch [1], Batch [893/938], Loss: 2.2919206619262695\n",
      "Train: Epoch [1], Batch [894/938], Loss: 2.290619373321533\n",
      "Train: Epoch [1], Batch [895/938], Loss: 2.2925143241882324\n",
      "Train: Epoch [1], Batch [896/938], Loss: 2.289386749267578\n",
      "Train: Epoch [1], Batch [897/938], Loss: 2.2851152420043945\n",
      "Train: Epoch [1], Batch [898/938], Loss: 2.290372371673584\n",
      "Train: Epoch [1], Batch [899/938], Loss: 2.2902557849884033\n",
      "Train: Epoch [1], Batch [900/938], Loss: 2.294489622116089\n",
      "Train: Epoch [1], Batch [901/938], Loss: 2.289092540740967\n",
      "Train: Epoch [1], Batch [902/938], Loss: 2.2860970497131348\n",
      "Train: Epoch [1], Batch [903/938], Loss: 2.2860286235809326\n",
      "Train: Epoch [1], Batch [904/938], Loss: 2.285677909851074\n",
      "Train: Epoch [1], Batch [905/938], Loss: 2.286170482635498\n",
      "Train: Epoch [1], Batch [906/938], Loss: 2.290180206298828\n",
      "Train: Epoch [1], Batch [907/938], Loss: 2.289318084716797\n",
      "Train: Epoch [1], Batch [908/938], Loss: 2.286802053451538\n",
      "Train: Epoch [1], Batch [909/938], Loss: 2.289360284805298\n",
      "Train: Epoch [1], Batch [910/938], Loss: 2.2844347953796387\n",
      "Train: Epoch [1], Batch [911/938], Loss: 2.2903828620910645\n",
      "Train: Epoch [1], Batch [912/938], Loss: 2.286973476409912\n",
      "Train: Epoch [1], Batch [913/938], Loss: 2.286245822906494\n",
      "Train: Epoch [1], Batch [914/938], Loss: 2.2926025390625\n",
      "Train: Epoch [1], Batch [915/938], Loss: 2.2890818119049072\n",
      "Train: Epoch [1], Batch [916/938], Loss: 2.291680335998535\n",
      "Train: Epoch [1], Batch [917/938], Loss: 2.2893505096435547\n",
      "Train: Epoch [1], Batch [918/938], Loss: 2.290802240371704\n",
      "Train: Epoch [1], Batch [919/938], Loss: 2.286717653274536\n",
      "Train: Epoch [1], Batch [920/938], Loss: 2.284928321838379\n",
      "Train: Epoch [1], Batch [921/938], Loss: 2.2880337238311768\n",
      "Train: Epoch [1], Batch [922/938], Loss: 2.288224697113037\n",
      "Train: Epoch [1], Batch [923/938], Loss: 2.2861649990081787\n",
      "Train: Epoch [1], Batch [924/938], Loss: 2.2880141735076904\n",
      "Train: Epoch [1], Batch [925/938], Loss: 2.291206121444702\n",
      "Train: Epoch [1], Batch [926/938], Loss: 2.2927114963531494\n",
      "Train: Epoch [1], Batch [927/938], Loss: 2.285712957382202\n",
      "Train: Epoch [1], Batch [928/938], Loss: 2.2930550575256348\n",
      "Train: Epoch [1], Batch [929/938], Loss: 2.287402629852295\n",
      "Train: Epoch [1], Batch [930/938], Loss: 2.2892184257507324\n",
      "Train: Epoch [1], Batch [931/938], Loss: 2.2906289100646973\n",
      "Train: Epoch [1], Batch [932/938], Loss: 2.2858364582061768\n",
      "Train: Epoch [1], Batch [933/938], Loss: 2.289252996444702\n",
      "Train: Epoch [1], Batch [934/938], Loss: 2.2909767627716064\n",
      "Train: Epoch [1], Batch [935/938], Loss: 2.2869222164154053\n",
      "Train: Epoch [1], Batch [936/938], Loss: 2.287123203277588\n",
      "Train: Epoch [1], Batch [937/938], Loss: 2.2921314239501953\n",
      "Train: Epoch [1], Batch [938/938], Loss: 2.285648822784424\n",
      "Accuracy of train set: 0.2929333333333333\n",
      "Validation: Epoch [1], Batch [1/938], Loss: 2.2882659435272217\n",
      "Validation: Epoch [1], Batch [2/938], Loss: 2.2882449626922607\n",
      "Validation: Epoch [1], Batch [3/938], Loss: 2.287651538848877\n",
      "Validation: Epoch [1], Batch [4/938], Loss: 2.286813735961914\n",
      "Validation: Epoch [1], Batch [5/938], Loss: 2.286130428314209\n",
      "Validation: Epoch [1], Batch [6/938], Loss: 2.2853939533233643\n",
      "Validation: Epoch [1], Batch [7/938], Loss: 2.2868525981903076\n",
      "Validation: Epoch [1], Batch [8/938], Loss: 2.283818483352661\n",
      "Validation: Epoch [1], Batch [9/938], Loss: 2.28694224357605\n",
      "Validation: Epoch [1], Batch [10/938], Loss: 2.2876665592193604\n",
      "Validation: Epoch [1], Batch [11/938], Loss: 2.289289951324463\n",
      "Validation: Epoch [1], Batch [12/938], Loss: 2.292611837387085\n",
      "Validation: Epoch [1], Batch [13/938], Loss: 2.287635326385498\n",
      "Validation: Epoch [1], Batch [14/938], Loss: 2.2833425998687744\n",
      "Validation: Epoch [1], Batch [15/938], Loss: 2.287966012954712\n",
      "Validation: Epoch [1], Batch [16/938], Loss: 2.284564971923828\n",
      "Validation: Epoch [1], Batch [17/938], Loss: 2.2883448600769043\n",
      "Validation: Epoch [1], Batch [18/938], Loss: 2.2894725799560547\n",
      "Validation: Epoch [1], Batch [19/938], Loss: 2.283705234527588\n",
      "Validation: Epoch [1], Batch [20/938], Loss: 2.290071964263916\n",
      "Validation: Epoch [1], Batch [21/938], Loss: 2.2844183444976807\n",
      "Validation: Epoch [1], Batch [22/938], Loss: 2.2946953773498535\n",
      "Validation: Epoch [1], Batch [23/938], Loss: 2.2862911224365234\n",
      "Validation: Epoch [1], Batch [24/938], Loss: 2.2809948921203613\n",
      "Validation: Epoch [1], Batch [25/938], Loss: 2.283092975616455\n",
      "Validation: Epoch [1], Batch [26/938], Loss: 2.2871742248535156\n",
      "Validation: Epoch [1], Batch [27/938], Loss: 2.292292833328247\n",
      "Validation: Epoch [1], Batch [28/938], Loss: 2.284872531890869\n",
      "Validation: Epoch [1], Batch [29/938], Loss: 2.291889190673828\n",
      "Validation: Epoch [1], Batch [30/938], Loss: 2.2908592224121094\n",
      "Validation: Epoch [1], Batch [31/938], Loss: 2.2861123085021973\n",
      "Validation: Epoch [1], Batch [32/938], Loss: 2.2866098880767822\n",
      "Validation: Epoch [1], Batch [33/938], Loss: 2.289757013320923\n",
      "Validation: Epoch [1], Batch [34/938], Loss: 2.2883195877075195\n",
      "Validation: Epoch [1], Batch [35/938], Loss: 2.2864184379577637\n",
      "Validation: Epoch [1], Batch [36/938], Loss: 2.2873382568359375\n",
      "Validation: Epoch [1], Batch [37/938], Loss: 2.288083791732788\n",
      "Validation: Epoch [1], Batch [38/938], Loss: 2.283500909805298\n",
      "Validation: Epoch [1], Batch [39/938], Loss: 2.286527156829834\n",
      "Validation: Epoch [1], Batch [40/938], Loss: 2.2881345748901367\n",
      "Validation: Epoch [1], Batch [41/938], Loss: 2.289994239807129\n",
      "Validation: Epoch [1], Batch [42/938], Loss: 2.2873148918151855\n",
      "Validation: Epoch [1], Batch [43/938], Loss: 2.2869462966918945\n",
      "Validation: Epoch [1], Batch [44/938], Loss: 2.2889695167541504\n",
      "Validation: Epoch [1], Batch [45/938], Loss: 2.2892136573791504\n",
      "Validation: Epoch [1], Batch [46/938], Loss: 2.2880380153656006\n",
      "Validation: Epoch [1], Batch [47/938], Loss: 2.2944118976593018\n",
      "Validation: Epoch [1], Batch [48/938], Loss: 2.2889108657836914\n",
      "Validation: Epoch [1], Batch [49/938], Loss: 2.2874603271484375\n",
      "Validation: Epoch [1], Batch [50/938], Loss: 2.287820339202881\n",
      "Validation: Epoch [1], Batch [51/938], Loss: 2.285879373550415\n",
      "Validation: Epoch [1], Batch [52/938], Loss: 2.2938802242279053\n",
      "Validation: Epoch [1], Batch [53/938], Loss: 2.2855892181396484\n",
      "Validation: Epoch [1], Batch [54/938], Loss: 2.2899060249328613\n",
      "Validation: Epoch [1], Batch [55/938], Loss: 2.2873857021331787\n",
      "Validation: Epoch [1], Batch [56/938], Loss: 2.288524866104126\n",
      "Validation: Epoch [1], Batch [57/938], Loss: 2.286231517791748\n",
      "Validation: Epoch [1], Batch [58/938], Loss: 2.286830425262451\n",
      "Validation: Epoch [1], Batch [59/938], Loss: 2.286569118499756\n",
      "Validation: Epoch [1], Batch [60/938], Loss: 2.288909673690796\n",
      "Validation: Epoch [1], Batch [61/938], Loss: 2.28550124168396\n",
      "Validation: Epoch [1], Batch [62/938], Loss: 2.288658618927002\n",
      "Validation: Epoch [1], Batch [63/938], Loss: 2.2902777194976807\n",
      "Validation: Epoch [1], Batch [64/938], Loss: 2.2882566452026367\n",
      "Validation: Epoch [1], Batch [65/938], Loss: 2.2842772006988525\n",
      "Validation: Epoch [1], Batch [66/938], Loss: 2.289088249206543\n",
      "Validation: Epoch [1], Batch [67/938], Loss: 2.2886898517608643\n",
      "Validation: Epoch [1], Batch [68/938], Loss: 2.2918312549591064\n",
      "Validation: Epoch [1], Batch [69/938], Loss: 2.2860193252563477\n",
      "Validation: Epoch [1], Batch [70/938], Loss: 2.2855772972106934\n",
      "Validation: Epoch [1], Batch [71/938], Loss: 2.2905941009521484\n",
      "Validation: Epoch [1], Batch [72/938], Loss: 2.282226324081421\n",
      "Validation: Epoch [1], Batch [73/938], Loss: 2.284116268157959\n",
      "Validation: Epoch [1], Batch [74/938], Loss: 2.2882938385009766\n",
      "Validation: Epoch [1], Batch [75/938], Loss: 2.2871146202087402\n",
      "Validation: Epoch [1], Batch [76/938], Loss: 2.292240858078003\n",
      "Validation: Epoch [1], Batch [77/938], Loss: 2.2876698970794678\n",
      "Validation: Epoch [1], Batch [78/938], Loss: 2.2917397022247314\n",
      "Validation: Epoch [1], Batch [79/938], Loss: 2.2845330238342285\n",
      "Validation: Epoch [1], Batch [80/938], Loss: 2.2862682342529297\n",
      "Validation: Epoch [1], Batch [81/938], Loss: 2.2846384048461914\n",
      "Validation: Epoch [1], Batch [82/938], Loss: 2.289383888244629\n",
      "Validation: Epoch [1], Batch [83/938], Loss: 2.289419174194336\n",
      "Validation: Epoch [1], Batch [84/938], Loss: 2.288674831390381\n",
      "Validation: Epoch [1], Batch [85/938], Loss: 2.2894532680511475\n",
      "Validation: Epoch [1], Batch [86/938], Loss: 2.2891860008239746\n",
      "Validation: Epoch [1], Batch [87/938], Loss: 2.2812678813934326\n",
      "Validation: Epoch [1], Batch [88/938], Loss: 2.291935443878174\n",
      "Validation: Epoch [1], Batch [89/938], Loss: 2.2878406047821045\n",
      "Validation: Epoch [1], Batch [90/938], Loss: 2.2895214557647705\n",
      "Validation: Epoch [1], Batch [91/938], Loss: 2.289889097213745\n",
      "Validation: Epoch [1], Batch [92/938], Loss: 2.290048599243164\n",
      "Validation: Epoch [1], Batch [93/938], Loss: 2.2867629528045654\n",
      "Validation: Epoch [1], Batch [94/938], Loss: 2.2840962409973145\n",
      "Validation: Epoch [1], Batch [95/938], Loss: 2.288980484008789\n",
      "Validation: Epoch [1], Batch [96/938], Loss: 2.283707618713379\n",
      "Validation: Epoch [1], Batch [97/938], Loss: 2.2849597930908203\n",
      "Validation: Epoch [1], Batch [98/938], Loss: 2.287827491760254\n",
      "Validation: Epoch [1], Batch [99/938], Loss: 2.2864537239074707\n",
      "Validation: Epoch [1], Batch [100/938], Loss: 2.2876570224761963\n",
      "Validation: Epoch [1], Batch [101/938], Loss: 2.28507399559021\n",
      "Validation: Epoch [1], Batch [102/938], Loss: 2.2858338356018066\n",
      "Validation: Epoch [1], Batch [103/938], Loss: 2.2878284454345703\n",
      "Validation: Epoch [1], Batch [104/938], Loss: 2.294795513153076\n",
      "Validation: Epoch [1], Batch [105/938], Loss: 2.2909271717071533\n",
      "Validation: Epoch [1], Batch [106/938], Loss: 2.2922017574310303\n",
      "Validation: Epoch [1], Batch [107/938], Loss: 2.2851269245147705\n",
      "Validation: Epoch [1], Batch [108/938], Loss: 2.2855091094970703\n",
      "Validation: Epoch [1], Batch [109/938], Loss: 2.2874295711517334\n",
      "Validation: Epoch [1], Batch [110/938], Loss: 2.2880024909973145\n",
      "Validation: Epoch [1], Batch [111/938], Loss: 2.2902565002441406\n",
      "Validation: Epoch [1], Batch [112/938], Loss: 2.2891159057617188\n",
      "Validation: Epoch [1], Batch [113/938], Loss: 2.2903013229370117\n",
      "Validation: Epoch [1], Batch [114/938], Loss: 2.2850911617279053\n",
      "Validation: Epoch [1], Batch [115/938], Loss: 2.291391134262085\n",
      "Validation: Epoch [1], Batch [116/938], Loss: 2.2928760051727295\n",
      "Validation: Epoch [1], Batch [117/938], Loss: 2.2929515838623047\n",
      "Validation: Epoch [1], Batch [118/938], Loss: 2.2859625816345215\n",
      "Validation: Epoch [1], Batch [119/938], Loss: 2.2866177558898926\n",
      "Validation: Epoch [1], Batch [120/938], Loss: 2.290055990219116\n",
      "Validation: Epoch [1], Batch [121/938], Loss: 2.288943290710449\n",
      "Validation: Epoch [1], Batch [122/938], Loss: 2.2862205505371094\n",
      "Validation: Epoch [1], Batch [123/938], Loss: 2.291217803955078\n",
      "Validation: Epoch [1], Batch [124/938], Loss: 2.287989616394043\n",
      "Validation: Epoch [1], Batch [125/938], Loss: 2.2882399559020996\n",
      "Validation: Epoch [1], Batch [126/938], Loss: 2.287670612335205\n",
      "Validation: Epoch [1], Batch [127/938], Loss: 2.287109851837158\n",
      "Validation: Epoch [1], Batch [128/938], Loss: 2.285482406616211\n",
      "Validation: Epoch [1], Batch [129/938], Loss: 2.2870099544525146\n",
      "Validation: Epoch [1], Batch [130/938], Loss: 2.286409378051758\n",
      "Validation: Epoch [1], Batch [131/938], Loss: 2.2853481769561768\n",
      "Validation: Epoch [1], Batch [132/938], Loss: 2.290966749191284\n",
      "Validation: Epoch [1], Batch [133/938], Loss: 2.2860589027404785\n",
      "Validation: Epoch [1], Batch [134/938], Loss: 2.2874464988708496\n",
      "Validation: Epoch [1], Batch [135/938], Loss: 2.2849042415618896\n",
      "Validation: Epoch [1], Batch [136/938], Loss: 2.2917118072509766\n",
      "Validation: Epoch [1], Batch [137/938], Loss: 2.286763906478882\n",
      "Validation: Epoch [1], Batch [138/938], Loss: 2.28446626663208\n",
      "Validation: Epoch [1], Batch [139/938], Loss: 2.282166004180908\n",
      "Validation: Epoch [1], Batch [140/938], Loss: 2.2860774993896484\n",
      "Validation: Epoch [1], Batch [141/938], Loss: 2.2881479263305664\n",
      "Validation: Epoch [1], Batch [142/938], Loss: 2.2898521423339844\n",
      "Validation: Epoch [1], Batch [143/938], Loss: 2.2866873741149902\n",
      "Validation: Epoch [1], Batch [144/938], Loss: 2.2840936183929443\n",
      "Validation: Epoch [1], Batch [145/938], Loss: 2.288329601287842\n",
      "Validation: Epoch [1], Batch [146/938], Loss: 2.2844159603118896\n",
      "Validation: Epoch [1], Batch [147/938], Loss: 2.2877683639526367\n",
      "Validation: Epoch [1], Batch [148/938], Loss: 2.289824962615967\n",
      "Validation: Epoch [1], Batch [149/938], Loss: 2.2879722118377686\n",
      "Validation: Epoch [1], Batch [150/938], Loss: 2.2898366451263428\n",
      "Validation: Epoch [1], Batch [151/938], Loss: 2.2871458530426025\n",
      "Validation: Epoch [1], Batch [152/938], Loss: 2.2862813472747803\n",
      "Validation: Epoch [1], Batch [153/938], Loss: 2.2861101627349854\n",
      "Validation: Epoch [1], Batch [154/938], Loss: 2.2894961833953857\n",
      "Validation: Epoch [1], Batch [155/938], Loss: 2.2884867191314697\n",
      "Validation: Epoch [1], Batch [156/938], Loss: 2.288769245147705\n",
      "Validation: Epoch [1], Batch [157/938], Loss: 2.2845261096954346\n",
      "Validation: Epoch [1], Batch [158/938], Loss: 2.2881698608398438\n",
      "Validation: Epoch [1], Batch [159/938], Loss: 2.288379192352295\n",
      "Validation: Epoch [1], Batch [160/938], Loss: 2.289393901824951\n",
      "Validation: Epoch [1], Batch [161/938], Loss: 2.285719156265259\n",
      "Validation: Epoch [1], Batch [162/938], Loss: 2.2914655208587646\n",
      "Validation: Epoch [1], Batch [163/938], Loss: 2.287053108215332\n",
      "Validation: Epoch [1], Batch [164/938], Loss: 2.28344988822937\n",
      "Validation: Epoch [1], Batch [165/938], Loss: 2.288672685623169\n",
      "Validation: Epoch [1], Batch [166/938], Loss: 2.291184425354004\n",
      "Validation: Epoch [1], Batch [167/938], Loss: 2.282799005508423\n",
      "Validation: Epoch [1], Batch [168/938], Loss: 2.2874345779418945\n",
      "Validation: Epoch [1], Batch [169/938], Loss: 2.286912441253662\n",
      "Validation: Epoch [1], Batch [170/938], Loss: 2.288318157196045\n",
      "Validation: Epoch [1], Batch [171/938], Loss: 2.283313751220703\n",
      "Validation: Epoch [1], Batch [172/938], Loss: 2.287900686264038\n",
      "Validation: Epoch [1], Batch [173/938], Loss: 2.288259983062744\n",
      "Validation: Epoch [1], Batch [174/938], Loss: 2.2867178916931152\n",
      "Validation: Epoch [1], Batch [175/938], Loss: 2.2867438793182373\n",
      "Validation: Epoch [1], Batch [176/938], Loss: 2.2857675552368164\n",
      "Validation: Epoch [1], Batch [177/938], Loss: 2.288320779800415\n",
      "Validation: Epoch [1], Batch [178/938], Loss: 2.294119358062744\n",
      "Validation: Epoch [1], Batch [179/938], Loss: 2.285982847213745\n",
      "Validation: Epoch [1], Batch [180/938], Loss: 2.2867562770843506\n",
      "Validation: Epoch [1], Batch [181/938], Loss: 2.2903411388397217\n",
      "Validation: Epoch [1], Batch [182/938], Loss: 2.2928600311279297\n",
      "Validation: Epoch [1], Batch [183/938], Loss: 2.282823324203491\n",
      "Validation: Epoch [1], Batch [184/938], Loss: 2.287062168121338\n",
      "Validation: Epoch [1], Batch [185/938], Loss: 2.28389835357666\n",
      "Validation: Epoch [1], Batch [186/938], Loss: 2.2842650413513184\n",
      "Validation: Epoch [1], Batch [187/938], Loss: 2.2873432636260986\n",
      "Validation: Epoch [1], Batch [188/938], Loss: 2.2896556854248047\n",
      "Validation: Epoch [1], Batch [189/938], Loss: 2.2912819385528564\n",
      "Validation: Epoch [1], Batch [190/938], Loss: 2.283749580383301\n",
      "Validation: Epoch [1], Batch [191/938], Loss: 2.2858963012695312\n",
      "Validation: Epoch [1], Batch [192/938], Loss: 2.28625226020813\n",
      "Validation: Epoch [1], Batch [193/938], Loss: 2.2899980545043945\n",
      "Validation: Epoch [1], Batch [194/938], Loss: 2.2883806228637695\n",
      "Validation: Epoch [1], Batch [195/938], Loss: 2.2916507720947266\n",
      "Validation: Epoch [1], Batch [196/938], Loss: 2.2909061908721924\n",
      "Validation: Epoch [1], Batch [197/938], Loss: 2.2876038551330566\n",
      "Validation: Epoch [1], Batch [198/938], Loss: 2.2912402153015137\n",
      "Validation: Epoch [1], Batch [199/938], Loss: 2.28800892829895\n",
      "Validation: Epoch [1], Batch [200/938], Loss: 2.2858619689941406\n",
      "Validation: Epoch [1], Batch [201/938], Loss: 2.290482521057129\n",
      "Validation: Epoch [1], Batch [202/938], Loss: 2.2888965606689453\n",
      "Validation: Epoch [1], Batch [203/938], Loss: 2.284391164779663\n",
      "Validation: Epoch [1], Batch [204/938], Loss: 2.2878332138061523\n",
      "Validation: Epoch [1], Batch [205/938], Loss: 2.291095018386841\n",
      "Validation: Epoch [1], Batch [206/938], Loss: 2.2870492935180664\n",
      "Validation: Epoch [1], Batch [207/938], Loss: 2.288317918777466\n",
      "Validation: Epoch [1], Batch [208/938], Loss: 2.2818331718444824\n",
      "Validation: Epoch [1], Batch [209/938], Loss: 2.2896885871887207\n",
      "Validation: Epoch [1], Batch [210/938], Loss: 2.2887051105499268\n",
      "Validation: Epoch [1], Batch [211/938], Loss: 2.2887463569641113\n",
      "Validation: Epoch [1], Batch [212/938], Loss: 2.2818727493286133\n",
      "Validation: Epoch [1], Batch [213/938], Loss: 2.289829969406128\n",
      "Validation: Epoch [1], Batch [214/938], Loss: 2.281742811203003\n",
      "Validation: Epoch [1], Batch [215/938], Loss: 2.285562038421631\n",
      "Validation: Epoch [1], Batch [216/938], Loss: 2.292717456817627\n",
      "Validation: Epoch [1], Batch [217/938], Loss: 2.2873008251190186\n",
      "Validation: Epoch [1], Batch [218/938], Loss: 2.2914748191833496\n",
      "Validation: Epoch [1], Batch [219/938], Loss: 2.292980670928955\n",
      "Validation: Epoch [1], Batch [220/938], Loss: 2.2884883880615234\n",
      "Validation: Epoch [1], Batch [221/938], Loss: 2.2928223609924316\n",
      "Validation: Epoch [1], Batch [222/938], Loss: 2.2851219177246094\n",
      "Validation: Epoch [1], Batch [223/938], Loss: 2.28513240814209\n",
      "Validation: Epoch [1], Batch [224/938], Loss: 2.290597677230835\n",
      "Validation: Epoch [1], Batch [225/938], Loss: 2.2853031158447266\n",
      "Validation: Epoch [1], Batch [226/938], Loss: 2.2900278568267822\n",
      "Validation: Epoch [1], Batch [227/938], Loss: 2.288898229598999\n",
      "Validation: Epoch [1], Batch [228/938], Loss: 2.2945871353149414\n",
      "Validation: Epoch [1], Batch [229/938], Loss: 2.288625478744507\n",
      "Validation: Epoch [1], Batch [230/938], Loss: 2.2893104553222656\n",
      "Validation: Epoch [1], Batch [231/938], Loss: 2.2884840965270996\n",
      "Validation: Epoch [1], Batch [232/938], Loss: 2.2878847122192383\n",
      "Validation: Epoch [1], Batch [233/938], Loss: 2.286635160446167\n",
      "Validation: Epoch [1], Batch [234/938], Loss: 2.29124116897583\n",
      "Validation: Epoch [1], Batch [235/938], Loss: 2.288638114929199\n",
      "Validation: Epoch [1], Batch [236/938], Loss: 2.2875216007232666\n",
      "Validation: Epoch [1], Batch [237/938], Loss: 2.2936151027679443\n",
      "Validation: Epoch [1], Batch [238/938], Loss: 2.2885324954986572\n",
      "Validation: Epoch [1], Batch [239/938], Loss: 2.2910654544830322\n",
      "Validation: Epoch [1], Batch [240/938], Loss: 2.288423776626587\n",
      "Validation: Epoch [1], Batch [241/938], Loss: 2.2830209732055664\n",
      "Validation: Epoch [1], Batch [242/938], Loss: 2.2867441177368164\n",
      "Validation: Epoch [1], Batch [243/938], Loss: 2.290231466293335\n",
      "Validation: Epoch [1], Batch [244/938], Loss: 2.288249969482422\n",
      "Validation: Epoch [1], Batch [245/938], Loss: 2.289829969406128\n",
      "Validation: Epoch [1], Batch [246/938], Loss: 2.282139539718628\n",
      "Validation: Epoch [1], Batch [247/938], Loss: 2.2854409217834473\n",
      "Validation: Epoch [1], Batch [248/938], Loss: 2.2893052101135254\n",
      "Validation: Epoch [1], Batch [249/938], Loss: 2.2853028774261475\n",
      "Validation: Epoch [1], Batch [250/938], Loss: 2.289459705352783\n",
      "Validation: Epoch [1], Batch [251/938], Loss: 2.2896642684936523\n",
      "Validation: Epoch [1], Batch [252/938], Loss: 2.2879607677459717\n",
      "Validation: Epoch [1], Batch [253/938], Loss: 2.283201217651367\n",
      "Validation: Epoch [1], Batch [254/938], Loss: 2.2827446460723877\n",
      "Validation: Epoch [1], Batch [255/938], Loss: 2.286550521850586\n",
      "Validation: Epoch [1], Batch [256/938], Loss: 2.2874467372894287\n",
      "Validation: Epoch [1], Batch [257/938], Loss: 2.286659002304077\n",
      "Validation: Epoch [1], Batch [258/938], Loss: 2.2891616821289062\n",
      "Validation: Epoch [1], Batch [259/938], Loss: 2.28456711769104\n",
      "Validation: Epoch [1], Batch [260/938], Loss: 2.287297487258911\n",
      "Validation: Epoch [1], Batch [261/938], Loss: 2.28791880607605\n",
      "Validation: Epoch [1], Batch [262/938], Loss: 2.2851667404174805\n",
      "Validation: Epoch [1], Batch [263/938], Loss: 2.285585403442383\n",
      "Validation: Epoch [1], Batch [264/938], Loss: 2.286454439163208\n",
      "Validation: Epoch [1], Batch [265/938], Loss: 2.289914131164551\n",
      "Validation: Epoch [1], Batch [266/938], Loss: 2.2792983055114746\n",
      "Validation: Epoch [1], Batch [267/938], Loss: 2.2827866077423096\n",
      "Validation: Epoch [1], Batch [268/938], Loss: 2.2845253944396973\n",
      "Validation: Epoch [1], Batch [269/938], Loss: 2.2875595092773438\n",
      "Validation: Epoch [1], Batch [270/938], Loss: 2.289569854736328\n",
      "Validation: Epoch [1], Batch [271/938], Loss: 2.285484790802002\n",
      "Validation: Epoch [1], Batch [272/938], Loss: 2.289975643157959\n",
      "Validation: Epoch [1], Batch [273/938], Loss: 2.2887299060821533\n",
      "Validation: Epoch [1], Batch [274/938], Loss: 2.286925792694092\n",
      "Validation: Epoch [1], Batch [275/938], Loss: 2.287543535232544\n",
      "Validation: Epoch [1], Batch [276/938], Loss: 2.28482723236084\n",
      "Validation: Epoch [1], Batch [277/938], Loss: 2.2852323055267334\n",
      "Validation: Epoch [1], Batch [278/938], Loss: 2.2902073860168457\n",
      "Validation: Epoch [1], Batch [279/938], Loss: 2.2882490158081055\n",
      "Validation: Epoch [1], Batch [280/938], Loss: 2.2862396240234375\n",
      "Validation: Epoch [1], Batch [281/938], Loss: 2.2827084064483643\n",
      "Validation: Epoch [1], Batch [282/938], Loss: 2.288662910461426\n",
      "Validation: Epoch [1], Batch [283/938], Loss: 2.2842981815338135\n",
      "Validation: Epoch [1], Batch [284/938], Loss: 2.2874879837036133\n",
      "Validation: Epoch [1], Batch [285/938], Loss: 2.2826955318450928\n",
      "Validation: Epoch [1], Batch [286/938], Loss: 2.2859301567077637\n",
      "Validation: Epoch [1], Batch [287/938], Loss: 2.2837073802948\n",
      "Validation: Epoch [1], Batch [288/938], Loss: 2.2832930088043213\n",
      "Validation: Epoch [1], Batch [289/938], Loss: 2.2905895709991455\n",
      "Validation: Epoch [1], Batch [290/938], Loss: 2.2874977588653564\n",
      "Validation: Epoch [1], Batch [291/938], Loss: 2.289992570877075\n",
      "Validation: Epoch [1], Batch [292/938], Loss: 2.2949399948120117\n",
      "Validation: Epoch [1], Batch [293/938], Loss: 2.2871673107147217\n",
      "Validation: Epoch [1], Batch [294/938], Loss: 2.288337469100952\n",
      "Validation: Epoch [1], Batch [295/938], Loss: 2.2872846126556396\n",
      "Validation: Epoch [1], Batch [296/938], Loss: 2.2893757820129395\n",
      "Validation: Epoch [1], Batch [297/938], Loss: 2.28605580329895\n",
      "Validation: Epoch [1], Batch [298/938], Loss: 2.2856340408325195\n",
      "Validation: Epoch [1], Batch [299/938], Loss: 2.2880001068115234\n",
      "Validation: Epoch [1], Batch [300/938], Loss: 2.2878501415252686\n",
      "Validation: Epoch [1], Batch [301/938], Loss: 2.288355827331543\n",
      "Validation: Epoch [1], Batch [302/938], Loss: 2.2849838733673096\n",
      "Validation: Epoch [1], Batch [303/938], Loss: 2.287684917449951\n",
      "Validation: Epoch [1], Batch [304/938], Loss: 2.2909045219421387\n",
      "Validation: Epoch [1], Batch [305/938], Loss: 2.2830638885498047\n",
      "Validation: Epoch [1], Batch [306/938], Loss: 2.28922176361084\n",
      "Validation: Epoch [1], Batch [307/938], Loss: 2.28558087348938\n",
      "Validation: Epoch [1], Batch [308/938], Loss: 2.2876038551330566\n",
      "Validation: Epoch [1], Batch [309/938], Loss: 2.290943145751953\n",
      "Validation: Epoch [1], Batch [310/938], Loss: 2.285749912261963\n",
      "Validation: Epoch [1], Batch [311/938], Loss: 2.283417224884033\n",
      "Validation: Epoch [1], Batch [312/938], Loss: 2.291369676589966\n",
      "Validation: Epoch [1], Batch [313/938], Loss: 2.2805674076080322\n",
      "Validation: Epoch [1], Batch [314/938], Loss: 2.287233352661133\n",
      "Validation: Epoch [1], Batch [315/938], Loss: 2.287262439727783\n",
      "Validation: Epoch [1], Batch [316/938], Loss: 2.288374900817871\n",
      "Validation: Epoch [1], Batch [317/938], Loss: 2.2903504371643066\n",
      "Validation: Epoch [1], Batch [318/938], Loss: 2.2842962741851807\n",
      "Validation: Epoch [1], Batch [319/938], Loss: 2.290503740310669\n",
      "Validation: Epoch [1], Batch [320/938], Loss: 2.281855821609497\n",
      "Validation: Epoch [1], Batch [321/938], Loss: 2.2868032455444336\n",
      "Validation: Epoch [1], Batch [322/938], Loss: 2.280588150024414\n",
      "Validation: Epoch [1], Batch [323/938], Loss: 2.2897324562072754\n",
      "Validation: Epoch [1], Batch [324/938], Loss: 2.287045478820801\n",
      "Validation: Epoch [1], Batch [325/938], Loss: 2.290273666381836\n",
      "Validation: Epoch [1], Batch [326/938], Loss: 2.281752347946167\n",
      "Validation: Epoch [1], Batch [327/938], Loss: 2.288241147994995\n",
      "Validation: Epoch [1], Batch [328/938], Loss: 2.2879490852355957\n",
      "Validation: Epoch [1], Batch [329/938], Loss: 2.2854790687561035\n",
      "Validation: Epoch [1], Batch [330/938], Loss: 2.2900731563568115\n",
      "Validation: Epoch [1], Batch [331/938], Loss: 2.283437967300415\n",
      "Validation: Epoch [1], Batch [332/938], Loss: 2.289181709289551\n",
      "Validation: Epoch [1], Batch [333/938], Loss: 2.2920589447021484\n",
      "Validation: Epoch [1], Batch [334/938], Loss: 2.289309501647949\n",
      "Validation: Epoch [1], Batch [335/938], Loss: 2.2948057651519775\n",
      "Validation: Epoch [1], Batch [336/938], Loss: 2.2891476154327393\n",
      "Validation: Epoch [1], Batch [337/938], Loss: 2.2855899333953857\n",
      "Validation: Epoch [1], Batch [338/938], Loss: 2.2892870903015137\n",
      "Validation: Epoch [1], Batch [339/938], Loss: 2.286078691482544\n",
      "Validation: Epoch [1], Batch [340/938], Loss: 2.288576364517212\n",
      "Validation: Epoch [1], Batch [341/938], Loss: 2.2864997386932373\n",
      "Validation: Epoch [1], Batch [342/938], Loss: 2.2818710803985596\n",
      "Validation: Epoch [1], Batch [343/938], Loss: 2.2875592708587646\n",
      "Validation: Epoch [1], Batch [344/938], Loss: 2.280442237854004\n",
      "Validation: Epoch [1], Batch [345/938], Loss: 2.2873167991638184\n",
      "Validation: Epoch [1], Batch [346/938], Loss: 2.287423849105835\n",
      "Validation: Epoch [1], Batch [347/938], Loss: 2.2855334281921387\n",
      "Validation: Epoch [1], Batch [348/938], Loss: 2.2856154441833496\n",
      "Validation: Epoch [1], Batch [349/938], Loss: 2.284604549407959\n",
      "Validation: Epoch [1], Batch [350/938], Loss: 2.288057804107666\n",
      "Validation: Epoch [1], Batch [351/938], Loss: 2.285327911376953\n",
      "Validation: Epoch [1], Batch [352/938], Loss: 2.281520366668701\n",
      "Validation: Epoch [1], Batch [353/938], Loss: 2.2908129692077637\n",
      "Validation: Epoch [1], Batch [354/938], Loss: 2.288052797317505\n",
      "Validation: Epoch [1], Batch [355/938], Loss: 2.2845730781555176\n",
      "Validation: Epoch [1], Batch [356/938], Loss: 2.2893881797790527\n",
      "Validation: Epoch [1], Batch [357/938], Loss: 2.2902872562408447\n",
      "Validation: Epoch [1], Batch [358/938], Loss: 2.293239116668701\n",
      "Validation: Epoch [1], Batch [359/938], Loss: 2.2832202911376953\n",
      "Validation: Epoch [1], Batch [360/938], Loss: 2.2899651527404785\n",
      "Validation: Epoch [1], Batch [361/938], Loss: 2.2894787788391113\n",
      "Validation: Epoch [1], Batch [362/938], Loss: 2.2936034202575684\n",
      "Validation: Epoch [1], Batch [363/938], Loss: 2.2911908626556396\n",
      "Validation: Epoch [1], Batch [364/938], Loss: 2.288283348083496\n",
      "Validation: Epoch [1], Batch [365/938], Loss: 2.293672561645508\n",
      "Validation: Epoch [1], Batch [366/938], Loss: 2.2818243503570557\n",
      "Validation: Epoch [1], Batch [367/938], Loss: 2.286508798599243\n",
      "Validation: Epoch [1], Batch [368/938], Loss: 2.2900924682617188\n",
      "Validation: Epoch [1], Batch [369/938], Loss: 2.289466142654419\n",
      "Validation: Epoch [1], Batch [370/938], Loss: 2.294027328491211\n",
      "Validation: Epoch [1], Batch [371/938], Loss: 2.290043830871582\n",
      "Validation: Epoch [1], Batch [372/938], Loss: 2.28434419631958\n",
      "Validation: Epoch [1], Batch [373/938], Loss: 2.2860891819000244\n",
      "Validation: Epoch [1], Batch [374/938], Loss: 2.290637731552124\n",
      "Validation: Epoch [1], Batch [375/938], Loss: 2.287088394165039\n",
      "Validation: Epoch [1], Batch [376/938], Loss: 2.2890844345092773\n",
      "Validation: Epoch [1], Batch [377/938], Loss: 2.2868309020996094\n",
      "Validation: Epoch [1], Batch [378/938], Loss: 2.2867631912231445\n",
      "Validation: Epoch [1], Batch [379/938], Loss: 2.2849361896514893\n",
      "Validation: Epoch [1], Batch [380/938], Loss: 2.292069911956787\n",
      "Validation: Epoch [1], Batch [381/938], Loss: 2.2808544635772705\n",
      "Validation: Epoch [1], Batch [382/938], Loss: 2.288083076477051\n",
      "Validation: Epoch [1], Batch [383/938], Loss: 2.2885353565216064\n",
      "Validation: Epoch [1], Batch [384/938], Loss: 2.2915871143341064\n",
      "Validation: Epoch [1], Batch [385/938], Loss: 2.2881815433502197\n",
      "Validation: Epoch [1], Batch [386/938], Loss: 2.2822442054748535\n",
      "Validation: Epoch [1], Batch [387/938], Loss: 2.284226894378662\n",
      "Validation: Epoch [1], Batch [388/938], Loss: 2.286417007446289\n",
      "Validation: Epoch [1], Batch [389/938], Loss: 2.289565324783325\n",
      "Validation: Epoch [1], Batch [390/938], Loss: 2.291316509246826\n",
      "Validation: Epoch [1], Batch [391/938], Loss: 2.293128490447998\n",
      "Validation: Epoch [1], Batch [392/938], Loss: 2.2904086112976074\n",
      "Validation: Epoch [1], Batch [393/938], Loss: 2.2820959091186523\n",
      "Validation: Epoch [1], Batch [394/938], Loss: 2.2861592769622803\n",
      "Validation: Epoch [1], Batch [395/938], Loss: 2.2895545959472656\n",
      "Validation: Epoch [1], Batch [396/938], Loss: 2.287816047668457\n",
      "Validation: Epoch [1], Batch [397/938], Loss: 2.2877767086029053\n",
      "Validation: Epoch [1], Batch [398/938], Loss: 2.2891407012939453\n",
      "Validation: Epoch [1], Batch [399/938], Loss: 2.284520387649536\n",
      "Validation: Epoch [1], Batch [400/938], Loss: 2.283015727996826\n",
      "Validation: Epoch [1], Batch [401/938], Loss: 2.2872254848480225\n",
      "Validation: Epoch [1], Batch [402/938], Loss: 2.2843642234802246\n",
      "Validation: Epoch [1], Batch [403/938], Loss: 2.2860960960388184\n",
      "Validation: Epoch [1], Batch [404/938], Loss: 2.289395332336426\n",
      "Validation: Epoch [1], Batch [405/938], Loss: 2.2869975566864014\n",
      "Validation: Epoch [1], Batch [406/938], Loss: 2.2895803451538086\n",
      "Validation: Epoch [1], Batch [407/938], Loss: 2.2820703983306885\n",
      "Validation: Epoch [1], Batch [408/938], Loss: 2.2878918647766113\n",
      "Validation: Epoch [1], Batch [409/938], Loss: 2.2850821018218994\n",
      "Validation: Epoch [1], Batch [410/938], Loss: 2.289064407348633\n",
      "Validation: Epoch [1], Batch [411/938], Loss: 2.2817318439483643\n",
      "Validation: Epoch [1], Batch [412/938], Loss: 2.284381628036499\n",
      "Validation: Epoch [1], Batch [413/938], Loss: 2.281472682952881\n",
      "Validation: Epoch [1], Batch [414/938], Loss: 2.2827606201171875\n",
      "Validation: Epoch [1], Batch [415/938], Loss: 2.285480260848999\n",
      "Validation: Epoch [1], Batch [416/938], Loss: 2.2837042808532715\n",
      "Validation: Epoch [1], Batch [417/938], Loss: 2.290541648864746\n",
      "Validation: Epoch [1], Batch [418/938], Loss: 2.287358045578003\n",
      "Validation: Epoch [1], Batch [419/938], Loss: 2.287865161895752\n",
      "Validation: Epoch [1], Batch [420/938], Loss: 2.2843732833862305\n",
      "Validation: Epoch [1], Batch [421/938], Loss: 2.291130542755127\n",
      "Validation: Epoch [1], Batch [422/938], Loss: 2.2832672595977783\n",
      "Validation: Epoch [1], Batch [423/938], Loss: 2.281709909439087\n",
      "Validation: Epoch [1], Batch [424/938], Loss: 2.2895920276641846\n",
      "Validation: Epoch [1], Batch [425/938], Loss: 2.2906734943389893\n",
      "Validation: Epoch [1], Batch [426/938], Loss: 2.285090684890747\n",
      "Validation: Epoch [1], Batch [427/938], Loss: 2.2872347831726074\n",
      "Validation: Epoch [1], Batch [428/938], Loss: 2.287961483001709\n",
      "Validation: Epoch [1], Batch [429/938], Loss: 2.2905726432800293\n",
      "Validation: Epoch [1], Batch [430/938], Loss: 2.2889504432678223\n",
      "Validation: Epoch [1], Batch [431/938], Loss: 2.2847230434417725\n",
      "Validation: Epoch [1], Batch [432/938], Loss: 2.2839274406433105\n",
      "Validation: Epoch [1], Batch [433/938], Loss: 2.286773681640625\n",
      "Validation: Epoch [1], Batch [434/938], Loss: 2.287083387374878\n",
      "Validation: Epoch [1], Batch [435/938], Loss: 2.284271717071533\n",
      "Validation: Epoch [1], Batch [436/938], Loss: 2.2846555709838867\n",
      "Validation: Epoch [1], Batch [437/938], Loss: 2.285515069961548\n",
      "Validation: Epoch [1], Batch [438/938], Loss: 2.289128065109253\n",
      "Validation: Epoch [1], Batch [439/938], Loss: 2.2834179401397705\n",
      "Validation: Epoch [1], Batch [440/938], Loss: 2.287923812866211\n",
      "Validation: Epoch [1], Batch [441/938], Loss: 2.2897894382476807\n",
      "Validation: Epoch [1], Batch [442/938], Loss: 2.2855212688446045\n",
      "Validation: Epoch [1], Batch [443/938], Loss: 2.288465738296509\n",
      "Validation: Epoch [1], Batch [444/938], Loss: 2.2944061756134033\n",
      "Validation: Epoch [1], Batch [445/938], Loss: 2.2872378826141357\n",
      "Validation: Epoch [1], Batch [446/938], Loss: 2.290968894958496\n",
      "Validation: Epoch [1], Batch [447/938], Loss: 2.2832212448120117\n",
      "Validation: Epoch [1], Batch [448/938], Loss: 2.2885420322418213\n",
      "Validation: Epoch [1], Batch [449/938], Loss: 2.2849321365356445\n",
      "Validation: Epoch [1], Batch [450/938], Loss: 2.2840800285339355\n",
      "Validation: Epoch [1], Batch [451/938], Loss: 2.2860851287841797\n",
      "Validation: Epoch [1], Batch [452/938], Loss: 2.2897701263427734\n",
      "Validation: Epoch [1], Batch [453/938], Loss: 2.2833809852600098\n",
      "Validation: Epoch [1], Batch [454/938], Loss: 2.2893478870391846\n",
      "Validation: Epoch [1], Batch [455/938], Loss: 2.2917914390563965\n",
      "Validation: Epoch [1], Batch [456/938], Loss: 2.287917375564575\n",
      "Validation: Epoch [1], Batch [457/938], Loss: 2.2864136695861816\n",
      "Validation: Epoch [1], Batch [458/938], Loss: 2.29769229888916\n",
      "Validation: Epoch [1], Batch [459/938], Loss: 2.2927932739257812\n",
      "Validation: Epoch [1], Batch [460/938], Loss: 2.2828893661499023\n",
      "Validation: Epoch [1], Batch [461/938], Loss: 2.2870500087738037\n",
      "Validation: Epoch [1], Batch [462/938], Loss: 2.2861251831054688\n",
      "Validation: Epoch [1], Batch [463/938], Loss: 2.291275978088379\n",
      "Validation: Epoch [1], Batch [464/938], Loss: 2.290203809738159\n",
      "Validation: Epoch [1], Batch [465/938], Loss: 2.287386655807495\n",
      "Validation: Epoch [1], Batch [466/938], Loss: 2.2846431732177734\n",
      "Validation: Epoch [1], Batch [467/938], Loss: 2.292470932006836\n",
      "Validation: Epoch [1], Batch [468/938], Loss: 2.283249855041504\n",
      "Validation: Epoch [1], Batch [469/938], Loss: 2.291733980178833\n",
      "Validation: Epoch [1], Batch [470/938], Loss: 2.2826032638549805\n",
      "Validation: Epoch [1], Batch [471/938], Loss: 2.2884624004364014\n",
      "Validation: Epoch [1], Batch [472/938], Loss: 2.2933404445648193\n",
      "Validation: Epoch [1], Batch [473/938], Loss: 2.2840569019317627\n",
      "Validation: Epoch [1], Batch [474/938], Loss: 2.2905266284942627\n",
      "Validation: Epoch [1], Batch [475/938], Loss: 2.2894585132598877\n",
      "Validation: Epoch [1], Batch [476/938], Loss: 2.291973352432251\n",
      "Validation: Epoch [1], Batch [477/938], Loss: 2.287224054336548\n",
      "Validation: Epoch [1], Batch [478/938], Loss: 2.2893598079681396\n",
      "Validation: Epoch [1], Batch [479/938], Loss: 2.296020984649658\n",
      "Validation: Epoch [1], Batch [480/938], Loss: 2.285856008529663\n",
      "Validation: Epoch [1], Batch [481/938], Loss: 2.2879080772399902\n",
      "Validation: Epoch [1], Batch [482/938], Loss: 2.2899420261383057\n",
      "Validation: Epoch [1], Batch [483/938], Loss: 2.2931392192840576\n",
      "Validation: Epoch [1], Batch [484/938], Loss: 2.2851667404174805\n",
      "Validation: Epoch [1], Batch [485/938], Loss: 2.29176664352417\n",
      "Validation: Epoch [1], Batch [486/938], Loss: 2.287548065185547\n",
      "Validation: Epoch [1], Batch [487/938], Loss: 2.2852871417999268\n",
      "Validation: Epoch [1], Batch [488/938], Loss: 2.284769058227539\n",
      "Validation: Epoch [1], Batch [489/938], Loss: 2.2856974601745605\n",
      "Validation: Epoch [1], Batch [490/938], Loss: 2.287048816680908\n",
      "Validation: Epoch [1], Batch [491/938], Loss: 2.2866053581237793\n",
      "Validation: Epoch [1], Batch [492/938], Loss: 2.28912091255188\n",
      "Validation: Epoch [1], Batch [493/938], Loss: 2.284452438354492\n",
      "Validation: Epoch [1], Batch [494/938], Loss: 2.298337459564209\n",
      "Validation: Epoch [1], Batch [495/938], Loss: 2.2882301807403564\n",
      "Validation: Epoch [1], Batch [496/938], Loss: 2.2843575477600098\n",
      "Validation: Epoch [1], Batch [497/938], Loss: 2.288247585296631\n",
      "Validation: Epoch [1], Batch [498/938], Loss: 2.2887461185455322\n",
      "Validation: Epoch [1], Batch [499/938], Loss: 2.285095453262329\n",
      "Validation: Epoch [1], Batch [500/938], Loss: 2.289778709411621\n",
      "Validation: Epoch [1], Batch [501/938], Loss: 2.2865285873413086\n",
      "Validation: Epoch [1], Batch [502/938], Loss: 2.2893049716949463\n",
      "Validation: Epoch [1], Batch [503/938], Loss: 2.288121461868286\n",
      "Validation: Epoch [1], Batch [504/938], Loss: 2.2859699726104736\n",
      "Validation: Epoch [1], Batch [505/938], Loss: 2.2877697944641113\n",
      "Validation: Epoch [1], Batch [506/938], Loss: 2.291778802871704\n",
      "Validation: Epoch [1], Batch [507/938], Loss: 2.2936441898345947\n",
      "Validation: Epoch [1], Batch [508/938], Loss: 2.290858507156372\n",
      "Validation: Epoch [1], Batch [509/938], Loss: 2.2876968383789062\n",
      "Validation: Epoch [1], Batch [510/938], Loss: 2.290494203567505\n",
      "Validation: Epoch [1], Batch [511/938], Loss: 2.2866475582122803\n",
      "Validation: Epoch [1], Batch [512/938], Loss: 2.285480499267578\n",
      "Validation: Epoch [1], Batch [513/938], Loss: 2.284384250640869\n",
      "Validation: Epoch [1], Batch [514/938], Loss: 2.286597967147827\n",
      "Validation: Epoch [1], Batch [515/938], Loss: 2.2889766693115234\n",
      "Validation: Epoch [1], Batch [516/938], Loss: 2.2904343605041504\n",
      "Validation: Epoch [1], Batch [517/938], Loss: 2.287000894546509\n",
      "Validation: Epoch [1], Batch [518/938], Loss: 2.2862863540649414\n",
      "Validation: Epoch [1], Batch [519/938], Loss: 2.2905659675598145\n",
      "Validation: Epoch [1], Batch [520/938], Loss: 2.2865238189697266\n",
      "Validation: Epoch [1], Batch [521/938], Loss: 2.2891685962677\n",
      "Validation: Epoch [1], Batch [522/938], Loss: 2.282078266143799\n",
      "Validation: Epoch [1], Batch [523/938], Loss: 2.290116548538208\n",
      "Validation: Epoch [1], Batch [524/938], Loss: 2.2872817516326904\n",
      "Validation: Epoch [1], Batch [525/938], Loss: 2.2888035774230957\n",
      "Validation: Epoch [1], Batch [526/938], Loss: 2.2850992679595947\n",
      "Validation: Epoch [1], Batch [527/938], Loss: 2.2852869033813477\n",
      "Validation: Epoch [1], Batch [528/938], Loss: 2.285222053527832\n",
      "Validation: Epoch [1], Batch [529/938], Loss: 2.2885046005249023\n",
      "Validation: Epoch [1], Batch [530/938], Loss: 2.2813212871551514\n",
      "Validation: Epoch [1], Batch [531/938], Loss: 2.2909493446350098\n",
      "Validation: Epoch [1], Batch [532/938], Loss: 2.2888097763061523\n",
      "Validation: Epoch [1], Batch [533/938], Loss: 2.289214611053467\n",
      "Validation: Epoch [1], Batch [534/938], Loss: 2.2861573696136475\n",
      "Validation: Epoch [1], Batch [535/938], Loss: 2.2885544300079346\n",
      "Validation: Epoch [1], Batch [536/938], Loss: 2.2913830280303955\n",
      "Validation: Epoch [1], Batch [537/938], Loss: 2.292628288269043\n",
      "Validation: Epoch [1], Batch [538/938], Loss: 2.2825393676757812\n",
      "Validation: Epoch [1], Batch [539/938], Loss: 2.284726142883301\n",
      "Validation: Epoch [1], Batch [540/938], Loss: 2.2926928997039795\n",
      "Validation: Epoch [1], Batch [541/938], Loss: 2.2814829349517822\n",
      "Validation: Epoch [1], Batch [542/938], Loss: 2.2892823219299316\n",
      "Validation: Epoch [1], Batch [543/938], Loss: 2.28938364982605\n",
      "Validation: Epoch [1], Batch [544/938], Loss: 2.2806921005249023\n",
      "Validation: Epoch [1], Batch [545/938], Loss: 2.2833237648010254\n",
      "Validation: Epoch [1], Batch [546/938], Loss: 2.291367769241333\n",
      "Validation: Epoch [1], Batch [547/938], Loss: 2.2841789722442627\n",
      "Validation: Epoch [1], Batch [548/938], Loss: 2.2884914875030518\n",
      "Validation: Epoch [1], Batch [549/938], Loss: 2.289473533630371\n",
      "Validation: Epoch [1], Batch [550/938], Loss: 2.280925989151001\n",
      "Validation: Epoch [1], Batch [551/938], Loss: 2.282069206237793\n",
      "Validation: Epoch [1], Batch [552/938], Loss: 2.2865889072418213\n",
      "Validation: Epoch [1], Batch [553/938], Loss: 2.2887489795684814\n",
      "Validation: Epoch [1], Batch [554/938], Loss: 2.288079261779785\n",
      "Validation: Epoch [1], Batch [555/938], Loss: 2.2862613201141357\n",
      "Validation: Epoch [1], Batch [556/938], Loss: 2.2874364852905273\n",
      "Validation: Epoch [1], Batch [557/938], Loss: 2.290914535522461\n",
      "Validation: Epoch [1], Batch [558/938], Loss: 2.28899884223938\n",
      "Validation: Epoch [1], Batch [559/938], Loss: 2.2908589839935303\n",
      "Validation: Epoch [1], Batch [560/938], Loss: 2.284273624420166\n",
      "Validation: Epoch [1], Batch [561/938], Loss: 2.2888545989990234\n",
      "Validation: Epoch [1], Batch [562/938], Loss: 2.2852280139923096\n",
      "Validation: Epoch [1], Batch [563/938], Loss: 2.285651445388794\n",
      "Validation: Epoch [1], Batch [564/938], Loss: 2.2908685207366943\n",
      "Validation: Epoch [1], Batch [565/938], Loss: 2.2872843742370605\n",
      "Validation: Epoch [1], Batch [566/938], Loss: 2.2863032817840576\n",
      "Validation: Epoch [1], Batch [567/938], Loss: 2.2849440574645996\n",
      "Validation: Epoch [1], Batch [568/938], Loss: 2.2890686988830566\n",
      "Validation: Epoch [1], Batch [569/938], Loss: 2.28826904296875\n",
      "Validation: Epoch [1], Batch [570/938], Loss: 2.2843446731567383\n",
      "Validation: Epoch [1], Batch [571/938], Loss: 2.2883267402648926\n",
      "Validation: Epoch [1], Batch [572/938], Loss: 2.2866859436035156\n",
      "Validation: Epoch [1], Batch [573/938], Loss: 2.285175085067749\n",
      "Validation: Epoch [1], Batch [574/938], Loss: 2.288539171218872\n",
      "Validation: Epoch [1], Batch [575/938], Loss: 2.287487030029297\n",
      "Validation: Epoch [1], Batch [576/938], Loss: 2.28446888923645\n",
      "Validation: Epoch [1], Batch [577/938], Loss: 2.2881038188934326\n",
      "Validation: Epoch [1], Batch [578/938], Loss: 2.286876678466797\n",
      "Validation: Epoch [1], Batch [579/938], Loss: 2.290559768676758\n",
      "Validation: Epoch [1], Batch [580/938], Loss: 2.2862887382507324\n",
      "Validation: Epoch [1], Batch [581/938], Loss: 2.290764093399048\n",
      "Validation: Epoch [1], Batch [582/938], Loss: 2.2858917713165283\n",
      "Validation: Epoch [1], Batch [583/938], Loss: 2.287520408630371\n",
      "Validation: Epoch [1], Batch [584/938], Loss: 2.2886362075805664\n",
      "Validation: Epoch [1], Batch [585/938], Loss: 2.288076162338257\n",
      "Validation: Epoch [1], Batch [586/938], Loss: 2.2899832725524902\n",
      "Validation: Epoch [1], Batch [587/938], Loss: 2.2880380153656006\n",
      "Validation: Epoch [1], Batch [588/938], Loss: 2.2818150520324707\n",
      "Validation: Epoch [1], Batch [589/938], Loss: 2.2875401973724365\n",
      "Validation: Epoch [1], Batch [590/938], Loss: 2.287242889404297\n",
      "Validation: Epoch [1], Batch [591/938], Loss: 2.284302234649658\n",
      "Validation: Epoch [1], Batch [592/938], Loss: 2.290109872817993\n",
      "Validation: Epoch [1], Batch [593/938], Loss: 2.288661479949951\n",
      "Validation: Epoch [1], Batch [594/938], Loss: 2.2907612323760986\n",
      "Validation: Epoch [1], Batch [595/938], Loss: 2.2795515060424805\n",
      "Validation: Epoch [1], Batch [596/938], Loss: 2.288780689239502\n",
      "Validation: Epoch [1], Batch [597/938], Loss: 2.286360025405884\n",
      "Validation: Epoch [1], Batch [598/938], Loss: 2.2913718223571777\n",
      "Validation: Epoch [1], Batch [599/938], Loss: 2.2855868339538574\n",
      "Validation: Epoch [1], Batch [600/938], Loss: 2.286425828933716\n",
      "Validation: Epoch [1], Batch [601/938], Loss: 2.2914202213287354\n",
      "Validation: Epoch [1], Batch [602/938], Loss: 2.28652024269104\n",
      "Validation: Epoch [1], Batch [603/938], Loss: 2.2840306758880615\n",
      "Validation: Epoch [1], Batch [604/938], Loss: 2.2927350997924805\n",
      "Validation: Epoch [1], Batch [605/938], Loss: 2.2875945568084717\n",
      "Validation: Epoch [1], Batch [606/938], Loss: 2.285318374633789\n",
      "Validation: Epoch [1], Batch [607/938], Loss: 2.2897021770477295\n",
      "Validation: Epoch [1], Batch [608/938], Loss: 2.286945343017578\n",
      "Validation: Epoch [1], Batch [609/938], Loss: 2.2848117351531982\n",
      "Validation: Epoch [1], Batch [610/938], Loss: 2.29250431060791\n",
      "Validation: Epoch [1], Batch [611/938], Loss: 2.287203311920166\n",
      "Validation: Epoch [1], Batch [612/938], Loss: 2.2827491760253906\n",
      "Validation: Epoch [1], Batch [613/938], Loss: 2.287551164627075\n",
      "Validation: Epoch [1], Batch [614/938], Loss: 2.2848494052886963\n",
      "Validation: Epoch [1], Batch [615/938], Loss: 2.285398006439209\n",
      "Validation: Epoch [1], Batch [616/938], Loss: 2.2888662815093994\n",
      "Validation: Epoch [1], Batch [617/938], Loss: 2.2861390113830566\n",
      "Validation: Epoch [1], Batch [618/938], Loss: 2.2882466316223145\n",
      "Validation: Epoch [1], Batch [619/938], Loss: 2.2874369621276855\n",
      "Validation: Epoch [1], Batch [620/938], Loss: 2.2867140769958496\n",
      "Validation: Epoch [1], Batch [621/938], Loss: 2.29132342338562\n",
      "Validation: Epoch [1], Batch [622/938], Loss: 2.2890779972076416\n",
      "Validation: Epoch [1], Batch [623/938], Loss: 2.287442684173584\n",
      "Validation: Epoch [1], Batch [624/938], Loss: 2.2869157791137695\n",
      "Validation: Epoch [1], Batch [625/938], Loss: 2.2888882160186768\n",
      "Validation: Epoch [1], Batch [626/938], Loss: 2.2892045974731445\n",
      "Validation: Epoch [1], Batch [627/938], Loss: 2.2876458168029785\n",
      "Validation: Epoch [1], Batch [628/938], Loss: 2.2810726165771484\n",
      "Validation: Epoch [1], Batch [629/938], Loss: 2.2947511672973633\n",
      "Validation: Epoch [1], Batch [630/938], Loss: 2.290937900543213\n",
      "Validation: Epoch [1], Batch [631/938], Loss: 2.2889580726623535\n",
      "Validation: Epoch [1], Batch [632/938], Loss: 2.285492420196533\n",
      "Validation: Epoch [1], Batch [633/938], Loss: 2.290311813354492\n",
      "Validation: Epoch [1], Batch [634/938], Loss: 2.289321184158325\n",
      "Validation: Epoch [1], Batch [635/938], Loss: 2.2884366512298584\n",
      "Validation: Epoch [1], Batch [636/938], Loss: 2.2903096675872803\n",
      "Validation: Epoch [1], Batch [637/938], Loss: 2.2845895290374756\n",
      "Validation: Epoch [1], Batch [638/938], Loss: 2.2858216762542725\n",
      "Validation: Epoch [1], Batch [639/938], Loss: 2.2852542400360107\n",
      "Validation: Epoch [1], Batch [640/938], Loss: 2.288076162338257\n",
      "Validation: Epoch [1], Batch [641/938], Loss: 2.285144329071045\n",
      "Validation: Epoch [1], Batch [642/938], Loss: 2.2878122329711914\n",
      "Validation: Epoch [1], Batch [643/938], Loss: 2.2859482765197754\n",
      "Validation: Epoch [1], Batch [644/938], Loss: 2.2901580333709717\n",
      "Validation: Epoch [1], Batch [645/938], Loss: 2.2886877059936523\n",
      "Validation: Epoch [1], Batch [646/938], Loss: 2.2883121967315674\n",
      "Validation: Epoch [1], Batch [647/938], Loss: 2.2899422645568848\n",
      "Validation: Epoch [1], Batch [648/938], Loss: 2.2867202758789062\n",
      "Validation: Epoch [1], Batch [649/938], Loss: 2.285531759262085\n",
      "Validation: Epoch [1], Batch [650/938], Loss: 2.288896083831787\n",
      "Validation: Epoch [1], Batch [651/938], Loss: 2.292213201522827\n",
      "Validation: Epoch [1], Batch [652/938], Loss: 2.288151264190674\n",
      "Validation: Epoch [1], Batch [653/938], Loss: 2.285703182220459\n",
      "Validation: Epoch [1], Batch [654/938], Loss: 2.2905972003936768\n",
      "Validation: Epoch [1], Batch [655/938], Loss: 2.2820212841033936\n",
      "Validation: Epoch [1], Batch [656/938], Loss: 2.284480571746826\n",
      "Validation: Epoch [1], Batch [657/938], Loss: 2.28928279876709\n",
      "Validation: Epoch [1], Batch [658/938], Loss: 2.2819252014160156\n",
      "Validation: Epoch [1], Batch [659/938], Loss: 2.286033868789673\n",
      "Validation: Epoch [1], Batch [660/938], Loss: 2.2878572940826416\n",
      "Validation: Epoch [1], Batch [661/938], Loss: 2.2873685359954834\n",
      "Validation: Epoch [1], Batch [662/938], Loss: 2.2928051948547363\n",
      "Validation: Epoch [1], Batch [663/938], Loss: 2.293125867843628\n",
      "Validation: Epoch [1], Batch [664/938], Loss: 2.286677360534668\n",
      "Validation: Epoch [1], Batch [665/938], Loss: 2.2843947410583496\n",
      "Validation: Epoch [1], Batch [666/938], Loss: 2.290015935897827\n",
      "Validation: Epoch [1], Batch [667/938], Loss: 2.290536880493164\n",
      "Validation: Epoch [1], Batch [668/938], Loss: 2.2817654609680176\n",
      "Validation: Epoch [1], Batch [669/938], Loss: 2.2873940467834473\n",
      "Validation: Epoch [1], Batch [670/938], Loss: 2.285749912261963\n",
      "Validation: Epoch [1], Batch [671/938], Loss: 2.2873170375823975\n",
      "Validation: Epoch [1], Batch [672/938], Loss: 2.2880022525787354\n",
      "Validation: Epoch [1], Batch [673/938], Loss: 2.2884738445281982\n",
      "Validation: Epoch [1], Batch [674/938], Loss: 2.2848944664001465\n",
      "Validation: Epoch [1], Batch [675/938], Loss: 2.2827138900756836\n",
      "Validation: Epoch [1], Batch [676/938], Loss: 2.287228584289551\n",
      "Validation: Epoch [1], Batch [677/938], Loss: 2.2889440059661865\n",
      "Validation: Epoch [1], Batch [678/938], Loss: 2.288334846496582\n",
      "Validation: Epoch [1], Batch [679/938], Loss: 2.288055896759033\n",
      "Validation: Epoch [1], Batch [680/938], Loss: 2.28895902633667\n",
      "Validation: Epoch [1], Batch [681/938], Loss: 2.292175769805908\n",
      "Validation: Epoch [1], Batch [682/938], Loss: 2.2846262454986572\n",
      "Validation: Epoch [1], Batch [683/938], Loss: 2.290855646133423\n",
      "Validation: Epoch [1], Batch [684/938], Loss: 2.2869157791137695\n",
      "Validation: Epoch [1], Batch [685/938], Loss: 2.280433177947998\n",
      "Validation: Epoch [1], Batch [686/938], Loss: 2.287749767303467\n",
      "Validation: Epoch [1], Batch [687/938], Loss: 2.2885212898254395\n",
      "Validation: Epoch [1], Batch [688/938], Loss: 2.291485071182251\n",
      "Validation: Epoch [1], Batch [689/938], Loss: 2.2900166511535645\n",
      "Validation: Epoch [1], Batch [690/938], Loss: 2.2913155555725098\n",
      "Validation: Epoch [1], Batch [691/938], Loss: 2.290851593017578\n",
      "Validation: Epoch [1], Batch [692/938], Loss: 2.2889411449432373\n",
      "Validation: Epoch [1], Batch [693/938], Loss: 2.289754867553711\n",
      "Validation: Epoch [1], Batch [694/938], Loss: 2.2862722873687744\n",
      "Validation: Epoch [1], Batch [695/938], Loss: 2.2898802757263184\n",
      "Validation: Epoch [1], Batch [696/938], Loss: 2.291597366333008\n",
      "Validation: Epoch [1], Batch [697/938], Loss: 2.287435531616211\n",
      "Validation: Epoch [1], Batch [698/938], Loss: 2.2916436195373535\n",
      "Validation: Epoch [1], Batch [699/938], Loss: 2.286959171295166\n",
      "Validation: Epoch [1], Batch [700/938], Loss: 2.2905428409576416\n",
      "Validation: Epoch [1], Batch [701/938], Loss: 2.2889132499694824\n",
      "Validation: Epoch [1], Batch [702/938], Loss: 2.2895050048828125\n",
      "Validation: Epoch [1], Batch [703/938], Loss: 2.2903404235839844\n",
      "Validation: Epoch [1], Batch [704/938], Loss: 2.2858023643493652\n",
      "Validation: Epoch [1], Batch [705/938], Loss: 2.28489351272583\n",
      "Validation: Epoch [1], Batch [706/938], Loss: 2.2888572216033936\n",
      "Validation: Epoch [1], Batch [707/938], Loss: 2.2875397205352783\n",
      "Validation: Epoch [1], Batch [708/938], Loss: 2.2873830795288086\n",
      "Validation: Epoch [1], Batch [709/938], Loss: 2.2938873767852783\n",
      "Validation: Epoch [1], Batch [710/938], Loss: 2.2891523838043213\n",
      "Validation: Epoch [1], Batch [711/938], Loss: 2.283827304840088\n",
      "Validation: Epoch [1], Batch [712/938], Loss: 2.2883403301239014\n",
      "Validation: Epoch [1], Batch [713/938], Loss: 2.28401780128479\n",
      "Validation: Epoch [1], Batch [714/938], Loss: 2.28983736038208\n",
      "Validation: Epoch [1], Batch [715/938], Loss: 2.292854070663452\n",
      "Validation: Epoch [1], Batch [716/938], Loss: 2.2922041416168213\n",
      "Validation: Epoch [1], Batch [717/938], Loss: 2.2885682582855225\n",
      "Validation: Epoch [1], Batch [718/938], Loss: 2.2905116081237793\n",
      "Validation: Epoch [1], Batch [719/938], Loss: 2.2880804538726807\n",
      "Validation: Epoch [1], Batch [720/938], Loss: 2.2861132621765137\n",
      "Validation: Epoch [1], Batch [721/938], Loss: 2.291738510131836\n",
      "Validation: Epoch [1], Batch [722/938], Loss: 2.287689208984375\n",
      "Validation: Epoch [1], Batch [723/938], Loss: 2.290292739868164\n",
      "Validation: Epoch [1], Batch [724/938], Loss: 2.28745698928833\n",
      "Validation: Epoch [1], Batch [725/938], Loss: 2.290292978286743\n",
      "Validation: Epoch [1], Batch [726/938], Loss: 2.287442684173584\n",
      "Validation: Epoch [1], Batch [727/938], Loss: 2.2891294956207275\n",
      "Validation: Epoch [1], Batch [728/938], Loss: 2.287675619125366\n",
      "Validation: Epoch [1], Batch [729/938], Loss: 2.2886130809783936\n",
      "Validation: Epoch [1], Batch [730/938], Loss: 2.2858386039733887\n",
      "Validation: Epoch [1], Batch [731/938], Loss: 2.2849934101104736\n",
      "Validation: Epoch [1], Batch [732/938], Loss: 2.2902209758758545\n",
      "Validation: Epoch [1], Batch [733/938], Loss: 2.285050630569458\n",
      "Validation: Epoch [1], Batch [734/938], Loss: 2.284754753112793\n",
      "Validation: Epoch [1], Batch [735/938], Loss: 2.284583806991577\n",
      "Validation: Epoch [1], Batch [736/938], Loss: 2.293653726577759\n",
      "Validation: Epoch [1], Batch [737/938], Loss: 2.2896463871002197\n",
      "Validation: Epoch [1], Batch [738/938], Loss: 2.289757251739502\n",
      "Validation: Epoch [1], Batch [739/938], Loss: 2.2892374992370605\n",
      "Validation: Epoch [1], Batch [740/938], Loss: 2.293395519256592\n",
      "Validation: Epoch [1], Batch [741/938], Loss: 2.285517930984497\n",
      "Validation: Epoch [1], Batch [742/938], Loss: 2.285916328430176\n",
      "Validation: Epoch [1], Batch [743/938], Loss: 2.2870779037475586\n",
      "Validation: Epoch [1], Batch [744/938], Loss: 2.283442974090576\n",
      "Validation: Epoch [1], Batch [745/938], Loss: 2.2881381511688232\n",
      "Validation: Epoch [1], Batch [746/938], Loss: 2.2861921787261963\n",
      "Validation: Epoch [1], Batch [747/938], Loss: 2.291234254837036\n",
      "Validation: Epoch [1], Batch [748/938], Loss: 2.2863855361938477\n",
      "Validation: Epoch [1], Batch [749/938], Loss: 2.2812094688415527\n",
      "Validation: Epoch [1], Batch [750/938], Loss: 2.2903592586517334\n",
      "Validation: Epoch [1], Batch [751/938], Loss: 2.28877329826355\n",
      "Validation: Epoch [1], Batch [752/938], Loss: 2.2918307781219482\n",
      "Validation: Epoch [1], Batch [753/938], Loss: 2.28069806098938\n",
      "Validation: Epoch [1], Batch [754/938], Loss: 2.2894721031188965\n",
      "Validation: Epoch [1], Batch [755/938], Loss: 2.293734550476074\n",
      "Validation: Epoch [1], Batch [756/938], Loss: 2.284851312637329\n",
      "Validation: Epoch [1], Batch [757/938], Loss: 2.2941925525665283\n",
      "Validation: Epoch [1], Batch [758/938], Loss: 2.2888190746307373\n",
      "Validation: Epoch [1], Batch [759/938], Loss: 2.2888145446777344\n",
      "Validation: Epoch [1], Batch [760/938], Loss: 2.28983736038208\n",
      "Validation: Epoch [1], Batch [761/938], Loss: 2.2881152629852295\n",
      "Validation: Epoch [1], Batch [762/938], Loss: 2.2863729000091553\n",
      "Validation: Epoch [1], Batch [763/938], Loss: 2.2883169651031494\n",
      "Validation: Epoch [1], Batch [764/938], Loss: 2.2849717140197754\n",
      "Validation: Epoch [1], Batch [765/938], Loss: 2.2897937297821045\n",
      "Validation: Epoch [1], Batch [766/938], Loss: 2.2899227142333984\n",
      "Validation: Epoch [1], Batch [767/938], Loss: 2.289350986480713\n",
      "Validation: Epoch [1], Batch [768/938], Loss: 2.2906100749969482\n",
      "Validation: Epoch [1], Batch [769/938], Loss: 2.2879228591918945\n",
      "Validation: Epoch [1], Batch [770/938], Loss: 2.2932324409484863\n",
      "Validation: Epoch [1], Batch [771/938], Loss: 2.288499593734741\n",
      "Validation: Epoch [1], Batch [772/938], Loss: 2.2838082313537598\n",
      "Validation: Epoch [1], Batch [773/938], Loss: 2.289478063583374\n",
      "Validation: Epoch [1], Batch [774/938], Loss: 2.2926759719848633\n",
      "Validation: Epoch [1], Batch [775/938], Loss: 2.2905702590942383\n",
      "Validation: Epoch [1], Batch [776/938], Loss: 2.2917330265045166\n",
      "Validation: Epoch [1], Batch [777/938], Loss: 2.2871360778808594\n",
      "Validation: Epoch [1], Batch [778/938], Loss: 2.2900545597076416\n",
      "Validation: Epoch [1], Batch [779/938], Loss: 2.287601947784424\n",
      "Validation: Epoch [1], Batch [780/938], Loss: 2.2867488861083984\n",
      "Validation: Epoch [1], Batch [781/938], Loss: 2.2868571281433105\n",
      "Validation: Epoch [1], Batch [782/938], Loss: 2.2912919521331787\n",
      "Validation: Epoch [1], Batch [783/938], Loss: 2.287476062774658\n",
      "Validation: Epoch [1], Batch [784/938], Loss: 2.2904319763183594\n",
      "Validation: Epoch [1], Batch [785/938], Loss: 2.2849106788635254\n",
      "Validation: Epoch [1], Batch [786/938], Loss: 2.2903153896331787\n",
      "Validation: Epoch [1], Batch [787/938], Loss: 2.293098211288452\n",
      "Validation: Epoch [1], Batch [788/938], Loss: 2.2846155166625977\n",
      "Validation: Epoch [1], Batch [789/938], Loss: 2.2810311317443848\n",
      "Validation: Epoch [1], Batch [790/938], Loss: 2.294076442718506\n",
      "Validation: Epoch [1], Batch [791/938], Loss: 2.2821569442749023\n",
      "Validation: Epoch [1], Batch [792/938], Loss: 2.2874245643615723\n",
      "Validation: Epoch [1], Batch [793/938], Loss: 2.2823212146759033\n",
      "Validation: Epoch [1], Batch [794/938], Loss: 2.2833564281463623\n",
      "Validation: Epoch [1], Batch [795/938], Loss: 2.290818929672241\n",
      "Validation: Epoch [1], Batch [796/938], Loss: 2.292609691619873\n",
      "Validation: Epoch [1], Batch [797/938], Loss: 2.2871909141540527\n",
      "Validation: Epoch [1], Batch [798/938], Loss: 2.2868335247039795\n",
      "Validation: Epoch [1], Batch [799/938], Loss: 2.2869622707366943\n",
      "Validation: Epoch [1], Batch [800/938], Loss: 2.2898647785186768\n",
      "Validation: Epoch [1], Batch [801/938], Loss: 2.283586025238037\n",
      "Validation: Epoch [1], Batch [802/938], Loss: 2.283778190612793\n",
      "Validation: Epoch [1], Batch [803/938], Loss: 2.2859015464782715\n",
      "Validation: Epoch [1], Batch [804/938], Loss: 2.2891671657562256\n",
      "Validation: Epoch [1], Batch [805/938], Loss: 2.291501045227051\n",
      "Validation: Epoch [1], Batch [806/938], Loss: 2.2868947982788086\n",
      "Validation: Epoch [1], Batch [807/938], Loss: 2.2885241508483887\n",
      "Validation: Epoch [1], Batch [808/938], Loss: 2.288332939147949\n",
      "Validation: Epoch [1], Batch [809/938], Loss: 2.2929344177246094\n",
      "Validation: Epoch [1], Batch [810/938], Loss: 2.288026809692383\n",
      "Validation: Epoch [1], Batch [811/938], Loss: 2.291229009628296\n",
      "Validation: Epoch [1], Batch [812/938], Loss: 2.290438652038574\n",
      "Validation: Epoch [1], Batch [813/938], Loss: 2.286714553833008\n",
      "Validation: Epoch [1], Batch [814/938], Loss: 2.2896506786346436\n",
      "Validation: Epoch [1], Batch [815/938], Loss: 2.289205312728882\n",
      "Validation: Epoch [1], Batch [816/938], Loss: 2.288562059402466\n",
      "Validation: Epoch [1], Batch [817/938], Loss: 2.2874159812927246\n",
      "Validation: Epoch [1], Batch [818/938], Loss: 2.285090923309326\n",
      "Validation: Epoch [1], Batch [819/938], Loss: 2.2858290672302246\n",
      "Validation: Epoch [1], Batch [820/938], Loss: 2.291799545288086\n",
      "Validation: Epoch [1], Batch [821/938], Loss: 2.280956268310547\n",
      "Validation: Epoch [1], Batch [822/938], Loss: 2.2834901809692383\n",
      "Validation: Epoch [1], Batch [823/938], Loss: 2.290032386779785\n",
      "Validation: Epoch [1], Batch [824/938], Loss: 2.288790702819824\n",
      "Validation: Epoch [1], Batch [825/938], Loss: 2.293848752975464\n",
      "Validation: Epoch [1], Batch [826/938], Loss: 2.287743330001831\n",
      "Validation: Epoch [1], Batch [827/938], Loss: 2.2877984046936035\n",
      "Validation: Epoch [1], Batch [828/938], Loss: 2.287564277648926\n",
      "Validation: Epoch [1], Batch [829/938], Loss: 2.286214590072632\n",
      "Validation: Epoch [1], Batch [830/938], Loss: 2.2856969833374023\n",
      "Validation: Epoch [1], Batch [831/938], Loss: 2.282376289367676\n",
      "Validation: Epoch [1], Batch [832/938], Loss: 2.2932887077331543\n",
      "Validation: Epoch [1], Batch [833/938], Loss: 2.2865378856658936\n",
      "Validation: Epoch [1], Batch [834/938], Loss: 2.2846765518188477\n",
      "Validation: Epoch [1], Batch [835/938], Loss: 2.2867226600646973\n",
      "Validation: Epoch [1], Batch [836/938], Loss: 2.291673183441162\n",
      "Validation: Epoch [1], Batch [837/938], Loss: 2.284468173980713\n",
      "Validation: Epoch [1], Batch [838/938], Loss: 2.28902530670166\n",
      "Validation: Epoch [1], Batch [839/938], Loss: 2.2873146533966064\n",
      "Validation: Epoch [1], Batch [840/938], Loss: 2.2888472080230713\n",
      "Validation: Epoch [1], Batch [841/938], Loss: 2.285846710205078\n",
      "Validation: Epoch [1], Batch [842/938], Loss: 2.289201259613037\n",
      "Validation: Epoch [1], Batch [843/938], Loss: 2.2847323417663574\n",
      "Validation: Epoch [1], Batch [844/938], Loss: 2.2854626178741455\n",
      "Validation: Epoch [1], Batch [845/938], Loss: 2.2888171672821045\n",
      "Validation: Epoch [1], Batch [846/938], Loss: 2.292848587036133\n",
      "Validation: Epoch [1], Batch [847/938], Loss: 2.287184000015259\n",
      "Validation: Epoch [1], Batch [848/938], Loss: 2.2879672050476074\n",
      "Validation: Epoch [1], Batch [849/938], Loss: 2.2928929328918457\n",
      "Validation: Epoch [1], Batch [850/938], Loss: 2.2916982173919678\n",
      "Validation: Epoch [1], Batch [851/938], Loss: 2.284862756729126\n",
      "Validation: Epoch [1], Batch [852/938], Loss: 2.28995943069458\n",
      "Validation: Epoch [1], Batch [853/938], Loss: 2.2927815914154053\n",
      "Validation: Epoch [1], Batch [854/938], Loss: 2.2906289100646973\n",
      "Validation: Epoch [1], Batch [855/938], Loss: 2.2841875553131104\n",
      "Validation: Epoch [1], Batch [856/938], Loss: 2.287628173828125\n",
      "Validation: Epoch [1], Batch [857/938], Loss: 2.2839150428771973\n",
      "Validation: Epoch [1], Batch [858/938], Loss: 2.2926535606384277\n",
      "Validation: Epoch [1], Batch [859/938], Loss: 2.2865536212921143\n",
      "Validation: Epoch [1], Batch [860/938], Loss: 2.2870733737945557\n",
      "Validation: Epoch [1], Batch [861/938], Loss: 2.286980390548706\n",
      "Validation: Epoch [1], Batch [862/938], Loss: 2.2880187034606934\n",
      "Validation: Epoch [1], Batch [863/938], Loss: 2.29118013381958\n",
      "Validation: Epoch [1], Batch [864/938], Loss: 2.280035972595215\n",
      "Validation: Epoch [1], Batch [865/938], Loss: 2.290886878967285\n",
      "Validation: Epoch [1], Batch [866/938], Loss: 2.2889325618743896\n",
      "Validation: Epoch [1], Batch [867/938], Loss: 2.2897047996520996\n",
      "Validation: Epoch [1], Batch [868/938], Loss: 2.2861671447753906\n",
      "Validation: Epoch [1], Batch [869/938], Loss: 2.28899884223938\n",
      "Validation: Epoch [1], Batch [870/938], Loss: 2.286133289337158\n",
      "Validation: Epoch [1], Batch [871/938], Loss: 2.286085605621338\n",
      "Validation: Epoch [1], Batch [872/938], Loss: 2.2929277420043945\n",
      "Validation: Epoch [1], Batch [873/938], Loss: 2.289137601852417\n",
      "Validation: Epoch [1], Batch [874/938], Loss: 2.2871391773223877\n",
      "Validation: Epoch [1], Batch [875/938], Loss: 2.2856526374816895\n",
      "Validation: Epoch [1], Batch [876/938], Loss: 2.2901926040649414\n",
      "Validation: Epoch [1], Batch [877/938], Loss: 2.2843308448791504\n",
      "Validation: Epoch [1], Batch [878/938], Loss: 2.289583206176758\n",
      "Validation: Epoch [1], Batch [879/938], Loss: 2.293454885482788\n",
      "Validation: Epoch [1], Batch [880/938], Loss: 2.2887730598449707\n",
      "Validation: Epoch [1], Batch [881/938], Loss: 2.2832415103912354\n",
      "Validation: Epoch [1], Batch [882/938], Loss: 2.291952610015869\n",
      "Validation: Epoch [1], Batch [883/938], Loss: 2.2827541828155518\n",
      "Validation: Epoch [1], Batch [884/938], Loss: 2.284421443939209\n",
      "Validation: Epoch [1], Batch [885/938], Loss: 2.2881851196289062\n",
      "Validation: Epoch [1], Batch [886/938], Loss: 2.287423610687256\n",
      "Validation: Epoch [1], Batch [887/938], Loss: 2.2841315269470215\n",
      "Validation: Epoch [1], Batch [888/938], Loss: 2.288344621658325\n",
      "Validation: Epoch [1], Batch [889/938], Loss: 2.283933162689209\n",
      "Validation: Epoch [1], Batch [890/938], Loss: 2.2891790866851807\n",
      "Validation: Epoch [1], Batch [891/938], Loss: 2.2912960052490234\n",
      "Validation: Epoch [1], Batch [892/938], Loss: 2.2843363285064697\n",
      "Validation: Epoch [1], Batch [893/938], Loss: 2.2907893657684326\n",
      "Validation: Epoch [1], Batch [894/938], Loss: 2.2914257049560547\n",
      "Validation: Epoch [1], Batch [895/938], Loss: 2.2886338233947754\n",
      "Validation: Epoch [1], Batch [896/938], Loss: 2.2878382205963135\n",
      "Validation: Epoch [1], Batch [897/938], Loss: 2.28566837310791\n",
      "Validation: Epoch [1], Batch [898/938], Loss: 2.2931127548217773\n",
      "Validation: Epoch [1], Batch [899/938], Loss: 2.289457321166992\n",
      "Validation: Epoch [1], Batch [900/938], Loss: 2.288590908050537\n",
      "Validation: Epoch [1], Batch [901/938], Loss: 2.284984588623047\n",
      "Validation: Epoch [1], Batch [902/938], Loss: 2.294255256652832\n",
      "Validation: Epoch [1], Batch [903/938], Loss: 2.2903120517730713\n",
      "Validation: Epoch [1], Batch [904/938], Loss: 2.286353588104248\n",
      "Validation: Epoch [1], Batch [905/938], Loss: 2.280237913131714\n",
      "Validation: Epoch [1], Batch [906/938], Loss: 2.288069725036621\n",
      "Validation: Epoch [1], Batch [907/938], Loss: 2.288114309310913\n",
      "Validation: Epoch [1], Batch [908/938], Loss: 2.2845096588134766\n",
      "Validation: Epoch [1], Batch [909/938], Loss: 2.286283254623413\n",
      "Validation: Epoch [1], Batch [910/938], Loss: 2.290597677230835\n",
      "Validation: Epoch [1], Batch [911/938], Loss: 2.288923978805542\n",
      "Validation: Epoch [1], Batch [912/938], Loss: 2.285743236541748\n",
      "Validation: Epoch [1], Batch [913/938], Loss: 2.285295248031616\n",
      "Validation: Epoch [1], Batch [914/938], Loss: 2.284207820892334\n",
      "Validation: Epoch [1], Batch [915/938], Loss: 2.284923791885376\n",
      "Validation: Epoch [1], Batch [916/938], Loss: 2.291294574737549\n",
      "Validation: Epoch [1], Batch [917/938], Loss: 2.2871217727661133\n",
      "Validation: Epoch [1], Batch [918/938], Loss: 2.2862751483917236\n",
      "Validation: Epoch [1], Batch [919/938], Loss: 2.2912933826446533\n",
      "Validation: Epoch [1], Batch [920/938], Loss: 2.2875871658325195\n",
      "Validation: Epoch [1], Batch [921/938], Loss: 2.286543846130371\n",
      "Validation: Epoch [1], Batch [922/938], Loss: 2.2873709201812744\n",
      "Validation: Epoch [1], Batch [923/938], Loss: 2.2833096981048584\n",
      "Validation: Epoch [1], Batch [924/938], Loss: 2.2844390869140625\n",
      "Validation: Epoch [1], Batch [925/938], Loss: 2.287062406539917\n",
      "Validation: Epoch [1], Batch [926/938], Loss: 2.2851040363311768\n",
      "Validation: Epoch [1], Batch [927/938], Loss: 2.290393114089966\n",
      "Validation: Epoch [1], Batch [928/938], Loss: 2.287215232849121\n",
      "Validation: Epoch [1], Batch [929/938], Loss: 2.2906248569488525\n",
      "Validation: Epoch [1], Batch [930/938], Loss: 2.2831649780273438\n",
      "Validation: Epoch [1], Batch [931/938], Loss: 2.289573907852173\n",
      "Validation: Epoch [1], Batch [932/938], Loss: 2.2935850620269775\n",
      "Validation: Epoch [1], Batch [933/938], Loss: 2.2933642864227295\n",
      "Validation: Epoch [1], Batch [934/938], Loss: 2.289483070373535\n",
      "Validation: Epoch [1], Batch [935/938], Loss: 2.2886245250701904\n",
      "Validation: Epoch [1], Batch [936/938], Loss: 2.2891478538513184\n",
      "Validation: Epoch [1], Batch [937/938], Loss: 2.2852954864501953\n",
      "Validation: Epoch [1], Batch [938/938], Loss: 2.2844855785369873\n",
      "Accuracy of test set: 0.2879\n",
      "Train: Epoch [2], Batch [1/938], Loss: 2.2916812896728516\n",
      "Train: Epoch [2], Batch [2/938], Loss: 2.2870564460754395\n",
      "Train: Epoch [2], Batch [3/938], Loss: 2.285177230834961\n",
      "Train: Epoch [2], Batch [4/938], Loss: 2.2864108085632324\n",
      "Train: Epoch [2], Batch [5/938], Loss: 2.2887916564941406\n",
      "Train: Epoch [2], Batch [6/938], Loss: 2.288735866546631\n",
      "Train: Epoch [2], Batch [7/938], Loss: 2.2905941009521484\n",
      "Train: Epoch [2], Batch [8/938], Loss: 2.2854063510894775\n",
      "Train: Epoch [2], Batch [9/938], Loss: 2.2871971130371094\n",
      "Train: Epoch [2], Batch [10/938], Loss: 2.28775691986084\n",
      "Train: Epoch [2], Batch [11/938], Loss: 2.2896981239318848\n",
      "Train: Epoch [2], Batch [12/938], Loss: 2.28810715675354\n",
      "Train: Epoch [2], Batch [13/938], Loss: 2.2939066886901855\n",
      "Train: Epoch [2], Batch [14/938], Loss: 2.281804323196411\n",
      "Train: Epoch [2], Batch [15/938], Loss: 2.2940480709075928\n",
      "Train: Epoch [2], Batch [16/938], Loss: 2.281550407409668\n",
      "Train: Epoch [2], Batch [17/938], Loss: 2.2813479900360107\n",
      "Train: Epoch [2], Batch [18/938], Loss: 2.2848434448242188\n",
      "Train: Epoch [2], Batch [19/938], Loss: 2.2898359298706055\n",
      "Train: Epoch [2], Batch [20/938], Loss: 2.285890579223633\n",
      "Train: Epoch [2], Batch [21/938], Loss: 2.2817461490631104\n",
      "Train: Epoch [2], Batch [22/938], Loss: 2.2837934494018555\n",
      "Train: Epoch [2], Batch [23/938], Loss: 2.287632942199707\n",
      "Train: Epoch [2], Batch [24/938], Loss: 2.289033889770508\n",
      "Train: Epoch [2], Batch [25/938], Loss: 2.2865710258483887\n",
      "Train: Epoch [2], Batch [26/938], Loss: 2.287383556365967\n",
      "Train: Epoch [2], Batch [27/938], Loss: 2.2867748737335205\n",
      "Train: Epoch [2], Batch [28/938], Loss: 2.2860851287841797\n",
      "Train: Epoch [2], Batch [29/938], Loss: 2.285921096801758\n",
      "Train: Epoch [2], Batch [30/938], Loss: 2.2836155891418457\n",
      "Train: Epoch [2], Batch [31/938], Loss: 2.2875583171844482\n",
      "Train: Epoch [2], Batch [32/938], Loss: 2.2850375175476074\n",
      "Train: Epoch [2], Batch [33/938], Loss: 2.2845101356506348\n",
      "Train: Epoch [2], Batch [34/938], Loss: 2.2886605262756348\n",
      "Train: Epoch [2], Batch [35/938], Loss: 2.2886154651641846\n",
      "Train: Epoch [2], Batch [36/938], Loss: 2.2840323448181152\n",
      "Train: Epoch [2], Batch [37/938], Loss: 2.2872226238250732\n",
      "Train: Epoch [2], Batch [38/938], Loss: 2.2897520065307617\n",
      "Train: Epoch [2], Batch [39/938], Loss: 2.2843940258026123\n",
      "Train: Epoch [2], Batch [40/938], Loss: 2.2804760932922363\n",
      "Train: Epoch [2], Batch [41/938], Loss: 2.2858097553253174\n",
      "Train: Epoch [2], Batch [42/938], Loss: 2.288607120513916\n",
      "Train: Epoch [2], Batch [43/938], Loss: 2.284078598022461\n",
      "Train: Epoch [2], Batch [44/938], Loss: 2.2883729934692383\n",
      "Train: Epoch [2], Batch [45/938], Loss: 2.28536057472229\n",
      "Train: Epoch [2], Batch [46/938], Loss: 2.289437770843506\n",
      "Train: Epoch [2], Batch [47/938], Loss: 2.2880752086639404\n",
      "Train: Epoch [2], Batch [48/938], Loss: 2.2856671810150146\n",
      "Train: Epoch [2], Batch [49/938], Loss: 2.2804105281829834\n",
      "Train: Epoch [2], Batch [50/938], Loss: 2.285696029663086\n",
      "Train: Epoch [2], Batch [51/938], Loss: 2.2881317138671875\n",
      "Train: Epoch [2], Batch [52/938], Loss: 2.2939770221710205\n",
      "Train: Epoch [2], Batch [53/938], Loss: 2.2855639457702637\n",
      "Train: Epoch [2], Batch [54/938], Loss: 2.2893784046173096\n",
      "Train: Epoch [2], Batch [55/938], Loss: 2.287001609802246\n",
      "Train: Epoch [2], Batch [56/938], Loss: 2.2814152240753174\n",
      "Train: Epoch [2], Batch [57/938], Loss: 2.2862088680267334\n",
      "Train: Epoch [2], Batch [58/938], Loss: 2.2904889583587646\n",
      "Train: Epoch [2], Batch [59/938], Loss: 2.284400463104248\n",
      "Train: Epoch [2], Batch [60/938], Loss: 2.28021240234375\n",
      "Train: Epoch [2], Batch [61/938], Loss: 2.287637710571289\n",
      "Train: Epoch [2], Batch [62/938], Loss: 2.2825798988342285\n",
      "Train: Epoch [2], Batch [63/938], Loss: 2.285566568374634\n",
      "Train: Epoch [2], Batch [64/938], Loss: 2.290534257888794\n",
      "Train: Epoch [2], Batch [65/938], Loss: 2.28591251373291\n",
      "Train: Epoch [2], Batch [66/938], Loss: 2.2808680534362793\n",
      "Train: Epoch [2], Batch [67/938], Loss: 2.292067289352417\n",
      "Train: Epoch [2], Batch [68/938], Loss: 2.2911815643310547\n",
      "Train: Epoch [2], Batch [69/938], Loss: 2.2870895862579346\n",
      "Train: Epoch [2], Batch [70/938], Loss: 2.2866995334625244\n",
      "Train: Epoch [2], Batch [71/938], Loss: 2.288613796234131\n",
      "Train: Epoch [2], Batch [72/938], Loss: 2.287489175796509\n",
      "Train: Epoch [2], Batch [73/938], Loss: 2.2842485904693604\n",
      "Train: Epoch [2], Batch [74/938], Loss: 2.2828357219696045\n",
      "Train: Epoch [2], Batch [75/938], Loss: 2.280630588531494\n",
      "Train: Epoch [2], Batch [76/938], Loss: 2.2850146293640137\n",
      "Train: Epoch [2], Batch [77/938], Loss: 2.283958673477173\n",
      "Train: Epoch [2], Batch [78/938], Loss: 2.2833092212677\n",
      "Train: Epoch [2], Batch [79/938], Loss: 2.283876657485962\n",
      "Train: Epoch [2], Batch [80/938], Loss: 2.2895967960357666\n",
      "Train: Epoch [2], Batch [81/938], Loss: 2.2841665744781494\n",
      "Train: Epoch [2], Batch [82/938], Loss: 2.285869836807251\n",
      "Train: Epoch [2], Batch [83/938], Loss: 2.279642105102539\n",
      "Train: Epoch [2], Batch [84/938], Loss: 2.2911293506622314\n",
      "Train: Epoch [2], Batch [85/938], Loss: 2.284219741821289\n",
      "Train: Epoch [2], Batch [86/938], Loss: 2.280557155609131\n",
      "Train: Epoch [2], Batch [87/938], Loss: 2.28582501411438\n",
      "Train: Epoch [2], Batch [88/938], Loss: 2.2803587913513184\n",
      "Train: Epoch [2], Batch [89/938], Loss: 2.2830758094787598\n",
      "Train: Epoch [2], Batch [90/938], Loss: 2.2852859497070312\n",
      "Train: Epoch [2], Batch [91/938], Loss: 2.2886369228363037\n",
      "Train: Epoch [2], Batch [92/938], Loss: 2.2876369953155518\n",
      "Train: Epoch [2], Batch [93/938], Loss: 2.2846739292144775\n",
      "Train: Epoch [2], Batch [94/938], Loss: 2.28920316696167\n",
      "Train: Epoch [2], Batch [95/938], Loss: 2.282283306121826\n",
      "Train: Epoch [2], Batch [96/938], Loss: 2.28971529006958\n",
      "Train: Epoch [2], Batch [97/938], Loss: 2.2886223793029785\n",
      "Train: Epoch [2], Batch [98/938], Loss: 2.2761788368225098\n",
      "Train: Epoch [2], Batch [99/938], Loss: 2.286266565322876\n",
      "Train: Epoch [2], Batch [100/938], Loss: 2.2871439456939697\n",
      "Train: Epoch [2], Batch [101/938], Loss: 2.282209634780884\n",
      "Train: Epoch [2], Batch [102/938], Loss: 2.293044090270996\n",
      "Train: Epoch [2], Batch [103/938], Loss: 2.286395788192749\n",
      "Train: Epoch [2], Batch [104/938], Loss: 2.280397415161133\n",
      "Train: Epoch [2], Batch [105/938], Loss: 2.280118942260742\n",
      "Train: Epoch [2], Batch [106/938], Loss: 2.285086154937744\n",
      "Train: Epoch [2], Batch [107/938], Loss: 2.2838892936706543\n",
      "Train: Epoch [2], Batch [108/938], Loss: 2.2839760780334473\n",
      "Train: Epoch [2], Batch [109/938], Loss: 2.2819888591766357\n",
      "Train: Epoch [2], Batch [110/938], Loss: 2.2860372066497803\n",
      "Train: Epoch [2], Batch [111/938], Loss: 2.283949851989746\n",
      "Train: Epoch [2], Batch [112/938], Loss: 2.283538818359375\n",
      "Train: Epoch [2], Batch [113/938], Loss: 2.2869277000427246\n",
      "Train: Epoch [2], Batch [114/938], Loss: 2.2831063270568848\n",
      "Train: Epoch [2], Batch [115/938], Loss: 2.285362720489502\n",
      "Train: Epoch [2], Batch [116/938], Loss: 2.2919585704803467\n",
      "Train: Epoch [2], Batch [117/938], Loss: 2.2822937965393066\n",
      "Train: Epoch [2], Batch [118/938], Loss: 2.2780380249023438\n",
      "Train: Epoch [2], Batch [119/938], Loss: 2.286156177520752\n",
      "Train: Epoch [2], Batch [120/938], Loss: 2.282745361328125\n",
      "Train: Epoch [2], Batch [121/938], Loss: 2.286684274673462\n",
      "Train: Epoch [2], Batch [122/938], Loss: 2.2873668670654297\n",
      "Train: Epoch [2], Batch [123/938], Loss: 2.281179666519165\n",
      "Train: Epoch [2], Batch [124/938], Loss: 2.279634952545166\n",
      "Train: Epoch [2], Batch [125/938], Loss: 2.2856030464172363\n",
      "Train: Epoch [2], Batch [126/938], Loss: 2.285287857055664\n",
      "Train: Epoch [2], Batch [127/938], Loss: 2.2821431159973145\n",
      "Train: Epoch [2], Batch [128/938], Loss: 2.2830970287323\n",
      "Train: Epoch [2], Batch [129/938], Loss: 2.290095329284668\n",
      "Train: Epoch [2], Batch [130/938], Loss: 2.2816176414489746\n",
      "Train: Epoch [2], Batch [131/938], Loss: 2.287191152572632\n",
      "Train: Epoch [2], Batch [132/938], Loss: 2.2832212448120117\n",
      "Train: Epoch [2], Batch [133/938], Loss: 2.2807397842407227\n",
      "Train: Epoch [2], Batch [134/938], Loss: 2.282949686050415\n",
      "Train: Epoch [2], Batch [135/938], Loss: 2.285353660583496\n",
      "Train: Epoch [2], Batch [136/938], Loss: 2.2782719135284424\n",
      "Train: Epoch [2], Batch [137/938], Loss: 2.2861337661743164\n",
      "Train: Epoch [2], Batch [138/938], Loss: 2.286284923553467\n",
      "Train: Epoch [2], Batch [139/938], Loss: 2.285578966140747\n",
      "Train: Epoch [2], Batch [140/938], Loss: 2.284324884414673\n",
      "Train: Epoch [2], Batch [141/938], Loss: 2.280730724334717\n",
      "Train: Epoch [2], Batch [142/938], Loss: 2.2824060916900635\n",
      "Train: Epoch [2], Batch [143/938], Loss: 2.285640001296997\n",
      "Train: Epoch [2], Batch [144/938], Loss: 2.2859809398651123\n",
      "Train: Epoch [2], Batch [145/938], Loss: 2.2786848545074463\n",
      "Train: Epoch [2], Batch [146/938], Loss: 2.2851154804229736\n",
      "Train: Epoch [2], Batch [147/938], Loss: 2.28238844871521\n",
      "Train: Epoch [2], Batch [148/938], Loss: 2.2823410034179688\n",
      "Train: Epoch [2], Batch [149/938], Loss: 2.289003372192383\n",
      "Train: Epoch [2], Batch [150/938], Loss: 2.282092809677124\n",
      "Train: Epoch [2], Batch [151/938], Loss: 2.2796905040740967\n",
      "Train: Epoch [2], Batch [152/938], Loss: 2.2772371768951416\n",
      "Train: Epoch [2], Batch [153/938], Loss: 2.2834343910217285\n",
      "Train: Epoch [2], Batch [154/938], Loss: 2.2823100090026855\n",
      "Train: Epoch [2], Batch [155/938], Loss: 2.279897451400757\n",
      "Train: Epoch [2], Batch [156/938], Loss: 2.2824454307556152\n",
      "Train: Epoch [2], Batch [157/938], Loss: 2.283146381378174\n",
      "Train: Epoch [2], Batch [158/938], Loss: 2.282618999481201\n",
      "Train: Epoch [2], Batch [159/938], Loss: 2.2823843955993652\n",
      "Train: Epoch [2], Batch [160/938], Loss: 2.2899558544158936\n",
      "Train: Epoch [2], Batch [161/938], Loss: 2.282005786895752\n",
      "Train: Epoch [2], Batch [162/938], Loss: 2.2806968688964844\n",
      "Train: Epoch [2], Batch [163/938], Loss: 2.2880029678344727\n",
      "Train: Epoch [2], Batch [164/938], Loss: 2.2830426692962646\n",
      "Train: Epoch [2], Batch [165/938], Loss: 2.2912042140960693\n",
      "Train: Epoch [2], Batch [166/938], Loss: 2.275006055831909\n",
      "Train: Epoch [2], Batch [167/938], Loss: 2.2796289920806885\n",
      "Train: Epoch [2], Batch [168/938], Loss: 2.285738945007324\n",
      "Train: Epoch [2], Batch [169/938], Loss: 2.284635305404663\n",
      "Train: Epoch [2], Batch [170/938], Loss: 2.283257007598877\n",
      "Train: Epoch [2], Batch [171/938], Loss: 2.289269208908081\n",
      "Train: Epoch [2], Batch [172/938], Loss: 2.27998685836792\n",
      "Train: Epoch [2], Batch [173/938], Loss: 2.2851595878601074\n",
      "Train: Epoch [2], Batch [174/938], Loss: 2.2841649055480957\n",
      "Train: Epoch [2], Batch [175/938], Loss: 2.280303478240967\n",
      "Train: Epoch [2], Batch [176/938], Loss: 2.289637565612793\n",
      "Train: Epoch [2], Batch [177/938], Loss: 2.284881114959717\n",
      "Train: Epoch [2], Batch [178/938], Loss: 2.285463809967041\n",
      "Train: Epoch [2], Batch [179/938], Loss: 2.2766013145446777\n",
      "Train: Epoch [2], Batch [180/938], Loss: 2.282525062561035\n",
      "Train: Epoch [2], Batch [181/938], Loss: 2.2773170471191406\n",
      "Train: Epoch [2], Batch [182/938], Loss: 2.2874507904052734\n",
      "Train: Epoch [2], Batch [183/938], Loss: 2.2792696952819824\n",
      "Train: Epoch [2], Batch [184/938], Loss: 2.277214288711548\n",
      "Train: Epoch [2], Batch [185/938], Loss: 2.278611183166504\n",
      "Train: Epoch [2], Batch [186/938], Loss: 2.2806386947631836\n",
      "Train: Epoch [2], Batch [187/938], Loss: 2.2830352783203125\n",
      "Train: Epoch [2], Batch [188/938], Loss: 2.28806471824646\n",
      "Train: Epoch [2], Batch [189/938], Loss: 2.2848551273345947\n",
      "Train: Epoch [2], Batch [190/938], Loss: 2.291649341583252\n",
      "Train: Epoch [2], Batch [191/938], Loss: 2.27666974067688\n",
      "Train: Epoch [2], Batch [192/938], Loss: 2.281851291656494\n",
      "Train: Epoch [2], Batch [193/938], Loss: 2.2891547679901123\n",
      "Train: Epoch [2], Batch [194/938], Loss: 2.2813029289245605\n",
      "Train: Epoch [2], Batch [195/938], Loss: 2.2848007678985596\n",
      "Train: Epoch [2], Batch [196/938], Loss: 2.285186767578125\n",
      "Train: Epoch [2], Batch [197/938], Loss: 2.2738590240478516\n",
      "Train: Epoch [2], Batch [198/938], Loss: 2.279085397720337\n",
      "Train: Epoch [2], Batch [199/938], Loss: 2.275084972381592\n",
      "Train: Epoch [2], Batch [200/938], Loss: 2.2843313217163086\n",
      "Train: Epoch [2], Batch [201/938], Loss: 2.281554698944092\n",
      "Train: Epoch [2], Batch [202/938], Loss: 2.2771363258361816\n",
      "Train: Epoch [2], Batch [203/938], Loss: 2.2733423709869385\n",
      "Train: Epoch [2], Batch [204/938], Loss: 2.278583288192749\n",
      "Train: Epoch [2], Batch [205/938], Loss: 2.2795705795288086\n",
      "Train: Epoch [2], Batch [206/938], Loss: 2.280684232711792\n",
      "Train: Epoch [2], Batch [207/938], Loss: 2.2836754322052\n",
      "Train: Epoch [2], Batch [208/938], Loss: 2.2815065383911133\n",
      "Train: Epoch [2], Batch [209/938], Loss: 2.2769737243652344\n",
      "Train: Epoch [2], Batch [210/938], Loss: 2.289835214614868\n",
      "Train: Epoch [2], Batch [211/938], Loss: 2.2829341888427734\n",
      "Train: Epoch [2], Batch [212/938], Loss: 2.2818684577941895\n",
      "Train: Epoch [2], Batch [213/938], Loss: 2.281017303466797\n",
      "Train: Epoch [2], Batch [214/938], Loss: 2.2811777591705322\n",
      "Train: Epoch [2], Batch [215/938], Loss: 2.282681465148926\n",
      "Train: Epoch [2], Batch [216/938], Loss: 2.277866840362549\n",
      "Train: Epoch [2], Batch [217/938], Loss: 2.286015033721924\n",
      "Train: Epoch [2], Batch [218/938], Loss: 2.2805466651916504\n",
      "Train: Epoch [2], Batch [219/938], Loss: 2.282937526702881\n",
      "Train: Epoch [2], Batch [220/938], Loss: 2.2812247276306152\n",
      "Train: Epoch [2], Batch [221/938], Loss: 2.2815279960632324\n",
      "Train: Epoch [2], Batch [222/938], Loss: 2.2793681621551514\n",
      "Train: Epoch [2], Batch [223/938], Loss: 2.276048183441162\n",
      "Train: Epoch [2], Batch [224/938], Loss: 2.283844232559204\n",
      "Train: Epoch [2], Batch [225/938], Loss: 2.2732138633728027\n",
      "Train: Epoch [2], Batch [226/938], Loss: 2.2731106281280518\n",
      "Train: Epoch [2], Batch [227/938], Loss: 2.29107928276062\n",
      "Train: Epoch [2], Batch [228/938], Loss: 2.2782561779022217\n",
      "Train: Epoch [2], Batch [229/938], Loss: 2.285200834274292\n",
      "Train: Epoch [2], Batch [230/938], Loss: 2.280280113220215\n",
      "Train: Epoch [2], Batch [231/938], Loss: 2.272279739379883\n",
      "Train: Epoch [2], Batch [232/938], Loss: 2.281161069869995\n",
      "Train: Epoch [2], Batch [233/938], Loss: 2.2827672958374023\n",
      "Train: Epoch [2], Batch [234/938], Loss: 2.2768423557281494\n",
      "Train: Epoch [2], Batch [235/938], Loss: 2.2845704555511475\n",
      "Train: Epoch [2], Batch [236/938], Loss: 2.2823357582092285\n",
      "Train: Epoch [2], Batch [237/938], Loss: 2.2809152603149414\n",
      "Train: Epoch [2], Batch [238/938], Loss: 2.2776894569396973\n",
      "Train: Epoch [2], Batch [239/938], Loss: 2.274684429168701\n",
      "Train: Epoch [2], Batch [240/938], Loss: 2.273521900177002\n",
      "Train: Epoch [2], Batch [241/938], Loss: 2.2780027389526367\n",
      "Train: Epoch [2], Batch [242/938], Loss: 2.2810587882995605\n",
      "Train: Epoch [2], Batch [243/938], Loss: 2.2784218788146973\n",
      "Train: Epoch [2], Batch [244/938], Loss: 2.283477544784546\n",
      "Train: Epoch [2], Batch [245/938], Loss: 2.266972780227661\n",
      "Train: Epoch [2], Batch [246/938], Loss: 2.27401065826416\n",
      "Train: Epoch [2], Batch [247/938], Loss: 2.273904323577881\n",
      "Train: Epoch [2], Batch [248/938], Loss: 2.2842113971710205\n",
      "Train: Epoch [2], Batch [249/938], Loss: 2.2859010696411133\n",
      "Train: Epoch [2], Batch [250/938], Loss: 2.280714750289917\n",
      "Train: Epoch [2], Batch [251/938], Loss: 2.2797064781188965\n",
      "Train: Epoch [2], Batch [252/938], Loss: 2.2830593585968018\n",
      "Train: Epoch [2], Batch [253/938], Loss: 2.2715392112731934\n",
      "Train: Epoch [2], Batch [254/938], Loss: 2.2792789936065674\n",
      "Train: Epoch [2], Batch [255/938], Loss: 2.2777180671691895\n",
      "Train: Epoch [2], Batch [256/938], Loss: 2.2826552391052246\n",
      "Train: Epoch [2], Batch [257/938], Loss: 2.276921033859253\n",
      "Train: Epoch [2], Batch [258/938], Loss: 2.283053398132324\n",
      "Train: Epoch [2], Batch [259/938], Loss: 2.2755119800567627\n",
      "Train: Epoch [2], Batch [260/938], Loss: 2.2750678062438965\n",
      "Train: Epoch [2], Batch [261/938], Loss: 2.2779037952423096\n",
      "Train: Epoch [2], Batch [262/938], Loss: 2.2751739025115967\n",
      "Train: Epoch [2], Batch [263/938], Loss: 2.2775917053222656\n",
      "Train: Epoch [2], Batch [264/938], Loss: 2.2745182514190674\n",
      "Train: Epoch [2], Batch [265/938], Loss: 2.2812705039978027\n",
      "Train: Epoch [2], Batch [266/938], Loss: 2.2718443870544434\n",
      "Train: Epoch [2], Batch [267/938], Loss: 2.27390718460083\n",
      "Train: Epoch [2], Batch [268/938], Loss: 2.2743186950683594\n",
      "Train: Epoch [2], Batch [269/938], Loss: 2.267976760864258\n",
      "Train: Epoch [2], Batch [270/938], Loss: 2.2788331508636475\n",
      "Train: Epoch [2], Batch [271/938], Loss: 2.2782986164093018\n",
      "Train: Epoch [2], Batch [272/938], Loss: 2.282900810241699\n",
      "Train: Epoch [2], Batch [273/938], Loss: 2.273622989654541\n",
      "Train: Epoch [2], Batch [274/938], Loss: 2.27883243560791\n",
      "Train: Epoch [2], Batch [275/938], Loss: 2.2791640758514404\n",
      "Train: Epoch [2], Batch [276/938], Loss: 2.281900644302368\n",
      "Train: Epoch [2], Batch [277/938], Loss: 2.2742252349853516\n",
      "Train: Epoch [2], Batch [278/938], Loss: 2.2727816104888916\n",
      "Train: Epoch [2], Batch [279/938], Loss: 2.2688426971435547\n",
      "Train: Epoch [2], Batch [280/938], Loss: 2.2783193588256836\n",
      "Train: Epoch [2], Batch [281/938], Loss: 2.2758264541625977\n",
      "Train: Epoch [2], Batch [282/938], Loss: 2.271143674850464\n",
      "Train: Epoch [2], Batch [283/938], Loss: 2.281796932220459\n",
      "Train: Epoch [2], Batch [284/938], Loss: 2.281583786010742\n",
      "Train: Epoch [2], Batch [285/938], Loss: 2.271723508834839\n",
      "Train: Epoch [2], Batch [286/938], Loss: 2.2763187885284424\n",
      "Train: Epoch [2], Batch [287/938], Loss: 2.2714014053344727\n",
      "Train: Epoch [2], Batch [288/938], Loss: 2.2814602851867676\n",
      "Train: Epoch [2], Batch [289/938], Loss: 2.274473190307617\n",
      "Train: Epoch [2], Batch [290/938], Loss: 2.277078151702881\n",
      "Train: Epoch [2], Batch [291/938], Loss: 2.2820396423339844\n",
      "Train: Epoch [2], Batch [292/938], Loss: 2.2756848335266113\n",
      "Train: Epoch [2], Batch [293/938], Loss: 2.279301881790161\n",
      "Train: Epoch [2], Batch [294/938], Loss: 2.266402006149292\n",
      "Train: Epoch [2], Batch [295/938], Loss: 2.2848122119903564\n",
      "Train: Epoch [2], Batch [296/938], Loss: 2.269172430038452\n",
      "Train: Epoch [2], Batch [297/938], Loss: 2.2717387676239014\n",
      "Train: Epoch [2], Batch [298/938], Loss: 2.2747867107391357\n",
      "Train: Epoch [2], Batch [299/938], Loss: 2.2790188789367676\n",
      "Train: Epoch [2], Batch [300/938], Loss: 2.280571460723877\n",
      "Train: Epoch [2], Batch [301/938], Loss: 2.267819881439209\n",
      "Train: Epoch [2], Batch [302/938], Loss: 2.2715535163879395\n",
      "Train: Epoch [2], Batch [303/938], Loss: 2.28226637840271\n",
      "Train: Epoch [2], Batch [304/938], Loss: 2.2746663093566895\n",
      "Train: Epoch [2], Batch [305/938], Loss: 2.270423173904419\n",
      "Train: Epoch [2], Batch [306/938], Loss: 2.2632381916046143\n",
      "Train: Epoch [2], Batch [307/938], Loss: 2.2717413902282715\n",
      "Train: Epoch [2], Batch [308/938], Loss: 2.2757139205932617\n",
      "Train: Epoch [2], Batch [309/938], Loss: 2.2766270637512207\n",
      "Train: Epoch [2], Batch [310/938], Loss: 2.278616428375244\n",
      "Train: Epoch [2], Batch [311/938], Loss: 2.275613784790039\n",
      "Train: Epoch [2], Batch [312/938], Loss: 2.2671847343444824\n",
      "Train: Epoch [2], Batch [313/938], Loss: 2.2794651985168457\n",
      "Train: Epoch [2], Batch [314/938], Loss: 2.2772059440612793\n",
      "Train: Epoch [2], Batch [315/938], Loss: 2.283169746398926\n",
      "Train: Epoch [2], Batch [316/938], Loss: 2.2865986824035645\n",
      "Train: Epoch [2], Batch [317/938], Loss: 2.2731704711914062\n",
      "Train: Epoch [2], Batch [318/938], Loss: 2.276313543319702\n",
      "Train: Epoch [2], Batch [319/938], Loss: 2.268232822418213\n",
      "Train: Epoch [2], Batch [320/938], Loss: 2.2745113372802734\n",
      "Train: Epoch [2], Batch [321/938], Loss: 2.2808613777160645\n",
      "Train: Epoch [2], Batch [322/938], Loss: 2.2773542404174805\n",
      "Train: Epoch [2], Batch [323/938], Loss: 2.2796566486358643\n",
      "Train: Epoch [2], Batch [324/938], Loss: 2.272148847579956\n",
      "Train: Epoch [2], Batch [325/938], Loss: 2.2767176628112793\n",
      "Train: Epoch [2], Batch [326/938], Loss: 2.2746429443359375\n",
      "Train: Epoch [2], Batch [327/938], Loss: 2.2791364192962646\n",
      "Train: Epoch [2], Batch [328/938], Loss: 2.276920795440674\n",
      "Train: Epoch [2], Batch [329/938], Loss: 2.2661073207855225\n",
      "Train: Epoch [2], Batch [330/938], Loss: 2.2854316234588623\n",
      "Train: Epoch [2], Batch [331/938], Loss: 2.2743849754333496\n",
      "Train: Epoch [2], Batch [332/938], Loss: 2.2718894481658936\n",
      "Train: Epoch [2], Batch [333/938], Loss: 2.2791924476623535\n",
      "Train: Epoch [2], Batch [334/938], Loss: 2.2753474712371826\n",
      "Train: Epoch [2], Batch [335/938], Loss: 2.2741801738739014\n",
      "Train: Epoch [2], Batch [336/938], Loss: 2.2730536460876465\n",
      "Train: Epoch [2], Batch [337/938], Loss: 2.275867223739624\n",
      "Train: Epoch [2], Batch [338/938], Loss: 2.2748281955718994\n",
      "Train: Epoch [2], Batch [339/938], Loss: 2.28047251701355\n",
      "Train: Epoch [2], Batch [340/938], Loss: 2.2782537937164307\n",
      "Train: Epoch [2], Batch [341/938], Loss: 2.2750606536865234\n",
      "Train: Epoch [2], Batch [342/938], Loss: 2.277829170227051\n",
      "Train: Epoch [2], Batch [343/938], Loss: 2.2762584686279297\n",
      "Train: Epoch [2], Batch [344/938], Loss: 2.2693135738372803\n",
      "Train: Epoch [2], Batch [345/938], Loss: 2.2737467288970947\n",
      "Train: Epoch [2], Batch [346/938], Loss: 2.2757837772369385\n",
      "Train: Epoch [2], Batch [347/938], Loss: 2.2811148166656494\n",
      "Train: Epoch [2], Batch [348/938], Loss: 2.273155689239502\n",
      "Train: Epoch [2], Batch [349/938], Loss: 2.2773678302764893\n",
      "Train: Epoch [2], Batch [350/938], Loss: 2.274414539337158\n",
      "Train: Epoch [2], Batch [351/938], Loss: 2.284298896789551\n",
      "Train: Epoch [2], Batch [352/938], Loss: 2.2753987312316895\n",
      "Train: Epoch [2], Batch [353/938], Loss: 2.270258665084839\n",
      "Train: Epoch [2], Batch [354/938], Loss: 2.27433443069458\n",
      "Train: Epoch [2], Batch [355/938], Loss: 2.272210121154785\n",
      "Train: Epoch [2], Batch [356/938], Loss: 2.2717037200927734\n",
      "Train: Epoch [2], Batch [357/938], Loss: 2.2757327556610107\n",
      "Train: Epoch [2], Batch [358/938], Loss: 2.2684195041656494\n",
      "Train: Epoch [2], Batch [359/938], Loss: 2.2790136337280273\n",
      "Train: Epoch [2], Batch [360/938], Loss: 2.265655040740967\n",
      "Train: Epoch [2], Batch [361/938], Loss: 2.26780366897583\n",
      "Train: Epoch [2], Batch [362/938], Loss: 2.273214817047119\n",
      "Train: Epoch [2], Batch [363/938], Loss: 2.2672109603881836\n",
      "Train: Epoch [2], Batch [364/938], Loss: 2.2683589458465576\n",
      "Train: Epoch [2], Batch [365/938], Loss: 2.265813112258911\n",
      "Train: Epoch [2], Batch [366/938], Loss: 2.2744674682617188\n",
      "Train: Epoch [2], Batch [367/938], Loss: 2.2752068042755127\n",
      "Train: Epoch [2], Batch [368/938], Loss: 2.2761945724487305\n",
      "Train: Epoch [2], Batch [369/938], Loss: 2.2693586349487305\n",
      "Train: Epoch [2], Batch [370/938], Loss: 2.27001690864563\n",
      "Train: Epoch [2], Batch [371/938], Loss: 2.2758126258850098\n",
      "Train: Epoch [2], Batch [372/938], Loss: 2.2695517539978027\n",
      "Train: Epoch [2], Batch [373/938], Loss: 2.2708141803741455\n",
      "Train: Epoch [2], Batch [374/938], Loss: 2.272552967071533\n",
      "Train: Epoch [2], Batch [375/938], Loss: 2.2817342281341553\n",
      "Train: Epoch [2], Batch [376/938], Loss: 2.277956247329712\n",
      "Train: Epoch [2], Batch [377/938], Loss: 2.271498680114746\n",
      "Train: Epoch [2], Batch [378/938], Loss: 2.2801318168640137\n",
      "Train: Epoch [2], Batch [379/938], Loss: 2.2631194591522217\n",
      "Train: Epoch [2], Batch [380/938], Loss: 2.276519298553467\n",
      "Train: Epoch [2], Batch [381/938], Loss: 2.2784790992736816\n",
      "Train: Epoch [2], Batch [382/938], Loss: 2.2726480960845947\n",
      "Train: Epoch [2], Batch [383/938], Loss: 2.264400005340576\n",
      "Train: Epoch [2], Batch [384/938], Loss: 2.275216817855835\n",
      "Train: Epoch [2], Batch [385/938], Loss: 2.26601505279541\n",
      "Train: Epoch [2], Batch [386/938], Loss: 2.269214153289795\n",
      "Train: Epoch [2], Batch [387/938], Loss: 2.273757219314575\n",
      "Train: Epoch [2], Batch [388/938], Loss: 2.2751786708831787\n",
      "Train: Epoch [2], Batch [389/938], Loss: 2.276034116744995\n",
      "Train: Epoch [2], Batch [390/938], Loss: 2.276412010192871\n",
      "Train: Epoch [2], Batch [391/938], Loss: 2.2590088844299316\n",
      "Train: Epoch [2], Batch [392/938], Loss: 2.277038335800171\n",
      "Train: Epoch [2], Batch [393/938], Loss: 2.2743356227874756\n",
      "Train: Epoch [2], Batch [394/938], Loss: 2.2740659713745117\n",
      "Train: Epoch [2], Batch [395/938], Loss: 2.2709434032440186\n",
      "Train: Epoch [2], Batch [396/938], Loss: 2.2655189037323\n",
      "Train: Epoch [2], Batch [397/938], Loss: 2.2724273204803467\n",
      "Train: Epoch [2], Batch [398/938], Loss: 2.277191638946533\n",
      "Train: Epoch [2], Batch [399/938], Loss: 2.2607009410858154\n",
      "Train: Epoch [2], Batch [400/938], Loss: 2.2696008682250977\n",
      "Train: Epoch [2], Batch [401/938], Loss: 2.2761952877044678\n",
      "Train: Epoch [2], Batch [402/938], Loss: 2.2667040824890137\n",
      "Train: Epoch [2], Batch [403/938], Loss: 2.28336238861084\n",
      "Train: Epoch [2], Batch [404/938], Loss: 2.2823703289031982\n",
      "Train: Epoch [2], Batch [405/938], Loss: 2.273366928100586\n",
      "Train: Epoch [2], Batch [406/938], Loss: 2.275543689727783\n",
      "Train: Epoch [2], Batch [407/938], Loss: 2.2594475746154785\n",
      "Train: Epoch [2], Batch [408/938], Loss: 2.269632339477539\n",
      "Train: Epoch [2], Batch [409/938], Loss: 2.2607319355010986\n",
      "Train: Epoch [2], Batch [410/938], Loss: 2.2749202251434326\n",
      "Train: Epoch [2], Batch [411/938], Loss: 2.2704920768737793\n",
      "Train: Epoch [2], Batch [412/938], Loss: 2.2736423015594482\n",
      "Train: Epoch [2], Batch [413/938], Loss: 2.276594638824463\n",
      "Train: Epoch [2], Batch [414/938], Loss: 2.2759053707122803\n",
      "Train: Epoch [2], Batch [415/938], Loss: 2.273921489715576\n",
      "Train: Epoch [2], Batch [416/938], Loss: 2.2831592559814453\n",
      "Train: Epoch [2], Batch [417/938], Loss: 2.2707223892211914\n",
      "Train: Epoch [2], Batch [418/938], Loss: 2.2777462005615234\n",
      "Train: Epoch [2], Batch [419/938], Loss: 2.2770590782165527\n",
      "Train: Epoch [2], Batch [420/938], Loss: 2.265542507171631\n",
      "Train: Epoch [2], Batch [421/938], Loss: 2.268085479736328\n",
      "Train: Epoch [2], Batch [422/938], Loss: 2.2756845951080322\n",
      "Train: Epoch [2], Batch [423/938], Loss: 2.265007495880127\n",
      "Train: Epoch [2], Batch [424/938], Loss: 2.2653675079345703\n",
      "Train: Epoch [2], Batch [425/938], Loss: 2.2695152759552\n",
      "Train: Epoch [2], Batch [426/938], Loss: 2.2718429565429688\n",
      "Train: Epoch [2], Batch [427/938], Loss: 2.2759928703308105\n",
      "Train: Epoch [2], Batch [428/938], Loss: 2.2588469982147217\n",
      "Train: Epoch [2], Batch [429/938], Loss: 2.274083375930786\n",
      "Train: Epoch [2], Batch [430/938], Loss: 2.271989345550537\n",
      "Train: Epoch [2], Batch [431/938], Loss: 2.266295909881592\n",
      "Train: Epoch [2], Batch [432/938], Loss: 2.2582662105560303\n",
      "Train: Epoch [2], Batch [433/938], Loss: 2.2694153785705566\n",
      "Train: Epoch [2], Batch [434/938], Loss: 2.2757604122161865\n",
      "Train: Epoch [2], Batch [435/938], Loss: 2.2656288146972656\n",
      "Train: Epoch [2], Batch [436/938], Loss: 2.274576187133789\n",
      "Train: Epoch [2], Batch [437/938], Loss: 2.2689414024353027\n",
      "Train: Epoch [2], Batch [438/938], Loss: 2.278218984603882\n",
      "Train: Epoch [2], Batch [439/938], Loss: 2.2628231048583984\n",
      "Train: Epoch [2], Batch [440/938], Loss: 2.2680182456970215\n",
      "Train: Epoch [2], Batch [441/938], Loss: 2.2616405487060547\n",
      "Train: Epoch [2], Batch [442/938], Loss: 2.2608680725097656\n",
      "Train: Epoch [2], Batch [443/938], Loss: 2.26584792137146\n",
      "Train: Epoch [2], Batch [444/938], Loss: 2.2729249000549316\n",
      "Train: Epoch [2], Batch [445/938], Loss: 2.265101909637451\n",
      "Train: Epoch [2], Batch [446/938], Loss: 2.273267984390259\n",
      "Train: Epoch [2], Batch [447/938], Loss: 2.271082639694214\n",
      "Train: Epoch [2], Batch [448/938], Loss: 2.2686550617218018\n",
      "Train: Epoch [2], Batch [449/938], Loss: 2.2641496658325195\n",
      "Train: Epoch [2], Batch [450/938], Loss: 2.274601936340332\n",
      "Train: Epoch [2], Batch [451/938], Loss: 2.267364978790283\n",
      "Train: Epoch [2], Batch [452/938], Loss: 2.2688984870910645\n",
      "Train: Epoch [2], Batch [453/938], Loss: 2.2764456272125244\n",
      "Train: Epoch [2], Batch [454/938], Loss: 2.257485866546631\n",
      "Train: Epoch [2], Batch [455/938], Loss: 2.264120101928711\n",
      "Train: Epoch [2], Batch [456/938], Loss: 2.2655038833618164\n",
      "Train: Epoch [2], Batch [457/938], Loss: 2.276042938232422\n",
      "Train: Epoch [2], Batch [458/938], Loss: 2.2597880363464355\n",
      "Train: Epoch [2], Batch [459/938], Loss: 2.2621357440948486\n",
      "Train: Epoch [2], Batch [460/938], Loss: 2.2757551670074463\n",
      "Train: Epoch [2], Batch [461/938], Loss: 2.26521897315979\n",
      "Train: Epoch [2], Batch [462/938], Loss: 2.270144462585449\n",
      "Train: Epoch [2], Batch [463/938], Loss: 2.2643930912017822\n",
      "Train: Epoch [2], Batch [464/938], Loss: 2.269362211227417\n",
      "Train: Epoch [2], Batch [465/938], Loss: 2.269122362136841\n",
      "Train: Epoch [2], Batch [466/938], Loss: 2.2602827548980713\n",
      "Train: Epoch [2], Batch [467/938], Loss: 2.2538349628448486\n",
      "Train: Epoch [2], Batch [468/938], Loss: 2.2669434547424316\n",
      "Train: Epoch [2], Batch [469/938], Loss: 2.2576072216033936\n",
      "Train: Epoch [2], Batch [470/938], Loss: 2.2647695541381836\n",
      "Train: Epoch [2], Batch [471/938], Loss: 2.2687129974365234\n",
      "Train: Epoch [2], Batch [472/938], Loss: 2.2647106647491455\n",
      "Train: Epoch [2], Batch [473/938], Loss: 2.2700142860412598\n",
      "Train: Epoch [2], Batch [474/938], Loss: 2.257016181945801\n",
      "Train: Epoch [2], Batch [475/938], Loss: 2.2581889629364014\n",
      "Train: Epoch [2], Batch [476/938], Loss: 2.2642557621002197\n",
      "Train: Epoch [2], Batch [477/938], Loss: 2.2590670585632324\n",
      "Train: Epoch [2], Batch [478/938], Loss: 2.2547473907470703\n",
      "Train: Epoch [2], Batch [479/938], Loss: 2.2658073902130127\n",
      "Train: Epoch [2], Batch [480/938], Loss: 2.2616543769836426\n",
      "Train: Epoch [2], Batch [481/938], Loss: 2.2684426307678223\n",
      "Train: Epoch [2], Batch [482/938], Loss: 2.2605292797088623\n",
      "Train: Epoch [2], Batch [483/938], Loss: 2.2673258781433105\n",
      "Train: Epoch [2], Batch [484/938], Loss: 2.2752881050109863\n",
      "Train: Epoch [2], Batch [485/938], Loss: 2.2698187828063965\n",
      "Train: Epoch [2], Batch [486/938], Loss: 2.2679126262664795\n",
      "Train: Epoch [2], Batch [487/938], Loss: 2.258028507232666\n",
      "Train: Epoch [2], Batch [488/938], Loss: 2.265150308609009\n",
      "Train: Epoch [2], Batch [489/938], Loss: 2.253896474838257\n",
      "Train: Epoch [2], Batch [490/938], Loss: 2.262798309326172\n",
      "Train: Epoch [2], Batch [491/938], Loss: 2.271345615386963\n",
      "Train: Epoch [2], Batch [492/938], Loss: 2.2656450271606445\n",
      "Train: Epoch [2], Batch [493/938], Loss: 2.264378309249878\n",
      "Train: Epoch [2], Batch [494/938], Loss: 2.257277011871338\n",
      "Train: Epoch [2], Batch [495/938], Loss: 2.2754616737365723\n",
      "Train: Epoch [2], Batch [496/938], Loss: 2.2668607234954834\n",
      "Train: Epoch [2], Batch [497/938], Loss: 2.267110824584961\n",
      "Train: Epoch [2], Batch [498/938], Loss: 2.263345956802368\n",
      "Train: Epoch [2], Batch [499/938], Loss: 2.2607452869415283\n",
      "Train: Epoch [2], Batch [500/938], Loss: 2.263145685195923\n",
      "Train: Epoch [2], Batch [501/938], Loss: 2.257650852203369\n",
      "Train: Epoch [2], Batch [502/938], Loss: 2.2637109756469727\n",
      "Train: Epoch [2], Batch [503/938], Loss: 2.2666773796081543\n",
      "Train: Epoch [2], Batch [504/938], Loss: 2.259101390838623\n",
      "Train: Epoch [2], Batch [505/938], Loss: 2.2639365196228027\n",
      "Train: Epoch [2], Batch [506/938], Loss: 2.2604799270629883\n",
      "Train: Epoch [2], Batch [507/938], Loss: 2.260758638381958\n",
      "Train: Epoch [2], Batch [508/938], Loss: 2.2740976810455322\n",
      "Train: Epoch [2], Batch [509/938], Loss: 2.277589797973633\n",
      "Train: Epoch [2], Batch [510/938], Loss: 2.256592035293579\n",
      "Train: Epoch [2], Batch [511/938], Loss: 2.2579197883605957\n",
      "Train: Epoch [2], Batch [512/938], Loss: 2.2594456672668457\n",
      "Train: Epoch [2], Batch [513/938], Loss: 2.2759408950805664\n",
      "Train: Epoch [2], Batch [514/938], Loss: 2.263453245162964\n",
      "Train: Epoch [2], Batch [515/938], Loss: 2.2614336013793945\n",
      "Train: Epoch [2], Batch [516/938], Loss: 2.250671625137329\n",
      "Train: Epoch [2], Batch [517/938], Loss: 2.264331579208374\n",
      "Train: Epoch [2], Batch [518/938], Loss: 2.2674036026000977\n",
      "Train: Epoch [2], Batch [519/938], Loss: 2.26214861869812\n",
      "Train: Epoch [2], Batch [520/938], Loss: 2.260532855987549\n",
      "Train: Epoch [2], Batch [521/938], Loss: 2.2477426528930664\n",
      "Train: Epoch [2], Batch [522/938], Loss: 2.260067939758301\n",
      "Train: Epoch [2], Batch [523/938], Loss: 2.2585153579711914\n",
      "Train: Epoch [2], Batch [524/938], Loss: 2.2622363567352295\n",
      "Train: Epoch [2], Batch [525/938], Loss: 2.2665562629699707\n",
      "Train: Epoch [2], Batch [526/938], Loss: 2.2547311782836914\n",
      "Train: Epoch [2], Batch [527/938], Loss: 2.2717292308807373\n",
      "Train: Epoch [2], Batch [528/938], Loss: 2.2558367252349854\n",
      "Train: Epoch [2], Batch [529/938], Loss: 2.2709977626800537\n",
      "Train: Epoch [2], Batch [530/938], Loss: 2.269331932067871\n",
      "Train: Epoch [2], Batch [531/938], Loss: 2.267850875854492\n",
      "Train: Epoch [2], Batch [532/938], Loss: 2.2721474170684814\n",
      "Train: Epoch [2], Batch [533/938], Loss: 2.2570366859436035\n",
      "Train: Epoch [2], Batch [534/938], Loss: 2.26226806640625\n",
      "Train: Epoch [2], Batch [535/938], Loss: 2.261972665786743\n",
      "Train: Epoch [2], Batch [536/938], Loss: 2.2643818855285645\n",
      "Train: Epoch [2], Batch [537/938], Loss: 2.2639927864074707\n",
      "Train: Epoch [2], Batch [538/938], Loss: 2.2683143615722656\n",
      "Train: Epoch [2], Batch [539/938], Loss: 2.254333972930908\n",
      "Train: Epoch [2], Batch [540/938], Loss: 2.2756097316741943\n",
      "Train: Epoch [2], Batch [541/938], Loss: 2.2606801986694336\n",
      "Train: Epoch [2], Batch [542/938], Loss: 2.267333507537842\n",
      "Train: Epoch [2], Batch [543/938], Loss: 2.259962320327759\n",
      "Train: Epoch [2], Batch [544/938], Loss: 2.267641544342041\n",
      "Train: Epoch [2], Batch [545/938], Loss: 2.2612545490264893\n",
      "Train: Epoch [2], Batch [546/938], Loss: 2.2593069076538086\n",
      "Train: Epoch [2], Batch [547/938], Loss: 2.2619259357452393\n",
      "Train: Epoch [2], Batch [548/938], Loss: 2.274358034133911\n",
      "Train: Epoch [2], Batch [549/938], Loss: 2.263646125793457\n",
      "Train: Epoch [2], Batch [550/938], Loss: 2.2638378143310547\n",
      "Train: Epoch [2], Batch [551/938], Loss: 2.263277053833008\n",
      "Train: Epoch [2], Batch [552/938], Loss: 2.252828598022461\n",
      "Train: Epoch [2], Batch [553/938], Loss: 2.244623899459839\n",
      "Train: Epoch [2], Batch [554/938], Loss: 2.2644386291503906\n",
      "Train: Epoch [2], Batch [555/938], Loss: 2.2678074836730957\n",
      "Train: Epoch [2], Batch [556/938], Loss: 2.261176347732544\n",
      "Train: Epoch [2], Batch [557/938], Loss: 2.2567238807678223\n",
      "Train: Epoch [2], Batch [558/938], Loss: 2.256756544113159\n",
      "Train: Epoch [2], Batch [559/938], Loss: 2.267559051513672\n",
      "Train: Epoch [2], Batch [560/938], Loss: 2.2615702152252197\n",
      "Train: Epoch [2], Batch [561/938], Loss: 2.251163959503174\n",
      "Train: Epoch [2], Batch [562/938], Loss: 2.2556521892547607\n",
      "Train: Epoch [2], Batch [563/938], Loss: 2.267270088195801\n",
      "Train: Epoch [2], Batch [564/938], Loss: 2.253152847290039\n",
      "Train: Epoch [2], Batch [565/938], Loss: 2.244572639465332\n",
      "Train: Epoch [2], Batch [566/938], Loss: 2.253024101257324\n",
      "Train: Epoch [2], Batch [567/938], Loss: 2.2552289962768555\n",
      "Train: Epoch [2], Batch [568/938], Loss: 2.2534947395324707\n",
      "Train: Epoch [2], Batch [569/938], Loss: 2.246622323989868\n",
      "Train: Epoch [2], Batch [570/938], Loss: 2.248166799545288\n",
      "Train: Epoch [2], Batch [571/938], Loss: 2.25852370262146\n",
      "Train: Epoch [2], Batch [572/938], Loss: 2.263657331466675\n",
      "Train: Epoch [2], Batch [573/938], Loss: 2.2592201232910156\n",
      "Train: Epoch [2], Batch [574/938], Loss: 2.2494051456451416\n",
      "Train: Epoch [2], Batch [575/938], Loss: 2.264878749847412\n",
      "Train: Epoch [2], Batch [576/938], Loss: 2.2700488567352295\n",
      "Train: Epoch [2], Batch [577/938], Loss: 2.2674450874328613\n",
      "Train: Epoch [2], Batch [578/938], Loss: 2.2589504718780518\n",
      "Train: Epoch [2], Batch [579/938], Loss: 2.255826711654663\n",
      "Train: Epoch [2], Batch [580/938], Loss: 2.254610538482666\n",
      "Train: Epoch [2], Batch [581/938], Loss: 2.2541282176971436\n",
      "Train: Epoch [2], Batch [582/938], Loss: 2.2566921710968018\n",
      "Train: Epoch [2], Batch [583/938], Loss: 2.2517499923706055\n",
      "Train: Epoch [2], Batch [584/938], Loss: 2.257920026779175\n",
      "Train: Epoch [2], Batch [585/938], Loss: 2.248203992843628\n",
      "Train: Epoch [2], Batch [586/938], Loss: 2.2646682262420654\n",
      "Train: Epoch [2], Batch [587/938], Loss: 2.2563798427581787\n",
      "Train: Epoch [2], Batch [588/938], Loss: 2.258124351501465\n",
      "Train: Epoch [2], Batch [589/938], Loss: 2.263784885406494\n",
      "Train: Epoch [2], Batch [590/938], Loss: 2.2552804946899414\n",
      "Train: Epoch [2], Batch [591/938], Loss: 2.2713234424591064\n",
      "Train: Epoch [2], Batch [592/938], Loss: 2.2492122650146484\n",
      "Train: Epoch [2], Batch [593/938], Loss: 2.266073226928711\n",
      "Train: Epoch [2], Batch [594/938], Loss: 2.2493784427642822\n",
      "Train: Epoch [2], Batch [595/938], Loss: 2.245443344116211\n",
      "Train: Epoch [2], Batch [596/938], Loss: 2.253688097000122\n",
      "Train: Epoch [2], Batch [597/938], Loss: 2.2409887313842773\n",
      "Train: Epoch [2], Batch [598/938], Loss: 2.246708869934082\n",
      "Train: Epoch [2], Batch [599/938], Loss: 2.242952346801758\n",
      "Train: Epoch [2], Batch [600/938], Loss: 2.25579571723938\n",
      "Train: Epoch [2], Batch [601/938], Loss: 2.2554590702056885\n",
      "Train: Epoch [2], Batch [602/938], Loss: 2.260525941848755\n",
      "Train: Epoch [2], Batch [603/938], Loss: 2.255388021469116\n",
      "Train: Epoch [2], Batch [604/938], Loss: 2.254525899887085\n",
      "Train: Epoch [2], Batch [605/938], Loss: 2.2586214542388916\n",
      "Train: Epoch [2], Batch [606/938], Loss: 2.254883050918579\n",
      "Train: Epoch [2], Batch [607/938], Loss: 2.253020763397217\n",
      "Train: Epoch [2], Batch [608/938], Loss: 2.259936571121216\n",
      "Train: Epoch [2], Batch [609/938], Loss: 2.2679190635681152\n",
      "Train: Epoch [2], Batch [610/938], Loss: 2.2596213817596436\n",
      "Train: Epoch [2], Batch [611/938], Loss: 2.2516512870788574\n",
      "Train: Epoch [2], Batch [612/938], Loss: 2.244929313659668\n",
      "Train: Epoch [2], Batch [613/938], Loss: 2.2586984634399414\n",
      "Train: Epoch [2], Batch [614/938], Loss: 2.2597124576568604\n",
      "Train: Epoch [2], Batch [615/938], Loss: 2.2530713081359863\n",
      "Train: Epoch [2], Batch [616/938], Loss: 2.243642568588257\n",
      "Train: Epoch [2], Batch [617/938], Loss: 2.2518577575683594\n",
      "Train: Epoch [2], Batch [618/938], Loss: 2.263136863708496\n",
      "Train: Epoch [2], Batch [619/938], Loss: 2.2568163871765137\n",
      "Train: Epoch [2], Batch [620/938], Loss: 2.24629282951355\n",
      "Train: Epoch [2], Batch [621/938], Loss: 2.254088878631592\n",
      "Train: Epoch [2], Batch [622/938], Loss: 2.246047019958496\n",
      "Train: Epoch [2], Batch [623/938], Loss: 2.255396842956543\n",
      "Train: Epoch [2], Batch [624/938], Loss: 2.2623071670532227\n",
      "Train: Epoch [2], Batch [625/938], Loss: 2.2625019550323486\n",
      "Train: Epoch [2], Batch [626/938], Loss: 2.254591464996338\n",
      "Train: Epoch [2], Batch [627/938], Loss: 2.250702381134033\n",
      "Train: Epoch [2], Batch [628/938], Loss: 2.2495577335357666\n",
      "Train: Epoch [2], Batch [629/938], Loss: 2.246546983718872\n",
      "Train: Epoch [2], Batch [630/938], Loss: 2.256662368774414\n",
      "Train: Epoch [2], Batch [631/938], Loss: 2.259006977081299\n",
      "Train: Epoch [2], Batch [632/938], Loss: 2.244549036026001\n",
      "Train: Epoch [2], Batch [633/938], Loss: 2.2429966926574707\n",
      "Train: Epoch [2], Batch [634/938], Loss: 2.250656843185425\n",
      "Train: Epoch [2], Batch [635/938], Loss: 2.2622554302215576\n",
      "Train: Epoch [2], Batch [636/938], Loss: 2.2454004287719727\n",
      "Train: Epoch [2], Batch [637/938], Loss: 2.250237464904785\n",
      "Train: Epoch [2], Batch [638/938], Loss: 2.2480502128601074\n",
      "Train: Epoch [2], Batch [639/938], Loss: 2.2540485858917236\n",
      "Train: Epoch [2], Batch [640/938], Loss: 2.246089220046997\n",
      "Train: Epoch [2], Batch [641/938], Loss: 2.2601912021636963\n",
      "Train: Epoch [2], Batch [642/938], Loss: 2.24991512298584\n",
      "Train: Epoch [2], Batch [643/938], Loss: 2.239866018295288\n",
      "Train: Epoch [2], Batch [644/938], Loss: 2.240518569946289\n",
      "Train: Epoch [2], Batch [645/938], Loss: 2.2434728145599365\n",
      "Train: Epoch [2], Batch [646/938], Loss: 2.2564549446105957\n",
      "Train: Epoch [2], Batch [647/938], Loss: 2.25651478767395\n",
      "Train: Epoch [2], Batch [648/938], Loss: 2.2549617290496826\n",
      "Train: Epoch [2], Batch [649/938], Loss: 2.241015911102295\n",
      "Train: Epoch [2], Batch [650/938], Loss: 2.247844696044922\n",
      "Train: Epoch [2], Batch [651/938], Loss: 2.249286651611328\n",
      "Train: Epoch [2], Batch [652/938], Loss: 2.2520976066589355\n",
      "Train: Epoch [2], Batch [653/938], Loss: 2.24580454826355\n",
      "Train: Epoch [2], Batch [654/938], Loss: 2.2522189617156982\n",
      "Train: Epoch [2], Batch [655/938], Loss: 2.248032569885254\n",
      "Train: Epoch [2], Batch [656/938], Loss: 2.2475428581237793\n",
      "Train: Epoch [2], Batch [657/938], Loss: 2.2556545734405518\n",
      "Train: Epoch [2], Batch [658/938], Loss: 2.2448582649230957\n",
      "Train: Epoch [2], Batch [659/938], Loss: 2.2670366764068604\n",
      "Train: Epoch [2], Batch [660/938], Loss: 2.2382097244262695\n",
      "Train: Epoch [2], Batch [661/938], Loss: 2.257786512374878\n",
      "Train: Epoch [2], Batch [662/938], Loss: 2.255584239959717\n",
      "Train: Epoch [2], Batch [663/938], Loss: 2.246084690093994\n",
      "Train: Epoch [2], Batch [664/938], Loss: 2.2493858337402344\n",
      "Train: Epoch [2], Batch [665/938], Loss: 2.2449703216552734\n",
      "Train: Epoch [2], Batch [666/938], Loss: 2.239797830581665\n",
      "Train: Epoch [2], Batch [667/938], Loss: 2.24542498588562\n",
      "Train: Epoch [2], Batch [668/938], Loss: 2.254178285598755\n",
      "Train: Epoch [2], Batch [669/938], Loss: 2.2478296756744385\n",
      "Train: Epoch [2], Batch [670/938], Loss: 2.2381060123443604\n",
      "Train: Epoch [2], Batch [671/938], Loss: 2.249910831451416\n",
      "Train: Epoch [2], Batch [672/938], Loss: 2.2486300468444824\n",
      "Train: Epoch [2], Batch [673/938], Loss: 2.2601876258850098\n",
      "Train: Epoch [2], Batch [674/938], Loss: 2.242917537689209\n",
      "Train: Epoch [2], Batch [675/938], Loss: 2.226426839828491\n",
      "Train: Epoch [2], Batch [676/938], Loss: 2.2495076656341553\n",
      "Train: Epoch [2], Batch [677/938], Loss: 2.2340481281280518\n",
      "Train: Epoch [2], Batch [678/938], Loss: 2.2536847591400146\n",
      "Train: Epoch [2], Batch [679/938], Loss: 2.243156909942627\n",
      "Train: Epoch [2], Batch [680/938], Loss: 2.265465021133423\n",
      "Train: Epoch [2], Batch [681/938], Loss: 2.251458168029785\n",
      "Train: Epoch [2], Batch [682/938], Loss: 2.2379541397094727\n",
      "Train: Epoch [2], Batch [683/938], Loss: 2.2417287826538086\n",
      "Train: Epoch [2], Batch [684/938], Loss: 2.2488348484039307\n",
      "Train: Epoch [2], Batch [685/938], Loss: 2.2411653995513916\n",
      "Train: Epoch [2], Batch [686/938], Loss: 2.261875629425049\n",
      "Train: Epoch [2], Batch [687/938], Loss: 2.2475337982177734\n",
      "Train: Epoch [2], Batch [688/938], Loss: 2.2402286529541016\n",
      "Train: Epoch [2], Batch [689/938], Loss: 2.246593713760376\n",
      "Train: Epoch [2], Batch [690/938], Loss: 2.262770414352417\n",
      "Train: Epoch [2], Batch [691/938], Loss: 2.241422653198242\n",
      "Train: Epoch [2], Batch [692/938], Loss: 2.2552242279052734\n",
      "Train: Epoch [2], Batch [693/938], Loss: 2.239180326461792\n",
      "Train: Epoch [2], Batch [694/938], Loss: 2.240748882293701\n",
      "Train: Epoch [2], Batch [695/938], Loss: 2.253175735473633\n",
      "Train: Epoch [2], Batch [696/938], Loss: 2.256359815597534\n",
      "Train: Epoch [2], Batch [697/938], Loss: 2.238858938217163\n",
      "Train: Epoch [2], Batch [698/938], Loss: 2.235962390899658\n",
      "Train: Epoch [2], Batch [699/938], Loss: 2.2491583824157715\n",
      "Train: Epoch [2], Batch [700/938], Loss: 2.231536388397217\n",
      "Train: Epoch [2], Batch [701/938], Loss: 2.2447023391723633\n",
      "Train: Epoch [2], Batch [702/938], Loss: 2.243095874786377\n",
      "Train: Epoch [2], Batch [703/938], Loss: 2.2487754821777344\n",
      "Train: Epoch [2], Batch [704/938], Loss: 2.236825704574585\n",
      "Train: Epoch [2], Batch [705/938], Loss: 2.244852304458618\n",
      "Train: Epoch [2], Batch [706/938], Loss: 2.2441844940185547\n",
      "Train: Epoch [2], Batch [707/938], Loss: 2.236083984375\n",
      "Train: Epoch [2], Batch [708/938], Loss: 2.236802816390991\n",
      "Train: Epoch [2], Batch [709/938], Loss: 2.248128890991211\n",
      "Train: Epoch [2], Batch [710/938], Loss: 2.2438619136810303\n",
      "Train: Epoch [2], Batch [711/938], Loss: 2.2417848110198975\n",
      "Train: Epoch [2], Batch [712/938], Loss: 2.2304344177246094\n",
      "Train: Epoch [2], Batch [713/938], Loss: 2.2390410900115967\n",
      "Train: Epoch [2], Batch [714/938], Loss: 2.2362022399902344\n",
      "Train: Epoch [2], Batch [715/938], Loss: 2.2362780570983887\n",
      "Train: Epoch [2], Batch [716/938], Loss: 2.250080108642578\n",
      "Train: Epoch [2], Batch [717/938], Loss: 2.2419426441192627\n",
      "Train: Epoch [2], Batch [718/938], Loss: 2.2448325157165527\n",
      "Train: Epoch [2], Batch [719/938], Loss: 2.241110324859619\n",
      "Train: Epoch [2], Batch [720/938], Loss: 2.2493433952331543\n",
      "Train: Epoch [2], Batch [721/938], Loss: 2.2411787509918213\n",
      "Train: Epoch [2], Batch [722/938], Loss: 2.241567611694336\n",
      "Train: Epoch [2], Batch [723/938], Loss: 2.2484018802642822\n",
      "Train: Epoch [2], Batch [724/938], Loss: 2.2351393699645996\n",
      "Train: Epoch [2], Batch [725/938], Loss: 2.233231782913208\n",
      "Train: Epoch [2], Batch [726/938], Loss: 2.240891695022583\n",
      "Train: Epoch [2], Batch [727/938], Loss: 2.256179094314575\n",
      "Train: Epoch [2], Batch [728/938], Loss: 2.240795612335205\n",
      "Train: Epoch [2], Batch [729/938], Loss: 2.2442479133605957\n",
      "Train: Epoch [2], Batch [730/938], Loss: 2.251739740371704\n",
      "Train: Epoch [2], Batch [731/938], Loss: 2.2560532093048096\n",
      "Train: Epoch [2], Batch [732/938], Loss: 2.239697217941284\n",
      "Train: Epoch [2], Batch [733/938], Loss: 2.223630666732788\n",
      "Train: Epoch [2], Batch [734/938], Loss: 2.240353584289551\n",
      "Train: Epoch [2], Batch [735/938], Loss: 2.24017596244812\n",
      "Train: Epoch [2], Batch [736/938], Loss: 2.2312979698181152\n",
      "Train: Epoch [2], Batch [737/938], Loss: 2.243394374847412\n",
      "Train: Epoch [2], Batch [738/938], Loss: 2.25109601020813\n",
      "Train: Epoch [2], Batch [739/938], Loss: 2.225379467010498\n",
      "Train: Epoch [2], Batch [740/938], Loss: 2.240769863128662\n",
      "Train: Epoch [2], Batch [741/938], Loss: 2.2363007068634033\n",
      "Train: Epoch [2], Batch [742/938], Loss: 2.2424240112304688\n",
      "Train: Epoch [2], Batch [743/938], Loss: 2.2546048164367676\n",
      "Train: Epoch [2], Batch [744/938], Loss: 2.2325761318206787\n",
      "Train: Epoch [2], Batch [745/938], Loss: 2.232680320739746\n",
      "Train: Epoch [2], Batch [746/938], Loss: 2.254763603210449\n",
      "Train: Epoch [2], Batch [747/938], Loss: 2.2459421157836914\n",
      "Train: Epoch [2], Batch [748/938], Loss: 2.2274370193481445\n",
      "Train: Epoch [2], Batch [749/938], Loss: 2.2485060691833496\n",
      "Train: Epoch [2], Batch [750/938], Loss: 2.242243766784668\n",
      "Train: Epoch [2], Batch [751/938], Loss: 2.2483620643615723\n",
      "Train: Epoch [2], Batch [752/938], Loss: 2.229588031768799\n",
      "Train: Epoch [2], Batch [753/938], Loss: 2.228308916091919\n",
      "Train: Epoch [2], Batch [754/938], Loss: 2.241325616836548\n",
      "Train: Epoch [2], Batch [755/938], Loss: 2.20641827583313\n",
      "Train: Epoch [2], Batch [756/938], Loss: 2.230433940887451\n",
      "Train: Epoch [2], Batch [757/938], Loss: 2.2343761920928955\n",
      "Train: Epoch [2], Batch [758/938], Loss: 2.244393825531006\n",
      "Train: Epoch [2], Batch [759/938], Loss: 2.2360172271728516\n",
      "Train: Epoch [2], Batch [760/938], Loss: 2.2343506813049316\n",
      "Train: Epoch [2], Batch [761/938], Loss: 2.24226713180542\n",
      "Train: Epoch [2], Batch [762/938], Loss: 2.2450294494628906\n",
      "Train: Epoch [2], Batch [763/938], Loss: 2.2276217937469482\n",
      "Train: Epoch [2], Batch [764/938], Loss: 2.237600326538086\n",
      "Train: Epoch [2], Batch [765/938], Loss: 2.239286184310913\n",
      "Train: Epoch [2], Batch [766/938], Loss: 2.2585949897766113\n",
      "Train: Epoch [2], Batch [767/938], Loss: 2.2378575801849365\n",
      "Train: Epoch [2], Batch [768/938], Loss: 2.2344002723693848\n",
      "Train: Epoch [2], Batch [769/938], Loss: 2.2231101989746094\n",
      "Train: Epoch [2], Batch [770/938], Loss: 2.232114791870117\n",
      "Train: Epoch [2], Batch [771/938], Loss: 2.218724250793457\n",
      "Train: Epoch [2], Batch [772/938], Loss: 2.2190604209899902\n",
      "Train: Epoch [2], Batch [773/938], Loss: 2.2469394207000732\n",
      "Train: Epoch [2], Batch [774/938], Loss: 2.2267491817474365\n",
      "Train: Epoch [2], Batch [775/938], Loss: 2.222123146057129\n",
      "Train: Epoch [2], Batch [776/938], Loss: 2.2062742710113525\n",
      "Train: Epoch [2], Batch [777/938], Loss: 2.2296693325042725\n",
      "Train: Epoch [2], Batch [778/938], Loss: 2.250861883163452\n",
      "Train: Epoch [2], Batch [779/938], Loss: 2.238156795501709\n",
      "Train: Epoch [2], Batch [780/938], Loss: 2.2296550273895264\n",
      "Train: Epoch [2], Batch [781/938], Loss: 2.2220571041107178\n",
      "Train: Epoch [2], Batch [782/938], Loss: 2.229288101196289\n",
      "Train: Epoch [2], Batch [783/938], Loss: 2.2316296100616455\n",
      "Train: Epoch [2], Batch [784/938], Loss: 2.215491533279419\n",
      "Train: Epoch [2], Batch [785/938], Loss: 2.207622528076172\n",
      "Train: Epoch [2], Batch [786/938], Loss: 2.2302019596099854\n",
      "Train: Epoch [2], Batch [787/938], Loss: 2.2468457221984863\n",
      "Train: Epoch [2], Batch [788/938], Loss: 2.2380666732788086\n",
      "Train: Epoch [2], Batch [789/938], Loss: 2.220679521560669\n",
      "Train: Epoch [2], Batch [790/938], Loss: 2.2414135932922363\n",
      "Train: Epoch [2], Batch [791/938], Loss: 2.2202296257019043\n",
      "Train: Epoch [2], Batch [792/938], Loss: 2.24629545211792\n",
      "Train: Epoch [2], Batch [793/938], Loss: 2.2357840538024902\n",
      "Train: Epoch [2], Batch [794/938], Loss: 2.2419817447662354\n",
      "Train: Epoch [2], Batch [795/938], Loss: 2.2391419410705566\n",
      "Train: Epoch [2], Batch [796/938], Loss: 2.2233824729919434\n",
      "Train: Epoch [2], Batch [797/938], Loss: 2.221353530883789\n",
      "Train: Epoch [2], Batch [798/938], Loss: 2.234851598739624\n",
      "Train: Epoch [2], Batch [799/938], Loss: 2.214130163192749\n",
      "Train: Epoch [2], Batch [800/938], Loss: 2.2395169734954834\n",
      "Train: Epoch [2], Batch [801/938], Loss: 2.213975429534912\n",
      "Train: Epoch [2], Batch [802/938], Loss: 2.2188591957092285\n",
      "Train: Epoch [2], Batch [803/938], Loss: 2.2296864986419678\n",
      "Train: Epoch [2], Batch [804/938], Loss: 2.227442979812622\n",
      "Train: Epoch [2], Batch [805/938], Loss: 2.2266111373901367\n",
      "Train: Epoch [2], Batch [806/938], Loss: 2.2212839126586914\n",
      "Train: Epoch [2], Batch [807/938], Loss: 2.2375731468200684\n",
      "Train: Epoch [2], Batch [808/938], Loss: 2.233354330062866\n",
      "Train: Epoch [2], Batch [809/938], Loss: 2.2253103256225586\n",
      "Train: Epoch [2], Batch [810/938], Loss: 2.222046375274658\n",
      "Train: Epoch [2], Batch [811/938], Loss: 2.229449510574341\n",
      "Train: Epoch [2], Batch [812/938], Loss: 2.2164947986602783\n",
      "Train: Epoch [2], Batch [813/938], Loss: 2.2311623096466064\n",
      "Train: Epoch [2], Batch [814/938], Loss: 2.2376251220703125\n",
      "Train: Epoch [2], Batch [815/938], Loss: 2.2033188343048096\n",
      "Train: Epoch [2], Batch [816/938], Loss: 2.2270560264587402\n",
      "Train: Epoch [2], Batch [817/938], Loss: 2.236314535140991\n",
      "Train: Epoch [2], Batch [818/938], Loss: 2.236490249633789\n",
      "Train: Epoch [2], Batch [819/938], Loss: 2.236356496810913\n",
      "Train: Epoch [2], Batch [820/938], Loss: 2.2222020626068115\n",
      "Train: Epoch [2], Batch [821/938], Loss: 2.237173557281494\n",
      "Train: Epoch [2], Batch [822/938], Loss: 2.2303531169891357\n",
      "Train: Epoch [2], Batch [823/938], Loss: 2.2081711292266846\n",
      "Train: Epoch [2], Batch [824/938], Loss: 2.228069543838501\n",
      "Train: Epoch [2], Batch [825/938], Loss: 2.238288402557373\n",
      "Train: Epoch [2], Batch [826/938], Loss: 2.2299082279205322\n",
      "Train: Epoch [2], Batch [827/938], Loss: 2.230360746383667\n",
      "Train: Epoch [2], Batch [828/938], Loss: 2.2262866497039795\n",
      "Train: Epoch [2], Batch [829/938], Loss: 2.2239537239074707\n",
      "Train: Epoch [2], Batch [830/938], Loss: 2.2306103706359863\n",
      "Train: Epoch [2], Batch [831/938], Loss: 2.219662666320801\n",
      "Train: Epoch [2], Batch [832/938], Loss: 2.209557056427002\n",
      "Train: Epoch [2], Batch [833/938], Loss: 2.2203264236450195\n",
      "Train: Epoch [2], Batch [834/938], Loss: 2.224778890609741\n",
      "Train: Epoch [2], Batch [835/938], Loss: 2.2391304969787598\n",
      "Train: Epoch [2], Batch [836/938], Loss: 2.2199149131774902\n",
      "Train: Epoch [2], Batch [837/938], Loss: 2.230245351791382\n",
      "Train: Epoch [2], Batch [838/938], Loss: 2.2242321968078613\n",
      "Train: Epoch [2], Batch [839/938], Loss: 2.223167896270752\n",
      "Train: Epoch [2], Batch [840/938], Loss: 2.2281579971313477\n",
      "Train: Epoch [2], Batch [841/938], Loss: 2.2223005294799805\n",
      "Train: Epoch [2], Batch [842/938], Loss: 2.2104716300964355\n",
      "Train: Epoch [2], Batch [843/938], Loss: 2.2291691303253174\n",
      "Train: Epoch [2], Batch [844/938], Loss: 2.2223453521728516\n",
      "Train: Epoch [2], Batch [845/938], Loss: 2.2198143005371094\n",
      "Train: Epoch [2], Batch [846/938], Loss: 2.229682207107544\n",
      "Train: Epoch [2], Batch [847/938], Loss: 2.205747604370117\n",
      "Train: Epoch [2], Batch [848/938], Loss: 2.2159879207611084\n",
      "Train: Epoch [2], Batch [849/938], Loss: 2.217057943344116\n",
      "Train: Epoch [2], Batch [850/938], Loss: 2.2166833877563477\n",
      "Train: Epoch [2], Batch [851/938], Loss: 2.2180795669555664\n",
      "Train: Epoch [2], Batch [852/938], Loss: 2.2118191719055176\n",
      "Train: Epoch [2], Batch [853/938], Loss: 2.195364475250244\n",
      "Train: Epoch [2], Batch [854/938], Loss: 2.2299084663391113\n",
      "Train: Epoch [2], Batch [855/938], Loss: 2.216918468475342\n",
      "Train: Epoch [2], Batch [856/938], Loss: 2.2179200649261475\n",
      "Train: Epoch [2], Batch [857/938], Loss: 2.2120041847229004\n",
      "Train: Epoch [2], Batch [858/938], Loss: 2.2324914932250977\n",
      "Train: Epoch [2], Batch [859/938], Loss: 2.2269623279571533\n",
      "Train: Epoch [2], Batch [860/938], Loss: 2.24601149559021\n",
      "Train: Epoch [2], Batch [861/938], Loss: 2.229710817337036\n",
      "Train: Epoch [2], Batch [862/938], Loss: 2.2413852214813232\n",
      "Train: Epoch [2], Batch [863/938], Loss: 2.1976609230041504\n",
      "Train: Epoch [2], Batch [864/938], Loss: 2.2312827110290527\n",
      "Train: Epoch [2], Batch [865/938], Loss: 2.224323034286499\n",
      "Train: Epoch [2], Batch [866/938], Loss: 2.2108051776885986\n",
      "Train: Epoch [2], Batch [867/938], Loss: 2.2073864936828613\n",
      "Train: Epoch [2], Batch [868/938], Loss: 2.224745273590088\n",
      "Train: Epoch [2], Batch [869/938], Loss: 2.2047250270843506\n",
      "Train: Epoch [2], Batch [870/938], Loss: 2.2124176025390625\n",
      "Train: Epoch [2], Batch [871/938], Loss: 2.2277283668518066\n",
      "Train: Epoch [2], Batch [872/938], Loss: 2.2192039489746094\n",
      "Train: Epoch [2], Batch [873/938], Loss: 2.226078987121582\n",
      "Train: Epoch [2], Batch [874/938], Loss: 2.1975717544555664\n",
      "Train: Epoch [2], Batch [875/938], Loss: 2.2108030319213867\n",
      "Train: Epoch [2], Batch [876/938], Loss: 2.206829071044922\n",
      "Train: Epoch [2], Batch [877/938], Loss: 2.2184560298919678\n",
      "Train: Epoch [2], Batch [878/938], Loss: 2.220794200897217\n",
      "Train: Epoch [2], Batch [879/938], Loss: 2.1853561401367188\n",
      "Train: Epoch [2], Batch [880/938], Loss: 2.217019557952881\n",
      "Train: Epoch [2], Batch [881/938], Loss: 2.189770460128784\n",
      "Train: Epoch [2], Batch [882/938], Loss: 2.223118782043457\n",
      "Train: Epoch [2], Batch [883/938], Loss: 2.187800407409668\n",
      "Train: Epoch [2], Batch [884/938], Loss: 2.2114624977111816\n",
      "Train: Epoch [2], Batch [885/938], Loss: 2.2136194705963135\n",
      "Train: Epoch [2], Batch [886/938], Loss: 2.2037179470062256\n",
      "Train: Epoch [2], Batch [887/938], Loss: 2.2063887119293213\n",
      "Train: Epoch [2], Batch [888/938], Loss: 2.21117901802063\n",
      "Train: Epoch [2], Batch [889/938], Loss: 2.213712215423584\n",
      "Train: Epoch [2], Batch [890/938], Loss: 2.208491325378418\n",
      "Train: Epoch [2], Batch [891/938], Loss: 2.203808546066284\n",
      "Train: Epoch [2], Batch [892/938], Loss: 2.2211711406707764\n",
      "Train: Epoch [2], Batch [893/938], Loss: 2.1930994987487793\n",
      "Train: Epoch [2], Batch [894/938], Loss: 2.2255382537841797\n",
      "Train: Epoch [2], Batch [895/938], Loss: 2.2104477882385254\n",
      "Train: Epoch [2], Batch [896/938], Loss: 2.2172117233276367\n",
      "Train: Epoch [2], Batch [897/938], Loss: 2.1656341552734375\n",
      "Train: Epoch [2], Batch [898/938], Loss: 2.1821351051330566\n",
      "Train: Epoch [2], Batch [899/938], Loss: 2.196371078491211\n",
      "Train: Epoch [2], Batch [900/938], Loss: 2.22091007232666\n",
      "Train: Epoch [2], Batch [901/938], Loss: 2.22377347946167\n",
      "Train: Epoch [2], Batch [902/938], Loss: 2.219423532485962\n",
      "Train: Epoch [2], Batch [903/938], Loss: 2.2265450954437256\n",
      "Train: Epoch [2], Batch [904/938], Loss: 2.214872360229492\n",
      "Train: Epoch [2], Batch [905/938], Loss: 2.2225539684295654\n",
      "Train: Epoch [2], Batch [906/938], Loss: 2.215402841567993\n",
      "Train: Epoch [2], Batch [907/938], Loss: 2.203378677368164\n",
      "Train: Epoch [2], Batch [908/938], Loss: 2.2269763946533203\n",
      "Train: Epoch [2], Batch [909/938], Loss: 2.2270801067352295\n",
      "Train: Epoch [2], Batch [910/938], Loss: 2.199575424194336\n",
      "Train: Epoch [2], Batch [911/938], Loss: 2.229328155517578\n",
      "Train: Epoch [2], Batch [912/938], Loss: 2.2261579036712646\n",
      "Train: Epoch [2], Batch [913/938], Loss: 2.2360873222351074\n",
      "Train: Epoch [2], Batch [914/938], Loss: 2.2337005138397217\n",
      "Train: Epoch [2], Batch [915/938], Loss: 2.236406087875366\n",
      "Train: Epoch [2], Batch [916/938], Loss: 2.199965000152588\n",
      "Train: Epoch [2], Batch [917/938], Loss: 2.217742919921875\n",
      "Train: Epoch [2], Batch [918/938], Loss: 2.1972780227661133\n",
      "Train: Epoch [2], Batch [919/938], Loss: 2.2125701904296875\n",
      "Train: Epoch [2], Batch [920/938], Loss: 2.2121663093566895\n",
      "Train: Epoch [2], Batch [921/938], Loss: 2.195479154586792\n",
      "Train: Epoch [2], Batch [922/938], Loss: 2.2209885120391846\n",
      "Train: Epoch [2], Batch [923/938], Loss: 2.2354044914245605\n",
      "Train: Epoch [2], Batch [924/938], Loss: 2.207204818725586\n",
      "Train: Epoch [2], Batch [925/938], Loss: 2.187011480331421\n",
      "Train: Epoch [2], Batch [926/938], Loss: 2.197209358215332\n",
      "Train: Epoch [2], Batch [927/938], Loss: 2.192392349243164\n",
      "Train: Epoch [2], Batch [928/938], Loss: 2.1839897632598877\n",
      "Train: Epoch [2], Batch [929/938], Loss: 2.2006893157958984\n",
      "Train: Epoch [2], Batch [930/938], Loss: 2.2075161933898926\n",
      "Train: Epoch [2], Batch [931/938], Loss: 2.2038636207580566\n",
      "Train: Epoch [2], Batch [932/938], Loss: 2.2286508083343506\n",
      "Train: Epoch [2], Batch [933/938], Loss: 2.20418119430542\n",
      "Train: Epoch [2], Batch [934/938], Loss: 2.212982416152954\n",
      "Train: Epoch [2], Batch [935/938], Loss: 2.1960151195526123\n",
      "Train: Epoch [2], Batch [936/938], Loss: 2.2006449699401855\n",
      "Train: Epoch [2], Batch [937/938], Loss: 2.1747398376464844\n",
      "Train: Epoch [2], Batch [938/938], Loss: 2.202934741973877\n",
      "Accuracy of train set: 0.2758333333333333\n",
      "Validation: Epoch [2], Batch [1/938], Loss: 2.204984426498413\n",
      "Validation: Epoch [2], Batch [2/938], Loss: 2.1862986087799072\n",
      "Validation: Epoch [2], Batch [3/938], Loss: 2.203616142272949\n",
      "Validation: Epoch [2], Batch [4/938], Loss: 2.2157111167907715\n",
      "Validation: Epoch [2], Batch [5/938], Loss: 2.1878390312194824\n",
      "Validation: Epoch [2], Batch [6/938], Loss: 2.1819756031036377\n",
      "Validation: Epoch [2], Batch [7/938], Loss: 2.1737210750579834\n",
      "Validation: Epoch [2], Batch [8/938], Loss: 2.203913450241089\n",
      "Validation: Epoch [2], Batch [9/938], Loss: 2.2145485877990723\n",
      "Validation: Epoch [2], Batch [10/938], Loss: 2.200031280517578\n",
      "Validation: Epoch [2], Batch [11/938], Loss: 2.1946046352386475\n",
      "Validation: Epoch [2], Batch [12/938], Loss: 2.223935127258301\n",
      "Validation: Epoch [2], Batch [13/938], Loss: 2.195652723312378\n",
      "Validation: Epoch [2], Batch [14/938], Loss: 2.180100917816162\n",
      "Validation: Epoch [2], Batch [15/938], Loss: 2.1965529918670654\n",
      "Validation: Epoch [2], Batch [16/938], Loss: 2.206200361251831\n",
      "Validation: Epoch [2], Batch [17/938], Loss: 2.217130661010742\n",
      "Validation: Epoch [2], Batch [18/938], Loss: 2.2252066135406494\n",
      "Validation: Epoch [2], Batch [19/938], Loss: 2.2132177352905273\n",
      "Validation: Epoch [2], Batch [20/938], Loss: 2.1827633380889893\n",
      "Validation: Epoch [2], Batch [21/938], Loss: 2.200045347213745\n",
      "Validation: Epoch [2], Batch [22/938], Loss: 2.154872417449951\n",
      "Validation: Epoch [2], Batch [23/938], Loss: 2.1936140060424805\n",
      "Validation: Epoch [2], Batch [24/938], Loss: 2.202493667602539\n",
      "Validation: Epoch [2], Batch [25/938], Loss: 2.1814959049224854\n",
      "Validation: Epoch [2], Batch [26/938], Loss: 2.20510196685791\n",
      "Validation: Epoch [2], Batch [27/938], Loss: 2.212110996246338\n",
      "Validation: Epoch [2], Batch [28/938], Loss: 2.1811041831970215\n",
      "Validation: Epoch [2], Batch [29/938], Loss: 2.2232792377471924\n",
      "Validation: Epoch [2], Batch [30/938], Loss: 2.191121816635132\n",
      "Validation: Epoch [2], Batch [31/938], Loss: 2.2023353576660156\n",
      "Validation: Epoch [2], Batch [32/938], Loss: 2.2061588764190674\n",
      "Validation: Epoch [2], Batch [33/938], Loss: 2.1850321292877197\n",
      "Validation: Epoch [2], Batch [34/938], Loss: 2.19691801071167\n",
      "Validation: Epoch [2], Batch [35/938], Loss: 2.181229829788208\n",
      "Validation: Epoch [2], Batch [36/938], Loss: 2.2168891429901123\n",
      "Validation: Epoch [2], Batch [37/938], Loss: 2.202876567840576\n",
      "Validation: Epoch [2], Batch [38/938], Loss: 2.2093589305877686\n",
      "Validation: Epoch [2], Batch [39/938], Loss: 2.1932191848754883\n",
      "Validation: Epoch [2], Batch [40/938], Loss: 2.178560733795166\n",
      "Validation: Epoch [2], Batch [41/938], Loss: 2.2045605182647705\n",
      "Validation: Epoch [2], Batch [42/938], Loss: 2.2061846256256104\n",
      "Validation: Epoch [2], Batch [43/938], Loss: 2.1990966796875\n",
      "Validation: Epoch [2], Batch [44/938], Loss: 2.1958541870117188\n",
      "Validation: Epoch [2], Batch [45/938], Loss: 2.1958770751953125\n",
      "Validation: Epoch [2], Batch [46/938], Loss: 2.2021679878234863\n",
      "Validation: Epoch [2], Batch [47/938], Loss: 2.2202329635620117\n",
      "Validation: Epoch [2], Batch [48/938], Loss: 2.206120491027832\n",
      "Validation: Epoch [2], Batch [49/938], Loss: 2.2105305194854736\n",
      "Validation: Epoch [2], Batch [50/938], Loss: 2.190131664276123\n",
      "Validation: Epoch [2], Batch [51/938], Loss: 2.2133400440216064\n",
      "Validation: Epoch [2], Batch [52/938], Loss: 2.218999147415161\n",
      "Validation: Epoch [2], Batch [53/938], Loss: 2.189413070678711\n",
      "Validation: Epoch [2], Batch [54/938], Loss: 2.191779851913452\n",
      "Validation: Epoch [2], Batch [55/938], Loss: 2.1859302520751953\n",
      "Validation: Epoch [2], Batch [56/938], Loss: 2.240471839904785\n",
      "Validation: Epoch [2], Batch [57/938], Loss: 2.2251996994018555\n",
      "Validation: Epoch [2], Batch [58/938], Loss: 2.1880810260772705\n",
      "Validation: Epoch [2], Batch [59/938], Loss: 2.19663405418396\n",
      "Validation: Epoch [2], Batch [60/938], Loss: 2.195197582244873\n",
      "Validation: Epoch [2], Batch [61/938], Loss: 2.2285139560699463\n",
      "Validation: Epoch [2], Batch [62/938], Loss: 2.205317497253418\n",
      "Validation: Epoch [2], Batch [63/938], Loss: 2.2118868827819824\n",
      "Validation: Epoch [2], Batch [64/938], Loss: 2.2047038078308105\n",
      "Validation: Epoch [2], Batch [65/938], Loss: 2.210419178009033\n",
      "Validation: Epoch [2], Batch [66/938], Loss: 2.214487314224243\n",
      "Validation: Epoch [2], Batch [67/938], Loss: 2.2193198204040527\n",
      "Validation: Epoch [2], Batch [68/938], Loss: 2.188997745513916\n",
      "Validation: Epoch [2], Batch [69/938], Loss: 2.2018089294433594\n",
      "Validation: Epoch [2], Batch [70/938], Loss: 2.187736988067627\n",
      "Validation: Epoch [2], Batch [71/938], Loss: 2.2225756645202637\n",
      "Validation: Epoch [2], Batch [72/938], Loss: 2.226180076599121\n",
      "Validation: Epoch [2], Batch [73/938], Loss: 2.188896894454956\n",
      "Validation: Epoch [2], Batch [74/938], Loss: 2.23124361038208\n",
      "Validation: Epoch [2], Batch [75/938], Loss: 2.216703414916992\n",
      "Validation: Epoch [2], Batch [76/938], Loss: 2.194279193878174\n",
      "Validation: Epoch [2], Batch [77/938], Loss: 2.2088565826416016\n",
      "Validation: Epoch [2], Batch [78/938], Loss: 2.193464756011963\n",
      "Validation: Epoch [2], Batch [79/938], Loss: 2.201369047164917\n",
      "Validation: Epoch [2], Batch [80/938], Loss: 2.2000956535339355\n",
      "Validation: Epoch [2], Batch [81/938], Loss: 2.235746383666992\n",
      "Validation: Epoch [2], Batch [82/938], Loss: 2.1937150955200195\n",
      "Validation: Epoch [2], Batch [83/938], Loss: 2.1743271350860596\n",
      "Validation: Epoch [2], Batch [84/938], Loss: 2.186854839324951\n",
      "Validation: Epoch [2], Batch [85/938], Loss: 2.229552984237671\n",
      "Validation: Epoch [2], Batch [86/938], Loss: 2.18677020072937\n",
      "Validation: Epoch [2], Batch [87/938], Loss: 2.208425521850586\n",
      "Validation: Epoch [2], Batch [88/938], Loss: 2.172215461730957\n",
      "Validation: Epoch [2], Batch [89/938], Loss: 2.2008748054504395\n",
      "Validation: Epoch [2], Batch [90/938], Loss: 2.1962249279022217\n",
      "Validation: Epoch [2], Batch [91/938], Loss: 2.1955623626708984\n",
      "Validation: Epoch [2], Batch [92/938], Loss: 2.1997733116149902\n",
      "Validation: Epoch [2], Batch [93/938], Loss: 2.2043285369873047\n",
      "Validation: Epoch [2], Batch [94/938], Loss: 2.1816952228546143\n",
      "Validation: Epoch [2], Batch [95/938], Loss: 2.1991257667541504\n",
      "Validation: Epoch [2], Batch [96/938], Loss: 2.21415638923645\n",
      "Validation: Epoch [2], Batch [97/938], Loss: 2.1914896965026855\n",
      "Validation: Epoch [2], Batch [98/938], Loss: 2.2013094425201416\n",
      "Validation: Epoch [2], Batch [99/938], Loss: 2.192615032196045\n",
      "Validation: Epoch [2], Batch [100/938], Loss: 2.1936841011047363\n",
      "Validation: Epoch [2], Batch [101/938], Loss: 2.202773094177246\n",
      "Validation: Epoch [2], Batch [102/938], Loss: 2.1869583129882812\n",
      "Validation: Epoch [2], Batch [103/938], Loss: 2.1815738677978516\n",
      "Validation: Epoch [2], Batch [104/938], Loss: 2.1941747665405273\n",
      "Validation: Epoch [2], Batch [105/938], Loss: 2.194603681564331\n",
      "Validation: Epoch [2], Batch [106/938], Loss: 2.188854694366455\n",
      "Validation: Epoch [2], Batch [107/938], Loss: 2.1919920444488525\n",
      "Validation: Epoch [2], Batch [108/938], Loss: 2.206737518310547\n",
      "Validation: Epoch [2], Batch [109/938], Loss: 2.2069458961486816\n",
      "Validation: Epoch [2], Batch [110/938], Loss: 2.2201263904571533\n",
      "Validation: Epoch [2], Batch [111/938], Loss: 2.204427719116211\n",
      "Validation: Epoch [2], Batch [112/938], Loss: 2.1869804859161377\n",
      "Validation: Epoch [2], Batch [113/938], Loss: 2.207778215408325\n",
      "Validation: Epoch [2], Batch [114/938], Loss: 2.195553779602051\n",
      "Validation: Epoch [2], Batch [115/938], Loss: 2.2054054737091064\n",
      "Validation: Epoch [2], Batch [116/938], Loss: 2.1952638626098633\n",
      "Validation: Epoch [2], Batch [117/938], Loss: 2.205883502960205\n",
      "Validation: Epoch [2], Batch [118/938], Loss: 2.175546169281006\n",
      "Validation: Epoch [2], Batch [119/938], Loss: 2.2102532386779785\n",
      "Validation: Epoch [2], Batch [120/938], Loss: 2.186478853225708\n",
      "Validation: Epoch [2], Batch [121/938], Loss: 2.191805124282837\n",
      "Validation: Epoch [2], Batch [122/938], Loss: 2.199279308319092\n",
      "Validation: Epoch [2], Batch [123/938], Loss: 2.2026078701019287\n",
      "Validation: Epoch [2], Batch [124/938], Loss: 2.188976764678955\n",
      "Validation: Epoch [2], Batch [125/938], Loss: 2.18332576751709\n",
      "Validation: Epoch [2], Batch [126/938], Loss: 2.183842658996582\n",
      "Validation: Epoch [2], Batch [127/938], Loss: 2.1798794269561768\n",
      "Validation: Epoch [2], Batch [128/938], Loss: 2.1899845600128174\n",
      "Validation: Epoch [2], Batch [129/938], Loss: 2.2221622467041016\n",
      "Validation: Epoch [2], Batch [130/938], Loss: 2.2097887992858887\n",
      "Validation: Epoch [2], Batch [131/938], Loss: 2.196187973022461\n",
      "Validation: Epoch [2], Batch [132/938], Loss: 2.206584930419922\n",
      "Validation: Epoch [2], Batch [133/938], Loss: 2.2131423950195312\n",
      "Validation: Epoch [2], Batch [134/938], Loss: 2.184084892272949\n",
      "Validation: Epoch [2], Batch [135/938], Loss: 2.2134270668029785\n",
      "Validation: Epoch [2], Batch [136/938], Loss: 2.184269428253174\n",
      "Validation: Epoch [2], Batch [137/938], Loss: 2.1926541328430176\n",
      "Validation: Epoch [2], Batch [138/938], Loss: 2.2048935890197754\n",
      "Validation: Epoch [2], Batch [139/938], Loss: 2.1932637691497803\n",
      "Validation: Epoch [2], Batch [140/938], Loss: 2.1828160285949707\n",
      "Validation: Epoch [2], Batch [141/938], Loss: 2.195338487625122\n",
      "Validation: Epoch [2], Batch [142/938], Loss: 2.209752321243286\n",
      "Validation: Epoch [2], Batch [143/938], Loss: 2.1977248191833496\n",
      "Validation: Epoch [2], Batch [144/938], Loss: 2.2008445262908936\n",
      "Validation: Epoch [2], Batch [145/938], Loss: 2.2099051475524902\n",
      "Validation: Epoch [2], Batch [146/938], Loss: 2.1851859092712402\n",
      "Validation: Epoch [2], Batch [147/938], Loss: 2.205857038497925\n",
      "Validation: Epoch [2], Batch [148/938], Loss: 2.2025058269500732\n",
      "Validation: Epoch [2], Batch [149/938], Loss: 2.190603256225586\n",
      "Validation: Epoch [2], Batch [150/938], Loss: 2.2116363048553467\n",
      "Validation: Epoch [2], Batch [151/938], Loss: 2.2054851055145264\n",
      "Validation: Epoch [2], Batch [152/938], Loss: 2.2225587368011475\n",
      "Validation: Epoch [2], Batch [153/938], Loss: 2.2219200134277344\n",
      "Validation: Epoch [2], Batch [154/938], Loss: 2.201807737350464\n",
      "Validation: Epoch [2], Batch [155/938], Loss: 2.2169981002807617\n",
      "Validation: Epoch [2], Batch [156/938], Loss: 2.210463762283325\n",
      "Validation: Epoch [2], Batch [157/938], Loss: 2.177561044692993\n",
      "Validation: Epoch [2], Batch [158/938], Loss: 2.193617343902588\n",
      "Validation: Epoch [2], Batch [159/938], Loss: 2.1847593784332275\n",
      "Validation: Epoch [2], Batch [160/938], Loss: 2.1887404918670654\n",
      "Validation: Epoch [2], Batch [161/938], Loss: 2.166154146194458\n",
      "Validation: Epoch [2], Batch [162/938], Loss: 2.2096121311187744\n",
      "Validation: Epoch [2], Batch [163/938], Loss: 2.2016196250915527\n",
      "Validation: Epoch [2], Batch [164/938], Loss: 2.198204278945923\n",
      "Validation: Epoch [2], Batch [165/938], Loss: 2.18497371673584\n",
      "Validation: Epoch [2], Batch [166/938], Loss: 2.19143009185791\n",
      "Validation: Epoch [2], Batch [167/938], Loss: 2.1935598850250244\n",
      "Validation: Epoch [2], Batch [168/938], Loss: 2.186823844909668\n",
      "Validation: Epoch [2], Batch [169/938], Loss: 2.184173107147217\n",
      "Validation: Epoch [2], Batch [170/938], Loss: 2.2127692699432373\n",
      "Validation: Epoch [2], Batch [171/938], Loss: 2.1748175621032715\n",
      "Validation: Epoch [2], Batch [172/938], Loss: 2.2091751098632812\n",
      "Validation: Epoch [2], Batch [173/938], Loss: 2.2132697105407715\n",
      "Validation: Epoch [2], Batch [174/938], Loss: 2.2048287391662598\n",
      "Validation: Epoch [2], Batch [175/938], Loss: 2.2037196159362793\n",
      "Validation: Epoch [2], Batch [176/938], Loss: 2.208617925643921\n",
      "Validation: Epoch [2], Batch [177/938], Loss: 2.202523946762085\n",
      "Validation: Epoch [2], Batch [178/938], Loss: 2.181018829345703\n",
      "Validation: Epoch [2], Batch [179/938], Loss: 2.1927666664123535\n",
      "Validation: Epoch [2], Batch [180/938], Loss: 2.2189290523529053\n",
      "Validation: Epoch [2], Batch [181/938], Loss: 2.1994638442993164\n",
      "Validation: Epoch [2], Batch [182/938], Loss: 2.183337688446045\n",
      "Validation: Epoch [2], Batch [183/938], Loss: 2.206078052520752\n",
      "Validation: Epoch [2], Batch [184/938], Loss: 2.222154140472412\n",
      "Validation: Epoch [2], Batch [185/938], Loss: 2.1795856952667236\n",
      "Validation: Epoch [2], Batch [186/938], Loss: 2.2287230491638184\n",
      "Validation: Epoch [2], Batch [187/938], Loss: 2.2138147354125977\n",
      "Validation: Epoch [2], Batch [188/938], Loss: 2.192599296569824\n",
      "Validation: Epoch [2], Batch [189/938], Loss: 2.1923768520355225\n",
      "Validation: Epoch [2], Batch [190/938], Loss: 2.1760342121124268\n",
      "Validation: Epoch [2], Batch [191/938], Loss: 2.1826696395874023\n",
      "Validation: Epoch [2], Batch [192/938], Loss: 2.213770627975464\n",
      "Validation: Epoch [2], Batch [193/938], Loss: 2.203686475753784\n",
      "Validation: Epoch [2], Batch [194/938], Loss: 2.1963276863098145\n",
      "Validation: Epoch [2], Batch [195/938], Loss: 2.184880256652832\n",
      "Validation: Epoch [2], Batch [196/938], Loss: 2.1819372177124023\n",
      "Validation: Epoch [2], Batch [197/938], Loss: 2.1907293796539307\n",
      "Validation: Epoch [2], Batch [198/938], Loss: 2.214437246322632\n",
      "Validation: Epoch [2], Batch [199/938], Loss: 2.1740176677703857\n",
      "Validation: Epoch [2], Batch [200/938], Loss: 2.2018845081329346\n",
      "Validation: Epoch [2], Batch [201/938], Loss: 2.2027721405029297\n",
      "Validation: Epoch [2], Batch [202/938], Loss: 2.2093210220336914\n",
      "Validation: Epoch [2], Batch [203/938], Loss: 2.2099483013153076\n",
      "Validation: Epoch [2], Batch [204/938], Loss: 2.179987668991089\n",
      "Validation: Epoch [2], Batch [205/938], Loss: 2.203653335571289\n",
      "Validation: Epoch [2], Batch [206/938], Loss: 2.229200601577759\n",
      "Validation: Epoch [2], Batch [207/938], Loss: 2.2021450996398926\n",
      "Validation: Epoch [2], Batch [208/938], Loss: 2.1913928985595703\n",
      "Validation: Epoch [2], Batch [209/938], Loss: 2.1882665157318115\n",
      "Validation: Epoch [2], Batch [210/938], Loss: 2.22166109085083\n",
      "Validation: Epoch [2], Batch [211/938], Loss: 2.1921873092651367\n",
      "Validation: Epoch [2], Batch [212/938], Loss: 2.2044901847839355\n",
      "Validation: Epoch [2], Batch [213/938], Loss: 2.2059335708618164\n",
      "Validation: Epoch [2], Batch [214/938], Loss: 2.216705083847046\n",
      "Validation: Epoch [2], Batch [215/938], Loss: 2.2043111324310303\n",
      "Validation: Epoch [2], Batch [216/938], Loss: 2.209214448928833\n",
      "Validation: Epoch [2], Batch [217/938], Loss: 2.2406105995178223\n",
      "Validation: Epoch [2], Batch [218/938], Loss: 2.2218177318573\n",
      "Validation: Epoch [2], Batch [219/938], Loss: 2.1999332904815674\n",
      "Validation: Epoch [2], Batch [220/938], Loss: 2.2077760696411133\n",
      "Validation: Epoch [2], Batch [221/938], Loss: 2.2049949169158936\n",
      "Validation: Epoch [2], Batch [222/938], Loss: 2.1924726963043213\n",
      "Validation: Epoch [2], Batch [223/938], Loss: 2.199955701828003\n",
      "Validation: Epoch [2], Batch [224/938], Loss: 2.2121918201446533\n",
      "Validation: Epoch [2], Batch [225/938], Loss: 2.2019474506378174\n",
      "Validation: Epoch [2], Batch [226/938], Loss: 2.2143821716308594\n",
      "Validation: Epoch [2], Batch [227/938], Loss: 2.2069990634918213\n",
      "Validation: Epoch [2], Batch [228/938], Loss: 2.19720458984375\n",
      "Validation: Epoch [2], Batch [229/938], Loss: 2.187019109725952\n",
      "Validation: Epoch [2], Batch [230/938], Loss: 2.196810722351074\n",
      "Validation: Epoch [2], Batch [231/938], Loss: 2.17920184135437\n",
      "Validation: Epoch [2], Batch [232/938], Loss: 2.209146499633789\n",
      "Validation: Epoch [2], Batch [233/938], Loss: 2.1973257064819336\n",
      "Validation: Epoch [2], Batch [234/938], Loss: 2.211219310760498\n",
      "Validation: Epoch [2], Batch [235/938], Loss: 2.2079193592071533\n",
      "Validation: Epoch [2], Batch [236/938], Loss: 2.1966912746429443\n",
      "Validation: Epoch [2], Batch [237/938], Loss: 2.201262950897217\n",
      "Validation: Epoch [2], Batch [238/938], Loss: 2.2156920433044434\n",
      "Validation: Epoch [2], Batch [239/938], Loss: 2.2138447761535645\n",
      "Validation: Epoch [2], Batch [240/938], Loss: 2.211954116821289\n",
      "Validation: Epoch [2], Batch [241/938], Loss: 2.2123067378997803\n",
      "Validation: Epoch [2], Batch [242/938], Loss: 2.2008299827575684\n",
      "Validation: Epoch [2], Batch [243/938], Loss: 2.1966042518615723\n",
      "Validation: Epoch [2], Batch [244/938], Loss: 2.213158130645752\n",
      "Validation: Epoch [2], Batch [245/938], Loss: 2.197810649871826\n",
      "Validation: Epoch [2], Batch [246/938], Loss: 2.193740129470825\n",
      "Validation: Epoch [2], Batch [247/938], Loss: 2.2043213844299316\n",
      "Validation: Epoch [2], Batch [248/938], Loss: 2.207415819168091\n",
      "Validation: Epoch [2], Batch [249/938], Loss: 2.175996780395508\n",
      "Validation: Epoch [2], Batch [250/938], Loss: 2.191046953201294\n",
      "Validation: Epoch [2], Batch [251/938], Loss: 2.2094063758850098\n",
      "Validation: Epoch [2], Batch [252/938], Loss: 2.1860947608947754\n",
      "Validation: Epoch [2], Batch [253/938], Loss: 2.2009153366088867\n",
      "Validation: Epoch [2], Batch [254/938], Loss: 2.2146754264831543\n",
      "Validation: Epoch [2], Batch [255/938], Loss: 2.214733123779297\n",
      "Validation: Epoch [2], Batch [256/938], Loss: 2.219761848449707\n",
      "Validation: Epoch [2], Batch [257/938], Loss: 2.217890739440918\n",
      "Validation: Epoch [2], Batch [258/938], Loss: 2.1743311882019043\n",
      "Validation: Epoch [2], Batch [259/938], Loss: 2.2297134399414062\n",
      "Validation: Epoch [2], Batch [260/938], Loss: 2.1870508193969727\n",
      "Validation: Epoch [2], Batch [261/938], Loss: 2.1595866680145264\n",
      "Validation: Epoch [2], Batch [262/938], Loss: 2.1974847316741943\n",
      "Validation: Epoch [2], Batch [263/938], Loss: 2.1669514179229736\n",
      "Validation: Epoch [2], Batch [264/938], Loss: 2.201385498046875\n",
      "Validation: Epoch [2], Batch [265/938], Loss: 2.1965651512145996\n",
      "Validation: Epoch [2], Batch [266/938], Loss: 2.1927404403686523\n",
      "Validation: Epoch [2], Batch [267/938], Loss: 2.216440439224243\n",
      "Validation: Epoch [2], Batch [268/938], Loss: 2.204238176345825\n",
      "Validation: Epoch [2], Batch [269/938], Loss: 2.2020418643951416\n",
      "Validation: Epoch [2], Batch [270/938], Loss: 2.196988105773926\n",
      "Validation: Epoch [2], Batch [271/938], Loss: 2.2220988273620605\n",
      "Validation: Epoch [2], Batch [272/938], Loss: 2.197788953781128\n",
      "Validation: Epoch [2], Batch [273/938], Loss: 2.2126126289367676\n",
      "Validation: Epoch [2], Batch [274/938], Loss: 2.1994376182556152\n",
      "Validation: Epoch [2], Batch [275/938], Loss: 2.2238049507141113\n",
      "Validation: Epoch [2], Batch [276/938], Loss: 2.196115493774414\n",
      "Validation: Epoch [2], Batch [277/938], Loss: 2.1990950107574463\n",
      "Validation: Epoch [2], Batch [278/938], Loss: 2.1826202869415283\n",
      "Validation: Epoch [2], Batch [279/938], Loss: 2.20338773727417\n",
      "Validation: Epoch [2], Batch [280/938], Loss: 2.1920745372772217\n",
      "Validation: Epoch [2], Batch [281/938], Loss: 2.213052749633789\n",
      "Validation: Epoch [2], Batch [282/938], Loss: 2.211747407913208\n",
      "Validation: Epoch [2], Batch [283/938], Loss: 2.212476968765259\n",
      "Validation: Epoch [2], Batch [284/938], Loss: 2.185671329498291\n",
      "Validation: Epoch [2], Batch [285/938], Loss: 2.220163345336914\n",
      "Validation: Epoch [2], Batch [286/938], Loss: 2.1907010078430176\n",
      "Validation: Epoch [2], Batch [287/938], Loss: 2.2000670433044434\n",
      "Validation: Epoch [2], Batch [288/938], Loss: 2.2107434272766113\n",
      "Validation: Epoch [2], Batch [289/938], Loss: 2.184971570968628\n",
      "Validation: Epoch [2], Batch [290/938], Loss: 2.2353882789611816\n",
      "Validation: Epoch [2], Batch [291/938], Loss: 2.2079334259033203\n",
      "Validation: Epoch [2], Batch [292/938], Loss: 2.187849521636963\n",
      "Validation: Epoch [2], Batch [293/938], Loss: 2.203141927719116\n",
      "Validation: Epoch [2], Batch [294/938], Loss: 2.1902832984924316\n",
      "Validation: Epoch [2], Batch [295/938], Loss: 2.20745849609375\n",
      "Validation: Epoch [2], Batch [296/938], Loss: 2.232123851776123\n",
      "Validation: Epoch [2], Batch [297/938], Loss: 2.184328556060791\n",
      "Validation: Epoch [2], Batch [298/938], Loss: 2.174553871154785\n",
      "Validation: Epoch [2], Batch [299/938], Loss: 2.2286500930786133\n",
      "Validation: Epoch [2], Batch [300/938], Loss: 2.1797573566436768\n",
      "Validation: Epoch [2], Batch [301/938], Loss: 2.165705919265747\n",
      "Validation: Epoch [2], Batch [302/938], Loss: 2.2426862716674805\n",
      "Validation: Epoch [2], Batch [303/938], Loss: 2.1925923824310303\n",
      "Validation: Epoch [2], Batch [304/938], Loss: 2.189586877822876\n",
      "Validation: Epoch [2], Batch [305/938], Loss: 2.1933791637420654\n",
      "Validation: Epoch [2], Batch [306/938], Loss: 2.2120120525360107\n",
      "Validation: Epoch [2], Batch [307/938], Loss: 2.190070390701294\n",
      "Validation: Epoch [2], Batch [308/938], Loss: 2.170875072479248\n",
      "Validation: Epoch [2], Batch [309/938], Loss: 2.180325984954834\n",
      "Validation: Epoch [2], Batch [310/938], Loss: 2.1881821155548096\n",
      "Validation: Epoch [2], Batch [311/938], Loss: 2.20365834236145\n",
      "Validation: Epoch [2], Batch [312/938], Loss: 2.1869125366210938\n",
      "Validation: Epoch [2], Batch [313/938], Loss: 2.166978359222412\n",
      "Validation: Epoch [2], Batch [314/938], Loss: 2.1850786209106445\n",
      "Validation: Epoch [2], Batch [315/938], Loss: 2.2199478149414062\n",
      "Validation: Epoch [2], Batch [316/938], Loss: 2.194275379180908\n",
      "Validation: Epoch [2], Batch [317/938], Loss: 2.1934008598327637\n",
      "Validation: Epoch [2], Batch [318/938], Loss: 2.203624725341797\n",
      "Validation: Epoch [2], Batch [319/938], Loss: 2.1951112747192383\n",
      "Validation: Epoch [2], Batch [320/938], Loss: 2.2218990325927734\n",
      "Validation: Epoch [2], Batch [321/938], Loss: 2.180715322494507\n",
      "Validation: Epoch [2], Batch [322/938], Loss: 2.180739164352417\n",
      "Validation: Epoch [2], Batch [323/938], Loss: 2.210836887359619\n",
      "Validation: Epoch [2], Batch [324/938], Loss: 2.211573600769043\n",
      "Validation: Epoch [2], Batch [325/938], Loss: 2.224454164505005\n",
      "Validation: Epoch [2], Batch [326/938], Loss: 2.2181315422058105\n",
      "Validation: Epoch [2], Batch [327/938], Loss: 2.1961755752563477\n",
      "Validation: Epoch [2], Batch [328/938], Loss: 2.1902856826782227\n",
      "Validation: Epoch [2], Batch [329/938], Loss: 2.169674873352051\n",
      "Validation: Epoch [2], Batch [330/938], Loss: 2.1938939094543457\n",
      "Validation: Epoch [2], Batch [331/938], Loss: 2.187286615371704\n",
      "Validation: Epoch [2], Batch [332/938], Loss: 2.1936097145080566\n",
      "Validation: Epoch [2], Batch [333/938], Loss: 2.2227108478546143\n",
      "Validation: Epoch [2], Batch [334/938], Loss: 2.1952381134033203\n",
      "Validation: Epoch [2], Batch [335/938], Loss: 2.215752124786377\n",
      "Validation: Epoch [2], Batch [336/938], Loss: 2.211761236190796\n",
      "Validation: Epoch [2], Batch [337/938], Loss: 2.2029032707214355\n",
      "Validation: Epoch [2], Batch [338/938], Loss: 2.2192788124084473\n",
      "Validation: Epoch [2], Batch [339/938], Loss: 2.223172426223755\n",
      "Validation: Epoch [2], Batch [340/938], Loss: 2.207425594329834\n",
      "Validation: Epoch [2], Batch [341/938], Loss: 2.175065279006958\n",
      "Validation: Epoch [2], Batch [342/938], Loss: 2.1938724517822266\n",
      "Validation: Epoch [2], Batch [343/938], Loss: 2.1717967987060547\n",
      "Validation: Epoch [2], Batch [344/938], Loss: 2.2299866676330566\n",
      "Validation: Epoch [2], Batch [345/938], Loss: 2.1854212284088135\n",
      "Validation: Epoch [2], Batch [346/938], Loss: 2.2007040977478027\n",
      "Validation: Epoch [2], Batch [347/938], Loss: 2.2003417015075684\n",
      "Validation: Epoch [2], Batch [348/938], Loss: 2.210768461227417\n",
      "Validation: Epoch [2], Batch [349/938], Loss: 2.213948965072632\n",
      "Validation: Epoch [2], Batch [350/938], Loss: 2.2080140113830566\n",
      "Validation: Epoch [2], Batch [351/938], Loss: 2.2029879093170166\n",
      "Validation: Epoch [2], Batch [352/938], Loss: 2.1893277168273926\n",
      "Validation: Epoch [2], Batch [353/938], Loss: 2.2158422470092773\n",
      "Validation: Epoch [2], Batch [354/938], Loss: 2.1891367435455322\n",
      "Validation: Epoch [2], Batch [355/938], Loss: 2.20377254486084\n",
      "Validation: Epoch [2], Batch [356/938], Loss: 2.2320940494537354\n",
      "Validation: Epoch [2], Batch [357/938], Loss: 2.2026114463806152\n",
      "Validation: Epoch [2], Batch [358/938], Loss: 2.212625026702881\n",
      "Validation: Epoch [2], Batch [359/938], Loss: 2.1832058429718018\n",
      "Validation: Epoch [2], Batch [360/938], Loss: 2.20939564704895\n",
      "Validation: Epoch [2], Batch [361/938], Loss: 2.2123401165008545\n",
      "Validation: Epoch [2], Batch [362/938], Loss: 2.2097396850585938\n",
      "Validation: Epoch [2], Batch [363/938], Loss: 2.206552505493164\n",
      "Validation: Epoch [2], Batch [364/938], Loss: 2.2390217781066895\n",
      "Validation: Epoch [2], Batch [365/938], Loss: 2.20851731300354\n",
      "Validation: Epoch [2], Batch [366/938], Loss: 2.197157621383667\n",
      "Validation: Epoch [2], Batch [367/938], Loss: 2.2224478721618652\n",
      "Validation: Epoch [2], Batch [368/938], Loss: 2.193387031555176\n",
      "Validation: Epoch [2], Batch [369/938], Loss: 2.18172025680542\n",
      "Validation: Epoch [2], Batch [370/938], Loss: 2.213773727416992\n",
      "Validation: Epoch [2], Batch [371/938], Loss: 2.189126491546631\n",
      "Validation: Epoch [2], Batch [372/938], Loss: 2.1927483081817627\n",
      "Validation: Epoch [2], Batch [373/938], Loss: 2.228764533996582\n",
      "Validation: Epoch [2], Batch [374/938], Loss: 2.1863577365875244\n",
      "Validation: Epoch [2], Batch [375/938], Loss: 2.2046239376068115\n",
      "Validation: Epoch [2], Batch [376/938], Loss: 2.1869828701019287\n",
      "Validation: Epoch [2], Batch [377/938], Loss: 2.202721357345581\n",
      "Validation: Epoch [2], Batch [378/938], Loss: 2.207563877105713\n",
      "Validation: Epoch [2], Batch [379/938], Loss: 2.200343132019043\n",
      "Validation: Epoch [2], Batch [380/938], Loss: 2.225520133972168\n",
      "Validation: Epoch [2], Batch [381/938], Loss: 2.217081069946289\n",
      "Validation: Epoch [2], Batch [382/938], Loss: 2.1987805366516113\n",
      "Validation: Epoch [2], Batch [383/938], Loss: 2.2041852474212646\n",
      "Validation: Epoch [2], Batch [384/938], Loss: 2.1892130374908447\n",
      "Validation: Epoch [2], Batch [385/938], Loss: 2.172741174697876\n",
      "Validation: Epoch [2], Batch [386/938], Loss: 2.1991524696350098\n",
      "Validation: Epoch [2], Batch [387/938], Loss: 2.184732675552368\n",
      "Validation: Epoch [2], Batch [388/938], Loss: 2.2075912952423096\n",
      "Validation: Epoch [2], Batch [389/938], Loss: 2.1940255165100098\n",
      "Validation: Epoch [2], Batch [390/938], Loss: 2.1768312454223633\n",
      "Validation: Epoch [2], Batch [391/938], Loss: 2.1860897541046143\n",
      "Validation: Epoch [2], Batch [392/938], Loss: 2.2111079692840576\n",
      "Validation: Epoch [2], Batch [393/938], Loss: 2.210585355758667\n",
      "Validation: Epoch [2], Batch [394/938], Loss: 2.1737821102142334\n",
      "Validation: Epoch [2], Batch [395/938], Loss: 2.2014944553375244\n",
      "Validation: Epoch [2], Batch [396/938], Loss: 2.2100026607513428\n",
      "Validation: Epoch [2], Batch [397/938], Loss: 2.1907968521118164\n",
      "Validation: Epoch [2], Batch [398/938], Loss: 2.191469192504883\n",
      "Validation: Epoch [2], Batch [399/938], Loss: 2.208332061767578\n",
      "Validation: Epoch [2], Batch [400/938], Loss: 2.1854665279388428\n",
      "Validation: Epoch [2], Batch [401/938], Loss: 2.189444065093994\n",
      "Validation: Epoch [2], Batch [402/938], Loss: 2.1881351470947266\n",
      "Validation: Epoch [2], Batch [403/938], Loss: 2.1801013946533203\n",
      "Validation: Epoch [2], Batch [404/938], Loss: 2.2027249336242676\n",
      "Validation: Epoch [2], Batch [405/938], Loss: 2.206411838531494\n",
      "Validation: Epoch [2], Batch [406/938], Loss: 2.2253870964050293\n",
      "Validation: Epoch [2], Batch [407/938], Loss: 2.1664981842041016\n",
      "Validation: Epoch [2], Batch [408/938], Loss: 2.1784603595733643\n",
      "Validation: Epoch [2], Batch [409/938], Loss: 2.2154054641723633\n",
      "Validation: Epoch [2], Batch [410/938], Loss: 2.1996850967407227\n",
      "Validation: Epoch [2], Batch [411/938], Loss: 2.196699380874634\n",
      "Validation: Epoch [2], Batch [412/938], Loss: 2.191770315170288\n",
      "Validation: Epoch [2], Batch [413/938], Loss: 2.227658987045288\n",
      "Validation: Epoch [2], Batch [414/938], Loss: 2.2283742427825928\n",
      "Validation: Epoch [2], Batch [415/938], Loss: 2.2003281116485596\n",
      "Validation: Epoch [2], Batch [416/938], Loss: 2.195772647857666\n",
      "Validation: Epoch [2], Batch [417/938], Loss: 2.2048888206481934\n",
      "Validation: Epoch [2], Batch [418/938], Loss: 2.215054988861084\n",
      "Validation: Epoch [2], Batch [419/938], Loss: 2.200573444366455\n",
      "Validation: Epoch [2], Batch [420/938], Loss: 2.2317612171173096\n",
      "Validation: Epoch [2], Batch [421/938], Loss: 2.227304458618164\n",
      "Validation: Epoch [2], Batch [422/938], Loss: 2.1974070072174072\n",
      "Validation: Epoch [2], Batch [423/938], Loss: 2.1998162269592285\n",
      "Validation: Epoch [2], Batch [424/938], Loss: 2.2424633502960205\n",
      "Validation: Epoch [2], Batch [425/938], Loss: 2.207669496536255\n",
      "Validation: Epoch [2], Batch [426/938], Loss: 2.181455135345459\n",
      "Validation: Epoch [2], Batch [427/938], Loss: 2.205733060836792\n",
      "Validation: Epoch [2], Batch [428/938], Loss: 2.1896917819976807\n",
      "Validation: Epoch [2], Batch [429/938], Loss: 2.2255570888519287\n",
      "Validation: Epoch [2], Batch [430/938], Loss: 2.231204032897949\n",
      "Validation: Epoch [2], Batch [431/938], Loss: 2.2375125885009766\n",
      "Validation: Epoch [2], Batch [432/938], Loss: 2.2113513946533203\n",
      "Validation: Epoch [2], Batch [433/938], Loss: 2.2005038261413574\n",
      "Validation: Epoch [2], Batch [434/938], Loss: 2.200270891189575\n",
      "Validation: Epoch [2], Batch [435/938], Loss: 2.1969761848449707\n",
      "Validation: Epoch [2], Batch [436/938], Loss: 2.190654993057251\n",
      "Validation: Epoch [2], Batch [437/938], Loss: 2.199535608291626\n",
      "Validation: Epoch [2], Batch [438/938], Loss: 2.195406198501587\n",
      "Validation: Epoch [2], Batch [439/938], Loss: 2.1955010890960693\n",
      "Validation: Epoch [2], Batch [440/938], Loss: 2.2090766429901123\n",
      "Validation: Epoch [2], Batch [441/938], Loss: 2.2041943073272705\n",
      "Validation: Epoch [2], Batch [442/938], Loss: 2.2071635723114014\n",
      "Validation: Epoch [2], Batch [443/938], Loss: 2.1967501640319824\n",
      "Validation: Epoch [2], Batch [444/938], Loss: 2.198300361633301\n",
      "Validation: Epoch [2], Batch [445/938], Loss: 2.246260404586792\n",
      "Validation: Epoch [2], Batch [446/938], Loss: 2.207435131072998\n",
      "Validation: Epoch [2], Batch [447/938], Loss: 2.208052158355713\n",
      "Validation: Epoch [2], Batch [448/938], Loss: 2.2096166610717773\n",
      "Validation: Epoch [2], Batch [449/938], Loss: 2.206206798553467\n",
      "Validation: Epoch [2], Batch [450/938], Loss: 2.2222814559936523\n",
      "Validation: Epoch [2], Batch [451/938], Loss: 2.198277473449707\n",
      "Validation: Epoch [2], Batch [452/938], Loss: 2.2034943103790283\n",
      "Validation: Epoch [2], Batch [453/938], Loss: 2.1897292137145996\n",
      "Validation: Epoch [2], Batch [454/938], Loss: 2.206026554107666\n",
      "Validation: Epoch [2], Batch [455/938], Loss: 2.1806275844573975\n",
      "Validation: Epoch [2], Batch [456/938], Loss: 2.179356336593628\n",
      "Validation: Epoch [2], Batch [457/938], Loss: 2.17020845413208\n",
      "Validation: Epoch [2], Batch [458/938], Loss: 2.206327438354492\n",
      "Validation: Epoch [2], Batch [459/938], Loss: 2.1857056617736816\n",
      "Validation: Epoch [2], Batch [460/938], Loss: 2.2047271728515625\n",
      "Validation: Epoch [2], Batch [461/938], Loss: 2.2249412536621094\n",
      "Validation: Epoch [2], Batch [462/938], Loss: 2.1804189682006836\n",
      "Validation: Epoch [2], Batch [463/938], Loss: 2.1861355304718018\n",
      "Validation: Epoch [2], Batch [464/938], Loss: 2.2209479808807373\n",
      "Validation: Epoch [2], Batch [465/938], Loss: 2.2076613903045654\n",
      "Validation: Epoch [2], Batch [466/938], Loss: 2.1993227005004883\n",
      "Validation: Epoch [2], Batch [467/938], Loss: 2.205982208251953\n",
      "Validation: Epoch [2], Batch [468/938], Loss: 2.1835036277770996\n",
      "Validation: Epoch [2], Batch [469/938], Loss: 2.2073731422424316\n",
      "Validation: Epoch [2], Batch [470/938], Loss: 2.2124149799346924\n",
      "Validation: Epoch [2], Batch [471/938], Loss: 2.219242811203003\n",
      "Validation: Epoch [2], Batch [472/938], Loss: 2.1987736225128174\n",
      "Validation: Epoch [2], Batch [473/938], Loss: 2.1894724369049072\n",
      "Validation: Epoch [2], Batch [474/938], Loss: 2.176151990890503\n",
      "Validation: Epoch [2], Batch [475/938], Loss: 2.1927812099456787\n",
      "Validation: Epoch [2], Batch [476/938], Loss: 2.2310783863067627\n",
      "Validation: Epoch [2], Batch [477/938], Loss: 2.20833683013916\n",
      "Validation: Epoch [2], Batch [478/938], Loss: 2.177490711212158\n",
      "Validation: Epoch [2], Batch [479/938], Loss: 2.1798019409179688\n",
      "Validation: Epoch [2], Batch [480/938], Loss: 2.2287635803222656\n",
      "Validation: Epoch [2], Batch [481/938], Loss: 2.2371435165405273\n",
      "Validation: Epoch [2], Batch [482/938], Loss: 2.1934003829956055\n",
      "Validation: Epoch [2], Batch [483/938], Loss: 2.205798864364624\n",
      "Validation: Epoch [2], Batch [484/938], Loss: 2.2176766395568848\n",
      "Validation: Epoch [2], Batch [485/938], Loss: 2.2043488025665283\n",
      "Validation: Epoch [2], Batch [486/938], Loss: 2.1728928089141846\n",
      "Validation: Epoch [2], Batch [487/938], Loss: 2.1814510822296143\n",
      "Validation: Epoch [2], Batch [488/938], Loss: 2.186469554901123\n",
      "Validation: Epoch [2], Batch [489/938], Loss: 2.190856456756592\n",
      "Validation: Epoch [2], Batch [490/938], Loss: 2.200748920440674\n",
      "Validation: Epoch [2], Batch [491/938], Loss: 2.2139101028442383\n",
      "Validation: Epoch [2], Batch [492/938], Loss: 2.178018808364868\n",
      "Validation: Epoch [2], Batch [493/938], Loss: 2.1948764324188232\n",
      "Validation: Epoch [2], Batch [494/938], Loss: 2.205315113067627\n",
      "Validation: Epoch [2], Batch [495/938], Loss: 2.2330806255340576\n",
      "Validation: Epoch [2], Batch [496/938], Loss: 2.2001137733459473\n",
      "Validation: Epoch [2], Batch [497/938], Loss: 2.200044870376587\n",
      "Validation: Epoch [2], Batch [498/938], Loss: 2.201585054397583\n",
      "Validation: Epoch [2], Batch [499/938], Loss: 2.19295072555542\n",
      "Validation: Epoch [2], Batch [500/938], Loss: 2.200690269470215\n",
      "Validation: Epoch [2], Batch [501/938], Loss: 2.235659122467041\n",
      "Validation: Epoch [2], Batch [502/938], Loss: 2.2159016132354736\n",
      "Validation: Epoch [2], Batch [503/938], Loss: 2.167403221130371\n",
      "Validation: Epoch [2], Batch [504/938], Loss: 2.192777633666992\n",
      "Validation: Epoch [2], Batch [505/938], Loss: 2.1895313262939453\n",
      "Validation: Epoch [2], Batch [506/938], Loss: 2.2148330211639404\n",
      "Validation: Epoch [2], Batch [507/938], Loss: 2.1703670024871826\n",
      "Validation: Epoch [2], Batch [508/938], Loss: 2.205134153366089\n",
      "Validation: Epoch [2], Batch [509/938], Loss: 2.1865766048431396\n",
      "Validation: Epoch [2], Batch [510/938], Loss: 2.210646152496338\n",
      "Validation: Epoch [2], Batch [511/938], Loss: 2.2061824798583984\n",
      "Validation: Epoch [2], Batch [512/938], Loss: 2.2000908851623535\n",
      "Validation: Epoch [2], Batch [513/938], Loss: 2.204076051712036\n",
      "Validation: Epoch [2], Batch [514/938], Loss: 2.1899478435516357\n",
      "Validation: Epoch [2], Batch [515/938], Loss: 2.209862470626831\n",
      "Validation: Epoch [2], Batch [516/938], Loss: 2.18703556060791\n",
      "Validation: Epoch [2], Batch [517/938], Loss: 2.2204723358154297\n",
      "Validation: Epoch [2], Batch [518/938], Loss: 2.217287540435791\n",
      "Validation: Epoch [2], Batch [519/938], Loss: 2.185781955718994\n",
      "Validation: Epoch [2], Batch [520/938], Loss: 2.19962477684021\n",
      "Validation: Epoch [2], Batch [521/938], Loss: 2.2060446739196777\n",
      "Validation: Epoch [2], Batch [522/938], Loss: 2.228762626647949\n",
      "Validation: Epoch [2], Batch [523/938], Loss: 2.1742522716522217\n",
      "Validation: Epoch [2], Batch [524/938], Loss: 2.19840145111084\n",
      "Validation: Epoch [2], Batch [525/938], Loss: 2.2042651176452637\n",
      "Validation: Epoch [2], Batch [526/938], Loss: 2.221143960952759\n",
      "Validation: Epoch [2], Batch [527/938], Loss: 2.2021806240081787\n",
      "Validation: Epoch [2], Batch [528/938], Loss: 2.1974945068359375\n",
      "Validation: Epoch [2], Batch [529/938], Loss: 2.214285373687744\n",
      "Validation: Epoch [2], Batch [530/938], Loss: 2.198955774307251\n",
      "Validation: Epoch [2], Batch [531/938], Loss: 2.241472005844116\n",
      "Validation: Epoch [2], Batch [532/938], Loss: 2.212989330291748\n",
      "Validation: Epoch [2], Batch [533/938], Loss: 2.2240190505981445\n",
      "Validation: Epoch [2], Batch [534/938], Loss: 2.211027145385742\n",
      "Validation: Epoch [2], Batch [535/938], Loss: 2.2027525901794434\n",
      "Validation: Epoch [2], Batch [536/938], Loss: 2.177554130554199\n",
      "Validation: Epoch [2], Batch [537/938], Loss: 2.190186023712158\n",
      "Validation: Epoch [2], Batch [538/938], Loss: 2.2152397632598877\n",
      "Validation: Epoch [2], Batch [539/938], Loss: 2.2118639945983887\n",
      "Validation: Epoch [2], Batch [540/938], Loss: 2.181394100189209\n",
      "Validation: Epoch [2], Batch [541/938], Loss: 2.231189727783203\n",
      "Validation: Epoch [2], Batch [542/938], Loss: 2.1986491680145264\n",
      "Validation: Epoch [2], Batch [543/938], Loss: 2.1661338806152344\n",
      "Validation: Epoch [2], Batch [544/938], Loss: 2.207066535949707\n",
      "Validation: Epoch [2], Batch [545/938], Loss: 2.216378688812256\n",
      "Validation: Epoch [2], Batch [546/938], Loss: 2.1660821437835693\n",
      "Validation: Epoch [2], Batch [547/938], Loss: 2.2062830924987793\n",
      "Validation: Epoch [2], Batch [548/938], Loss: 2.2098870277404785\n",
      "Validation: Epoch [2], Batch [549/938], Loss: 2.191218137741089\n",
      "Validation: Epoch [2], Batch [550/938], Loss: 2.2353506088256836\n",
      "Validation: Epoch [2], Batch [551/938], Loss: 2.184838056564331\n",
      "Validation: Epoch [2], Batch [552/938], Loss: 2.1983742713928223\n",
      "Validation: Epoch [2], Batch [553/938], Loss: 2.189805030822754\n",
      "Validation: Epoch [2], Batch [554/938], Loss: 2.2144312858581543\n",
      "Validation: Epoch [2], Batch [555/938], Loss: 2.1925978660583496\n",
      "Validation: Epoch [2], Batch [556/938], Loss: 2.208937644958496\n",
      "Validation: Epoch [2], Batch [557/938], Loss: 2.208533525466919\n",
      "Validation: Epoch [2], Batch [558/938], Loss: 2.197326421737671\n",
      "Validation: Epoch [2], Batch [559/938], Loss: 2.2127251625061035\n",
      "Validation: Epoch [2], Batch [560/938], Loss: 2.202165126800537\n",
      "Validation: Epoch [2], Batch [561/938], Loss: 2.21036958694458\n",
      "Validation: Epoch [2], Batch [562/938], Loss: 2.2229838371276855\n",
      "Validation: Epoch [2], Batch [563/938], Loss: 2.1826789379119873\n",
      "Validation: Epoch [2], Batch [564/938], Loss: 2.2019155025482178\n",
      "Validation: Epoch [2], Batch [565/938], Loss: 2.216876983642578\n",
      "Validation: Epoch [2], Batch [566/938], Loss: 2.208949089050293\n",
      "Validation: Epoch [2], Batch [567/938], Loss: 2.207089424133301\n",
      "Validation: Epoch [2], Batch [568/938], Loss: 2.2101924419403076\n",
      "Validation: Epoch [2], Batch [569/938], Loss: 2.2029409408569336\n",
      "Validation: Epoch [2], Batch [570/938], Loss: 2.1980583667755127\n",
      "Validation: Epoch [2], Batch [571/938], Loss: 2.19704008102417\n",
      "Validation: Epoch [2], Batch [572/938], Loss: 2.193354606628418\n",
      "Validation: Epoch [2], Batch [573/938], Loss: 2.2062878608703613\n",
      "Validation: Epoch [2], Batch [574/938], Loss: 2.198276996612549\n",
      "Validation: Epoch [2], Batch [575/938], Loss: 2.203416585922241\n",
      "Validation: Epoch [2], Batch [576/938], Loss: 2.218616485595703\n",
      "Validation: Epoch [2], Batch [577/938], Loss: 2.2145142555236816\n",
      "Validation: Epoch [2], Batch [578/938], Loss: 2.179962396621704\n",
      "Validation: Epoch [2], Batch [579/938], Loss: 2.1828389167785645\n",
      "Validation: Epoch [2], Batch [580/938], Loss: 2.1928775310516357\n",
      "Validation: Epoch [2], Batch [581/938], Loss: 2.21830153465271\n",
      "Validation: Epoch [2], Batch [582/938], Loss: 2.2186975479125977\n",
      "Validation: Epoch [2], Batch [583/938], Loss: 2.1978960037231445\n",
      "Validation: Epoch [2], Batch [584/938], Loss: 2.2241806983947754\n",
      "Validation: Epoch [2], Batch [585/938], Loss: 2.1800780296325684\n",
      "Validation: Epoch [2], Batch [586/938], Loss: 2.2112045288085938\n",
      "Validation: Epoch [2], Batch [587/938], Loss: 2.2038333415985107\n",
      "Validation: Epoch [2], Batch [588/938], Loss: 2.2141053676605225\n",
      "Validation: Epoch [2], Batch [589/938], Loss: 2.2031917572021484\n",
      "Validation: Epoch [2], Batch [590/938], Loss: 2.207280158996582\n",
      "Validation: Epoch [2], Batch [591/938], Loss: 2.2113046646118164\n",
      "Validation: Epoch [2], Batch [592/938], Loss: 2.2150187492370605\n",
      "Validation: Epoch [2], Batch [593/938], Loss: 2.1947717666625977\n",
      "Validation: Epoch [2], Batch [594/938], Loss: 2.20108962059021\n",
      "Validation: Epoch [2], Batch [595/938], Loss: 2.197983980178833\n",
      "Validation: Epoch [2], Batch [596/938], Loss: 2.1964221000671387\n",
      "Validation: Epoch [2], Batch [597/938], Loss: 2.202638864517212\n",
      "Validation: Epoch [2], Batch [598/938], Loss: 2.211505651473999\n",
      "Validation: Epoch [2], Batch [599/938], Loss: 2.2050392627716064\n",
      "Validation: Epoch [2], Batch [600/938], Loss: 2.179309844970703\n",
      "Validation: Epoch [2], Batch [601/938], Loss: 2.2021677494049072\n",
      "Validation: Epoch [2], Batch [602/938], Loss: 2.192613363265991\n",
      "Validation: Epoch [2], Batch [603/938], Loss: 2.1905369758605957\n",
      "Validation: Epoch [2], Batch [604/938], Loss: 2.1945300102233887\n",
      "Validation: Epoch [2], Batch [605/938], Loss: 2.2137084007263184\n",
      "Validation: Epoch [2], Batch [606/938], Loss: 2.214536666870117\n",
      "Validation: Epoch [2], Batch [607/938], Loss: 2.1927103996276855\n",
      "Validation: Epoch [2], Batch [608/938], Loss: 2.2097060680389404\n",
      "Validation: Epoch [2], Batch [609/938], Loss: 2.2265281677246094\n",
      "Validation: Epoch [2], Batch [610/938], Loss: 2.201111316680908\n",
      "Validation: Epoch [2], Batch [611/938], Loss: 2.1929738521575928\n",
      "Validation: Epoch [2], Batch [612/938], Loss: 2.2267260551452637\n",
      "Validation: Epoch [2], Batch [613/938], Loss: 2.2073283195495605\n",
      "Validation: Epoch [2], Batch [614/938], Loss: 2.2089571952819824\n",
      "Validation: Epoch [2], Batch [615/938], Loss: 2.1853725910186768\n",
      "Validation: Epoch [2], Batch [616/938], Loss: 2.203516960144043\n",
      "Validation: Epoch [2], Batch [617/938], Loss: 2.1658754348754883\n",
      "Validation: Epoch [2], Batch [618/938], Loss: 2.1794228553771973\n",
      "Validation: Epoch [2], Batch [619/938], Loss: 2.211287021636963\n",
      "Validation: Epoch [2], Batch [620/938], Loss: 2.2154715061187744\n",
      "Validation: Epoch [2], Batch [621/938], Loss: 2.229844570159912\n",
      "Validation: Epoch [2], Batch [622/938], Loss: 2.1885461807250977\n",
      "Validation: Epoch [2], Batch [623/938], Loss: 2.1951708793640137\n",
      "Validation: Epoch [2], Batch [624/938], Loss: 2.2134969234466553\n",
      "Validation: Epoch [2], Batch [625/938], Loss: 2.162917137145996\n",
      "Validation: Epoch [2], Batch [626/938], Loss: 2.2051730155944824\n",
      "Validation: Epoch [2], Batch [627/938], Loss: 2.1889801025390625\n",
      "Validation: Epoch [2], Batch [628/938], Loss: 2.1849565505981445\n",
      "Validation: Epoch [2], Batch [629/938], Loss: 2.174018383026123\n",
      "Validation: Epoch [2], Batch [630/938], Loss: 2.1903247833251953\n",
      "Validation: Epoch [2], Batch [631/938], Loss: 2.2102904319763184\n",
      "Validation: Epoch [2], Batch [632/938], Loss: 2.1691884994506836\n",
      "Validation: Epoch [2], Batch [633/938], Loss: 2.2062017917633057\n",
      "Validation: Epoch [2], Batch [634/938], Loss: 2.186943531036377\n",
      "Validation: Epoch [2], Batch [635/938], Loss: 2.1895267963409424\n",
      "Validation: Epoch [2], Batch [636/938], Loss: 2.1958324909210205\n",
      "Validation: Epoch [2], Batch [637/938], Loss: 2.2052431106567383\n",
      "Validation: Epoch [2], Batch [638/938], Loss: 2.210127353668213\n",
      "Validation: Epoch [2], Batch [639/938], Loss: 2.20145320892334\n",
      "Validation: Epoch [2], Batch [640/938], Loss: 2.207322359085083\n",
      "Validation: Epoch [2], Batch [641/938], Loss: 2.2070937156677246\n",
      "Validation: Epoch [2], Batch [642/938], Loss: 2.188817024230957\n",
      "Validation: Epoch [2], Batch [643/938], Loss: 2.1991796493530273\n",
      "Validation: Epoch [2], Batch [644/938], Loss: 2.173640251159668\n",
      "Validation: Epoch [2], Batch [645/938], Loss: 2.177314043045044\n",
      "Validation: Epoch [2], Batch [646/938], Loss: 2.190709114074707\n",
      "Validation: Epoch [2], Batch [647/938], Loss: 2.191279172897339\n",
      "Validation: Epoch [2], Batch [648/938], Loss: 2.1612420082092285\n",
      "Validation: Epoch [2], Batch [649/938], Loss: 2.1858205795288086\n",
      "Validation: Epoch [2], Batch [650/938], Loss: 2.1941118240356445\n",
      "Validation: Epoch [2], Batch [651/938], Loss: 2.203758478164673\n",
      "Validation: Epoch [2], Batch [652/938], Loss: 2.2116827964782715\n",
      "Validation: Epoch [2], Batch [653/938], Loss: 2.2045061588287354\n",
      "Validation: Epoch [2], Batch [654/938], Loss: 2.173466205596924\n",
      "Validation: Epoch [2], Batch [655/938], Loss: 2.202235221862793\n",
      "Validation: Epoch [2], Batch [656/938], Loss: 2.2071404457092285\n",
      "Validation: Epoch [2], Batch [657/938], Loss: 2.2276451587677\n",
      "Validation: Epoch [2], Batch [658/938], Loss: 2.1743829250335693\n",
      "Validation: Epoch [2], Batch [659/938], Loss: 2.202420473098755\n",
      "Validation: Epoch [2], Batch [660/938], Loss: 2.2158875465393066\n",
      "Validation: Epoch [2], Batch [661/938], Loss: 2.1783246994018555\n",
      "Validation: Epoch [2], Batch [662/938], Loss: 2.1926639080047607\n",
      "Validation: Epoch [2], Batch [663/938], Loss: 2.2011325359344482\n",
      "Validation: Epoch [2], Batch [664/938], Loss: 2.2127578258514404\n",
      "Validation: Epoch [2], Batch [665/938], Loss: 2.182126045227051\n",
      "Validation: Epoch [2], Batch [666/938], Loss: 2.1757473945617676\n",
      "Validation: Epoch [2], Batch [667/938], Loss: 2.1928014755249023\n",
      "Validation: Epoch [2], Batch [668/938], Loss: 2.1832613945007324\n",
      "Validation: Epoch [2], Batch [669/938], Loss: 2.1908066272735596\n",
      "Validation: Epoch [2], Batch [670/938], Loss: 2.2213857173919678\n",
      "Validation: Epoch [2], Batch [671/938], Loss: 2.1865015029907227\n",
      "Validation: Epoch [2], Batch [672/938], Loss: 2.2239487171173096\n",
      "Validation: Epoch [2], Batch [673/938], Loss: 2.203948974609375\n",
      "Validation: Epoch [2], Batch [674/938], Loss: 2.1913795471191406\n",
      "Validation: Epoch [2], Batch [675/938], Loss: 2.2039542198181152\n",
      "Validation: Epoch [2], Batch [676/938], Loss: 2.21486234664917\n",
      "Validation: Epoch [2], Batch [677/938], Loss: 2.2073848247528076\n",
      "Validation: Epoch [2], Batch [678/938], Loss: 2.2099978923797607\n",
      "Validation: Epoch [2], Batch [679/938], Loss: 2.193180799484253\n",
      "Validation: Epoch [2], Batch [680/938], Loss: 2.184356689453125\n",
      "Validation: Epoch [2], Batch [681/938], Loss: 2.1912736892700195\n",
      "Validation: Epoch [2], Batch [682/938], Loss: 2.1942081451416016\n",
      "Validation: Epoch [2], Batch [683/938], Loss: 2.226327896118164\n",
      "Validation: Epoch [2], Batch [684/938], Loss: 2.200464963912964\n",
      "Validation: Epoch [2], Batch [685/938], Loss: 2.2107937335968018\n",
      "Validation: Epoch [2], Batch [686/938], Loss: 2.2049190998077393\n",
      "Validation: Epoch [2], Batch [687/938], Loss: 2.200838327407837\n",
      "Validation: Epoch [2], Batch [688/938], Loss: 2.1973350048065186\n",
      "Validation: Epoch [2], Batch [689/938], Loss: 2.182335376739502\n",
      "Validation: Epoch [2], Batch [690/938], Loss: 2.213984966278076\n",
      "Validation: Epoch [2], Batch [691/938], Loss: 2.185279369354248\n",
      "Validation: Epoch [2], Batch [692/938], Loss: 2.2158498764038086\n",
      "Validation: Epoch [2], Batch [693/938], Loss: 2.1813981533050537\n",
      "Validation: Epoch [2], Batch [694/938], Loss: 2.1648552417755127\n",
      "Validation: Epoch [2], Batch [695/938], Loss: 2.2115089893341064\n",
      "Validation: Epoch [2], Batch [696/938], Loss: 2.2172749042510986\n",
      "Validation: Epoch [2], Batch [697/938], Loss: 2.198823928833008\n",
      "Validation: Epoch [2], Batch [698/938], Loss: 2.1939263343811035\n",
      "Validation: Epoch [2], Batch [699/938], Loss: 2.176539421081543\n",
      "Validation: Epoch [2], Batch [700/938], Loss: 2.2019917964935303\n",
      "Validation: Epoch [2], Batch [701/938], Loss: 2.194324493408203\n",
      "Validation: Epoch [2], Batch [702/938], Loss: 2.2045910358428955\n",
      "Validation: Epoch [2], Batch [703/938], Loss: 2.2138562202453613\n",
      "Validation: Epoch [2], Batch [704/938], Loss: 2.196132183074951\n",
      "Validation: Epoch [2], Batch [705/938], Loss: 2.2074835300445557\n",
      "Validation: Epoch [2], Batch [706/938], Loss: 2.2061424255371094\n",
      "Validation: Epoch [2], Batch [707/938], Loss: 2.2025163173675537\n",
      "Validation: Epoch [2], Batch [708/938], Loss: 2.2001774311065674\n",
      "Validation: Epoch [2], Batch [709/938], Loss: 2.2146155834198\n",
      "Validation: Epoch [2], Batch [710/938], Loss: 2.218329906463623\n",
      "Validation: Epoch [2], Batch [711/938], Loss: 2.2057230472564697\n",
      "Validation: Epoch [2], Batch [712/938], Loss: 2.1913275718688965\n",
      "Validation: Epoch [2], Batch [713/938], Loss: 2.2138099670410156\n",
      "Validation: Epoch [2], Batch [714/938], Loss: 2.204519510269165\n",
      "Validation: Epoch [2], Batch [715/938], Loss: 2.211015224456787\n",
      "Validation: Epoch [2], Batch [716/938], Loss: 2.2031033039093018\n",
      "Validation: Epoch [2], Batch [717/938], Loss: 2.1789183616638184\n",
      "Validation: Epoch [2], Batch [718/938], Loss: 2.193918228149414\n",
      "Validation: Epoch [2], Batch [719/938], Loss: 2.2119202613830566\n",
      "Validation: Epoch [2], Batch [720/938], Loss: 2.1766107082366943\n",
      "Validation: Epoch [2], Batch [721/938], Loss: 2.190605640411377\n",
      "Validation: Epoch [2], Batch [722/938], Loss: 2.2074122428894043\n",
      "Validation: Epoch [2], Batch [723/938], Loss: 2.1964340209960938\n",
      "Validation: Epoch [2], Batch [724/938], Loss: 2.193307399749756\n",
      "Validation: Epoch [2], Batch [725/938], Loss: 2.2396669387817383\n",
      "Validation: Epoch [2], Batch [726/938], Loss: 2.1917266845703125\n",
      "Validation: Epoch [2], Batch [727/938], Loss: 2.212212324142456\n",
      "Validation: Epoch [2], Batch [728/938], Loss: 2.1999402046203613\n",
      "Validation: Epoch [2], Batch [729/938], Loss: 2.207956314086914\n",
      "Validation: Epoch [2], Batch [730/938], Loss: 2.2189996242523193\n",
      "Validation: Epoch [2], Batch [731/938], Loss: 2.2055723667144775\n",
      "Validation: Epoch [2], Batch [732/938], Loss: 2.2087645530700684\n",
      "Validation: Epoch [2], Batch [733/938], Loss: 2.2113428115844727\n",
      "Validation: Epoch [2], Batch [734/938], Loss: 2.194889783859253\n",
      "Validation: Epoch [2], Batch [735/938], Loss: 2.1963558197021484\n",
      "Validation: Epoch [2], Batch [736/938], Loss: 2.190375566482544\n",
      "Validation: Epoch [2], Batch [737/938], Loss: 2.2064785957336426\n",
      "Validation: Epoch [2], Batch [738/938], Loss: 2.1868274211883545\n",
      "Validation: Epoch [2], Batch [739/938], Loss: 2.2150912284851074\n",
      "Validation: Epoch [2], Batch [740/938], Loss: 2.221538543701172\n",
      "Validation: Epoch [2], Batch [741/938], Loss: 2.2132604122161865\n",
      "Validation: Epoch [2], Batch [742/938], Loss: 2.213646173477173\n",
      "Validation: Epoch [2], Batch [743/938], Loss: 2.1705946922302246\n",
      "Validation: Epoch [2], Batch [744/938], Loss: 2.200652599334717\n",
      "Validation: Epoch [2], Batch [745/938], Loss: 2.1962454319000244\n",
      "Validation: Epoch [2], Batch [746/938], Loss: 2.2147133350372314\n",
      "Validation: Epoch [2], Batch [747/938], Loss: 2.1925928592681885\n",
      "Validation: Epoch [2], Batch [748/938], Loss: 2.2083072662353516\n",
      "Validation: Epoch [2], Batch [749/938], Loss: 2.2112905979156494\n",
      "Validation: Epoch [2], Batch [750/938], Loss: 2.1774075031280518\n",
      "Validation: Epoch [2], Batch [751/938], Loss: 2.218599319458008\n",
      "Validation: Epoch [2], Batch [752/938], Loss: 2.210179090499878\n",
      "Validation: Epoch [2], Batch [753/938], Loss: 2.1880528926849365\n",
      "Validation: Epoch [2], Batch [754/938], Loss: 2.2283952236175537\n",
      "Validation: Epoch [2], Batch [755/938], Loss: 2.215714931488037\n",
      "Validation: Epoch [2], Batch [756/938], Loss: 2.19551944732666\n",
      "Validation: Epoch [2], Batch [757/938], Loss: 2.202510118484497\n",
      "Validation: Epoch [2], Batch [758/938], Loss: 2.208482027053833\n",
      "Validation: Epoch [2], Batch [759/938], Loss: 2.2009782791137695\n",
      "Validation: Epoch [2], Batch [760/938], Loss: 2.223027229309082\n",
      "Validation: Epoch [2], Batch [761/938], Loss: 2.234971761703491\n",
      "Validation: Epoch [2], Batch [762/938], Loss: 2.225125789642334\n",
      "Validation: Epoch [2], Batch [763/938], Loss: 2.1901912689208984\n",
      "Validation: Epoch [2], Batch [764/938], Loss: 2.2021191120147705\n",
      "Validation: Epoch [2], Batch [765/938], Loss: 2.210108757019043\n",
      "Validation: Epoch [2], Batch [766/938], Loss: 2.1752126216888428\n",
      "Validation: Epoch [2], Batch [767/938], Loss: 2.2176294326782227\n",
      "Validation: Epoch [2], Batch [768/938], Loss: 2.2044711112976074\n",
      "Validation: Epoch [2], Batch [769/938], Loss: 2.1748008728027344\n",
      "Validation: Epoch [2], Batch [770/938], Loss: 2.1935436725616455\n",
      "Validation: Epoch [2], Batch [771/938], Loss: 2.2203738689422607\n",
      "Validation: Epoch [2], Batch [772/938], Loss: 2.175564765930176\n",
      "Validation: Epoch [2], Batch [773/938], Loss: 2.2081143856048584\n",
      "Validation: Epoch [2], Batch [774/938], Loss: 2.2099123001098633\n",
      "Validation: Epoch [2], Batch [775/938], Loss: 2.1831111907958984\n",
      "Validation: Epoch [2], Batch [776/938], Loss: 2.231961965560913\n",
      "Validation: Epoch [2], Batch [777/938], Loss: 2.1831228733062744\n",
      "Validation: Epoch [2], Batch [778/938], Loss: 2.1994869709014893\n",
      "Validation: Epoch [2], Batch [779/938], Loss: 2.246755361557007\n",
      "Validation: Epoch [2], Batch [780/938], Loss: 2.1872634887695312\n",
      "Validation: Epoch [2], Batch [781/938], Loss: 2.203691005706787\n",
      "Validation: Epoch [2], Batch [782/938], Loss: 2.1972203254699707\n",
      "Validation: Epoch [2], Batch [783/938], Loss: 2.212552547454834\n",
      "Validation: Epoch [2], Batch [784/938], Loss: 2.187533140182495\n",
      "Validation: Epoch [2], Batch [785/938], Loss: 2.227522850036621\n",
      "Validation: Epoch [2], Batch [786/938], Loss: 2.197319746017456\n",
      "Validation: Epoch [2], Batch [787/938], Loss: 2.2099480628967285\n",
      "Validation: Epoch [2], Batch [788/938], Loss: 2.2276620864868164\n",
      "Validation: Epoch [2], Batch [789/938], Loss: 2.1950149536132812\n",
      "Validation: Epoch [2], Batch [790/938], Loss: 2.2174429893493652\n",
      "Validation: Epoch [2], Batch [791/938], Loss: 2.1909430027008057\n",
      "Validation: Epoch [2], Batch [792/938], Loss: 2.209463119506836\n",
      "Validation: Epoch [2], Batch [793/938], Loss: 2.2084994316101074\n",
      "Validation: Epoch [2], Batch [794/938], Loss: 2.202043294906616\n",
      "Validation: Epoch [2], Batch [795/938], Loss: 2.196495532989502\n",
      "Validation: Epoch [2], Batch [796/938], Loss: 2.2289717197418213\n",
      "Validation: Epoch [2], Batch [797/938], Loss: 2.1780903339385986\n",
      "Validation: Epoch [2], Batch [798/938], Loss: 2.1949517726898193\n",
      "Validation: Epoch [2], Batch [799/938], Loss: 2.1982762813568115\n",
      "Validation: Epoch [2], Batch [800/938], Loss: 2.232469320297241\n",
      "Validation: Epoch [2], Batch [801/938], Loss: 2.199925422668457\n",
      "Validation: Epoch [2], Batch [802/938], Loss: 2.209146738052368\n",
      "Validation: Epoch [2], Batch [803/938], Loss: 2.19667649269104\n",
      "Validation: Epoch [2], Batch [804/938], Loss: 2.1893110275268555\n",
      "Validation: Epoch [2], Batch [805/938], Loss: 2.1859002113342285\n",
      "Validation: Epoch [2], Batch [806/938], Loss: 2.221358060836792\n",
      "Validation: Epoch [2], Batch [807/938], Loss: 2.2080094814300537\n",
      "Validation: Epoch [2], Batch [808/938], Loss: 2.2138237953186035\n",
      "Validation: Epoch [2], Batch [809/938], Loss: 2.1868953704833984\n",
      "Validation: Epoch [2], Batch [810/938], Loss: 2.201925754547119\n",
      "Validation: Epoch [2], Batch [811/938], Loss: 2.177647352218628\n",
      "Validation: Epoch [2], Batch [812/938], Loss: 2.221778392791748\n",
      "Validation: Epoch [2], Batch [813/938], Loss: 2.217407703399658\n",
      "Validation: Epoch [2], Batch [814/938], Loss: 2.1964285373687744\n",
      "Validation: Epoch [2], Batch [815/938], Loss: 2.1922826766967773\n",
      "Validation: Epoch [2], Batch [816/938], Loss: 2.2044734954833984\n",
      "Validation: Epoch [2], Batch [817/938], Loss: 2.209164619445801\n",
      "Validation: Epoch [2], Batch [818/938], Loss: 2.172639846801758\n",
      "Validation: Epoch [2], Batch [819/938], Loss: 2.203075647354126\n",
      "Validation: Epoch [2], Batch [820/938], Loss: 2.208982229232788\n",
      "Validation: Epoch [2], Batch [821/938], Loss: 2.203488349914551\n",
      "Validation: Epoch [2], Batch [822/938], Loss: 2.197927474975586\n",
      "Validation: Epoch [2], Batch [823/938], Loss: 2.209113597869873\n",
      "Validation: Epoch [2], Batch [824/938], Loss: 2.1875953674316406\n",
      "Validation: Epoch [2], Batch [825/938], Loss: 2.2073845863342285\n",
      "Validation: Epoch [2], Batch [826/938], Loss: 2.178676128387451\n",
      "Validation: Epoch [2], Batch [827/938], Loss: 2.189737558364868\n",
      "Validation: Epoch [2], Batch [828/938], Loss: 2.194803237915039\n",
      "Validation: Epoch [2], Batch [829/938], Loss: 2.17828106880188\n",
      "Validation: Epoch [2], Batch [830/938], Loss: 2.1846208572387695\n",
      "Validation: Epoch [2], Batch [831/938], Loss: 2.19838285446167\n",
      "Validation: Epoch [2], Batch [832/938], Loss: 2.201634168624878\n",
      "Validation: Epoch [2], Batch [833/938], Loss: 2.179424524307251\n",
      "Validation: Epoch [2], Batch [834/938], Loss: 2.1939454078674316\n",
      "Validation: Epoch [2], Batch [835/938], Loss: 2.179492950439453\n",
      "Validation: Epoch [2], Batch [836/938], Loss: 2.2105705738067627\n",
      "Validation: Epoch [2], Batch [837/938], Loss: 2.218424081802368\n",
      "Validation: Epoch [2], Batch [838/938], Loss: 2.217087745666504\n",
      "Validation: Epoch [2], Batch [839/938], Loss: 2.219857692718506\n",
      "Validation: Epoch [2], Batch [840/938], Loss: 2.212477684020996\n",
      "Validation: Epoch [2], Batch [841/938], Loss: 2.2149291038513184\n",
      "Validation: Epoch [2], Batch [842/938], Loss: 2.1989920139312744\n",
      "Validation: Epoch [2], Batch [843/938], Loss: 2.1979527473449707\n",
      "Validation: Epoch [2], Batch [844/938], Loss: 2.1959729194641113\n",
      "Validation: Epoch [2], Batch [845/938], Loss: 2.169285297393799\n",
      "Validation: Epoch [2], Batch [846/938], Loss: 2.2062132358551025\n",
      "Validation: Epoch [2], Batch [847/938], Loss: 2.1929726600646973\n",
      "Validation: Epoch [2], Batch [848/938], Loss: 2.1886978149414062\n",
      "Validation: Epoch [2], Batch [849/938], Loss: 2.2087924480438232\n",
      "Validation: Epoch [2], Batch [850/938], Loss: 2.2201147079467773\n",
      "Validation: Epoch [2], Batch [851/938], Loss: 2.2383434772491455\n",
      "Validation: Epoch [2], Batch [852/938], Loss: 2.2247915267944336\n",
      "Validation: Epoch [2], Batch [853/938], Loss: 2.2123804092407227\n",
      "Validation: Epoch [2], Batch [854/938], Loss: 2.194648027420044\n",
      "Validation: Epoch [2], Batch [855/938], Loss: 2.186823606491089\n",
      "Validation: Epoch [2], Batch [856/938], Loss: 2.203681468963623\n",
      "Validation: Epoch [2], Batch [857/938], Loss: 2.17246413230896\n",
      "Validation: Epoch [2], Batch [858/938], Loss: 2.2116925716400146\n",
      "Validation: Epoch [2], Batch [859/938], Loss: 2.216055393218994\n",
      "Validation: Epoch [2], Batch [860/938], Loss: 2.1798551082611084\n",
      "Validation: Epoch [2], Batch [861/938], Loss: 2.1988017559051514\n",
      "Validation: Epoch [2], Batch [862/938], Loss: 2.235292911529541\n",
      "Validation: Epoch [2], Batch [863/938], Loss: 2.2049520015716553\n",
      "Validation: Epoch [2], Batch [864/938], Loss: 2.2250664234161377\n",
      "Validation: Epoch [2], Batch [865/938], Loss: 2.1937053203582764\n",
      "Validation: Epoch [2], Batch [866/938], Loss: 2.2083990573883057\n",
      "Validation: Epoch [2], Batch [867/938], Loss: 2.2301650047302246\n",
      "Validation: Epoch [2], Batch [868/938], Loss: 2.2257814407348633\n",
      "Validation: Epoch [2], Batch [869/938], Loss: 2.199068784713745\n",
      "Validation: Epoch [2], Batch [870/938], Loss: 2.2162013053894043\n",
      "Validation: Epoch [2], Batch [871/938], Loss: 2.207409143447876\n",
      "Validation: Epoch [2], Batch [872/938], Loss: 2.178706645965576\n",
      "Validation: Epoch [2], Batch [873/938], Loss: 2.198829174041748\n",
      "Validation: Epoch [2], Batch [874/938], Loss: 2.2189323902130127\n",
      "Validation: Epoch [2], Batch [875/938], Loss: 2.186722993850708\n",
      "Validation: Epoch [2], Batch [876/938], Loss: 2.2137880325317383\n",
      "Validation: Epoch [2], Batch [877/938], Loss: 2.196467876434326\n",
      "Validation: Epoch [2], Batch [878/938], Loss: 2.216848611831665\n",
      "Validation: Epoch [2], Batch [879/938], Loss: 2.1935603618621826\n",
      "Validation: Epoch [2], Batch [880/938], Loss: 2.215336561203003\n",
      "Validation: Epoch [2], Batch [881/938], Loss: 2.2056825160980225\n",
      "Validation: Epoch [2], Batch [882/938], Loss: 2.2167556285858154\n",
      "Validation: Epoch [2], Batch [883/938], Loss: 2.2023749351501465\n",
      "Validation: Epoch [2], Batch [884/938], Loss: 2.2172954082489014\n",
      "Validation: Epoch [2], Batch [885/938], Loss: 2.2117927074432373\n",
      "Validation: Epoch [2], Batch [886/938], Loss: 2.1871399879455566\n",
      "Validation: Epoch [2], Batch [887/938], Loss: 2.220172882080078\n",
      "Validation: Epoch [2], Batch [888/938], Loss: 2.1856038570404053\n",
      "Validation: Epoch [2], Batch [889/938], Loss: 2.2180352210998535\n",
      "Validation: Epoch [2], Batch [890/938], Loss: 2.199564218521118\n",
      "Validation: Epoch [2], Batch [891/938], Loss: 2.231393575668335\n",
      "Validation: Epoch [2], Batch [892/938], Loss: 2.170196056365967\n",
      "Validation: Epoch [2], Batch [893/938], Loss: 2.200817108154297\n",
      "Validation: Epoch [2], Batch [894/938], Loss: 2.2029950618743896\n",
      "Validation: Epoch [2], Batch [895/938], Loss: 2.195009231567383\n",
      "Validation: Epoch [2], Batch [896/938], Loss: 2.2026565074920654\n",
      "Validation: Epoch [2], Batch [897/938], Loss: 2.208597421646118\n",
      "Validation: Epoch [2], Batch [898/938], Loss: 2.18710994720459\n",
      "Validation: Epoch [2], Batch [899/938], Loss: 2.2030928134918213\n",
      "Validation: Epoch [2], Batch [900/938], Loss: 2.20725154876709\n",
      "Validation: Epoch [2], Batch [901/938], Loss: 2.2142646312713623\n",
      "Validation: Epoch [2], Batch [902/938], Loss: 2.1736245155334473\n",
      "Validation: Epoch [2], Batch [903/938], Loss: 2.2011117935180664\n",
      "Validation: Epoch [2], Batch [904/938], Loss: 2.187746286392212\n",
      "Validation: Epoch [2], Batch [905/938], Loss: 2.2121410369873047\n",
      "Validation: Epoch [2], Batch [906/938], Loss: 2.182215929031372\n",
      "Validation: Epoch [2], Batch [907/938], Loss: 2.200531005859375\n",
      "Validation: Epoch [2], Batch [908/938], Loss: 2.2378811836242676\n",
      "Validation: Epoch [2], Batch [909/938], Loss: 2.221449613571167\n",
      "Validation: Epoch [2], Batch [910/938], Loss: 2.212467670440674\n",
      "Validation: Epoch [2], Batch [911/938], Loss: 2.2177085876464844\n",
      "Validation: Epoch [2], Batch [912/938], Loss: 2.2044591903686523\n",
      "Validation: Epoch [2], Batch [913/938], Loss: 2.2266573905944824\n",
      "Validation: Epoch [2], Batch [914/938], Loss: 2.20900821685791\n",
      "Validation: Epoch [2], Batch [915/938], Loss: 2.218886137008667\n",
      "Validation: Epoch [2], Batch [916/938], Loss: 2.197031259536743\n",
      "Validation: Epoch [2], Batch [917/938], Loss: 2.2090702056884766\n",
      "Validation: Epoch [2], Batch [918/938], Loss: 2.22048020362854\n",
      "Validation: Epoch [2], Batch [919/938], Loss: 2.2017173767089844\n",
      "Validation: Epoch [2], Batch [920/938], Loss: 2.2150368690490723\n",
      "Validation: Epoch [2], Batch [921/938], Loss: 2.177065849304199\n",
      "Validation: Epoch [2], Batch [922/938], Loss: 2.2212581634521484\n",
      "Validation: Epoch [2], Batch [923/938], Loss: 2.2165584564208984\n",
      "Validation: Epoch [2], Batch [924/938], Loss: 2.222896099090576\n",
      "Validation: Epoch [2], Batch [925/938], Loss: 2.1862878799438477\n",
      "Validation: Epoch [2], Batch [926/938], Loss: 2.217966318130493\n",
      "Validation: Epoch [2], Batch [927/938], Loss: 2.202138662338257\n",
      "Validation: Epoch [2], Batch [928/938], Loss: 2.208446502685547\n",
      "Validation: Epoch [2], Batch [929/938], Loss: 2.206286907196045\n",
      "Validation: Epoch [2], Batch [930/938], Loss: 2.212383270263672\n",
      "Validation: Epoch [2], Batch [931/938], Loss: 2.203651189804077\n",
      "Validation: Epoch [2], Batch [932/938], Loss: 2.2028915882110596\n",
      "Validation: Epoch [2], Batch [933/938], Loss: 2.1933610439300537\n",
      "Validation: Epoch [2], Batch [934/938], Loss: 2.212463855743408\n",
      "Validation: Epoch [2], Batch [935/938], Loss: 2.2085070610046387\n",
      "Validation: Epoch [2], Batch [936/938], Loss: 2.209463596343994\n",
      "Validation: Epoch [2], Batch [937/938], Loss: 2.1941287517547607\n",
      "Validation: Epoch [2], Batch [938/938], Loss: 2.2189791202545166\n",
      "Accuracy of test set: 0.32083333333333336\n",
      "Train: Epoch [3], Batch [1/938], Loss: 2.205889940261841\n",
      "Train: Epoch [3], Batch [2/938], Loss: 2.2337210178375244\n",
      "Train: Epoch [3], Batch [3/938], Loss: 2.202902317047119\n",
      "Train: Epoch [3], Batch [4/938], Loss: 2.208728313446045\n",
      "Train: Epoch [3], Batch [5/938], Loss: 2.2176339626312256\n",
      "Train: Epoch [3], Batch [6/938], Loss: 2.1794803142547607\n",
      "Train: Epoch [3], Batch [7/938], Loss: 2.193321704864502\n",
      "Train: Epoch [3], Batch [8/938], Loss: 2.191192626953125\n",
      "Train: Epoch [3], Batch [9/938], Loss: 2.202284336090088\n",
      "Train: Epoch [3], Batch [10/938], Loss: 2.180659294128418\n",
      "Train: Epoch [3], Batch [11/938], Loss: 2.197532892227173\n",
      "Train: Epoch [3], Batch [12/938], Loss: 2.1756703853607178\n",
      "Train: Epoch [3], Batch [13/938], Loss: 2.216618776321411\n",
      "Train: Epoch [3], Batch [14/938], Loss: 2.1855130195617676\n",
      "Train: Epoch [3], Batch [15/938], Loss: 2.1715667247772217\n",
      "Train: Epoch [3], Batch [16/938], Loss: 2.204921007156372\n",
      "Train: Epoch [3], Batch [17/938], Loss: 2.214296579360962\n",
      "Train: Epoch [3], Batch [18/938], Loss: 2.2172069549560547\n",
      "Train: Epoch [3], Batch [19/938], Loss: 2.204890251159668\n",
      "Train: Epoch [3], Batch [20/938], Loss: 2.192617893218994\n",
      "Train: Epoch [3], Batch [21/938], Loss: 2.2062175273895264\n",
      "Train: Epoch [3], Batch [22/938], Loss: 2.2134528160095215\n",
      "Train: Epoch [3], Batch [23/938], Loss: 2.1913585662841797\n",
      "Train: Epoch [3], Batch [24/938], Loss: 2.1803417205810547\n",
      "Train: Epoch [3], Batch [25/938], Loss: 2.187429189682007\n",
      "Train: Epoch [3], Batch [26/938], Loss: 2.1826348304748535\n",
      "Train: Epoch [3], Batch [27/938], Loss: 2.1955690383911133\n",
      "Train: Epoch [3], Batch [28/938], Loss: 2.2015323638916016\n",
      "Train: Epoch [3], Batch [29/938], Loss: 2.2017312049865723\n",
      "Train: Epoch [3], Batch [30/938], Loss: 2.1918933391571045\n",
      "Train: Epoch [3], Batch [31/938], Loss: 2.191788911819458\n",
      "Train: Epoch [3], Batch [32/938], Loss: 2.1577906608581543\n",
      "Train: Epoch [3], Batch [33/938], Loss: 2.2034754753112793\n",
      "Train: Epoch [3], Batch [34/938], Loss: 2.230356454849243\n",
      "Train: Epoch [3], Batch [35/938], Loss: 2.178447961807251\n",
      "Train: Epoch [3], Batch [36/938], Loss: 2.1996264457702637\n",
      "Train: Epoch [3], Batch [37/938], Loss: 2.178417205810547\n",
      "Train: Epoch [3], Batch [38/938], Loss: 2.2061209678649902\n",
      "Train: Epoch [3], Batch [39/938], Loss: 2.201052665710449\n",
      "Train: Epoch [3], Batch [40/938], Loss: 2.196439266204834\n",
      "Train: Epoch [3], Batch [41/938], Loss: 2.2150063514709473\n",
      "Train: Epoch [3], Batch [42/938], Loss: 2.2122910022735596\n",
      "Train: Epoch [3], Batch [43/938], Loss: 2.2182986736297607\n",
      "Train: Epoch [3], Batch [44/938], Loss: 2.2110748291015625\n",
      "Train: Epoch [3], Batch [45/938], Loss: 2.2204127311706543\n",
      "Train: Epoch [3], Batch [46/938], Loss: 2.211674213409424\n",
      "Train: Epoch [3], Batch [47/938], Loss: 2.1863961219787598\n",
      "Train: Epoch [3], Batch [48/938], Loss: 2.1752712726593018\n",
      "Train: Epoch [3], Batch [49/938], Loss: 2.176722288131714\n",
      "Train: Epoch [3], Batch [50/938], Loss: 2.180732011795044\n",
      "Train: Epoch [3], Batch [51/938], Loss: 2.1763534545898438\n",
      "Train: Epoch [3], Batch [52/938], Loss: 2.1932144165039062\n",
      "Train: Epoch [3], Batch [53/938], Loss: 2.202462673187256\n",
      "Train: Epoch [3], Batch [54/938], Loss: 2.1939005851745605\n",
      "Train: Epoch [3], Batch [55/938], Loss: 2.196516990661621\n",
      "Train: Epoch [3], Batch [56/938], Loss: 2.1731343269348145\n",
      "Train: Epoch [3], Batch [57/938], Loss: 2.183938980102539\n",
      "Train: Epoch [3], Batch [58/938], Loss: 2.1690146923065186\n",
      "Train: Epoch [3], Batch [59/938], Loss: 2.179927110671997\n",
      "Train: Epoch [3], Batch [60/938], Loss: 2.1810641288757324\n",
      "Train: Epoch [3], Batch [61/938], Loss: 2.2156741619110107\n",
      "Train: Epoch [3], Batch [62/938], Loss: 2.1686458587646484\n",
      "Train: Epoch [3], Batch [63/938], Loss: 2.2046444416046143\n",
      "Train: Epoch [3], Batch [64/938], Loss: 2.1610031127929688\n",
      "Train: Epoch [3], Batch [65/938], Loss: 2.1842868328094482\n",
      "Train: Epoch [3], Batch [66/938], Loss: 2.20072078704834\n",
      "Train: Epoch [3], Batch [67/938], Loss: 2.1861612796783447\n",
      "Train: Epoch [3], Batch [68/938], Loss: 2.190744638442993\n",
      "Train: Epoch [3], Batch [69/938], Loss: 2.165616035461426\n",
      "Train: Epoch [3], Batch [70/938], Loss: 2.188323974609375\n",
      "Train: Epoch [3], Batch [71/938], Loss: 2.182992696762085\n",
      "Train: Epoch [3], Batch [72/938], Loss: 2.178964138031006\n",
      "Train: Epoch [3], Batch [73/938], Loss: 2.2187981605529785\n",
      "Train: Epoch [3], Batch [74/938], Loss: 2.1757164001464844\n",
      "Train: Epoch [3], Batch [75/938], Loss: 2.179250478744507\n",
      "Train: Epoch [3], Batch [76/938], Loss: 2.179248332977295\n",
      "Train: Epoch [3], Batch [77/938], Loss: 2.162454128265381\n",
      "Train: Epoch [3], Batch [78/938], Loss: 2.16487193107605\n",
      "Train: Epoch [3], Batch [79/938], Loss: 2.192006826400757\n",
      "Train: Epoch [3], Batch [80/938], Loss: 2.166369676589966\n",
      "Train: Epoch [3], Batch [81/938], Loss: 2.161216974258423\n",
      "Train: Epoch [3], Batch [82/938], Loss: 2.1801247596740723\n",
      "Train: Epoch [3], Batch [83/938], Loss: 2.176388740539551\n",
      "Train: Epoch [3], Batch [84/938], Loss: 2.1795077323913574\n",
      "Train: Epoch [3], Batch [85/938], Loss: 2.1864969730377197\n",
      "Train: Epoch [3], Batch [86/938], Loss: 2.1958041191101074\n",
      "Train: Epoch [3], Batch [87/938], Loss: 2.182316541671753\n",
      "Train: Epoch [3], Batch [88/938], Loss: 2.143174886703491\n",
      "Train: Epoch [3], Batch [89/938], Loss: 2.192000150680542\n",
      "Train: Epoch [3], Batch [90/938], Loss: 2.1920595169067383\n",
      "Train: Epoch [3], Batch [91/938], Loss: 2.1548542976379395\n",
      "Train: Epoch [3], Batch [92/938], Loss: 2.1893951892852783\n",
      "Train: Epoch [3], Batch [93/938], Loss: 2.1646888256073\n",
      "Train: Epoch [3], Batch [94/938], Loss: 2.172271251678467\n",
      "Train: Epoch [3], Batch [95/938], Loss: 2.181878089904785\n",
      "Train: Epoch [3], Batch [96/938], Loss: 2.1836249828338623\n",
      "Train: Epoch [3], Batch [97/938], Loss: 2.174360990524292\n",
      "Train: Epoch [3], Batch [98/938], Loss: 2.214402437210083\n",
      "Train: Epoch [3], Batch [99/938], Loss: 2.1785621643066406\n",
      "Train: Epoch [3], Batch [100/938], Loss: 2.162844181060791\n",
      "Train: Epoch [3], Batch [101/938], Loss: 2.1624059677124023\n",
      "Train: Epoch [3], Batch [102/938], Loss: 2.147880792617798\n",
      "Train: Epoch [3], Batch [103/938], Loss: 2.1847548484802246\n",
      "Train: Epoch [3], Batch [104/938], Loss: 2.177258014678955\n",
      "Train: Epoch [3], Batch [105/938], Loss: 2.1577908992767334\n",
      "Train: Epoch [3], Batch [106/938], Loss: 2.1817140579223633\n",
      "Train: Epoch [3], Batch [107/938], Loss: 2.1908304691314697\n",
      "Train: Epoch [3], Batch [108/938], Loss: 2.199871063232422\n",
      "Train: Epoch [3], Batch [109/938], Loss: 2.1828041076660156\n",
      "Train: Epoch [3], Batch [110/938], Loss: 2.1584181785583496\n",
      "Train: Epoch [3], Batch [111/938], Loss: 2.146294116973877\n",
      "Train: Epoch [3], Batch [112/938], Loss: 2.184178113937378\n",
      "Train: Epoch [3], Batch [113/938], Loss: 2.182994842529297\n",
      "Train: Epoch [3], Batch [114/938], Loss: 2.123621940612793\n",
      "Train: Epoch [3], Batch [115/938], Loss: 2.1936233043670654\n",
      "Train: Epoch [3], Batch [116/938], Loss: 2.114637851715088\n",
      "Train: Epoch [3], Batch [117/938], Loss: 2.1646995544433594\n",
      "Train: Epoch [3], Batch [118/938], Loss: 2.1737453937530518\n",
      "Train: Epoch [3], Batch [119/938], Loss: 2.165571689605713\n",
      "Train: Epoch [3], Batch [120/938], Loss: 2.197526693344116\n",
      "Train: Epoch [3], Batch [121/938], Loss: 2.1871721744537354\n",
      "Train: Epoch [3], Batch [122/938], Loss: 2.163022994995117\n",
      "Train: Epoch [3], Batch [123/938], Loss: 2.231010675430298\n",
      "Train: Epoch [3], Batch [124/938], Loss: 2.142367362976074\n",
      "Train: Epoch [3], Batch [125/938], Loss: 2.145392656326294\n",
      "Train: Epoch [3], Batch [126/938], Loss: 2.1701483726501465\n",
      "Train: Epoch [3], Batch [127/938], Loss: 2.1703720092773438\n",
      "Train: Epoch [3], Batch [128/938], Loss: 2.1420397758483887\n",
      "Train: Epoch [3], Batch [129/938], Loss: 2.1984171867370605\n",
      "Train: Epoch [3], Batch [130/938], Loss: 2.1320273876190186\n",
      "Train: Epoch [3], Batch [131/938], Loss: 2.177842855453491\n",
      "Train: Epoch [3], Batch [132/938], Loss: 2.148099422454834\n",
      "Train: Epoch [3], Batch [133/938], Loss: 2.1997885704040527\n",
      "Train: Epoch [3], Batch [134/938], Loss: 2.152759552001953\n",
      "Train: Epoch [3], Batch [135/938], Loss: 2.1644041538238525\n",
      "Train: Epoch [3], Batch [136/938], Loss: 2.1979804039001465\n",
      "Train: Epoch [3], Batch [137/938], Loss: 2.152614116668701\n",
      "Train: Epoch [3], Batch [138/938], Loss: 2.166865348815918\n",
      "Train: Epoch [3], Batch [139/938], Loss: 2.1364121437072754\n",
      "Train: Epoch [3], Batch [140/938], Loss: 2.1831579208374023\n",
      "Train: Epoch [3], Batch [141/938], Loss: 2.165903329849243\n",
      "Train: Epoch [3], Batch [142/938], Loss: 2.167060136795044\n",
      "Train: Epoch [3], Batch [143/938], Loss: 2.192159414291382\n",
      "Train: Epoch [3], Batch [144/938], Loss: 2.150627374649048\n",
      "Train: Epoch [3], Batch [145/938], Loss: 2.180997848510742\n",
      "Train: Epoch [3], Batch [146/938], Loss: 2.1695024967193604\n",
      "Train: Epoch [3], Batch [147/938], Loss: 2.151338815689087\n",
      "Train: Epoch [3], Batch [148/938], Loss: 2.1360836029052734\n",
      "Train: Epoch [3], Batch [149/938], Loss: 2.141362190246582\n",
      "Train: Epoch [3], Batch [150/938], Loss: 2.133133888244629\n",
      "Train: Epoch [3], Batch [151/938], Loss: 2.1742963790893555\n",
      "Train: Epoch [3], Batch [152/938], Loss: 2.1218135356903076\n",
      "Train: Epoch [3], Batch [153/938], Loss: 2.1401641368865967\n",
      "Train: Epoch [3], Batch [154/938], Loss: 2.149353265762329\n",
      "Train: Epoch [3], Batch [155/938], Loss: 2.1460349559783936\n",
      "Train: Epoch [3], Batch [156/938], Loss: 2.1517016887664795\n",
      "Train: Epoch [3], Batch [157/938], Loss: 2.1560182571411133\n",
      "Train: Epoch [3], Batch [158/938], Loss: 2.153858184814453\n",
      "Train: Epoch [3], Batch [159/938], Loss: 2.126540422439575\n",
      "Train: Epoch [3], Batch [160/938], Loss: 2.149348497390747\n",
      "Train: Epoch [3], Batch [161/938], Loss: 2.1483654975891113\n",
      "Train: Epoch [3], Batch [162/938], Loss: 2.1657965183258057\n",
      "Train: Epoch [3], Batch [163/938], Loss: 2.149397134780884\n",
      "Train: Epoch [3], Batch [164/938], Loss: 2.147458553314209\n",
      "Train: Epoch [3], Batch [165/938], Loss: 2.137807846069336\n",
      "Train: Epoch [3], Batch [166/938], Loss: 2.162208318710327\n",
      "Train: Epoch [3], Batch [167/938], Loss: 2.171431064605713\n",
      "Train: Epoch [3], Batch [168/938], Loss: 2.2016515731811523\n",
      "Train: Epoch [3], Batch [169/938], Loss: 2.115142583847046\n",
      "Train: Epoch [3], Batch [170/938], Loss: 2.1398544311523438\n",
      "Train: Epoch [3], Batch [171/938], Loss: 2.091702699661255\n",
      "Train: Epoch [3], Batch [172/938], Loss: 2.1996569633483887\n",
      "Train: Epoch [3], Batch [173/938], Loss: 2.1487627029418945\n",
      "Train: Epoch [3], Batch [174/938], Loss: 2.135580062866211\n",
      "Train: Epoch [3], Batch [175/938], Loss: 2.137068510055542\n",
      "Train: Epoch [3], Batch [176/938], Loss: 2.123978853225708\n",
      "Train: Epoch [3], Batch [177/938], Loss: 2.1488382816314697\n",
      "Train: Epoch [3], Batch [178/938], Loss: 2.152252197265625\n",
      "Train: Epoch [3], Batch [179/938], Loss: 2.122098445892334\n",
      "Train: Epoch [3], Batch [180/938], Loss: 2.1396541595458984\n",
      "Train: Epoch [3], Batch [181/938], Loss: 2.1437325477600098\n",
      "Train: Epoch [3], Batch [182/938], Loss: 2.162719964981079\n",
      "Train: Epoch [3], Batch [183/938], Loss: 2.124737024307251\n",
      "Train: Epoch [3], Batch [184/938], Loss: 2.163792610168457\n",
      "Train: Epoch [3], Batch [185/938], Loss: 2.14322566986084\n",
      "Train: Epoch [3], Batch [186/938], Loss: 2.1599977016448975\n",
      "Train: Epoch [3], Batch [187/938], Loss: 2.107243299484253\n",
      "Train: Epoch [3], Batch [188/938], Loss: 2.1209821701049805\n",
      "Train: Epoch [3], Batch [189/938], Loss: 2.1646366119384766\n",
      "Train: Epoch [3], Batch [190/938], Loss: 2.136608839035034\n",
      "Train: Epoch [3], Batch [191/938], Loss: 2.1508922576904297\n",
      "Train: Epoch [3], Batch [192/938], Loss: 2.144514799118042\n",
      "Train: Epoch [3], Batch [193/938], Loss: 2.0915706157684326\n",
      "Train: Epoch [3], Batch [194/938], Loss: 2.1118083000183105\n",
      "Train: Epoch [3], Batch [195/938], Loss: 2.141698122024536\n",
      "Train: Epoch [3], Batch [196/938], Loss: 2.101901054382324\n",
      "Train: Epoch [3], Batch [197/938], Loss: 2.1280927658081055\n",
      "Train: Epoch [3], Batch [198/938], Loss: 2.1133873462677\n",
      "Train: Epoch [3], Batch [199/938], Loss: 2.150693416595459\n",
      "Train: Epoch [3], Batch [200/938], Loss: 2.1378934383392334\n",
      "Train: Epoch [3], Batch [201/938], Loss: 2.120314598083496\n",
      "Train: Epoch [3], Batch [202/938], Loss: 2.1251068115234375\n",
      "Train: Epoch [3], Batch [203/938], Loss: 2.196659803390503\n",
      "Train: Epoch [3], Batch [204/938], Loss: 2.132493495941162\n",
      "Train: Epoch [3], Batch [205/938], Loss: 2.1232354640960693\n",
      "Train: Epoch [3], Batch [206/938], Loss: 2.1510512828826904\n",
      "Train: Epoch [3], Batch [207/938], Loss: 2.1157238483428955\n",
      "Train: Epoch [3], Batch [208/938], Loss: 2.1472299098968506\n",
      "Train: Epoch [3], Batch [209/938], Loss: 2.1423513889312744\n",
      "Train: Epoch [3], Batch [210/938], Loss: 2.134033679962158\n",
      "Train: Epoch [3], Batch [211/938], Loss: 2.1652238368988037\n",
      "Train: Epoch [3], Batch [212/938], Loss: 2.105426073074341\n",
      "Train: Epoch [3], Batch [213/938], Loss: 2.1034295558929443\n",
      "Train: Epoch [3], Batch [214/938], Loss: 2.1414406299591064\n",
      "Train: Epoch [3], Batch [215/938], Loss: 2.1291677951812744\n",
      "Train: Epoch [3], Batch [216/938], Loss: 2.1302390098571777\n",
      "Train: Epoch [3], Batch [217/938], Loss: 2.114649772644043\n",
      "Train: Epoch [3], Batch [218/938], Loss: 2.1271142959594727\n",
      "Train: Epoch [3], Batch [219/938], Loss: 2.123602867126465\n",
      "Train: Epoch [3], Batch [220/938], Loss: 2.149803876876831\n",
      "Train: Epoch [3], Batch [221/938], Loss: 2.1715660095214844\n",
      "Train: Epoch [3], Batch [222/938], Loss: 2.146333694458008\n",
      "Train: Epoch [3], Batch [223/938], Loss: 2.136908531188965\n",
      "Train: Epoch [3], Batch [224/938], Loss: 2.11672306060791\n",
      "Train: Epoch [3], Batch [225/938], Loss: 2.1512529850006104\n",
      "Train: Epoch [3], Batch [226/938], Loss: 2.090672016143799\n",
      "Train: Epoch [3], Batch [227/938], Loss: 2.1136045455932617\n",
      "Train: Epoch [3], Batch [228/938], Loss: 2.0654211044311523\n",
      "Train: Epoch [3], Batch [229/938], Loss: 2.1186647415161133\n",
      "Train: Epoch [3], Batch [230/938], Loss: 2.081043243408203\n",
      "Train: Epoch [3], Batch [231/938], Loss: 2.1509323120117188\n",
      "Train: Epoch [3], Batch [232/938], Loss: 2.166283369064331\n",
      "Train: Epoch [3], Batch [233/938], Loss: 2.100187063217163\n",
      "Train: Epoch [3], Batch [234/938], Loss: 2.0946552753448486\n",
      "Train: Epoch [3], Batch [235/938], Loss: 2.0819122791290283\n",
      "Train: Epoch [3], Batch [236/938], Loss: 2.1268739700317383\n",
      "Train: Epoch [3], Batch [237/938], Loss: 2.1441030502319336\n",
      "Train: Epoch [3], Batch [238/938], Loss: 2.118669033050537\n",
      "Train: Epoch [3], Batch [239/938], Loss: 2.09941029548645\n",
      "Train: Epoch [3], Batch [240/938], Loss: 2.1447043418884277\n",
      "Train: Epoch [3], Batch [241/938], Loss: 2.076205015182495\n",
      "Train: Epoch [3], Batch [242/938], Loss: 2.057588815689087\n",
      "Train: Epoch [3], Batch [243/938], Loss: 2.148979902267456\n",
      "Train: Epoch [3], Batch [244/938], Loss: 2.1709656715393066\n",
      "Train: Epoch [3], Batch [245/938], Loss: 2.084930658340454\n",
      "Train: Epoch [3], Batch [246/938], Loss: 2.0906474590301514\n",
      "Train: Epoch [3], Batch [247/938], Loss: 2.093754768371582\n",
      "Train: Epoch [3], Batch [248/938], Loss: 2.1054811477661133\n",
      "Train: Epoch [3], Batch [249/938], Loss: 2.122718334197998\n",
      "Train: Epoch [3], Batch [250/938], Loss: 2.125309467315674\n",
      "Train: Epoch [3], Batch [251/938], Loss: 2.1011593341827393\n",
      "Train: Epoch [3], Batch [252/938], Loss: 2.0952253341674805\n",
      "Train: Epoch [3], Batch [253/938], Loss: 2.066807508468628\n",
      "Train: Epoch [3], Batch [254/938], Loss: 2.087273120880127\n",
      "Train: Epoch [3], Batch [255/938], Loss: 2.104356288909912\n",
      "Train: Epoch [3], Batch [256/938], Loss: 2.061615467071533\n",
      "Train: Epoch [3], Batch [257/938], Loss: 2.1194920539855957\n",
      "Train: Epoch [3], Batch [258/938], Loss: 2.094592571258545\n",
      "Train: Epoch [3], Batch [259/938], Loss: 2.028433322906494\n",
      "Train: Epoch [3], Batch [260/938], Loss: 2.116044044494629\n",
      "Train: Epoch [3], Batch [261/938], Loss: 2.082756996154785\n",
      "Train: Epoch [3], Batch [262/938], Loss: 2.109959602355957\n",
      "Train: Epoch [3], Batch [263/938], Loss: 2.082017660140991\n",
      "Train: Epoch [3], Batch [264/938], Loss: 2.1262505054473877\n",
      "Train: Epoch [3], Batch [265/938], Loss: 2.126401901245117\n",
      "Train: Epoch [3], Batch [266/938], Loss: 2.1131668090820312\n",
      "Train: Epoch [3], Batch [267/938], Loss: 2.0625600814819336\n",
      "Train: Epoch [3], Batch [268/938], Loss: 2.101909637451172\n",
      "Train: Epoch [3], Batch [269/938], Loss: 2.0715749263763428\n",
      "Train: Epoch [3], Batch [270/938], Loss: 2.1319727897644043\n",
      "Train: Epoch [3], Batch [271/938], Loss: 2.090116262435913\n",
      "Train: Epoch [3], Batch [272/938], Loss: 2.0866880416870117\n",
      "Train: Epoch [3], Batch [273/938], Loss: 2.0678114891052246\n",
      "Train: Epoch [3], Batch [274/938], Loss: 2.0941760540008545\n",
      "Train: Epoch [3], Batch [275/938], Loss: 2.052212953567505\n",
      "Train: Epoch [3], Batch [276/938], Loss: 2.0715463161468506\n",
      "Train: Epoch [3], Batch [277/938], Loss: 2.070930242538452\n",
      "Train: Epoch [3], Batch [278/938], Loss: 2.081188201904297\n",
      "Train: Epoch [3], Batch [279/938], Loss: 1.9960278272628784\n",
      "Train: Epoch [3], Batch [280/938], Loss: 2.0999913215637207\n",
      "Train: Epoch [3], Batch [281/938], Loss: 2.1336474418640137\n",
      "Train: Epoch [3], Batch [282/938], Loss: 2.0815141201019287\n",
      "Train: Epoch [3], Batch [283/938], Loss: 2.118276357650757\n",
      "Train: Epoch [3], Batch [284/938], Loss: 2.1218676567077637\n",
      "Train: Epoch [3], Batch [285/938], Loss: 2.0696394443511963\n",
      "Train: Epoch [3], Batch [286/938], Loss: 2.0520575046539307\n",
      "Train: Epoch [3], Batch [287/938], Loss: 2.0503032207489014\n",
      "Train: Epoch [3], Batch [288/938], Loss: 2.183194398880005\n",
      "Train: Epoch [3], Batch [289/938], Loss: 2.114321708679199\n",
      "Train: Epoch [3], Batch [290/938], Loss: 2.0561695098876953\n",
      "Train: Epoch [3], Batch [291/938], Loss: 2.0891034603118896\n",
      "Train: Epoch [3], Batch [292/938], Loss: 2.1145379543304443\n",
      "Train: Epoch [3], Batch [293/938], Loss: 2.0850870609283447\n",
      "Train: Epoch [3], Batch [294/938], Loss: 2.085623025894165\n",
      "Train: Epoch [3], Batch [295/938], Loss: 2.0985429286956787\n",
      "Train: Epoch [3], Batch [296/938], Loss: 2.091237783432007\n",
      "Train: Epoch [3], Batch [297/938], Loss: 2.0433685779571533\n",
      "Train: Epoch [3], Batch [298/938], Loss: 2.0478572845458984\n",
      "Train: Epoch [3], Batch [299/938], Loss: 2.0771353244781494\n",
      "Train: Epoch [3], Batch [300/938], Loss: 2.069816827774048\n",
      "Train: Epoch [3], Batch [301/938], Loss: 2.0758888721466064\n",
      "Train: Epoch [3], Batch [302/938], Loss: 2.0086240768432617\n",
      "Train: Epoch [3], Batch [303/938], Loss: 2.0851821899414062\n",
      "Train: Epoch [3], Batch [304/938], Loss: 2.077362537384033\n",
      "Train: Epoch [3], Batch [305/938], Loss: 2.1363162994384766\n",
      "Train: Epoch [3], Batch [306/938], Loss: 2.0610735416412354\n",
      "Train: Epoch [3], Batch [307/938], Loss: 2.1000306606292725\n",
      "Train: Epoch [3], Batch [308/938], Loss: 2.114434003829956\n",
      "Train: Epoch [3], Batch [309/938], Loss: 2.069810390472412\n",
      "Train: Epoch [3], Batch [310/938], Loss: 2.013495683670044\n",
      "Train: Epoch [3], Batch [311/938], Loss: 2.124330997467041\n",
      "Train: Epoch [3], Batch [312/938], Loss: 2.06107759475708\n",
      "Train: Epoch [3], Batch [313/938], Loss: 2.067246437072754\n",
      "Train: Epoch [3], Batch [314/938], Loss: 2.0635194778442383\n",
      "Train: Epoch [3], Batch [315/938], Loss: 2.119946002960205\n",
      "Train: Epoch [3], Batch [316/938], Loss: 2.0684080123901367\n",
      "Train: Epoch [3], Batch [317/938], Loss: 2.107048511505127\n",
      "Train: Epoch [3], Batch [318/938], Loss: 2.0239651203155518\n",
      "Train: Epoch [3], Batch [319/938], Loss: 2.0533392429351807\n",
      "Train: Epoch [3], Batch [320/938], Loss: 2.029500722885132\n",
      "Train: Epoch [3], Batch [321/938], Loss: 2.052995443344116\n",
      "Train: Epoch [3], Batch [322/938], Loss: 2.0634188652038574\n",
      "Train: Epoch [3], Batch [323/938], Loss: 2.180140495300293\n",
      "Train: Epoch [3], Batch [324/938], Loss: 2.1558918952941895\n",
      "Train: Epoch [3], Batch [325/938], Loss: 2.0651419162750244\n",
      "Train: Epoch [3], Batch [326/938], Loss: 2.0327091217041016\n",
      "Train: Epoch [3], Batch [327/938], Loss: 2.0033531188964844\n",
      "Train: Epoch [3], Batch [328/938], Loss: 2.04659366607666\n",
      "Train: Epoch [3], Batch [329/938], Loss: 1.991316795349121\n",
      "Train: Epoch [3], Batch [330/938], Loss: 2.0253236293792725\n",
      "Train: Epoch [3], Batch [331/938], Loss: 2.06083345413208\n",
      "Train: Epoch [3], Batch [332/938], Loss: 2.0414180755615234\n",
      "Train: Epoch [3], Batch [333/938], Loss: 2.0640933513641357\n",
      "Train: Epoch [3], Batch [334/938], Loss: 2.0563998222351074\n",
      "Train: Epoch [3], Batch [335/938], Loss: 2.0950028896331787\n",
      "Train: Epoch [3], Batch [336/938], Loss: 2.0239639282226562\n",
      "Train: Epoch [3], Batch [337/938], Loss: 2.041198253631592\n",
      "Train: Epoch [3], Batch [338/938], Loss: 2.113826274871826\n",
      "Train: Epoch [3], Batch [339/938], Loss: 1.9709289073944092\n",
      "Train: Epoch [3], Batch [340/938], Loss: 2.0406177043914795\n",
      "Train: Epoch [3], Batch [341/938], Loss: 2.06365704536438\n",
      "Train: Epoch [3], Batch [342/938], Loss: 2.0484423637390137\n",
      "Train: Epoch [3], Batch [343/938], Loss: 2.0977351665496826\n",
      "Train: Epoch [3], Batch [344/938], Loss: 2.0382702350616455\n",
      "Train: Epoch [3], Batch [345/938], Loss: 2.088564872741699\n",
      "Train: Epoch [3], Batch [346/938], Loss: 2.029454231262207\n",
      "Train: Epoch [3], Batch [347/938], Loss: 2.022632360458374\n",
      "Train: Epoch [3], Batch [348/938], Loss: 2.015279531478882\n",
      "Train: Epoch [3], Batch [349/938], Loss: 2.0518832206726074\n",
      "Train: Epoch [3], Batch [350/938], Loss: 2.0616953372955322\n",
      "Train: Epoch [3], Batch [351/938], Loss: 1.9939968585968018\n",
      "Train: Epoch [3], Batch [352/938], Loss: 2.0354135036468506\n",
      "Train: Epoch [3], Batch [353/938], Loss: 2.0353753566741943\n",
      "Train: Epoch [3], Batch [354/938], Loss: 2.0641136169433594\n",
      "Train: Epoch [3], Batch [355/938], Loss: 2.0293729305267334\n",
      "Train: Epoch [3], Batch [356/938], Loss: 2.0455307960510254\n",
      "Train: Epoch [3], Batch [357/938], Loss: 2.0334131717681885\n",
      "Train: Epoch [3], Batch [358/938], Loss: 2.0038583278656006\n",
      "Train: Epoch [3], Batch [359/938], Loss: 2.0812087059020996\n",
      "Train: Epoch [3], Batch [360/938], Loss: 2.04105281829834\n",
      "Train: Epoch [3], Batch [361/938], Loss: 2.0679073333740234\n",
      "Train: Epoch [3], Batch [362/938], Loss: 2.021803617477417\n",
      "Train: Epoch [3], Batch [363/938], Loss: 2.035310745239258\n",
      "Train: Epoch [3], Batch [364/938], Loss: 2.0175247192382812\n",
      "Train: Epoch [3], Batch [365/938], Loss: 2.0767855644226074\n",
      "Train: Epoch [3], Batch [366/938], Loss: 2.0162265300750732\n",
      "Train: Epoch [3], Batch [367/938], Loss: 2.0350823402404785\n",
      "Train: Epoch [3], Batch [368/938], Loss: 2.003037691116333\n",
      "Train: Epoch [3], Batch [369/938], Loss: 2.0530266761779785\n",
      "Train: Epoch [3], Batch [370/938], Loss: 2.0565102100372314\n",
      "Train: Epoch [3], Batch [371/938], Loss: 2.016265392303467\n",
      "Train: Epoch [3], Batch [372/938], Loss: 2.0241646766662598\n",
      "Train: Epoch [3], Batch [373/938], Loss: 1.9884852170944214\n",
      "Train: Epoch [3], Batch [374/938], Loss: 2.0251965522766113\n",
      "Train: Epoch [3], Batch [375/938], Loss: 2.138545274734497\n",
      "Train: Epoch [3], Batch [376/938], Loss: 1.9542540311813354\n",
      "Train: Epoch [3], Batch [377/938], Loss: 2.0278825759887695\n",
      "Train: Epoch [3], Batch [378/938], Loss: 2.020702600479126\n",
      "Train: Epoch [3], Batch [379/938], Loss: 2.003139019012451\n",
      "Train: Epoch [3], Batch [380/938], Loss: 2.134324073791504\n",
      "Train: Epoch [3], Batch [381/938], Loss: 1.960053563117981\n",
      "Train: Epoch [3], Batch [382/938], Loss: 2.018385410308838\n",
      "Train: Epoch [3], Batch [383/938], Loss: 2.0726265907287598\n",
      "Train: Epoch [3], Batch [384/938], Loss: 2.0641627311706543\n",
      "Train: Epoch [3], Batch [385/938], Loss: 2.0068020820617676\n",
      "Train: Epoch [3], Batch [386/938], Loss: 1.9909814596176147\n",
      "Train: Epoch [3], Batch [387/938], Loss: 2.0012314319610596\n",
      "Train: Epoch [3], Batch [388/938], Loss: 2.0630898475646973\n",
      "Train: Epoch [3], Batch [389/938], Loss: 2.013916015625\n",
      "Train: Epoch [3], Batch [390/938], Loss: 2.024170398712158\n",
      "Train: Epoch [3], Batch [391/938], Loss: 2.019657611846924\n",
      "Train: Epoch [3], Batch [392/938], Loss: 2.0178539752960205\n",
      "Train: Epoch [3], Batch [393/938], Loss: 1.9894754886627197\n",
      "Train: Epoch [3], Batch [394/938], Loss: 2.032439708709717\n",
      "Train: Epoch [3], Batch [395/938], Loss: 2.144923686981201\n",
      "Train: Epoch [3], Batch [396/938], Loss: 2.0372955799102783\n",
      "Train: Epoch [3], Batch [397/938], Loss: 1.9874415397644043\n",
      "Train: Epoch [3], Batch [398/938], Loss: 2.0589778423309326\n",
      "Train: Epoch [3], Batch [399/938], Loss: 1.997515320777893\n",
      "Train: Epoch [3], Batch [400/938], Loss: 2.0514159202575684\n",
      "Train: Epoch [3], Batch [401/938], Loss: 2.126443862915039\n",
      "Train: Epoch [3], Batch [402/938], Loss: 2.0178439617156982\n",
      "Train: Epoch [3], Batch [403/938], Loss: 2.0772058963775635\n",
      "Train: Epoch [3], Batch [404/938], Loss: 1.9463896751403809\n",
      "Train: Epoch [3], Batch [405/938], Loss: 2.056166172027588\n",
      "Train: Epoch [3], Batch [406/938], Loss: 2.0935535430908203\n",
      "Train: Epoch [3], Batch [407/938], Loss: 1.95155668258667\n",
      "Train: Epoch [3], Batch [408/938], Loss: 1.9717638492584229\n",
      "Train: Epoch [3], Batch [409/938], Loss: 1.98444664478302\n",
      "Train: Epoch [3], Batch [410/938], Loss: 2.0026769638061523\n",
      "Train: Epoch [3], Batch [411/938], Loss: 1.9625283479690552\n",
      "Train: Epoch [3], Batch [412/938], Loss: 1.9900860786437988\n",
      "Train: Epoch [3], Batch [413/938], Loss: 1.9623281955718994\n",
      "Train: Epoch [3], Batch [414/938], Loss: 2.029043197631836\n",
      "Train: Epoch [3], Batch [415/938], Loss: 1.9267634153366089\n",
      "Train: Epoch [3], Batch [416/938], Loss: 1.9510399103164673\n",
      "Train: Epoch [3], Batch [417/938], Loss: 2.0538809299468994\n",
      "Train: Epoch [3], Batch [418/938], Loss: 2.0216970443725586\n",
      "Train: Epoch [3], Batch [419/938], Loss: 1.953493356704712\n",
      "Train: Epoch [3], Batch [420/938], Loss: 2.024718999862671\n",
      "Train: Epoch [3], Batch [421/938], Loss: 2.038217306137085\n",
      "Train: Epoch [3], Batch [422/938], Loss: 1.94346284866333\n",
      "Train: Epoch [3], Batch [423/938], Loss: 1.9520368576049805\n",
      "Train: Epoch [3], Batch [424/938], Loss: 2.0100483894348145\n",
      "Train: Epoch [3], Batch [425/938], Loss: 2.031435012817383\n",
      "Train: Epoch [3], Batch [426/938], Loss: 2.0382394790649414\n",
      "Train: Epoch [3], Batch [427/938], Loss: 2.0385468006134033\n",
      "Train: Epoch [3], Batch [428/938], Loss: 2.04127836227417\n",
      "Train: Epoch [3], Batch [429/938], Loss: 1.9502391815185547\n",
      "Train: Epoch [3], Batch [430/938], Loss: 1.9950470924377441\n",
      "Train: Epoch [3], Batch [431/938], Loss: 1.9585871696472168\n",
      "Train: Epoch [3], Batch [432/938], Loss: 2.017897367477417\n",
      "Train: Epoch [3], Batch [433/938], Loss: 1.9965192079544067\n",
      "Train: Epoch [3], Batch [434/938], Loss: 2.0206139087677\n",
      "Train: Epoch [3], Batch [435/938], Loss: 1.9495267868041992\n",
      "Train: Epoch [3], Batch [436/938], Loss: 1.949743628501892\n",
      "Train: Epoch [3], Batch [437/938], Loss: 1.9843772649765015\n",
      "Train: Epoch [3], Batch [438/938], Loss: 2.0140297412872314\n",
      "Train: Epoch [3], Batch [439/938], Loss: 1.9901585578918457\n",
      "Train: Epoch [3], Batch [440/938], Loss: 1.987516164779663\n",
      "Train: Epoch [3], Batch [441/938], Loss: 1.9674054384231567\n",
      "Train: Epoch [3], Batch [442/938], Loss: 1.932032585144043\n",
      "Train: Epoch [3], Batch [443/938], Loss: 2.0150275230407715\n",
      "Train: Epoch [3], Batch [444/938], Loss: 1.9394291639328003\n",
      "Train: Epoch [3], Batch [445/938], Loss: 2.0134451389312744\n",
      "Train: Epoch [3], Batch [446/938], Loss: 1.9857292175292969\n",
      "Train: Epoch [3], Batch [447/938], Loss: 1.949435830116272\n",
      "Train: Epoch [3], Batch [448/938], Loss: 1.9236351251602173\n",
      "Train: Epoch [3], Batch [449/938], Loss: 1.8965017795562744\n",
      "Train: Epoch [3], Batch [450/938], Loss: 1.9627172946929932\n",
      "Train: Epoch [3], Batch [451/938], Loss: 1.9673596620559692\n",
      "Train: Epoch [3], Batch [452/938], Loss: 2.004749298095703\n",
      "Train: Epoch [3], Batch [453/938], Loss: 1.9590439796447754\n",
      "Train: Epoch [3], Batch [454/938], Loss: 1.908888578414917\n",
      "Train: Epoch [3], Batch [455/938], Loss: 1.954980492591858\n",
      "Train: Epoch [3], Batch [456/938], Loss: 1.9675936698913574\n",
      "Train: Epoch [3], Batch [457/938], Loss: 2.020996332168579\n",
      "Train: Epoch [3], Batch [458/938], Loss: 1.9913216829299927\n",
      "Train: Epoch [3], Batch [459/938], Loss: 1.911586880683899\n",
      "Train: Epoch [3], Batch [460/938], Loss: 2.0138304233551025\n",
      "Train: Epoch [3], Batch [461/938], Loss: 1.966265082359314\n",
      "Train: Epoch [3], Batch [462/938], Loss: 1.9511405229568481\n",
      "Train: Epoch [3], Batch [463/938], Loss: 1.9390840530395508\n",
      "Train: Epoch [3], Batch [464/938], Loss: 1.9837524890899658\n",
      "Train: Epoch [3], Batch [465/938], Loss: 2.043833017349243\n",
      "Train: Epoch [3], Batch [466/938], Loss: 1.992509365081787\n",
      "Train: Epoch [3], Batch [467/938], Loss: 1.9408775568008423\n",
      "Train: Epoch [3], Batch [468/938], Loss: 1.9554719924926758\n",
      "Train: Epoch [3], Batch [469/938], Loss: 1.913236379623413\n",
      "Train: Epoch [3], Batch [470/938], Loss: 1.9982926845550537\n",
      "Train: Epoch [3], Batch [471/938], Loss: 1.8779551982879639\n",
      "Train: Epoch [3], Batch [472/938], Loss: 1.9639579057693481\n",
      "Train: Epoch [3], Batch [473/938], Loss: 1.9722568988800049\n",
      "Train: Epoch [3], Batch [474/938], Loss: 1.8786565065383911\n",
      "Train: Epoch [3], Batch [475/938], Loss: 1.858504295349121\n",
      "Train: Epoch [3], Batch [476/938], Loss: 1.8922276496887207\n",
      "Train: Epoch [3], Batch [477/938], Loss: 1.9552491903305054\n",
      "Train: Epoch [3], Batch [478/938], Loss: 2.041423797607422\n",
      "Train: Epoch [3], Batch [479/938], Loss: 1.9148495197296143\n",
      "Train: Epoch [3], Batch [480/938], Loss: 1.9317723512649536\n",
      "Train: Epoch [3], Batch [481/938], Loss: 1.9010956287384033\n",
      "Train: Epoch [3], Batch [482/938], Loss: 1.9405730962753296\n",
      "Train: Epoch [3], Batch [483/938], Loss: 1.877636432647705\n",
      "Train: Epoch [3], Batch [484/938], Loss: 2.0061511993408203\n",
      "Train: Epoch [3], Batch [485/938], Loss: 1.93074369430542\n",
      "Train: Epoch [3], Batch [486/938], Loss: 1.895505666732788\n",
      "Train: Epoch [3], Batch [487/938], Loss: 1.998593807220459\n",
      "Train: Epoch [3], Batch [488/938], Loss: 1.8762421607971191\n",
      "Train: Epoch [3], Batch [489/938], Loss: 1.9516263008117676\n",
      "Train: Epoch [3], Batch [490/938], Loss: 1.9257514476776123\n",
      "Train: Epoch [3], Batch [491/938], Loss: 1.9521498680114746\n",
      "Train: Epoch [3], Batch [492/938], Loss: 1.8829046487808228\n",
      "Train: Epoch [3], Batch [493/938], Loss: 1.9462559223175049\n",
      "Train: Epoch [3], Batch [494/938], Loss: 1.9598746299743652\n",
      "Train: Epoch [3], Batch [495/938], Loss: 1.902090311050415\n",
      "Train: Epoch [3], Batch [496/938], Loss: 1.8875763416290283\n",
      "Train: Epoch [3], Batch [497/938], Loss: 1.8865195512771606\n",
      "Train: Epoch [3], Batch [498/938], Loss: 1.838310956954956\n",
      "Train: Epoch [3], Batch [499/938], Loss: 1.8421598672866821\n",
      "Train: Epoch [3], Batch [500/938], Loss: 1.79832923412323\n",
      "Train: Epoch [3], Batch [501/938], Loss: 1.896396279335022\n",
      "Train: Epoch [3], Batch [502/938], Loss: 1.9178194999694824\n",
      "Train: Epoch [3], Batch [503/938], Loss: 1.9762568473815918\n",
      "Train: Epoch [3], Batch [504/938], Loss: 1.886823058128357\n",
      "Train: Epoch [3], Batch [505/938], Loss: 1.9148101806640625\n",
      "Train: Epoch [3], Batch [506/938], Loss: 1.8112633228302002\n",
      "Train: Epoch [3], Batch [507/938], Loss: 1.8309111595153809\n",
      "Train: Epoch [3], Batch [508/938], Loss: 1.9749788045883179\n",
      "Train: Epoch [3], Batch [509/938], Loss: 1.803637981414795\n",
      "Train: Epoch [3], Batch [510/938], Loss: 1.9435619115829468\n",
      "Train: Epoch [3], Batch [511/938], Loss: 1.9391660690307617\n",
      "Train: Epoch [3], Batch [512/938], Loss: 1.8579208850860596\n",
      "Train: Epoch [3], Batch [513/938], Loss: 1.9576210975646973\n",
      "Train: Epoch [3], Batch [514/938], Loss: 1.9633400440216064\n",
      "Train: Epoch [3], Batch [515/938], Loss: 1.8924452066421509\n",
      "Train: Epoch [3], Batch [516/938], Loss: 1.9406867027282715\n",
      "Train: Epoch [3], Batch [517/938], Loss: 1.8604856729507446\n",
      "Train: Epoch [3], Batch [518/938], Loss: 1.877349615097046\n",
      "Train: Epoch [3], Batch [519/938], Loss: 1.8518555164337158\n",
      "Train: Epoch [3], Batch [520/938], Loss: 1.8051118850708008\n",
      "Train: Epoch [3], Batch [521/938], Loss: 1.9021327495574951\n",
      "Train: Epoch [3], Batch [522/938], Loss: 1.9543452262878418\n",
      "Train: Epoch [3], Batch [523/938], Loss: 1.9235738515853882\n",
      "Train: Epoch [3], Batch [524/938], Loss: 1.9243133068084717\n",
      "Train: Epoch [3], Batch [525/938], Loss: 1.9103114604949951\n",
      "Train: Epoch [3], Batch [526/938], Loss: 1.9635392427444458\n",
      "Train: Epoch [3], Batch [527/938], Loss: 1.9234731197357178\n",
      "Train: Epoch [3], Batch [528/938], Loss: 1.7902215719223022\n",
      "Train: Epoch [3], Batch [529/938], Loss: 2.0063958168029785\n",
      "Train: Epoch [3], Batch [530/938], Loss: 1.8516106605529785\n",
      "Train: Epoch [3], Batch [531/938], Loss: 1.9682894945144653\n",
      "Train: Epoch [3], Batch [532/938], Loss: 1.7754552364349365\n",
      "Train: Epoch [3], Batch [533/938], Loss: 1.9343534708023071\n",
      "Train: Epoch [3], Batch [534/938], Loss: 1.8791486024856567\n",
      "Train: Epoch [3], Batch [535/938], Loss: 2.003298282623291\n",
      "Train: Epoch [3], Batch [536/938], Loss: 1.830870270729065\n",
      "Train: Epoch [3], Batch [537/938], Loss: 2.0029473304748535\n",
      "Train: Epoch [3], Batch [538/938], Loss: 1.8973482847213745\n",
      "Train: Epoch [3], Batch [539/938], Loss: 1.917389154434204\n",
      "Train: Epoch [3], Batch [540/938], Loss: 1.834647536277771\n",
      "Train: Epoch [3], Batch [541/938], Loss: 1.7634243965148926\n",
      "Train: Epoch [3], Batch [542/938], Loss: 1.8916776180267334\n",
      "Train: Epoch [3], Batch [543/938], Loss: 1.9589706659317017\n",
      "Train: Epoch [3], Batch [544/938], Loss: 1.823486566543579\n",
      "Train: Epoch [3], Batch [545/938], Loss: 2.0201494693756104\n",
      "Train: Epoch [3], Batch [546/938], Loss: 1.9156274795532227\n",
      "Train: Epoch [3], Batch [547/938], Loss: 1.824533462524414\n",
      "Train: Epoch [3], Batch [548/938], Loss: 1.81756591796875\n",
      "Train: Epoch [3], Batch [549/938], Loss: 1.7992212772369385\n",
      "Train: Epoch [3], Batch [550/938], Loss: 1.7728842496871948\n",
      "Train: Epoch [3], Batch [551/938], Loss: 1.9315813779830933\n",
      "Train: Epoch [3], Batch [552/938], Loss: 1.8985891342163086\n",
      "Train: Epoch [3], Batch [553/938], Loss: 1.8925774097442627\n",
      "Train: Epoch [3], Batch [554/938], Loss: 1.856158971786499\n",
      "Train: Epoch [3], Batch [555/938], Loss: 1.7583684921264648\n",
      "Train: Epoch [3], Batch [556/938], Loss: 1.8748804330825806\n",
      "Train: Epoch [3], Batch [557/938], Loss: 1.8640460968017578\n",
      "Train: Epoch [3], Batch [558/938], Loss: 1.822732925415039\n",
      "Train: Epoch [3], Batch [559/938], Loss: 1.9429301023483276\n",
      "Train: Epoch [3], Batch [560/938], Loss: 1.809924602508545\n",
      "Train: Epoch [3], Batch [561/938], Loss: 1.6882508993148804\n",
      "Train: Epoch [3], Batch [562/938], Loss: 1.9014086723327637\n",
      "Train: Epoch [3], Batch [563/938], Loss: 2.008808135986328\n",
      "Train: Epoch [3], Batch [564/938], Loss: 1.8655786514282227\n",
      "Train: Epoch [3], Batch [565/938], Loss: 1.9584494829177856\n",
      "Train: Epoch [3], Batch [566/938], Loss: 1.7727588415145874\n",
      "Train: Epoch [3], Batch [567/938], Loss: 1.9674110412597656\n",
      "Train: Epoch [3], Batch [568/938], Loss: 1.7367069721221924\n",
      "Train: Epoch [3], Batch [569/938], Loss: 1.989224910736084\n",
      "Train: Epoch [3], Batch [570/938], Loss: 1.9283998012542725\n",
      "Train: Epoch [3], Batch [571/938], Loss: 1.7227108478546143\n",
      "Train: Epoch [3], Batch [572/938], Loss: 1.806528925895691\n",
      "Train: Epoch [3], Batch [573/938], Loss: 2.0120460987091064\n",
      "Train: Epoch [3], Batch [574/938], Loss: 1.9101382493972778\n",
      "Train: Epoch [3], Batch [575/938], Loss: 1.9238296747207642\n",
      "Train: Epoch [3], Batch [576/938], Loss: 1.9387248754501343\n",
      "Train: Epoch [3], Batch [577/938], Loss: 1.8168541193008423\n",
      "Train: Epoch [3], Batch [578/938], Loss: 1.7954990863800049\n",
      "Train: Epoch [3], Batch [579/938], Loss: 1.8426252603530884\n",
      "Train: Epoch [3], Batch [580/938], Loss: 1.9028451442718506\n",
      "Train: Epoch [3], Batch [581/938], Loss: 1.818574070930481\n",
      "Train: Epoch [3], Batch [582/938], Loss: 1.9852591753005981\n",
      "Train: Epoch [3], Batch [583/938], Loss: 1.8136793375015259\n",
      "Train: Epoch [3], Batch [584/938], Loss: 1.8923423290252686\n",
      "Train: Epoch [3], Batch [585/938], Loss: 1.7401785850524902\n",
      "Train: Epoch [3], Batch [586/938], Loss: 1.8694372177124023\n",
      "Train: Epoch [3], Batch [587/938], Loss: 1.8215305805206299\n",
      "Train: Epoch [3], Batch [588/938], Loss: 1.9083912372589111\n",
      "Train: Epoch [3], Batch [589/938], Loss: 1.8946969509124756\n",
      "Train: Epoch [3], Batch [590/938], Loss: 1.7501195669174194\n",
      "Train: Epoch [3], Batch [591/938], Loss: 1.7811996936798096\n",
      "Train: Epoch [3], Batch [592/938], Loss: 1.824397325515747\n",
      "Train: Epoch [3], Batch [593/938], Loss: 1.7299163341522217\n",
      "Train: Epoch [3], Batch [594/938], Loss: 1.7393684387207031\n",
      "Train: Epoch [3], Batch [595/938], Loss: 1.9805357456207275\n",
      "Train: Epoch [3], Batch [596/938], Loss: 1.728615641593933\n",
      "Train: Epoch [3], Batch [597/938], Loss: 1.8384233713150024\n",
      "Train: Epoch [3], Batch [598/938], Loss: 1.7646684646606445\n",
      "Train: Epoch [3], Batch [599/938], Loss: 1.8756564855575562\n",
      "Train: Epoch [3], Batch [600/938], Loss: 1.8239433765411377\n",
      "Train: Epoch [3], Batch [601/938], Loss: 2.016526460647583\n",
      "Train: Epoch [3], Batch [602/938], Loss: 1.9886548519134521\n",
      "Train: Epoch [3], Batch [603/938], Loss: 1.9473329782485962\n",
      "Train: Epoch [3], Batch [604/938], Loss: 1.9125503301620483\n",
      "Train: Epoch [3], Batch [605/938], Loss: 1.8007159233093262\n",
      "Train: Epoch [3], Batch [606/938], Loss: 1.8942201137542725\n",
      "Train: Epoch [3], Batch [607/938], Loss: 1.8019615411758423\n",
      "Train: Epoch [3], Batch [608/938], Loss: 1.9943934679031372\n",
      "Train: Epoch [3], Batch [609/938], Loss: 1.8330130577087402\n",
      "Train: Epoch [3], Batch [610/938], Loss: 1.9879159927368164\n",
      "Train: Epoch [3], Batch [611/938], Loss: 1.8326685428619385\n",
      "Train: Epoch [3], Batch [612/938], Loss: 1.7696772813796997\n",
      "Train: Epoch [3], Batch [613/938], Loss: 1.6727100610733032\n",
      "Train: Epoch [3], Batch [614/938], Loss: 1.8159756660461426\n",
      "Train: Epoch [3], Batch [615/938], Loss: 1.682163119316101\n",
      "Train: Epoch [3], Batch [616/938], Loss: 1.6798095703125\n",
      "Train: Epoch [3], Batch [617/938], Loss: 1.68994140625\n",
      "Train: Epoch [3], Batch [618/938], Loss: 1.7943388223648071\n",
      "Train: Epoch [3], Batch [619/938], Loss: 1.834317922592163\n",
      "Train: Epoch [3], Batch [620/938], Loss: 1.8413935899734497\n",
      "Train: Epoch [3], Batch [621/938], Loss: 1.8820827007293701\n",
      "Train: Epoch [3], Batch [622/938], Loss: 1.774219036102295\n",
      "Train: Epoch [3], Batch [623/938], Loss: 1.9112346172332764\n",
      "Train: Epoch [3], Batch [624/938], Loss: 1.7757214307785034\n",
      "Train: Epoch [3], Batch [625/938], Loss: 1.8701387643814087\n",
      "Train: Epoch [3], Batch [626/938], Loss: 1.9543315172195435\n",
      "Train: Epoch [3], Batch [627/938], Loss: 1.6822434663772583\n",
      "Train: Epoch [3], Batch [628/938], Loss: 1.7376388311386108\n",
      "Train: Epoch [3], Batch [629/938], Loss: 1.7112348079681396\n",
      "Train: Epoch [3], Batch [630/938], Loss: 2.0098140239715576\n",
      "Train: Epoch [3], Batch [631/938], Loss: 1.9940791130065918\n",
      "Train: Epoch [3], Batch [632/938], Loss: 1.8398149013519287\n",
      "Train: Epoch [3], Batch [633/938], Loss: 1.8071520328521729\n",
      "Train: Epoch [3], Batch [634/938], Loss: 1.7578167915344238\n",
      "Train: Epoch [3], Batch [635/938], Loss: 1.8679744005203247\n",
      "Train: Epoch [3], Batch [636/938], Loss: 1.7721203565597534\n",
      "Train: Epoch [3], Batch [637/938], Loss: 1.9668300151824951\n",
      "Train: Epoch [3], Batch [638/938], Loss: 1.8112295866012573\n",
      "Train: Epoch [3], Batch [639/938], Loss: 1.707364797592163\n",
      "Train: Epoch [3], Batch [640/938], Loss: 1.7891491651535034\n",
      "Train: Epoch [3], Batch [641/938], Loss: 1.8315942287445068\n",
      "Train: Epoch [3], Batch [642/938], Loss: 1.8245466947555542\n",
      "Train: Epoch [3], Batch [643/938], Loss: 1.876981258392334\n",
      "Train: Epoch [3], Batch [644/938], Loss: 1.871708869934082\n",
      "Train: Epoch [3], Batch [645/938], Loss: 1.7567076683044434\n",
      "Train: Epoch [3], Batch [646/938], Loss: 1.8030338287353516\n",
      "Train: Epoch [3], Batch [647/938], Loss: 1.8666961193084717\n",
      "Train: Epoch [3], Batch [648/938], Loss: 1.902343988418579\n",
      "Train: Epoch [3], Batch [649/938], Loss: 1.8306337594985962\n",
      "Train: Epoch [3], Batch [650/938], Loss: 1.7513391971588135\n",
      "Train: Epoch [3], Batch [651/938], Loss: 1.8807196617126465\n",
      "Train: Epoch [3], Batch [652/938], Loss: 1.7288259267807007\n",
      "Train: Epoch [3], Batch [653/938], Loss: 1.7709665298461914\n",
      "Train: Epoch [3], Batch [654/938], Loss: 1.7816240787506104\n",
      "Train: Epoch [3], Batch [655/938], Loss: 1.8478833436965942\n",
      "Train: Epoch [3], Batch [656/938], Loss: 1.822301983833313\n",
      "Train: Epoch [3], Batch [657/938], Loss: 1.7770931720733643\n",
      "Train: Epoch [3], Batch [658/938], Loss: 1.9104653596878052\n",
      "Train: Epoch [3], Batch [659/938], Loss: 1.8258098363876343\n",
      "Train: Epoch [3], Batch [660/938], Loss: 1.7648661136627197\n",
      "Train: Epoch [3], Batch [661/938], Loss: 1.8075177669525146\n",
      "Train: Epoch [3], Batch [662/938], Loss: 1.8260011672973633\n",
      "Train: Epoch [3], Batch [663/938], Loss: 1.7803901433944702\n",
      "Train: Epoch [3], Batch [664/938], Loss: 1.9122225046157837\n",
      "Train: Epoch [3], Batch [665/938], Loss: 1.787485957145691\n",
      "Train: Epoch [3], Batch [666/938], Loss: 1.8424322605133057\n",
      "Train: Epoch [3], Batch [667/938], Loss: 1.7770116329193115\n",
      "Train: Epoch [3], Batch [668/938], Loss: 1.8534934520721436\n",
      "Train: Epoch [3], Batch [669/938], Loss: 1.7620524168014526\n",
      "Train: Epoch [3], Batch [670/938], Loss: 1.8347588777542114\n",
      "Train: Epoch [3], Batch [671/938], Loss: 1.804125428199768\n",
      "Train: Epoch [3], Batch [672/938], Loss: 1.6547434329986572\n",
      "Train: Epoch [3], Batch [673/938], Loss: 1.8613924980163574\n",
      "Train: Epoch [3], Batch [674/938], Loss: 1.9584174156188965\n",
      "Train: Epoch [3], Batch [675/938], Loss: 1.7197930812835693\n",
      "Train: Epoch [3], Batch [676/938], Loss: 1.8208274841308594\n",
      "Train: Epoch [3], Batch [677/938], Loss: 1.7264755964279175\n",
      "Train: Epoch [3], Batch [678/938], Loss: 1.8611199855804443\n",
      "Train: Epoch [3], Batch [679/938], Loss: 1.8191142082214355\n",
      "Train: Epoch [3], Batch [680/938], Loss: 1.7351891994476318\n",
      "Train: Epoch [3], Batch [681/938], Loss: 1.8443987369537354\n",
      "Train: Epoch [3], Batch [682/938], Loss: 1.7275300025939941\n",
      "Train: Epoch [3], Batch [683/938], Loss: 1.8991674184799194\n",
      "Train: Epoch [3], Batch [684/938], Loss: 1.7689350843429565\n",
      "Train: Epoch [3], Batch [685/938], Loss: 1.8276479244232178\n",
      "Train: Epoch [3], Batch [686/938], Loss: 1.7031095027923584\n",
      "Train: Epoch [3], Batch [687/938], Loss: 1.9121966361999512\n",
      "Train: Epoch [3], Batch [688/938], Loss: 1.8666925430297852\n",
      "Train: Epoch [3], Batch [689/938], Loss: 1.7979730367660522\n",
      "Train: Epoch [3], Batch [690/938], Loss: 1.8319389820098877\n",
      "Train: Epoch [3], Batch [691/938], Loss: 1.8200993537902832\n",
      "Train: Epoch [3], Batch [692/938], Loss: 1.7558833360671997\n",
      "Train: Epoch [3], Batch [693/938], Loss: 1.6563494205474854\n",
      "Train: Epoch [3], Batch [694/938], Loss: 1.913010597229004\n",
      "Train: Epoch [3], Batch [695/938], Loss: 1.791269302368164\n",
      "Train: Epoch [3], Batch [696/938], Loss: 1.9361401796340942\n",
      "Train: Epoch [3], Batch [697/938], Loss: 1.716679573059082\n",
      "Train: Epoch [3], Batch [698/938], Loss: 1.7778671979904175\n",
      "Train: Epoch [3], Batch [699/938], Loss: 1.7752183675765991\n",
      "Train: Epoch [3], Batch [700/938], Loss: 1.6894199848175049\n",
      "Train: Epoch [3], Batch [701/938], Loss: 1.7417527437210083\n",
      "Train: Epoch [3], Batch [702/938], Loss: 1.7575674057006836\n",
      "Train: Epoch [3], Batch [703/938], Loss: 1.6408026218414307\n",
      "Train: Epoch [3], Batch [704/938], Loss: 1.8812055587768555\n",
      "Train: Epoch [3], Batch [705/938], Loss: 1.71688711643219\n",
      "Train: Epoch [3], Batch [706/938], Loss: 1.8415919542312622\n",
      "Train: Epoch [3], Batch [707/938], Loss: 1.7724286317825317\n",
      "Train: Epoch [3], Batch [708/938], Loss: 1.7004284858703613\n",
      "Train: Epoch [3], Batch [709/938], Loss: 1.7316724061965942\n",
      "Train: Epoch [3], Batch [710/938], Loss: 1.8122093677520752\n",
      "Train: Epoch [3], Batch [711/938], Loss: 1.7661691904067993\n",
      "Train: Epoch [3], Batch [712/938], Loss: 1.7453712224960327\n",
      "Train: Epoch [3], Batch [713/938], Loss: 1.7298316955566406\n",
      "Train: Epoch [3], Batch [714/938], Loss: 1.9003112316131592\n",
      "Train: Epoch [3], Batch [715/938], Loss: 1.7540549039840698\n",
      "Train: Epoch [3], Batch [716/938], Loss: 1.6125245094299316\n",
      "Train: Epoch [3], Batch [717/938], Loss: 1.7305816411972046\n",
      "Train: Epoch [3], Batch [718/938], Loss: 1.784611463546753\n",
      "Train: Epoch [3], Batch [719/938], Loss: 1.8206408023834229\n",
      "Train: Epoch [3], Batch [720/938], Loss: 1.7341333627700806\n",
      "Train: Epoch [3], Batch [721/938], Loss: 1.867469310760498\n",
      "Train: Epoch [3], Batch [722/938], Loss: 1.916272759437561\n",
      "Train: Epoch [3], Batch [723/938], Loss: 1.761642575263977\n",
      "Train: Epoch [3], Batch [724/938], Loss: 1.7584607601165771\n",
      "Train: Epoch [3], Batch [725/938], Loss: 1.7223517894744873\n",
      "Train: Epoch [3], Batch [726/938], Loss: 1.660489559173584\n",
      "Train: Epoch [3], Batch [727/938], Loss: 1.780841588973999\n",
      "Train: Epoch [3], Batch [728/938], Loss: 1.6814000606536865\n",
      "Train: Epoch [3], Batch [729/938], Loss: 1.9234652519226074\n",
      "Train: Epoch [3], Batch [730/938], Loss: 1.8411495685577393\n",
      "Train: Epoch [3], Batch [731/938], Loss: 1.6904646158218384\n",
      "Train: Epoch [3], Batch [732/938], Loss: 1.8389968872070312\n",
      "Train: Epoch [3], Batch [733/938], Loss: 1.8875477313995361\n",
      "Train: Epoch [3], Batch [734/938], Loss: 1.9045984745025635\n",
      "Train: Epoch [3], Batch [735/938], Loss: 1.694168210029602\n",
      "Train: Epoch [3], Batch [736/938], Loss: 1.6944431066513062\n",
      "Train: Epoch [3], Batch [737/938], Loss: 1.7430280447006226\n",
      "Train: Epoch [3], Batch [738/938], Loss: 1.68502676486969\n",
      "Train: Epoch [3], Batch [739/938], Loss: 1.5701621770858765\n",
      "Train: Epoch [3], Batch [740/938], Loss: 1.6937816143035889\n",
      "Train: Epoch [3], Batch [741/938], Loss: 1.8475321531295776\n",
      "Train: Epoch [3], Batch [742/938], Loss: 1.5296416282653809\n",
      "Train: Epoch [3], Batch [743/938], Loss: 1.6891250610351562\n",
      "Train: Epoch [3], Batch [744/938], Loss: 1.7401444911956787\n",
      "Train: Epoch [3], Batch [745/938], Loss: 1.6929773092269897\n",
      "Train: Epoch [3], Batch [746/938], Loss: 1.7857102155685425\n",
      "Train: Epoch [3], Batch [747/938], Loss: 1.6382639408111572\n",
      "Train: Epoch [3], Batch [748/938], Loss: 1.7707583904266357\n",
      "Train: Epoch [3], Batch [749/938], Loss: 1.6955450773239136\n",
      "Train: Epoch [3], Batch [750/938], Loss: 1.8437334299087524\n",
      "Train: Epoch [3], Batch [751/938], Loss: 1.721463680267334\n",
      "Train: Epoch [3], Batch [752/938], Loss: 1.7047137022018433\n",
      "Train: Epoch [3], Batch [753/938], Loss: 1.7121152877807617\n",
      "Train: Epoch [3], Batch [754/938], Loss: 1.721486210823059\n",
      "Train: Epoch [3], Batch [755/938], Loss: 1.6895172595977783\n",
      "Train: Epoch [3], Batch [756/938], Loss: 1.8054986000061035\n",
      "Train: Epoch [3], Batch [757/938], Loss: 1.6598091125488281\n",
      "Train: Epoch [3], Batch [758/938], Loss: 1.7496038675308228\n",
      "Train: Epoch [3], Batch [759/938], Loss: 1.728427529335022\n",
      "Train: Epoch [3], Batch [760/938], Loss: 1.7143056392669678\n",
      "Train: Epoch [3], Batch [761/938], Loss: 1.8434100151062012\n",
      "Train: Epoch [3], Batch [762/938], Loss: 1.8476488590240479\n",
      "Train: Epoch [3], Batch [763/938], Loss: 1.7989389896392822\n",
      "Train: Epoch [3], Batch [764/938], Loss: 1.8779401779174805\n",
      "Train: Epoch [3], Batch [765/938], Loss: 1.59010648727417\n",
      "Train: Epoch [3], Batch [766/938], Loss: 1.6373355388641357\n",
      "Train: Epoch [3], Batch [767/938], Loss: 1.8087975978851318\n",
      "Train: Epoch [3], Batch [768/938], Loss: 1.6317179203033447\n",
      "Train: Epoch [3], Batch [769/938], Loss: 1.6468614339828491\n",
      "Train: Epoch [3], Batch [770/938], Loss: 1.678794026374817\n",
      "Train: Epoch [3], Batch [771/938], Loss: 1.820474624633789\n",
      "Train: Epoch [3], Batch [772/938], Loss: 1.747127890586853\n",
      "Train: Epoch [3], Batch [773/938], Loss: 1.7556607723236084\n",
      "Train: Epoch [3], Batch [774/938], Loss: 1.9156116247177124\n",
      "Train: Epoch [3], Batch [775/938], Loss: 1.625490427017212\n",
      "Train: Epoch [3], Batch [776/938], Loss: 1.6412549018859863\n",
      "Train: Epoch [3], Batch [777/938], Loss: 1.750881552696228\n",
      "Train: Epoch [3], Batch [778/938], Loss: 1.5534383058547974\n",
      "Train: Epoch [3], Batch [779/938], Loss: 1.723212480545044\n",
      "Train: Epoch [3], Batch [780/938], Loss: 1.7101396322250366\n",
      "Train: Epoch [3], Batch [781/938], Loss: 1.5496798753738403\n",
      "Train: Epoch [3], Batch [782/938], Loss: 1.9897115230560303\n",
      "Train: Epoch [3], Batch [783/938], Loss: 1.8128315210342407\n",
      "Train: Epoch [3], Batch [784/938], Loss: 1.7890583276748657\n",
      "Train: Epoch [3], Batch [785/938], Loss: 1.8808075189590454\n",
      "Train: Epoch [3], Batch [786/938], Loss: 1.6478770971298218\n",
      "Train: Epoch [3], Batch [787/938], Loss: 1.7475944757461548\n",
      "Train: Epoch [3], Batch [788/938], Loss: 1.9005860090255737\n",
      "Train: Epoch [3], Batch [789/938], Loss: 1.7402573823928833\n",
      "Train: Epoch [3], Batch [790/938], Loss: 1.7604082822799683\n",
      "Train: Epoch [3], Batch [791/938], Loss: 1.784257173538208\n",
      "Train: Epoch [3], Batch [792/938], Loss: 1.7884340286254883\n",
      "Train: Epoch [3], Batch [793/938], Loss: 1.6649893522262573\n",
      "Train: Epoch [3], Batch [794/938], Loss: 1.7924156188964844\n",
      "Train: Epoch [3], Batch [795/938], Loss: 1.7158093452453613\n",
      "Train: Epoch [3], Batch [796/938], Loss: 1.685293436050415\n",
      "Train: Epoch [3], Batch [797/938], Loss: 1.8109830617904663\n",
      "Train: Epoch [3], Batch [798/938], Loss: 1.5827305316925049\n",
      "Train: Epoch [3], Batch [799/938], Loss: 1.646451473236084\n",
      "Train: Epoch [3], Batch [800/938], Loss: 1.8280431032180786\n",
      "Train: Epoch [3], Batch [801/938], Loss: 1.669578194618225\n",
      "Train: Epoch [3], Batch [802/938], Loss: 1.6729402542114258\n",
      "Train: Epoch [3], Batch [803/938], Loss: 1.7769391536712646\n",
      "Train: Epoch [3], Batch [804/938], Loss: 1.7146254777908325\n",
      "Train: Epoch [3], Batch [805/938], Loss: 1.4511148929595947\n",
      "Train: Epoch [3], Batch [806/938], Loss: 1.7347334623336792\n",
      "Train: Epoch [3], Batch [807/938], Loss: 1.7980639934539795\n",
      "Train: Epoch [3], Batch [808/938], Loss: 1.6492220163345337\n",
      "Train: Epoch [3], Batch [809/938], Loss: 1.6268517971038818\n",
      "Train: Epoch [3], Batch [810/938], Loss: 1.778066635131836\n",
      "Train: Epoch [3], Batch [811/938], Loss: 1.5180083513259888\n",
      "Train: Epoch [3], Batch [812/938], Loss: 1.7991039752960205\n",
      "Train: Epoch [3], Batch [813/938], Loss: 1.72856867313385\n",
      "Train: Epoch [3], Batch [814/938], Loss: 1.6765365600585938\n",
      "Train: Epoch [3], Batch [815/938], Loss: 1.7953510284423828\n",
      "Train: Epoch [3], Batch [816/938], Loss: 1.6248581409454346\n",
      "Train: Epoch [3], Batch [817/938], Loss: 1.7912555932998657\n",
      "Train: Epoch [3], Batch [818/938], Loss: 1.7169427871704102\n",
      "Train: Epoch [3], Batch [819/938], Loss: 1.7139384746551514\n",
      "Train: Epoch [3], Batch [820/938], Loss: 1.6609669923782349\n",
      "Train: Epoch [3], Batch [821/938], Loss: 1.6855005025863647\n",
      "Train: Epoch [3], Batch [822/938], Loss: 1.6708533763885498\n",
      "Train: Epoch [3], Batch [823/938], Loss: 1.6999319791793823\n",
      "Train: Epoch [3], Batch [824/938], Loss: 1.6499892473220825\n",
      "Train: Epoch [3], Batch [825/938], Loss: 1.7570549249649048\n",
      "Train: Epoch [3], Batch [826/938], Loss: 1.6671299934387207\n",
      "Train: Epoch [3], Batch [827/938], Loss: 1.7848509550094604\n",
      "Train: Epoch [3], Batch [828/938], Loss: 1.6160427331924438\n",
      "Train: Epoch [3], Batch [829/938], Loss: 1.6656205654144287\n",
      "Train: Epoch [3], Batch [830/938], Loss: 1.677399754524231\n",
      "Train: Epoch [3], Batch [831/938], Loss: 1.7484791278839111\n",
      "Train: Epoch [3], Batch [832/938], Loss: 1.6689599752426147\n",
      "Train: Epoch [3], Batch [833/938], Loss: 1.6465166807174683\n",
      "Train: Epoch [3], Batch [834/938], Loss: 1.5998587608337402\n",
      "Train: Epoch [3], Batch [835/938], Loss: 1.7418696880340576\n",
      "Train: Epoch [3], Batch [836/938], Loss: 1.7925794124603271\n",
      "Train: Epoch [3], Batch [837/938], Loss: 1.517026424407959\n",
      "Train: Epoch [3], Batch [838/938], Loss: 1.7171144485473633\n",
      "Train: Epoch [3], Batch [839/938], Loss: 1.618815302848816\n",
      "Train: Epoch [3], Batch [840/938], Loss: 1.641460657119751\n",
      "Train: Epoch [3], Batch [841/938], Loss: 1.5983421802520752\n",
      "Train: Epoch [3], Batch [842/938], Loss: 1.707976222038269\n",
      "Train: Epoch [3], Batch [843/938], Loss: 1.6394230127334595\n",
      "Train: Epoch [3], Batch [844/938], Loss: 1.7461717128753662\n",
      "Train: Epoch [3], Batch [845/938], Loss: 1.8833454847335815\n",
      "Train: Epoch [3], Batch [846/938], Loss: 1.7747952938079834\n",
      "Train: Epoch [3], Batch [847/938], Loss: 1.761128306388855\n",
      "Train: Epoch [3], Batch [848/938], Loss: 1.9197356700897217\n",
      "Train: Epoch [3], Batch [849/938], Loss: 1.8384267091751099\n",
      "Train: Epoch [3], Batch [850/938], Loss: 1.6234259605407715\n",
      "Train: Epoch [3], Batch [851/938], Loss: 1.5758535861968994\n",
      "Train: Epoch [3], Batch [852/938], Loss: 1.6382241249084473\n",
      "Train: Epoch [3], Batch [853/938], Loss: 1.846612572669983\n",
      "Train: Epoch [3], Batch [854/938], Loss: 1.5082463026046753\n",
      "Train: Epoch [3], Batch [855/938], Loss: 1.866243839263916\n",
      "Train: Epoch [3], Batch [856/938], Loss: 1.6238919496536255\n",
      "Train: Epoch [3], Batch [857/938], Loss: 1.7417858839035034\n",
      "Train: Epoch [3], Batch [858/938], Loss: 1.6958848237991333\n",
      "Train: Epoch [3], Batch [859/938], Loss: 1.6962268352508545\n",
      "Train: Epoch [3], Batch [860/938], Loss: 1.6510677337646484\n",
      "Train: Epoch [3], Batch [861/938], Loss: 1.603255033493042\n",
      "Train: Epoch [3], Batch [862/938], Loss: 1.849058985710144\n",
      "Train: Epoch [3], Batch [863/938], Loss: 1.6818712949752808\n",
      "Train: Epoch [3], Batch [864/938], Loss: 1.5109305381774902\n",
      "Train: Epoch [3], Batch [865/938], Loss: 1.6426645517349243\n",
      "Train: Epoch [3], Batch [866/938], Loss: 1.8341012001037598\n",
      "Train: Epoch [3], Batch [867/938], Loss: 1.783936858177185\n",
      "Train: Epoch [3], Batch [868/938], Loss: 1.7242016792297363\n",
      "Train: Epoch [3], Batch [869/938], Loss: 1.6188737154006958\n",
      "Train: Epoch [3], Batch [870/938], Loss: 1.6517058610916138\n",
      "Train: Epoch [3], Batch [871/938], Loss: 1.6646041870117188\n",
      "Train: Epoch [3], Batch [872/938], Loss: 1.7240562438964844\n",
      "Train: Epoch [3], Batch [873/938], Loss: 1.5890462398529053\n",
      "Train: Epoch [3], Batch [874/938], Loss: 1.5683879852294922\n",
      "Train: Epoch [3], Batch [875/938], Loss: 1.5332297086715698\n",
      "Train: Epoch [3], Batch [876/938], Loss: 1.7268545627593994\n",
      "Train: Epoch [3], Batch [877/938], Loss: 1.6796061992645264\n",
      "Train: Epoch [3], Batch [878/938], Loss: 1.776667594909668\n",
      "Train: Epoch [3], Batch [879/938], Loss: 1.7240816354751587\n",
      "Train: Epoch [3], Batch [880/938], Loss: 1.768800973892212\n",
      "Train: Epoch [3], Batch [881/938], Loss: 1.6226885318756104\n",
      "Train: Epoch [3], Batch [882/938], Loss: 1.5710150003433228\n",
      "Train: Epoch [3], Batch [883/938], Loss: 1.7311166524887085\n",
      "Train: Epoch [3], Batch [884/938], Loss: 1.5895799398422241\n",
      "Train: Epoch [3], Batch [885/938], Loss: 1.6248409748077393\n",
      "Train: Epoch [3], Batch [886/938], Loss: 1.5220367908477783\n",
      "Train: Epoch [3], Batch [887/938], Loss: 1.7869962453842163\n",
      "Train: Epoch [3], Batch [888/938], Loss: 1.7045085430145264\n",
      "Train: Epoch [3], Batch [889/938], Loss: 1.7213170528411865\n",
      "Train: Epoch [3], Batch [890/938], Loss: 1.5770972967147827\n",
      "Train: Epoch [3], Batch [891/938], Loss: 1.7582117319107056\n",
      "Train: Epoch [3], Batch [892/938], Loss: 1.8244036436080933\n",
      "Train: Epoch [3], Batch [893/938], Loss: 1.6835911273956299\n",
      "Train: Epoch [3], Batch [894/938], Loss: 1.6382315158843994\n",
      "Train: Epoch [3], Batch [895/938], Loss: 1.397544503211975\n",
      "Train: Epoch [3], Batch [896/938], Loss: 1.8082993030548096\n",
      "Train: Epoch [3], Batch [897/938], Loss: 1.57597815990448\n",
      "Train: Epoch [3], Batch [898/938], Loss: 1.6167619228363037\n",
      "Train: Epoch [3], Batch [899/938], Loss: 1.6935582160949707\n",
      "Train: Epoch [3], Batch [900/938], Loss: 1.6201434135437012\n",
      "Train: Epoch [3], Batch [901/938], Loss: 1.7178857326507568\n",
      "Train: Epoch [3], Batch [902/938], Loss: 1.6599373817443848\n",
      "Train: Epoch [3], Batch [903/938], Loss: 1.6090192794799805\n",
      "Train: Epoch [3], Batch [904/938], Loss: 1.6841814517974854\n",
      "Train: Epoch [3], Batch [905/938], Loss: 1.5949066877365112\n",
      "Train: Epoch [3], Batch [906/938], Loss: 1.7440476417541504\n",
      "Train: Epoch [3], Batch [907/938], Loss: 1.486989974975586\n",
      "Train: Epoch [3], Batch [908/938], Loss: 1.7826861143112183\n",
      "Train: Epoch [3], Batch [909/938], Loss: 1.6805907487869263\n",
      "Train: Epoch [3], Batch [910/938], Loss: 1.5088326930999756\n",
      "Train: Epoch [3], Batch [911/938], Loss: 1.6947473287582397\n",
      "Train: Epoch [3], Batch [912/938], Loss: 1.593858242034912\n",
      "Train: Epoch [3], Batch [913/938], Loss: 1.6316418647766113\n",
      "Train: Epoch [3], Batch [914/938], Loss: 1.5909348726272583\n",
      "Train: Epoch [3], Batch [915/938], Loss: 1.6812779903411865\n",
      "Train: Epoch [3], Batch [916/938], Loss: 1.8851401805877686\n",
      "Train: Epoch [3], Batch [917/938], Loss: 1.6873548030853271\n",
      "Train: Epoch [3], Batch [918/938], Loss: 1.5968149900436401\n",
      "Train: Epoch [3], Batch [919/938], Loss: 1.5015385150909424\n",
      "Train: Epoch [3], Batch [920/938], Loss: 1.5176475048065186\n",
      "Train: Epoch [3], Batch [921/938], Loss: 1.4834260940551758\n",
      "Train: Epoch [3], Batch [922/938], Loss: 1.6235954761505127\n",
      "Train: Epoch [3], Batch [923/938], Loss: 1.4684514999389648\n",
      "Train: Epoch [3], Batch [924/938], Loss: 1.7832152843475342\n",
      "Train: Epoch [3], Batch [925/938], Loss: 1.713546633720398\n",
      "Train: Epoch [3], Batch [926/938], Loss: 1.5854400396347046\n",
      "Train: Epoch [3], Batch [927/938], Loss: 1.6041420698165894\n",
      "Train: Epoch [3], Batch [928/938], Loss: 1.6840590238571167\n",
      "Train: Epoch [3], Batch [929/938], Loss: 1.6980425119400024\n",
      "Train: Epoch [3], Batch [930/938], Loss: 1.570493221282959\n",
      "Train: Epoch [3], Batch [931/938], Loss: 1.4711627960205078\n",
      "Train: Epoch [3], Batch [932/938], Loss: 1.6409577131271362\n",
      "Train: Epoch [3], Batch [933/938], Loss: 1.7889621257781982\n",
      "Train: Epoch [3], Batch [934/938], Loss: 1.643080711364746\n",
      "Train: Epoch [3], Batch [935/938], Loss: 1.6924149990081787\n",
      "Train: Epoch [3], Batch [936/938], Loss: 1.7326549291610718\n",
      "Train: Epoch [3], Batch [937/938], Loss: 1.5789995193481445\n",
      "Train: Epoch [3], Batch [938/938], Loss: 1.7172346115112305\n",
      "Accuracy of train set: 0.3809166666666667\n",
      "Validation: Epoch [3], Batch [1/938], Loss: 1.6994881629943848\n",
      "Validation: Epoch [3], Batch [2/938], Loss: 1.6276073455810547\n",
      "Validation: Epoch [3], Batch [3/938], Loss: 1.586679220199585\n",
      "Validation: Epoch [3], Batch [4/938], Loss: 1.633230447769165\n",
      "Validation: Epoch [3], Batch [5/938], Loss: 1.783615231513977\n",
      "Validation: Epoch [3], Batch [6/938], Loss: 1.6867371797561646\n",
      "Validation: Epoch [3], Batch [7/938], Loss: 1.614423394203186\n",
      "Validation: Epoch [3], Batch [8/938], Loss: 1.4154034852981567\n",
      "Validation: Epoch [3], Batch [9/938], Loss: 1.5562067031860352\n",
      "Validation: Epoch [3], Batch [10/938], Loss: 1.6357474327087402\n",
      "Validation: Epoch [3], Batch [11/938], Loss: 1.7615344524383545\n",
      "Validation: Epoch [3], Batch [12/938], Loss: 1.5606911182403564\n",
      "Validation: Epoch [3], Batch [13/938], Loss: 1.6980540752410889\n",
      "Validation: Epoch [3], Batch [14/938], Loss: 1.484532356262207\n",
      "Validation: Epoch [3], Batch [15/938], Loss: 1.5896207094192505\n",
      "Validation: Epoch [3], Batch [16/938], Loss: 1.6111524105072021\n",
      "Validation: Epoch [3], Batch [17/938], Loss: 1.4558743238449097\n",
      "Validation: Epoch [3], Batch [18/938], Loss: 1.8283213376998901\n",
      "Validation: Epoch [3], Batch [19/938], Loss: 1.8801782131195068\n",
      "Validation: Epoch [3], Batch [20/938], Loss: 1.6438086032867432\n",
      "Validation: Epoch [3], Batch [21/938], Loss: 1.6629377603530884\n",
      "Validation: Epoch [3], Batch [22/938], Loss: 1.5265414714813232\n",
      "Validation: Epoch [3], Batch [23/938], Loss: 1.405997633934021\n",
      "Validation: Epoch [3], Batch [24/938], Loss: 1.6678417921066284\n",
      "Validation: Epoch [3], Batch [25/938], Loss: 1.7984282970428467\n",
      "Validation: Epoch [3], Batch [26/938], Loss: 1.6264315843582153\n",
      "Validation: Epoch [3], Batch [27/938], Loss: 1.5871028900146484\n",
      "Validation: Epoch [3], Batch [28/938], Loss: 1.7795687913894653\n",
      "Validation: Epoch [3], Batch [29/938], Loss: 1.7182812690734863\n",
      "Validation: Epoch [3], Batch [30/938], Loss: 1.5863572359085083\n",
      "Validation: Epoch [3], Batch [31/938], Loss: 1.5136579275131226\n",
      "Validation: Epoch [3], Batch [32/938], Loss: 1.4965652227401733\n",
      "Validation: Epoch [3], Batch [33/938], Loss: 1.8402957916259766\n",
      "Validation: Epoch [3], Batch [34/938], Loss: 1.692679762840271\n",
      "Validation: Epoch [3], Batch [35/938], Loss: 1.581308126449585\n",
      "Validation: Epoch [3], Batch [36/938], Loss: 1.623490333557129\n",
      "Validation: Epoch [3], Batch [37/938], Loss: 1.7761908769607544\n",
      "Validation: Epoch [3], Batch [38/938], Loss: 1.5312217473983765\n",
      "Validation: Epoch [3], Batch [39/938], Loss: 1.617546558380127\n",
      "Validation: Epoch [3], Batch [40/938], Loss: 1.6389013528823853\n",
      "Validation: Epoch [3], Batch [41/938], Loss: 1.6846565008163452\n",
      "Validation: Epoch [3], Batch [42/938], Loss: 1.647848129272461\n",
      "Validation: Epoch [3], Batch [43/938], Loss: 1.5598052740097046\n",
      "Validation: Epoch [3], Batch [44/938], Loss: 1.4924280643463135\n",
      "Validation: Epoch [3], Batch [45/938], Loss: 1.7329639196395874\n",
      "Validation: Epoch [3], Batch [46/938], Loss: 1.475278615951538\n",
      "Validation: Epoch [3], Batch [47/938], Loss: 1.6378389596939087\n",
      "Validation: Epoch [3], Batch [48/938], Loss: 1.595027208328247\n",
      "Validation: Epoch [3], Batch [49/938], Loss: 1.6178860664367676\n",
      "Validation: Epoch [3], Batch [50/938], Loss: 1.7100872993469238\n",
      "Validation: Epoch [3], Batch [51/938], Loss: 1.6850357055664062\n",
      "Validation: Epoch [3], Batch [52/938], Loss: 1.593098521232605\n",
      "Validation: Epoch [3], Batch [53/938], Loss: 1.5887137651443481\n",
      "Validation: Epoch [3], Batch [54/938], Loss: 1.6689385175704956\n",
      "Validation: Epoch [3], Batch [55/938], Loss: 1.6534242630004883\n",
      "Validation: Epoch [3], Batch [56/938], Loss: 1.628667950630188\n",
      "Validation: Epoch [3], Batch [57/938], Loss: 1.7216213941574097\n",
      "Validation: Epoch [3], Batch [58/938], Loss: 1.7356503009796143\n",
      "Validation: Epoch [3], Batch [59/938], Loss: 1.6396372318267822\n",
      "Validation: Epoch [3], Batch [60/938], Loss: 1.6160534620285034\n",
      "Validation: Epoch [3], Batch [61/938], Loss: 1.6116496324539185\n",
      "Validation: Epoch [3], Batch [62/938], Loss: 1.5856212377548218\n",
      "Validation: Epoch [3], Batch [63/938], Loss: 1.768378496170044\n",
      "Validation: Epoch [3], Batch [64/938], Loss: 1.6529768705368042\n",
      "Validation: Epoch [3], Batch [65/938], Loss: 1.5180989503860474\n",
      "Validation: Epoch [3], Batch [66/938], Loss: 1.7287261486053467\n",
      "Validation: Epoch [3], Batch [67/938], Loss: 1.5800708532333374\n",
      "Validation: Epoch [3], Batch [68/938], Loss: 1.7339770793914795\n",
      "Validation: Epoch [3], Batch [69/938], Loss: 1.642951250076294\n",
      "Validation: Epoch [3], Batch [70/938], Loss: 1.688287377357483\n",
      "Validation: Epoch [3], Batch [71/938], Loss: 1.5969083309173584\n",
      "Validation: Epoch [3], Batch [72/938], Loss: 1.7888927459716797\n",
      "Validation: Epoch [3], Batch [73/938], Loss: 1.6384062767028809\n",
      "Validation: Epoch [3], Batch [74/938], Loss: 1.6605948209762573\n",
      "Validation: Epoch [3], Batch [75/938], Loss: 1.607320785522461\n",
      "Validation: Epoch [3], Batch [76/938], Loss: 1.7157440185546875\n",
      "Validation: Epoch [3], Batch [77/938], Loss: 1.7247951030731201\n",
      "Validation: Epoch [3], Batch [78/938], Loss: 1.6363511085510254\n",
      "Validation: Epoch [3], Batch [79/938], Loss: 1.547674298286438\n",
      "Validation: Epoch [3], Batch [80/938], Loss: 1.6803874969482422\n",
      "Validation: Epoch [3], Batch [81/938], Loss: 1.7652490139007568\n",
      "Validation: Epoch [3], Batch [82/938], Loss: 1.5779200792312622\n",
      "Validation: Epoch [3], Batch [83/938], Loss: 1.4686317443847656\n",
      "Validation: Epoch [3], Batch [84/938], Loss: 1.6169620752334595\n",
      "Validation: Epoch [3], Batch [85/938], Loss: 1.6698126792907715\n",
      "Validation: Epoch [3], Batch [86/938], Loss: 1.7235682010650635\n",
      "Validation: Epoch [3], Batch [87/938], Loss: 1.6561188697814941\n",
      "Validation: Epoch [3], Batch [88/938], Loss: 1.796528697013855\n",
      "Validation: Epoch [3], Batch [89/938], Loss: 1.6471840143203735\n",
      "Validation: Epoch [3], Batch [90/938], Loss: 1.7090604305267334\n",
      "Validation: Epoch [3], Batch [91/938], Loss: 1.6842451095581055\n",
      "Validation: Epoch [3], Batch [92/938], Loss: 1.633850336074829\n",
      "Validation: Epoch [3], Batch [93/938], Loss: 1.6836475133895874\n",
      "Validation: Epoch [3], Batch [94/938], Loss: 1.6291519403457642\n",
      "Validation: Epoch [3], Batch [95/938], Loss: 1.5972230434417725\n",
      "Validation: Epoch [3], Batch [96/938], Loss: 1.745807409286499\n",
      "Validation: Epoch [3], Batch [97/938], Loss: 1.506542682647705\n",
      "Validation: Epoch [3], Batch [98/938], Loss: 1.783714771270752\n",
      "Validation: Epoch [3], Batch [99/938], Loss: 1.4817513227462769\n",
      "Validation: Epoch [3], Batch [100/938], Loss: 1.5424052476882935\n",
      "Validation: Epoch [3], Batch [101/938], Loss: 1.6433463096618652\n",
      "Validation: Epoch [3], Batch [102/938], Loss: 1.5651843547821045\n",
      "Validation: Epoch [3], Batch [103/938], Loss: 1.6047230958938599\n",
      "Validation: Epoch [3], Batch [104/938], Loss: 1.46417236328125\n",
      "Validation: Epoch [3], Batch [105/938], Loss: 1.566942811012268\n",
      "Validation: Epoch [3], Batch [106/938], Loss: 1.6698555946350098\n",
      "Validation: Epoch [3], Batch [107/938], Loss: 1.685793399810791\n",
      "Validation: Epoch [3], Batch [108/938], Loss: 1.513598918914795\n",
      "Validation: Epoch [3], Batch [109/938], Loss: 1.7065010070800781\n",
      "Validation: Epoch [3], Batch [110/938], Loss: 1.7257252931594849\n",
      "Validation: Epoch [3], Batch [111/938], Loss: 1.6093634366989136\n",
      "Validation: Epoch [3], Batch [112/938], Loss: 1.650041103363037\n",
      "Validation: Epoch [3], Batch [113/938], Loss: 1.6707305908203125\n",
      "Validation: Epoch [3], Batch [114/938], Loss: 1.7124577760696411\n",
      "Validation: Epoch [3], Batch [115/938], Loss: 1.5523046255111694\n",
      "Validation: Epoch [3], Batch [116/938], Loss: 1.8025418519973755\n",
      "Validation: Epoch [3], Batch [117/938], Loss: 1.6002469062805176\n",
      "Validation: Epoch [3], Batch [118/938], Loss: 1.6550298929214478\n",
      "Validation: Epoch [3], Batch [119/938], Loss: 1.6951347589492798\n",
      "Validation: Epoch [3], Batch [120/938], Loss: 1.7823352813720703\n",
      "Validation: Epoch [3], Batch [121/938], Loss: 1.7010090351104736\n",
      "Validation: Epoch [3], Batch [122/938], Loss: 1.5415089130401611\n",
      "Validation: Epoch [3], Batch [123/938], Loss: 1.6994211673736572\n",
      "Validation: Epoch [3], Batch [124/938], Loss: 1.7776093482971191\n",
      "Validation: Epoch [3], Batch [125/938], Loss: 1.626587152481079\n",
      "Validation: Epoch [3], Batch [126/938], Loss: 1.5555481910705566\n",
      "Validation: Epoch [3], Batch [127/938], Loss: 1.5152183771133423\n",
      "Validation: Epoch [3], Batch [128/938], Loss: 1.691394329071045\n",
      "Validation: Epoch [3], Batch [129/938], Loss: 1.6962463855743408\n",
      "Validation: Epoch [3], Batch [130/938], Loss: 1.697342872619629\n",
      "Validation: Epoch [3], Batch [131/938], Loss: 1.4316937923431396\n",
      "Validation: Epoch [3], Batch [132/938], Loss: 1.5352503061294556\n",
      "Validation: Epoch [3], Batch [133/938], Loss: 1.678229570388794\n",
      "Validation: Epoch [3], Batch [134/938], Loss: 1.7195559740066528\n",
      "Validation: Epoch [3], Batch [135/938], Loss: 1.660592794418335\n",
      "Validation: Epoch [3], Batch [136/938], Loss: 1.5406105518341064\n",
      "Validation: Epoch [3], Batch [137/938], Loss: 1.6384806632995605\n",
      "Validation: Epoch [3], Batch [138/938], Loss: 1.693888783454895\n",
      "Validation: Epoch [3], Batch [139/938], Loss: 1.651350736618042\n",
      "Validation: Epoch [3], Batch [140/938], Loss: 1.6781280040740967\n",
      "Validation: Epoch [3], Batch [141/938], Loss: 1.7075353860855103\n",
      "Validation: Epoch [3], Batch [142/938], Loss: 1.8205572366714478\n",
      "Validation: Epoch [3], Batch [143/938], Loss: 1.4374151229858398\n",
      "Validation: Epoch [3], Batch [144/938], Loss: 1.6625561714172363\n",
      "Validation: Epoch [3], Batch [145/938], Loss: 1.6507512331008911\n",
      "Validation: Epoch [3], Batch [146/938], Loss: 1.5566787719726562\n",
      "Validation: Epoch [3], Batch [147/938], Loss: 1.6745151281356812\n",
      "Validation: Epoch [3], Batch [148/938], Loss: 1.6831063032150269\n",
      "Validation: Epoch [3], Batch [149/938], Loss: 1.8410744667053223\n",
      "Validation: Epoch [3], Batch [150/938], Loss: 1.655659556388855\n",
      "Validation: Epoch [3], Batch [151/938], Loss: 1.6016978025436401\n",
      "Validation: Epoch [3], Batch [152/938], Loss: 1.5091981887817383\n",
      "Validation: Epoch [3], Batch [153/938], Loss: 1.6798315048217773\n",
      "Validation: Epoch [3], Batch [154/938], Loss: 1.810522437095642\n",
      "Validation: Epoch [3], Batch [155/938], Loss: 1.5558092594146729\n",
      "Validation: Epoch [3], Batch [156/938], Loss: 1.5903342962265015\n",
      "Validation: Epoch [3], Batch [157/938], Loss: 1.519713282585144\n",
      "Validation: Epoch [3], Batch [158/938], Loss: 1.5155433416366577\n",
      "Validation: Epoch [3], Batch [159/938], Loss: 1.520845651626587\n",
      "Validation: Epoch [3], Batch [160/938], Loss: 1.7372136116027832\n",
      "Validation: Epoch [3], Batch [161/938], Loss: 1.805600643157959\n",
      "Validation: Epoch [3], Batch [162/938], Loss: 1.5751385688781738\n",
      "Validation: Epoch [3], Batch [163/938], Loss: 1.5845001935958862\n",
      "Validation: Epoch [3], Batch [164/938], Loss: 1.6137551069259644\n",
      "Validation: Epoch [3], Batch [165/938], Loss: 1.7863104343414307\n",
      "Validation: Epoch [3], Batch [166/938], Loss: 1.5418049097061157\n",
      "Validation: Epoch [3], Batch [167/938], Loss: 1.5182243585586548\n",
      "Validation: Epoch [3], Batch [168/938], Loss: 1.592022180557251\n",
      "Validation: Epoch [3], Batch [169/938], Loss: 1.6149003505706787\n",
      "Validation: Epoch [3], Batch [170/938], Loss: 1.6077419519424438\n",
      "Validation: Epoch [3], Batch [171/938], Loss: 1.7716820240020752\n",
      "Validation: Epoch [3], Batch [172/938], Loss: 1.7730019092559814\n",
      "Validation: Epoch [3], Batch [173/938], Loss: 1.520383596420288\n",
      "Validation: Epoch [3], Batch [174/938], Loss: 1.6767356395721436\n",
      "Validation: Epoch [3], Batch [175/938], Loss: 1.631258249282837\n",
      "Validation: Epoch [3], Batch [176/938], Loss: 1.631571888923645\n",
      "Validation: Epoch [3], Batch [177/938], Loss: 1.6149699687957764\n",
      "Validation: Epoch [3], Batch [178/938], Loss: 1.7115777730941772\n",
      "Validation: Epoch [3], Batch [179/938], Loss: 1.55758798122406\n",
      "Validation: Epoch [3], Batch [180/938], Loss: 1.710515022277832\n",
      "Validation: Epoch [3], Batch [181/938], Loss: 1.8518060445785522\n",
      "Validation: Epoch [3], Batch [182/938], Loss: 1.5345534086227417\n",
      "Validation: Epoch [3], Batch [183/938], Loss: 1.6623735427856445\n",
      "Validation: Epoch [3], Batch [184/938], Loss: 1.483333945274353\n",
      "Validation: Epoch [3], Batch [185/938], Loss: 1.7455811500549316\n",
      "Validation: Epoch [3], Batch [186/938], Loss: 1.7685898542404175\n",
      "Validation: Epoch [3], Batch [187/938], Loss: 1.5389829874038696\n",
      "Validation: Epoch [3], Batch [188/938], Loss: 1.6384961605072021\n",
      "Validation: Epoch [3], Batch [189/938], Loss: 1.775229573249817\n",
      "Validation: Epoch [3], Batch [190/938], Loss: 1.5967310667037964\n",
      "Validation: Epoch [3], Batch [191/938], Loss: 1.6180312633514404\n",
      "Validation: Epoch [3], Batch [192/938], Loss: 1.6887685060501099\n",
      "Validation: Epoch [3], Batch [193/938], Loss: 1.7783491611480713\n",
      "Validation: Epoch [3], Batch [194/938], Loss: 1.4212732315063477\n",
      "Validation: Epoch [3], Batch [195/938], Loss: 1.8011608123779297\n",
      "Validation: Epoch [3], Batch [196/938], Loss: 1.5487260818481445\n",
      "Validation: Epoch [3], Batch [197/938], Loss: 1.6091549396514893\n",
      "Validation: Epoch [3], Batch [198/938], Loss: 1.6360704898834229\n",
      "Validation: Epoch [3], Batch [199/938], Loss: 1.5110068321228027\n",
      "Validation: Epoch [3], Batch [200/938], Loss: 1.6171514987945557\n",
      "Validation: Epoch [3], Batch [201/938], Loss: 1.5697548389434814\n",
      "Validation: Epoch [3], Batch [202/938], Loss: 1.7515016794204712\n",
      "Validation: Epoch [3], Batch [203/938], Loss: 1.8188332319259644\n",
      "Validation: Epoch [3], Batch [204/938], Loss: 1.577101230621338\n",
      "Validation: Epoch [3], Batch [205/938], Loss: 1.6111348867416382\n",
      "Validation: Epoch [3], Batch [206/938], Loss: 1.8333626985549927\n",
      "Validation: Epoch [3], Batch [207/938], Loss: 1.585265040397644\n",
      "Validation: Epoch [3], Batch [208/938], Loss: 1.6969417333602905\n",
      "Validation: Epoch [3], Batch [209/938], Loss: 1.591804027557373\n",
      "Validation: Epoch [3], Batch [210/938], Loss: 1.5814827680587769\n",
      "Validation: Epoch [3], Batch [211/938], Loss: 1.6648198366165161\n",
      "Validation: Epoch [3], Batch [212/938], Loss: 1.4218335151672363\n",
      "Validation: Epoch [3], Batch [213/938], Loss: 1.7744040489196777\n",
      "Validation: Epoch [3], Batch [214/938], Loss: 1.556083083152771\n",
      "Validation: Epoch [3], Batch [215/938], Loss: 1.7909414768218994\n",
      "Validation: Epoch [3], Batch [216/938], Loss: 1.5874769687652588\n",
      "Validation: Epoch [3], Batch [217/938], Loss: 1.5892665386199951\n",
      "Validation: Epoch [3], Batch [218/938], Loss: 1.6254520416259766\n",
      "Validation: Epoch [3], Batch [219/938], Loss: 1.570469617843628\n",
      "Validation: Epoch [3], Batch [220/938], Loss: 1.6601314544677734\n",
      "Validation: Epoch [3], Batch [221/938], Loss: 1.597062587738037\n",
      "Validation: Epoch [3], Batch [222/938], Loss: 1.6284170150756836\n",
      "Validation: Epoch [3], Batch [223/938], Loss: 1.6626410484313965\n",
      "Validation: Epoch [3], Batch [224/938], Loss: 1.519569993019104\n",
      "Validation: Epoch [3], Batch [225/938], Loss: 1.7088189125061035\n",
      "Validation: Epoch [3], Batch [226/938], Loss: 1.7887637615203857\n",
      "Validation: Epoch [3], Batch [227/938], Loss: 1.7285047769546509\n",
      "Validation: Epoch [3], Batch [228/938], Loss: 1.6081712245941162\n",
      "Validation: Epoch [3], Batch [229/938], Loss: 1.693508505821228\n",
      "Validation: Epoch [3], Batch [230/938], Loss: 1.4269812107086182\n",
      "Validation: Epoch [3], Batch [231/938], Loss: 1.5102193355560303\n",
      "Validation: Epoch [3], Batch [232/938], Loss: 1.6702780723571777\n",
      "Validation: Epoch [3], Batch [233/938], Loss: 1.5874485969543457\n",
      "Validation: Epoch [3], Batch [234/938], Loss: 1.5947861671447754\n",
      "Validation: Epoch [3], Batch [235/938], Loss: 1.6995604038238525\n",
      "Validation: Epoch [3], Batch [236/938], Loss: 1.673750877380371\n",
      "Validation: Epoch [3], Batch [237/938], Loss: 1.667593002319336\n",
      "Validation: Epoch [3], Batch [238/938], Loss: 1.5954160690307617\n",
      "Validation: Epoch [3], Batch [239/938], Loss: 1.6972486972808838\n",
      "Validation: Epoch [3], Batch [240/938], Loss: 1.6690367460250854\n",
      "Validation: Epoch [3], Batch [241/938], Loss: 1.5885246992111206\n",
      "Validation: Epoch [3], Batch [242/938], Loss: 1.660564661026001\n",
      "Validation: Epoch [3], Batch [243/938], Loss: 1.5969630479812622\n",
      "Validation: Epoch [3], Batch [244/938], Loss: 1.745924472808838\n",
      "Validation: Epoch [3], Batch [245/938], Loss: 1.6192113161087036\n",
      "Validation: Epoch [3], Batch [246/938], Loss: 1.5550549030303955\n",
      "Validation: Epoch [3], Batch [247/938], Loss: 1.591714859008789\n",
      "Validation: Epoch [3], Batch [248/938], Loss: 1.7326825857162476\n",
      "Validation: Epoch [3], Batch [249/938], Loss: 1.4572679996490479\n",
      "Validation: Epoch [3], Batch [250/938], Loss: 1.64711594581604\n",
      "Validation: Epoch [3], Batch [251/938], Loss: 1.519181251525879\n",
      "Validation: Epoch [3], Batch [252/938], Loss: 1.6178629398345947\n",
      "Validation: Epoch [3], Batch [253/938], Loss: 1.7149734497070312\n",
      "Validation: Epoch [3], Batch [254/938], Loss: 1.6864114999771118\n",
      "Validation: Epoch [3], Batch [255/938], Loss: 1.4844202995300293\n",
      "Validation: Epoch [3], Batch [256/938], Loss: 1.4671128988265991\n",
      "Validation: Epoch [3], Batch [257/938], Loss: 1.8176448345184326\n",
      "Validation: Epoch [3], Batch [258/938], Loss: 1.7423840761184692\n",
      "Validation: Epoch [3], Batch [259/938], Loss: 1.5794202089309692\n",
      "Validation: Epoch [3], Batch [260/938], Loss: 1.694866418838501\n",
      "Validation: Epoch [3], Batch [261/938], Loss: 1.6329431533813477\n",
      "Validation: Epoch [3], Batch [262/938], Loss: 1.6543534994125366\n",
      "Validation: Epoch [3], Batch [263/938], Loss: 1.5838607549667358\n",
      "Validation: Epoch [3], Batch [264/938], Loss: 1.70582914352417\n",
      "Validation: Epoch [3], Batch [265/938], Loss: 1.7967875003814697\n",
      "Validation: Epoch [3], Batch [266/938], Loss: 1.657829761505127\n",
      "Validation: Epoch [3], Batch [267/938], Loss: 1.745806097984314\n",
      "Validation: Epoch [3], Batch [268/938], Loss: 1.516055941581726\n",
      "Validation: Epoch [3], Batch [269/938], Loss: 1.717898964881897\n",
      "Validation: Epoch [3], Batch [270/938], Loss: 1.51797616481781\n",
      "Validation: Epoch [3], Batch [271/938], Loss: 1.8730396032333374\n",
      "Validation: Epoch [3], Batch [272/938], Loss: 1.5496901273727417\n",
      "Validation: Epoch [3], Batch [273/938], Loss: 1.626573920249939\n",
      "Validation: Epoch [3], Batch [274/938], Loss: 1.6238231658935547\n",
      "Validation: Epoch [3], Batch [275/938], Loss: 1.5129059553146362\n",
      "Validation: Epoch [3], Batch [276/938], Loss: 1.6187845468521118\n",
      "Validation: Epoch [3], Batch [277/938], Loss: 1.655373215675354\n",
      "Validation: Epoch [3], Batch [278/938], Loss: 1.6703033447265625\n",
      "Validation: Epoch [3], Batch [279/938], Loss: 1.7363100051879883\n",
      "Validation: Epoch [3], Batch [280/938], Loss: 1.621766448020935\n",
      "Validation: Epoch [3], Batch [281/938], Loss: 1.8989824056625366\n",
      "Validation: Epoch [3], Batch [282/938], Loss: 1.7181509733200073\n",
      "Validation: Epoch [3], Batch [283/938], Loss: 1.543898105621338\n",
      "Validation: Epoch [3], Batch [284/938], Loss: 1.629286289215088\n",
      "Validation: Epoch [3], Batch [285/938], Loss: 1.663354754447937\n",
      "Validation: Epoch [3], Batch [286/938], Loss: 1.6730685234069824\n",
      "Validation: Epoch [3], Batch [287/938], Loss: 1.7024149894714355\n",
      "Validation: Epoch [3], Batch [288/938], Loss: 1.5823830366134644\n",
      "Validation: Epoch [3], Batch [289/938], Loss: 1.7329323291778564\n",
      "Validation: Epoch [3], Batch [290/938], Loss: 1.522065281867981\n",
      "Validation: Epoch [3], Batch [291/938], Loss: 1.36115300655365\n",
      "Validation: Epoch [3], Batch [292/938], Loss: 1.6329816579818726\n",
      "Validation: Epoch [3], Batch [293/938], Loss: 1.6080220937728882\n",
      "Validation: Epoch [3], Batch [294/938], Loss: 1.5873160362243652\n",
      "Validation: Epoch [3], Batch [295/938], Loss: 1.6201555728912354\n",
      "Validation: Epoch [3], Batch [296/938], Loss: 1.579019546508789\n",
      "Validation: Epoch [3], Batch [297/938], Loss: 1.6156071424484253\n",
      "Validation: Epoch [3], Batch [298/938], Loss: 1.786110758781433\n",
      "Validation: Epoch [3], Batch [299/938], Loss: 1.7521910667419434\n",
      "Validation: Epoch [3], Batch [300/938], Loss: 1.728463888168335\n",
      "Validation: Epoch [3], Batch [301/938], Loss: 1.6946115493774414\n",
      "Validation: Epoch [3], Batch [302/938], Loss: 1.7529877424240112\n",
      "Validation: Epoch [3], Batch [303/938], Loss: 1.578872561454773\n",
      "Validation: Epoch [3], Batch [304/938], Loss: 1.5466668605804443\n",
      "Validation: Epoch [3], Batch [305/938], Loss: 1.5715411901474\n",
      "Validation: Epoch [3], Batch [306/938], Loss: 1.6686487197875977\n",
      "Validation: Epoch [3], Batch [307/938], Loss: 1.492906928062439\n",
      "Validation: Epoch [3], Batch [308/938], Loss: 1.615486979484558\n",
      "Validation: Epoch [3], Batch [309/938], Loss: 1.549556016921997\n",
      "Validation: Epoch [3], Batch [310/938], Loss: 1.4677760601043701\n",
      "Validation: Epoch [3], Batch [311/938], Loss: 1.590421438217163\n",
      "Validation: Epoch [3], Batch [312/938], Loss: 1.7785663604736328\n",
      "Validation: Epoch [3], Batch [313/938], Loss: 1.6018749475479126\n",
      "Validation: Epoch [3], Batch [314/938], Loss: 1.6508018970489502\n",
      "Validation: Epoch [3], Batch [315/938], Loss: 1.7526617050170898\n",
      "Validation: Epoch [3], Batch [316/938], Loss: 1.5024027824401855\n",
      "Validation: Epoch [3], Batch [317/938], Loss: 1.6988716125488281\n",
      "Validation: Epoch [3], Batch [318/938], Loss: 1.5633445978164673\n",
      "Validation: Epoch [3], Batch [319/938], Loss: 1.548467993736267\n",
      "Validation: Epoch [3], Batch [320/938], Loss: 1.680059790611267\n",
      "Validation: Epoch [3], Batch [321/938], Loss: 1.6407439708709717\n",
      "Validation: Epoch [3], Batch [322/938], Loss: 1.5878241062164307\n",
      "Validation: Epoch [3], Batch [323/938], Loss: 1.7523444890975952\n",
      "Validation: Epoch [3], Batch [324/938], Loss: 1.6203665733337402\n",
      "Validation: Epoch [3], Batch [325/938], Loss: 1.6597782373428345\n",
      "Validation: Epoch [3], Batch [326/938], Loss: 1.6833139657974243\n",
      "Validation: Epoch [3], Batch [327/938], Loss: 1.658728003501892\n",
      "Validation: Epoch [3], Batch [328/938], Loss: 1.5603928565979004\n",
      "Validation: Epoch [3], Batch [329/938], Loss: 1.7126713991165161\n",
      "Validation: Epoch [3], Batch [330/938], Loss: 1.6855002641677856\n",
      "Validation: Epoch [3], Batch [331/938], Loss: 1.6358274221420288\n",
      "Validation: Epoch [3], Batch [332/938], Loss: 1.6530239582061768\n",
      "Validation: Epoch [3], Batch [333/938], Loss: 1.7185977697372437\n",
      "Validation: Epoch [3], Batch [334/938], Loss: 1.6872090101242065\n",
      "Validation: Epoch [3], Batch [335/938], Loss: 1.5972661972045898\n",
      "Validation: Epoch [3], Batch [336/938], Loss: 1.5363938808441162\n",
      "Validation: Epoch [3], Batch [337/938], Loss: 1.570425033569336\n",
      "Validation: Epoch [3], Batch [338/938], Loss: 1.713208794593811\n",
      "Validation: Epoch [3], Batch [339/938], Loss: 1.670406699180603\n",
      "Validation: Epoch [3], Batch [340/938], Loss: 1.6495224237442017\n",
      "Validation: Epoch [3], Batch [341/938], Loss: 1.703080177307129\n",
      "Validation: Epoch [3], Batch [342/938], Loss: 1.6758365631103516\n",
      "Validation: Epoch [3], Batch [343/938], Loss: 1.5911113023757935\n",
      "Validation: Epoch [3], Batch [344/938], Loss: 1.513019323348999\n",
      "Validation: Epoch [3], Batch [345/938], Loss: 1.4927774667739868\n",
      "Validation: Epoch [3], Batch [346/938], Loss: 1.6920993328094482\n",
      "Validation: Epoch [3], Batch [347/938], Loss: 1.7493443489074707\n",
      "Validation: Epoch [3], Batch [348/938], Loss: 1.6792761087417603\n",
      "Validation: Epoch [3], Batch [349/938], Loss: 1.6153159141540527\n",
      "Validation: Epoch [3], Batch [350/938], Loss: 1.5323175191879272\n",
      "Validation: Epoch [3], Batch [351/938], Loss: 1.661268711090088\n",
      "Validation: Epoch [3], Batch [352/938], Loss: 1.6607396602630615\n",
      "Validation: Epoch [3], Batch [353/938], Loss: 1.52348792552948\n",
      "Validation: Epoch [3], Batch [354/938], Loss: 1.6241735219955444\n",
      "Validation: Epoch [3], Batch [355/938], Loss: 1.6834619045257568\n",
      "Validation: Epoch [3], Batch [356/938], Loss: 1.6376820802688599\n",
      "Validation: Epoch [3], Batch [357/938], Loss: 1.688295841217041\n",
      "Validation: Epoch [3], Batch [358/938], Loss: 1.5260732173919678\n",
      "Validation: Epoch [3], Batch [359/938], Loss: 1.6486427783966064\n",
      "Validation: Epoch [3], Batch [360/938], Loss: 1.733001470565796\n",
      "Validation: Epoch [3], Batch [361/938], Loss: 1.8189764022827148\n",
      "Validation: Epoch [3], Batch [362/938], Loss: 1.6294162273406982\n",
      "Validation: Epoch [3], Batch [363/938], Loss: 1.6549092531204224\n",
      "Validation: Epoch [3], Batch [364/938], Loss: 1.5966403484344482\n",
      "Validation: Epoch [3], Batch [365/938], Loss: 1.6673274040222168\n",
      "Validation: Epoch [3], Batch [366/938], Loss: 1.7367784976959229\n",
      "Validation: Epoch [3], Batch [367/938], Loss: 1.6102296113967896\n",
      "Validation: Epoch [3], Batch [368/938], Loss: 1.6300609111785889\n",
      "Validation: Epoch [3], Batch [369/938], Loss: 1.7516539096832275\n",
      "Validation: Epoch [3], Batch [370/938], Loss: 1.7356069087982178\n",
      "Validation: Epoch [3], Batch [371/938], Loss: 1.628897786140442\n",
      "Validation: Epoch [3], Batch [372/938], Loss: 1.487189531326294\n",
      "Validation: Epoch [3], Batch [373/938], Loss: 1.7196892499923706\n",
      "Validation: Epoch [3], Batch [374/938], Loss: 1.5246574878692627\n",
      "Validation: Epoch [3], Batch [375/938], Loss: 1.661879062652588\n",
      "Validation: Epoch [3], Batch [376/938], Loss: 1.5615907907485962\n",
      "Validation: Epoch [3], Batch [377/938], Loss: 1.5458979606628418\n",
      "Validation: Epoch [3], Batch [378/938], Loss: 1.6162809133529663\n",
      "Validation: Epoch [3], Batch [379/938], Loss: 1.6893682479858398\n",
      "Validation: Epoch [3], Batch [380/938], Loss: 1.554049015045166\n",
      "Validation: Epoch [3], Batch [381/938], Loss: 1.6583564281463623\n",
      "Validation: Epoch [3], Batch [382/938], Loss: 1.612109899520874\n",
      "Validation: Epoch [3], Batch [383/938], Loss: 1.5402004718780518\n",
      "Validation: Epoch [3], Batch [384/938], Loss: 1.6404626369476318\n",
      "Validation: Epoch [3], Batch [385/938], Loss: 1.5866304636001587\n",
      "Validation: Epoch [3], Batch [386/938], Loss: 1.603880763053894\n",
      "Validation: Epoch [3], Batch [387/938], Loss: 1.5036401748657227\n",
      "Validation: Epoch [3], Batch [388/938], Loss: 1.4551812410354614\n",
      "Validation: Epoch [3], Batch [389/938], Loss: 1.638627052307129\n",
      "Validation: Epoch [3], Batch [390/938], Loss: 1.6871802806854248\n",
      "Validation: Epoch [3], Batch [391/938], Loss: 1.5972541570663452\n",
      "Validation: Epoch [3], Batch [392/938], Loss: 1.6329504251480103\n",
      "Validation: Epoch [3], Batch [393/938], Loss: 1.6491141319274902\n",
      "Validation: Epoch [3], Batch [394/938], Loss: 1.6220369338989258\n",
      "Validation: Epoch [3], Batch [395/938], Loss: 1.5641874074935913\n",
      "Validation: Epoch [3], Batch [396/938], Loss: 1.639674425125122\n",
      "Validation: Epoch [3], Batch [397/938], Loss: 1.6703685522079468\n",
      "Validation: Epoch [3], Batch [398/938], Loss: 1.5037238597869873\n",
      "Validation: Epoch [3], Batch [399/938], Loss: 1.7008583545684814\n",
      "Validation: Epoch [3], Batch [400/938], Loss: 1.659150242805481\n",
      "Validation: Epoch [3], Batch [401/938], Loss: 1.6013426780700684\n",
      "Validation: Epoch [3], Batch [402/938], Loss: 1.5935994386672974\n",
      "Validation: Epoch [3], Batch [403/938], Loss: 1.4611104726791382\n",
      "Validation: Epoch [3], Batch [404/938], Loss: 1.605088233947754\n",
      "Validation: Epoch [3], Batch [405/938], Loss: 1.6817665100097656\n",
      "Validation: Epoch [3], Batch [406/938], Loss: 1.6256440877914429\n",
      "Validation: Epoch [3], Batch [407/938], Loss: 1.652649998664856\n",
      "Validation: Epoch [3], Batch [408/938], Loss: 1.5569953918457031\n",
      "Validation: Epoch [3], Batch [409/938], Loss: 1.635489821434021\n",
      "Validation: Epoch [3], Batch [410/938], Loss: 1.7514848709106445\n",
      "Validation: Epoch [3], Batch [411/938], Loss: 1.7519792318344116\n",
      "Validation: Epoch [3], Batch [412/938], Loss: 1.7543721199035645\n",
      "Validation: Epoch [3], Batch [413/938], Loss: 1.723387598991394\n",
      "Validation: Epoch [3], Batch [414/938], Loss: 1.4841129779815674\n",
      "Validation: Epoch [3], Batch [415/938], Loss: 1.641112208366394\n",
      "Validation: Epoch [3], Batch [416/938], Loss: 1.5928980112075806\n",
      "Validation: Epoch [3], Batch [417/938], Loss: 1.6052029132843018\n",
      "Validation: Epoch [3], Batch [418/938], Loss: 1.6177818775177002\n",
      "Validation: Epoch [3], Batch [419/938], Loss: 1.4792554378509521\n",
      "Validation: Epoch [3], Batch [420/938], Loss: 1.6035317182540894\n",
      "Validation: Epoch [3], Batch [421/938], Loss: 1.676168441772461\n",
      "Validation: Epoch [3], Batch [422/938], Loss: 1.6677241325378418\n",
      "Validation: Epoch [3], Batch [423/938], Loss: 1.6297801733016968\n",
      "Validation: Epoch [3], Batch [424/938], Loss: 1.8102611303329468\n",
      "Validation: Epoch [3], Batch [425/938], Loss: 1.753312349319458\n",
      "Validation: Epoch [3], Batch [426/938], Loss: 1.7776902914047241\n",
      "Validation: Epoch [3], Batch [427/938], Loss: 1.4913856983184814\n",
      "Validation: Epoch [3], Batch [428/938], Loss: 1.5022245645523071\n",
      "Validation: Epoch [3], Batch [429/938], Loss: 1.5973701477050781\n",
      "Validation: Epoch [3], Batch [430/938], Loss: 1.656587839126587\n",
      "Validation: Epoch [3], Batch [431/938], Loss: 1.6764112710952759\n",
      "Validation: Epoch [3], Batch [432/938], Loss: 1.83525550365448\n",
      "Validation: Epoch [3], Batch [433/938], Loss: 1.6593172550201416\n",
      "Validation: Epoch [3], Batch [434/938], Loss: 1.588276743888855\n",
      "Validation: Epoch [3], Batch [435/938], Loss: 1.663398027420044\n",
      "Validation: Epoch [3], Batch [436/938], Loss: 1.5515801906585693\n",
      "Validation: Epoch [3], Batch [437/938], Loss: 1.7903735637664795\n",
      "Validation: Epoch [3], Batch [438/938], Loss: 1.6658893823623657\n",
      "Validation: Epoch [3], Batch [439/938], Loss: 1.5671547651290894\n",
      "Validation: Epoch [3], Batch [440/938], Loss: 1.5156810283660889\n",
      "Validation: Epoch [3], Batch [441/938], Loss: 1.5439444780349731\n",
      "Validation: Epoch [3], Batch [442/938], Loss: 1.751746416091919\n",
      "Validation: Epoch [3], Batch [443/938], Loss: 1.6594511270523071\n",
      "Validation: Epoch [3], Batch [444/938], Loss: 1.655295968055725\n",
      "Validation: Epoch [3], Batch [445/938], Loss: 1.641213297843933\n",
      "Validation: Epoch [3], Batch [446/938], Loss: 1.4846289157867432\n",
      "Validation: Epoch [3], Batch [447/938], Loss: 1.8889909982681274\n",
      "Validation: Epoch [3], Batch [448/938], Loss: 1.5870624780654907\n",
      "Validation: Epoch [3], Batch [449/938], Loss: 1.3484553098678589\n",
      "Validation: Epoch [3], Batch [450/938], Loss: 1.6931992769241333\n",
      "Validation: Epoch [3], Batch [451/938], Loss: 1.682389259338379\n",
      "Validation: Epoch [3], Batch [452/938], Loss: 1.6353721618652344\n",
      "Validation: Epoch [3], Batch [453/938], Loss: 1.793735146522522\n",
      "Validation: Epoch [3], Batch [454/938], Loss: 1.620154857635498\n",
      "Validation: Epoch [3], Batch [455/938], Loss: 1.6251349449157715\n",
      "Validation: Epoch [3], Batch [456/938], Loss: 1.853400468826294\n",
      "Validation: Epoch [3], Batch [457/938], Loss: 1.677022099494934\n",
      "Validation: Epoch [3], Batch [458/938], Loss: 1.6090470552444458\n",
      "Validation: Epoch [3], Batch [459/938], Loss: 1.6822677850723267\n",
      "Validation: Epoch [3], Batch [460/938], Loss: 1.8905673027038574\n",
      "Validation: Epoch [3], Batch [461/938], Loss: 1.828447699546814\n",
      "Validation: Epoch [3], Batch [462/938], Loss: 1.8274294137954712\n",
      "Validation: Epoch [3], Batch [463/938], Loss: 1.5967681407928467\n",
      "Validation: Epoch [3], Batch [464/938], Loss: 1.6324344873428345\n",
      "Validation: Epoch [3], Batch [465/938], Loss: 1.7738196849822998\n",
      "Validation: Epoch [3], Batch [466/938], Loss: 1.6956701278686523\n",
      "Validation: Epoch [3], Batch [467/938], Loss: 1.5319684743881226\n",
      "Validation: Epoch [3], Batch [468/938], Loss: 1.5065864324569702\n",
      "Validation: Epoch [3], Batch [469/938], Loss: 1.6558796167373657\n",
      "Validation: Epoch [3], Batch [470/938], Loss: 1.6064982414245605\n",
      "Validation: Epoch [3], Batch [471/938], Loss: 1.5227110385894775\n",
      "Validation: Epoch [3], Batch [472/938], Loss: 1.5261626243591309\n",
      "Validation: Epoch [3], Batch [473/938], Loss: 1.5897330045700073\n",
      "Validation: Epoch [3], Batch [474/938], Loss: 1.551094651222229\n",
      "Validation: Epoch [3], Batch [475/938], Loss: 1.5776281356811523\n",
      "Validation: Epoch [3], Batch [476/938], Loss: 1.5556963682174683\n",
      "Validation: Epoch [3], Batch [477/938], Loss: 1.6845697164535522\n",
      "Validation: Epoch [3], Batch [478/938], Loss: 1.74135160446167\n",
      "Validation: Epoch [3], Batch [479/938], Loss: 1.469521164894104\n",
      "Validation: Epoch [3], Batch [480/938], Loss: 1.6542258262634277\n",
      "Validation: Epoch [3], Batch [481/938], Loss: 1.5897725820541382\n",
      "Validation: Epoch [3], Batch [482/938], Loss: 1.5565108060836792\n",
      "Validation: Epoch [3], Batch [483/938], Loss: 1.699735164642334\n",
      "Validation: Epoch [3], Batch [484/938], Loss: 1.4948654174804688\n",
      "Validation: Epoch [3], Batch [485/938], Loss: 1.6468470096588135\n",
      "Validation: Epoch [3], Batch [486/938], Loss: 1.6172127723693848\n",
      "Validation: Epoch [3], Batch [487/938], Loss: 1.6926891803741455\n",
      "Validation: Epoch [3], Batch [488/938], Loss: 1.5580388307571411\n",
      "Validation: Epoch [3], Batch [489/938], Loss: 1.5693506002426147\n",
      "Validation: Epoch [3], Batch [490/938], Loss: 1.591396450996399\n",
      "Validation: Epoch [3], Batch [491/938], Loss: 1.6561896800994873\n",
      "Validation: Epoch [3], Batch [492/938], Loss: 1.6813292503356934\n",
      "Validation: Epoch [3], Batch [493/938], Loss: 1.717496633529663\n",
      "Validation: Epoch [3], Batch [494/938], Loss: 1.6981205940246582\n",
      "Validation: Epoch [3], Batch [495/938], Loss: 1.7062746286392212\n",
      "Validation: Epoch [3], Batch [496/938], Loss: 1.6164854764938354\n",
      "Validation: Epoch [3], Batch [497/938], Loss: 1.6991980075836182\n",
      "Validation: Epoch [3], Batch [498/938], Loss: 1.7329422235488892\n",
      "Validation: Epoch [3], Batch [499/938], Loss: 1.7854359149932861\n",
      "Validation: Epoch [3], Batch [500/938], Loss: 1.5624130964279175\n",
      "Validation: Epoch [3], Batch [501/938], Loss: 1.606597661972046\n",
      "Validation: Epoch [3], Batch [502/938], Loss: 1.6018438339233398\n",
      "Validation: Epoch [3], Batch [503/938], Loss: 1.5245192050933838\n",
      "Validation: Epoch [3], Batch [504/938], Loss: 1.6768860816955566\n",
      "Validation: Epoch [3], Batch [505/938], Loss: 1.6875536441802979\n",
      "Validation: Epoch [3], Batch [506/938], Loss: 1.8379886150360107\n",
      "Validation: Epoch [3], Batch [507/938], Loss: 1.8027234077453613\n",
      "Validation: Epoch [3], Batch [508/938], Loss: 1.6186063289642334\n",
      "Validation: Epoch [3], Batch [509/938], Loss: 1.5709015130996704\n",
      "Validation: Epoch [3], Batch [510/938], Loss: 1.7261402606964111\n",
      "Validation: Epoch [3], Batch [511/938], Loss: 1.6368504762649536\n",
      "Validation: Epoch [3], Batch [512/938], Loss: 1.6460390090942383\n",
      "Validation: Epoch [3], Batch [513/938], Loss: 1.624815583229065\n",
      "Validation: Epoch [3], Batch [514/938], Loss: 1.5144257545471191\n",
      "Validation: Epoch [3], Batch [515/938], Loss: 1.6427005529403687\n",
      "Validation: Epoch [3], Batch [516/938], Loss: 1.6135305166244507\n",
      "Validation: Epoch [3], Batch [517/938], Loss: 1.6426717042922974\n",
      "Validation: Epoch [3], Batch [518/938], Loss: 1.7259126901626587\n",
      "Validation: Epoch [3], Batch [519/938], Loss: 1.576659917831421\n",
      "Validation: Epoch [3], Batch [520/938], Loss: 1.7241512537002563\n",
      "Validation: Epoch [3], Batch [521/938], Loss: 1.6560131311416626\n",
      "Validation: Epoch [3], Batch [522/938], Loss: 1.7766165733337402\n",
      "Validation: Epoch [3], Batch [523/938], Loss: 1.6971439123153687\n",
      "Validation: Epoch [3], Batch [524/938], Loss: 1.7681841850280762\n",
      "Validation: Epoch [3], Batch [525/938], Loss: 1.6898152828216553\n",
      "Validation: Epoch [3], Batch [526/938], Loss: 1.674928069114685\n",
      "Validation: Epoch [3], Batch [527/938], Loss: 1.6608351469039917\n",
      "Validation: Epoch [3], Batch [528/938], Loss: 1.5139353275299072\n",
      "Validation: Epoch [3], Batch [529/938], Loss: 1.4380161762237549\n",
      "Validation: Epoch [3], Batch [530/938], Loss: 1.6736598014831543\n",
      "Validation: Epoch [3], Batch [531/938], Loss: 1.7410337924957275\n",
      "Validation: Epoch [3], Batch [532/938], Loss: 1.5112111568450928\n",
      "Validation: Epoch [3], Batch [533/938], Loss: 1.5460100173950195\n",
      "Validation: Epoch [3], Batch [534/938], Loss: 1.5824145078659058\n",
      "Validation: Epoch [3], Batch [535/938], Loss: 1.535844087600708\n",
      "Validation: Epoch [3], Batch [536/938], Loss: 1.5965570211410522\n",
      "Validation: Epoch [3], Batch [537/938], Loss: 1.6855356693267822\n",
      "Validation: Epoch [3], Batch [538/938], Loss: 1.7172906398773193\n",
      "Validation: Epoch [3], Batch [539/938], Loss: 1.6529885530471802\n",
      "Validation: Epoch [3], Batch [540/938], Loss: 1.6372193098068237\n",
      "Validation: Epoch [3], Batch [541/938], Loss: 1.635401725769043\n",
      "Validation: Epoch [3], Batch [542/938], Loss: 1.6019099950790405\n",
      "Validation: Epoch [3], Batch [543/938], Loss: 1.6208349466323853\n",
      "Validation: Epoch [3], Batch [544/938], Loss: 1.7602845430374146\n",
      "Validation: Epoch [3], Batch [545/938], Loss: 1.75679612159729\n",
      "Validation: Epoch [3], Batch [546/938], Loss: 1.4636576175689697\n",
      "Validation: Epoch [3], Batch [547/938], Loss: 1.617623209953308\n",
      "Validation: Epoch [3], Batch [548/938], Loss: 1.5680224895477295\n",
      "Validation: Epoch [3], Batch [549/938], Loss: 1.513702630996704\n",
      "Validation: Epoch [3], Batch [550/938], Loss: 1.6730254888534546\n",
      "Validation: Epoch [3], Batch [551/938], Loss: 1.5780929327011108\n",
      "Validation: Epoch [3], Batch [552/938], Loss: 1.5888049602508545\n",
      "Validation: Epoch [3], Batch [553/938], Loss: 1.6633539199829102\n",
      "Validation: Epoch [3], Batch [554/938], Loss: 1.6295957565307617\n",
      "Validation: Epoch [3], Batch [555/938], Loss: 1.6636860370635986\n",
      "Validation: Epoch [3], Batch [556/938], Loss: 1.6209721565246582\n",
      "Validation: Epoch [3], Batch [557/938], Loss: 1.6664292812347412\n",
      "Validation: Epoch [3], Batch [558/938], Loss: 1.7982479333877563\n",
      "Validation: Epoch [3], Batch [559/938], Loss: 1.676195502281189\n",
      "Validation: Epoch [3], Batch [560/938], Loss: 1.7279634475708008\n",
      "Validation: Epoch [3], Batch [561/938], Loss: 1.58965003490448\n",
      "Validation: Epoch [3], Batch [562/938], Loss: 1.7645847797393799\n",
      "Validation: Epoch [3], Batch [563/938], Loss: 1.6552823781967163\n",
      "Validation: Epoch [3], Batch [564/938], Loss: 1.5603623390197754\n",
      "Validation: Epoch [3], Batch [565/938], Loss: 1.8015590906143188\n",
      "Validation: Epoch [3], Batch [566/938], Loss: 1.741731882095337\n",
      "Validation: Epoch [3], Batch [567/938], Loss: 1.826341986656189\n",
      "Validation: Epoch [3], Batch [568/938], Loss: 1.7380179166793823\n",
      "Validation: Epoch [3], Batch [569/938], Loss: 1.7493791580200195\n",
      "Validation: Epoch [3], Batch [570/938], Loss: 1.6078531742095947\n",
      "Validation: Epoch [3], Batch [571/938], Loss: 1.7172465324401855\n",
      "Validation: Epoch [3], Batch [572/938], Loss: 1.4812657833099365\n",
      "Validation: Epoch [3], Batch [573/938], Loss: 1.6935999393463135\n",
      "Validation: Epoch [3], Batch [574/938], Loss: 1.650400161743164\n",
      "Validation: Epoch [3], Batch [575/938], Loss: 1.629363775253296\n",
      "Validation: Epoch [3], Batch [576/938], Loss: 1.774438738822937\n",
      "Validation: Epoch [3], Batch [577/938], Loss: 1.5882443189620972\n",
      "Validation: Epoch [3], Batch [578/938], Loss: 1.6728544235229492\n",
      "Validation: Epoch [3], Batch [579/938], Loss: 1.6612743139266968\n",
      "Validation: Epoch [3], Batch [580/938], Loss: 1.6579300165176392\n",
      "Validation: Epoch [3], Batch [581/938], Loss: 1.6639958620071411\n",
      "Validation: Epoch [3], Batch [582/938], Loss: 1.6155892610549927\n",
      "Validation: Epoch [3], Batch [583/938], Loss: 1.7839165925979614\n",
      "Validation: Epoch [3], Batch [584/938], Loss: 1.5259186029434204\n",
      "Validation: Epoch [3], Batch [585/938], Loss: 1.752485752105713\n",
      "Validation: Epoch [3], Batch [586/938], Loss: 1.6380234956741333\n",
      "Validation: Epoch [3], Batch [587/938], Loss: 1.646677017211914\n",
      "Validation: Epoch [3], Batch [588/938], Loss: 1.5504549741744995\n",
      "Validation: Epoch [3], Batch [589/938], Loss: 1.534592628479004\n",
      "Validation: Epoch [3], Batch [590/938], Loss: 1.7641801834106445\n",
      "Validation: Epoch [3], Batch [591/938], Loss: 1.541055679321289\n",
      "Validation: Epoch [3], Batch [592/938], Loss: 1.5553110837936401\n",
      "Validation: Epoch [3], Batch [593/938], Loss: 1.6954482793807983\n",
      "Validation: Epoch [3], Batch [594/938], Loss: 1.5904285907745361\n",
      "Validation: Epoch [3], Batch [595/938], Loss: 1.679009199142456\n",
      "Validation: Epoch [3], Batch [596/938], Loss: 1.5489997863769531\n",
      "Validation: Epoch [3], Batch [597/938], Loss: 1.5806958675384521\n",
      "Validation: Epoch [3], Batch [598/938], Loss: 1.795019507408142\n",
      "Validation: Epoch [3], Batch [599/938], Loss: 1.549904704093933\n",
      "Validation: Epoch [3], Batch [600/938], Loss: 1.7651418447494507\n",
      "Validation: Epoch [3], Batch [601/938], Loss: 1.626088261604309\n",
      "Validation: Epoch [3], Batch [602/938], Loss: 1.7764389514923096\n",
      "Validation: Epoch [3], Batch [603/938], Loss: 1.671047568321228\n",
      "Validation: Epoch [3], Batch [604/938], Loss: 1.6704967021942139\n",
      "Validation: Epoch [3], Batch [605/938], Loss: 1.8395133018493652\n",
      "Validation: Epoch [3], Batch [606/938], Loss: 1.592913269996643\n",
      "Validation: Epoch [3], Batch [607/938], Loss: 1.6752755641937256\n",
      "Validation: Epoch [3], Batch [608/938], Loss: 1.5814166069030762\n",
      "Validation: Epoch [3], Batch [609/938], Loss: 1.7039463520050049\n",
      "Validation: Epoch [3], Batch [610/938], Loss: 1.6709011793136597\n",
      "Validation: Epoch [3], Batch [611/938], Loss: 1.4810583591461182\n",
      "Validation: Epoch [3], Batch [612/938], Loss: 1.899298906326294\n",
      "Validation: Epoch [3], Batch [613/938], Loss: 1.6234194040298462\n",
      "Validation: Epoch [3], Batch [614/938], Loss: 1.6599955558776855\n",
      "Validation: Epoch [3], Batch [615/938], Loss: 1.6050220727920532\n",
      "Validation: Epoch [3], Batch [616/938], Loss: 1.565927267074585\n",
      "Validation: Epoch [3], Batch [617/938], Loss: 1.7963539361953735\n",
      "Validation: Epoch [3], Batch [618/938], Loss: 1.6943283081054688\n",
      "Validation: Epoch [3], Batch [619/938], Loss: 1.560143232345581\n",
      "Validation: Epoch [3], Batch [620/938], Loss: 1.8474376201629639\n",
      "Validation: Epoch [3], Batch [621/938], Loss: 1.87959885597229\n",
      "Validation: Epoch [3], Batch [622/938], Loss: 1.6060384511947632\n",
      "Validation: Epoch [3], Batch [623/938], Loss: 1.7134100198745728\n",
      "Validation: Epoch [3], Batch [624/938], Loss: 1.6563074588775635\n",
      "Validation: Epoch [3], Batch [625/938], Loss: 1.592897891998291\n",
      "Validation: Epoch [3], Batch [626/938], Loss: 1.7954457998275757\n",
      "Validation: Epoch [3], Batch [627/938], Loss: 1.6194020509719849\n",
      "Validation: Epoch [3], Batch [628/938], Loss: 1.72030770778656\n",
      "Validation: Epoch [3], Batch [629/938], Loss: 1.6157411336898804\n",
      "Validation: Epoch [3], Batch [630/938], Loss: 1.5476068258285522\n",
      "Validation: Epoch [3], Batch [631/938], Loss: 1.489457368850708\n",
      "Validation: Epoch [3], Batch [632/938], Loss: 1.6556363105773926\n",
      "Validation: Epoch [3], Batch [633/938], Loss: 1.504478931427002\n",
      "Validation: Epoch [3], Batch [634/938], Loss: 1.667006254196167\n",
      "Validation: Epoch [3], Batch [635/938], Loss: 1.7163896560668945\n",
      "Validation: Epoch [3], Batch [636/938], Loss: 1.5441091060638428\n",
      "Validation: Epoch [3], Batch [637/938], Loss: 1.755292296409607\n",
      "Validation: Epoch [3], Batch [638/938], Loss: 1.6471614837646484\n",
      "Validation: Epoch [3], Batch [639/938], Loss: 1.7151869535446167\n",
      "Validation: Epoch [3], Batch [640/938], Loss: 1.573188304901123\n",
      "Validation: Epoch [3], Batch [641/938], Loss: 1.628377914428711\n",
      "Validation: Epoch [3], Batch [642/938], Loss: 1.4802912473678589\n",
      "Validation: Epoch [3], Batch [643/938], Loss: 1.6732920408248901\n",
      "Validation: Epoch [3], Batch [644/938], Loss: 1.7208969593048096\n",
      "Validation: Epoch [3], Batch [645/938], Loss: 1.6873610019683838\n",
      "Validation: Epoch [3], Batch [646/938], Loss: 1.7338987588882446\n",
      "Validation: Epoch [3], Batch [647/938], Loss: 1.5666753053665161\n",
      "Validation: Epoch [3], Batch [648/938], Loss: 1.6657063961029053\n",
      "Validation: Epoch [3], Batch [649/938], Loss: 1.6563154458999634\n",
      "Validation: Epoch [3], Batch [650/938], Loss: 1.4744954109191895\n",
      "Validation: Epoch [3], Batch [651/938], Loss: 1.617281198501587\n",
      "Validation: Epoch [3], Batch [652/938], Loss: 1.656240701675415\n",
      "Validation: Epoch [3], Batch [653/938], Loss: 1.5684096813201904\n",
      "Validation: Epoch [3], Batch [654/938], Loss: 1.6768304109573364\n",
      "Validation: Epoch [3], Batch [655/938], Loss: 1.691721796989441\n",
      "Validation: Epoch [3], Batch [656/938], Loss: 1.6737799644470215\n",
      "Validation: Epoch [3], Batch [657/938], Loss: 1.6689823865890503\n",
      "Validation: Epoch [3], Batch [658/938], Loss: 1.6730666160583496\n",
      "Validation: Epoch [3], Batch [659/938], Loss: 1.641075611114502\n",
      "Validation: Epoch [3], Batch [660/938], Loss: 1.7167671918869019\n",
      "Validation: Epoch [3], Batch [661/938], Loss: 1.5522987842559814\n",
      "Validation: Epoch [3], Batch [662/938], Loss: 1.6968834400177002\n",
      "Validation: Epoch [3], Batch [663/938], Loss: 1.570956826210022\n",
      "Validation: Epoch [3], Batch [664/938], Loss: 1.4637221097946167\n",
      "Validation: Epoch [3], Batch [665/938], Loss: 1.7935724258422852\n",
      "Validation: Epoch [3], Batch [666/938], Loss: 1.5454883575439453\n",
      "Validation: Epoch [3], Batch [667/938], Loss: 1.6979235410690308\n",
      "Validation: Epoch [3], Batch [668/938], Loss: 1.4829812049865723\n",
      "Validation: Epoch [3], Batch [669/938], Loss: 1.7067780494689941\n",
      "Validation: Epoch [3], Batch [670/938], Loss: 1.7565178871154785\n",
      "Validation: Epoch [3], Batch [671/938], Loss: 1.7754565477371216\n",
      "Validation: Epoch [3], Batch [672/938], Loss: 1.5718069076538086\n",
      "Validation: Epoch [3], Batch [673/938], Loss: 1.8403040170669556\n",
      "Validation: Epoch [3], Batch [674/938], Loss: 1.7839648723602295\n",
      "Validation: Epoch [3], Batch [675/938], Loss: 1.7322638034820557\n",
      "Validation: Epoch [3], Batch [676/938], Loss: 1.673905611038208\n",
      "Validation: Epoch [3], Batch [677/938], Loss: 1.7807797193527222\n",
      "Validation: Epoch [3], Batch [678/938], Loss: 1.6236883401870728\n",
      "Validation: Epoch [3], Batch [679/938], Loss: 1.4440422058105469\n",
      "Validation: Epoch [3], Batch [680/938], Loss: 1.5124173164367676\n",
      "Validation: Epoch [3], Batch [681/938], Loss: 1.5253022909164429\n",
      "Validation: Epoch [3], Batch [682/938], Loss: 1.8234608173370361\n",
      "Validation: Epoch [3], Batch [683/938], Loss: 1.5414105653762817\n",
      "Validation: Epoch [3], Batch [684/938], Loss: 1.5434746742248535\n",
      "Validation: Epoch [3], Batch [685/938], Loss: 1.7517741918563843\n",
      "Validation: Epoch [3], Batch [686/938], Loss: 1.691603183746338\n",
      "Validation: Epoch [3], Batch [687/938], Loss: 1.718016266822815\n",
      "Validation: Epoch [3], Batch [688/938], Loss: 1.880177617073059\n",
      "Validation: Epoch [3], Batch [689/938], Loss: 1.6698108911514282\n",
      "Validation: Epoch [3], Batch [690/938], Loss: 1.3505687713623047\n",
      "Validation: Epoch [3], Batch [691/938], Loss: 1.7285425662994385\n",
      "Validation: Epoch [3], Batch [692/938], Loss: 1.809314489364624\n",
      "Validation: Epoch [3], Batch [693/938], Loss: 1.605943202972412\n",
      "Validation: Epoch [3], Batch [694/938], Loss: 1.5233535766601562\n",
      "Validation: Epoch [3], Batch [695/938], Loss: 1.6203044652938843\n",
      "Validation: Epoch [3], Batch [696/938], Loss: 1.6631008386611938\n",
      "Validation: Epoch [3], Batch [697/938], Loss: 1.6658923625946045\n",
      "Validation: Epoch [3], Batch [698/938], Loss: 1.598602294921875\n",
      "Validation: Epoch [3], Batch [699/938], Loss: 1.4601240158081055\n",
      "Validation: Epoch [3], Batch [700/938], Loss: 1.793452501296997\n",
      "Validation: Epoch [3], Batch [701/938], Loss: 1.6903270483016968\n",
      "Validation: Epoch [3], Batch [702/938], Loss: 1.7515465021133423\n",
      "Validation: Epoch [3], Batch [703/938], Loss: 1.5666334629058838\n",
      "Validation: Epoch [3], Batch [704/938], Loss: 1.7362663745880127\n",
      "Validation: Epoch [3], Batch [705/938], Loss: 1.6273843050003052\n",
      "Validation: Epoch [3], Batch [706/938], Loss: 1.8222230672836304\n",
      "Validation: Epoch [3], Batch [707/938], Loss: 1.6242084503173828\n",
      "Validation: Epoch [3], Batch [708/938], Loss: 1.5545399188995361\n",
      "Validation: Epoch [3], Batch [709/938], Loss: 1.6792901754379272\n",
      "Validation: Epoch [3], Batch [710/938], Loss: 1.5895648002624512\n",
      "Validation: Epoch [3], Batch [711/938], Loss: 1.7163305282592773\n",
      "Validation: Epoch [3], Batch [712/938], Loss: 1.4611976146697998\n",
      "Validation: Epoch [3], Batch [713/938], Loss: 1.489697813987732\n",
      "Validation: Epoch [3], Batch [714/938], Loss: 1.7253751754760742\n",
      "Validation: Epoch [3], Batch [715/938], Loss: 1.532292366027832\n",
      "Validation: Epoch [3], Batch [716/938], Loss: 1.5383220911026\n",
      "Validation: Epoch [3], Batch [717/938], Loss: 1.6781692504882812\n",
      "Validation: Epoch [3], Batch [718/938], Loss: 1.6068702936172485\n",
      "Validation: Epoch [3], Batch [719/938], Loss: 1.6937543153762817\n",
      "Validation: Epoch [3], Batch [720/938], Loss: 1.8186289072036743\n",
      "Validation: Epoch [3], Batch [721/938], Loss: 1.6104642152786255\n",
      "Validation: Epoch [3], Batch [722/938], Loss: 1.7332490682601929\n",
      "Validation: Epoch [3], Batch [723/938], Loss: 1.7682908773422241\n",
      "Validation: Epoch [3], Batch [724/938], Loss: 1.7651374340057373\n",
      "Validation: Epoch [3], Batch [725/938], Loss: 1.52241051197052\n",
      "Validation: Epoch [3], Batch [726/938], Loss: 1.5724269151687622\n",
      "Validation: Epoch [3], Batch [727/938], Loss: 1.67850923538208\n",
      "Validation: Epoch [3], Batch [728/938], Loss: 1.5239437818527222\n",
      "Validation: Epoch [3], Batch [729/938], Loss: 1.7009294033050537\n",
      "Validation: Epoch [3], Batch [730/938], Loss: 1.5197408199310303\n",
      "Validation: Epoch [3], Batch [731/938], Loss: 1.6770132780075073\n",
      "Validation: Epoch [3], Batch [732/938], Loss: 1.4755728244781494\n",
      "Validation: Epoch [3], Batch [733/938], Loss: 1.6799049377441406\n",
      "Validation: Epoch [3], Batch [734/938], Loss: 1.693582534790039\n",
      "Validation: Epoch [3], Batch [735/938], Loss: 1.4390395879745483\n",
      "Validation: Epoch [3], Batch [736/938], Loss: 1.6429193019866943\n",
      "Validation: Epoch [3], Batch [737/938], Loss: 1.6402459144592285\n",
      "Validation: Epoch [3], Batch [738/938], Loss: 1.5976753234863281\n",
      "Validation: Epoch [3], Batch [739/938], Loss: 1.5793168544769287\n",
      "Validation: Epoch [3], Batch [740/938], Loss: 1.861142635345459\n",
      "Validation: Epoch [3], Batch [741/938], Loss: 1.7898180484771729\n",
      "Validation: Epoch [3], Batch [742/938], Loss: 1.7150923013687134\n",
      "Validation: Epoch [3], Batch [743/938], Loss: 1.5585055351257324\n",
      "Validation: Epoch [3], Batch [744/938], Loss: 1.545963168144226\n",
      "Validation: Epoch [3], Batch [745/938], Loss: 1.6907620429992676\n",
      "Validation: Epoch [3], Batch [746/938], Loss: 1.6213687658309937\n",
      "Validation: Epoch [3], Batch [747/938], Loss: 1.7208080291748047\n",
      "Validation: Epoch [3], Batch [748/938], Loss: 1.7090024948120117\n",
      "Validation: Epoch [3], Batch [749/938], Loss: 1.9518609046936035\n",
      "Validation: Epoch [3], Batch [750/938], Loss: 1.6438556909561157\n",
      "Validation: Epoch [3], Batch [751/938], Loss: 1.735259771347046\n",
      "Validation: Epoch [3], Batch [752/938], Loss: 1.5123419761657715\n",
      "Validation: Epoch [3], Batch [753/938], Loss: 1.6368842124938965\n",
      "Validation: Epoch [3], Batch [754/938], Loss: 1.61812162399292\n",
      "Validation: Epoch [3], Batch [755/938], Loss: 1.4771440029144287\n",
      "Validation: Epoch [3], Batch [756/938], Loss: 1.7526153326034546\n",
      "Validation: Epoch [3], Batch [757/938], Loss: 1.7014039754867554\n",
      "Validation: Epoch [3], Batch [758/938], Loss: 1.7180709838867188\n",
      "Validation: Epoch [3], Batch [759/938], Loss: 1.6073538064956665\n",
      "Validation: Epoch [3], Batch [760/938], Loss: 1.7395131587982178\n",
      "Validation: Epoch [3], Batch [761/938], Loss: 1.5789438486099243\n",
      "Validation: Epoch [3], Batch [762/938], Loss: 1.7659772634506226\n",
      "Validation: Epoch [3], Batch [763/938], Loss: 1.5717231035232544\n",
      "Validation: Epoch [3], Batch [764/938], Loss: 1.6415989398956299\n",
      "Validation: Epoch [3], Batch [765/938], Loss: 1.5793458223342896\n",
      "Validation: Epoch [3], Batch [766/938], Loss: 1.6511902809143066\n",
      "Validation: Epoch [3], Batch [767/938], Loss: 1.6652247905731201\n",
      "Validation: Epoch [3], Batch [768/938], Loss: 1.670631766319275\n",
      "Validation: Epoch [3], Batch [769/938], Loss: 1.7501444816589355\n",
      "Validation: Epoch [3], Batch [770/938], Loss: 1.597951054573059\n",
      "Validation: Epoch [3], Batch [771/938], Loss: 1.7191550731658936\n",
      "Validation: Epoch [3], Batch [772/938], Loss: 1.4234018325805664\n",
      "Validation: Epoch [3], Batch [773/938], Loss: 1.5292550325393677\n",
      "Validation: Epoch [3], Batch [774/938], Loss: 1.662407398223877\n",
      "Validation: Epoch [3], Batch [775/938], Loss: 1.777329683303833\n",
      "Validation: Epoch [3], Batch [776/938], Loss: 1.5295441150665283\n",
      "Validation: Epoch [3], Batch [777/938], Loss: 1.651199460029602\n",
      "Validation: Epoch [3], Batch [778/938], Loss: 1.5836682319641113\n",
      "Validation: Epoch [3], Batch [779/938], Loss: 1.4817216396331787\n",
      "Validation: Epoch [3], Batch [780/938], Loss: 1.522513508796692\n",
      "Validation: Epoch [3], Batch [781/938], Loss: 1.7659653425216675\n",
      "Validation: Epoch [3], Batch [782/938], Loss: 1.690732479095459\n",
      "Validation: Epoch [3], Batch [783/938], Loss: 1.62367582321167\n",
      "Validation: Epoch [3], Batch [784/938], Loss: 1.524095892906189\n",
      "Validation: Epoch [3], Batch [785/938], Loss: 1.6593507528305054\n",
      "Validation: Epoch [3], Batch [786/938], Loss: 1.763767957687378\n",
      "Validation: Epoch [3], Batch [787/938], Loss: 1.5796236991882324\n",
      "Validation: Epoch [3], Batch [788/938], Loss: 1.5257099866867065\n",
      "Validation: Epoch [3], Batch [789/938], Loss: 1.667077898979187\n",
      "Validation: Epoch [3], Batch [790/938], Loss: 1.615969181060791\n",
      "Validation: Epoch [3], Batch [791/938], Loss: 1.6201039552688599\n",
      "Validation: Epoch [3], Batch [792/938], Loss: 1.655454158782959\n",
      "Validation: Epoch [3], Batch [793/938], Loss: 1.7784650325775146\n",
      "Validation: Epoch [3], Batch [794/938], Loss: 1.5319950580596924\n",
      "Validation: Epoch [3], Batch [795/938], Loss: 1.8549259901046753\n",
      "Validation: Epoch [3], Batch [796/938], Loss: 1.5376262664794922\n",
      "Validation: Epoch [3], Batch [797/938], Loss: 2.0196731090545654\n",
      "Validation: Epoch [3], Batch [798/938], Loss: 1.7303259372711182\n",
      "Validation: Epoch [3], Batch [799/938], Loss: 1.6979745626449585\n",
      "Validation: Epoch [3], Batch [800/938], Loss: 1.7698125839233398\n",
      "Validation: Epoch [3], Batch [801/938], Loss: 1.7758740186691284\n",
      "Validation: Epoch [3], Batch [802/938], Loss: 1.7001609802246094\n",
      "Validation: Epoch [3], Batch [803/938], Loss: 1.4583362340927124\n",
      "Validation: Epoch [3], Batch [804/938], Loss: 1.6958633661270142\n",
      "Validation: Epoch [3], Batch [805/938], Loss: 1.4203526973724365\n",
      "Validation: Epoch [3], Batch [806/938], Loss: 1.6701165437698364\n",
      "Validation: Epoch [3], Batch [807/938], Loss: 1.6361685991287231\n",
      "Validation: Epoch [3], Batch [808/938], Loss: 1.6400883197784424\n",
      "Validation: Epoch [3], Batch [809/938], Loss: 1.595763921737671\n",
      "Validation: Epoch [3], Batch [810/938], Loss: 1.6853463649749756\n",
      "Validation: Epoch [3], Batch [811/938], Loss: 1.6248700618743896\n",
      "Validation: Epoch [3], Batch [812/938], Loss: 1.7968875169754028\n",
      "Validation: Epoch [3], Batch [813/938], Loss: 1.6984106302261353\n",
      "Validation: Epoch [3], Batch [814/938], Loss: 1.5205069780349731\n",
      "Validation: Epoch [3], Batch [815/938], Loss: 1.7144527435302734\n",
      "Validation: Epoch [3], Batch [816/938], Loss: 1.5585942268371582\n",
      "Validation: Epoch [3], Batch [817/938], Loss: 1.6481868028640747\n",
      "Validation: Epoch [3], Batch [818/938], Loss: 1.8015272617340088\n",
      "Validation: Epoch [3], Batch [819/938], Loss: 1.513722538948059\n",
      "Validation: Epoch [3], Batch [820/938], Loss: 1.581735610961914\n",
      "Validation: Epoch [3], Batch [821/938], Loss: 1.8641080856323242\n",
      "Validation: Epoch [3], Batch [822/938], Loss: 1.5680937767028809\n",
      "Validation: Epoch [3], Batch [823/938], Loss: 1.4213898181915283\n",
      "Validation: Epoch [3], Batch [824/938], Loss: 1.7719041109085083\n",
      "Validation: Epoch [3], Batch [825/938], Loss: 1.721624493598938\n",
      "Validation: Epoch [3], Batch [826/938], Loss: 1.6386651992797852\n",
      "Validation: Epoch [3], Batch [827/938], Loss: 1.7308493852615356\n",
      "Validation: Epoch [3], Batch [828/938], Loss: 1.4465800523757935\n",
      "Validation: Epoch [3], Batch [829/938], Loss: 1.4558758735656738\n",
      "Validation: Epoch [3], Batch [830/938], Loss: 1.6129369735717773\n",
      "Validation: Epoch [3], Batch [831/938], Loss: 1.6984148025512695\n",
      "Validation: Epoch [3], Batch [832/938], Loss: 1.8113532066345215\n",
      "Validation: Epoch [3], Batch [833/938], Loss: 1.649829387664795\n",
      "Validation: Epoch [3], Batch [834/938], Loss: 1.7024033069610596\n",
      "Validation: Epoch [3], Batch [835/938], Loss: 1.6159085035324097\n",
      "Validation: Epoch [3], Batch [836/938], Loss: 1.4697558879852295\n",
      "Validation: Epoch [3], Batch [837/938], Loss: 1.68474280834198\n",
      "Validation: Epoch [3], Batch [838/938], Loss: 1.6351096630096436\n",
      "Validation: Epoch [3], Batch [839/938], Loss: 1.6222444772720337\n",
      "Validation: Epoch [3], Batch [840/938], Loss: 1.6985095739364624\n",
      "Validation: Epoch [3], Batch [841/938], Loss: 1.4276494979858398\n",
      "Validation: Epoch [3], Batch [842/938], Loss: 1.7976382970809937\n",
      "Validation: Epoch [3], Batch [843/938], Loss: 1.6828715801239014\n",
      "Validation: Epoch [3], Batch [844/938], Loss: 1.6010234355926514\n",
      "Validation: Epoch [3], Batch [845/938], Loss: 1.7045636177062988\n",
      "Validation: Epoch [3], Batch [846/938], Loss: 1.6373790502548218\n",
      "Validation: Epoch [3], Batch [847/938], Loss: 1.580881953239441\n",
      "Validation: Epoch [3], Batch [848/938], Loss: 1.5679314136505127\n",
      "Validation: Epoch [3], Batch [849/938], Loss: 1.725024938583374\n",
      "Validation: Epoch [3], Batch [850/938], Loss: 1.5940319299697876\n",
      "Validation: Epoch [3], Batch [851/938], Loss: 1.5958595275878906\n",
      "Validation: Epoch [3], Batch [852/938], Loss: 1.550102710723877\n",
      "Validation: Epoch [3], Batch [853/938], Loss: 1.5496842861175537\n",
      "Validation: Epoch [3], Batch [854/938], Loss: 1.737311840057373\n",
      "Validation: Epoch [3], Batch [855/938], Loss: 1.6110923290252686\n",
      "Validation: Epoch [3], Batch [856/938], Loss: 1.694273829460144\n",
      "Validation: Epoch [3], Batch [857/938], Loss: 1.5647003650665283\n",
      "Validation: Epoch [3], Batch [858/938], Loss: 1.6327098608016968\n",
      "Validation: Epoch [3], Batch [859/938], Loss: 1.7779444456100464\n",
      "Validation: Epoch [3], Batch [860/938], Loss: 1.569793701171875\n",
      "Validation: Epoch [3], Batch [861/938], Loss: 1.6353836059570312\n",
      "Validation: Epoch [3], Batch [862/938], Loss: 1.811767339706421\n",
      "Validation: Epoch [3], Batch [863/938], Loss: 1.6649501323699951\n",
      "Validation: Epoch [3], Batch [864/938], Loss: 1.7956719398498535\n",
      "Validation: Epoch [3], Batch [865/938], Loss: 1.5419291257858276\n",
      "Validation: Epoch [3], Batch [866/938], Loss: 1.6260671615600586\n",
      "Validation: Epoch [3], Batch [867/938], Loss: 1.5617854595184326\n",
      "Validation: Epoch [3], Batch [868/938], Loss: 1.6112223863601685\n",
      "Validation: Epoch [3], Batch [869/938], Loss: 1.4974086284637451\n",
      "Validation: Epoch [3], Batch [870/938], Loss: 1.748870611190796\n",
      "Validation: Epoch [3], Batch [871/938], Loss: 1.575215220451355\n",
      "Validation: Epoch [3], Batch [872/938], Loss: 1.5566333532333374\n",
      "Validation: Epoch [3], Batch [873/938], Loss: 1.7179986238479614\n",
      "Validation: Epoch [3], Batch [874/938], Loss: 1.5476863384246826\n",
      "Validation: Epoch [3], Batch [875/938], Loss: 1.5227068662643433\n",
      "Validation: Epoch [3], Batch [876/938], Loss: 1.5832109451293945\n",
      "Validation: Epoch [3], Batch [877/938], Loss: 1.6232714653015137\n",
      "Validation: Epoch [3], Batch [878/938], Loss: 1.6591449975967407\n",
      "Validation: Epoch [3], Batch [879/938], Loss: 1.5676130056381226\n",
      "Validation: Epoch [3], Batch [880/938], Loss: 1.5663795471191406\n",
      "Validation: Epoch [3], Batch [881/938], Loss: 1.7540936470031738\n",
      "Validation: Epoch [3], Batch [882/938], Loss: 1.6800806522369385\n",
      "Validation: Epoch [3], Batch [883/938], Loss: 1.5602792501449585\n",
      "Validation: Epoch [3], Batch [884/938], Loss: 1.647567868232727\n",
      "Validation: Epoch [3], Batch [885/938], Loss: 1.5988081693649292\n",
      "Validation: Epoch [3], Batch [886/938], Loss: 1.578802227973938\n",
      "Validation: Epoch [3], Batch [887/938], Loss: 1.6304470300674438\n",
      "Validation: Epoch [3], Batch [888/938], Loss: 1.5953028202056885\n",
      "Validation: Epoch [3], Batch [889/938], Loss: 1.7566605806350708\n",
      "Validation: Epoch [3], Batch [890/938], Loss: 1.5933609008789062\n",
      "Validation: Epoch [3], Batch [891/938], Loss: 1.7299995422363281\n",
      "Validation: Epoch [3], Batch [892/938], Loss: 1.7131026983261108\n",
      "Validation: Epoch [3], Batch [893/938], Loss: 1.430671215057373\n",
      "Validation: Epoch [3], Batch [894/938], Loss: 1.7630325555801392\n",
      "Validation: Epoch [3], Batch [895/938], Loss: 1.4871830940246582\n",
      "Validation: Epoch [3], Batch [896/938], Loss: 1.5550414323806763\n",
      "Validation: Epoch [3], Batch [897/938], Loss: 1.6199287176132202\n",
      "Validation: Epoch [3], Batch [898/938], Loss: 1.452881097793579\n",
      "Validation: Epoch [3], Batch [899/938], Loss: 1.54395592212677\n",
      "Validation: Epoch [3], Batch [900/938], Loss: 1.7038679122924805\n",
      "Validation: Epoch [3], Batch [901/938], Loss: 1.564316987991333\n",
      "Validation: Epoch [3], Batch [902/938], Loss: 1.654276967048645\n",
      "Validation: Epoch [3], Batch [903/938], Loss: 1.5241779088974\n",
      "Validation: Epoch [3], Batch [904/938], Loss: 1.7168221473693848\n",
      "Validation: Epoch [3], Batch [905/938], Loss: 1.648165225982666\n",
      "Validation: Epoch [3], Batch [906/938], Loss: 1.6348216533660889\n",
      "Validation: Epoch [3], Batch [907/938], Loss: 1.5687158107757568\n",
      "Validation: Epoch [3], Batch [908/938], Loss: 1.541702389717102\n",
      "Validation: Epoch [3], Batch [909/938], Loss: 1.6663610935211182\n",
      "Validation: Epoch [3], Batch [910/938], Loss: 1.6669535636901855\n",
      "Validation: Epoch [3], Batch [911/938], Loss: 1.7706912755966187\n",
      "Validation: Epoch [3], Batch [912/938], Loss: 1.586449146270752\n",
      "Validation: Epoch [3], Batch [913/938], Loss: 1.70851469039917\n",
      "Validation: Epoch [3], Batch [914/938], Loss: 1.5843449831008911\n",
      "Validation: Epoch [3], Batch [915/938], Loss: 1.7499942779541016\n",
      "Validation: Epoch [3], Batch [916/938], Loss: 1.5115208625793457\n",
      "Validation: Epoch [3], Batch [917/938], Loss: 1.8068187236785889\n",
      "Validation: Epoch [3], Batch [918/938], Loss: 1.6406923532485962\n",
      "Validation: Epoch [3], Batch [919/938], Loss: 1.637758731842041\n",
      "Validation: Epoch [3], Batch [920/938], Loss: 1.594946265220642\n",
      "Validation: Epoch [3], Batch [921/938], Loss: 1.582808256149292\n",
      "Validation: Epoch [3], Batch [922/938], Loss: 1.7131468057632446\n",
      "Validation: Epoch [3], Batch [923/938], Loss: 1.723837971687317\n",
      "Validation: Epoch [3], Batch [924/938], Loss: 1.6938680410385132\n",
      "Validation: Epoch [3], Batch [925/938], Loss: 1.7240941524505615\n",
      "Validation: Epoch [3], Batch [926/938], Loss: 1.5640544891357422\n",
      "Validation: Epoch [3], Batch [927/938], Loss: 1.5130337476730347\n",
      "Validation: Epoch [3], Batch [928/938], Loss: 1.4884655475616455\n",
      "Validation: Epoch [3], Batch [929/938], Loss: 1.5645560026168823\n",
      "Validation: Epoch [3], Batch [930/938], Loss: 1.6918026208877563\n",
      "Validation: Epoch [3], Batch [931/938], Loss: 1.7535911798477173\n",
      "Validation: Epoch [3], Batch [932/938], Loss: 1.6826839447021484\n",
      "Validation: Epoch [3], Batch [933/938], Loss: 1.6165382862091064\n",
      "Validation: Epoch [3], Batch [934/938], Loss: 1.6397387981414795\n",
      "Validation: Epoch [3], Batch [935/938], Loss: 1.6146358251571655\n",
      "Validation: Epoch [3], Batch [936/938], Loss: 1.619193196296692\n",
      "Validation: Epoch [3], Batch [937/938], Loss: 1.5939655303955078\n",
      "Validation: Epoch [3], Batch [938/938], Loss: 1.6974844932556152\n",
      "Accuracy of test set: 0.40996666666666665\n",
      "Train: Epoch [4], Batch [1/938], Loss: 1.6144381761550903\n",
      "Train: Epoch [4], Batch [2/938], Loss: 1.5770678520202637\n",
      "Train: Epoch [4], Batch [3/938], Loss: 1.6888909339904785\n",
      "Train: Epoch [4], Batch [4/938], Loss: 1.4835717678070068\n",
      "Train: Epoch [4], Batch [5/938], Loss: 1.7221237421035767\n",
      "Train: Epoch [4], Batch [6/938], Loss: 1.6207306385040283\n",
      "Train: Epoch [4], Batch [7/938], Loss: 1.6286569833755493\n",
      "Train: Epoch [4], Batch [8/938], Loss: 1.6275306940078735\n",
      "Train: Epoch [4], Batch [9/938], Loss: 1.681142807006836\n",
      "Train: Epoch [4], Batch [10/938], Loss: 1.655800223350525\n",
      "Train: Epoch [4], Batch [11/938], Loss: 1.6708250045776367\n",
      "Train: Epoch [4], Batch [12/938], Loss: 1.5142232179641724\n",
      "Train: Epoch [4], Batch [13/938], Loss: 1.4869964122772217\n",
      "Train: Epoch [4], Batch [14/938], Loss: 1.6484787464141846\n",
      "Train: Epoch [4], Batch [15/938], Loss: 1.507332444190979\n",
      "Train: Epoch [4], Batch [16/938], Loss: 1.5888748168945312\n",
      "Train: Epoch [4], Batch [17/938], Loss: 1.645076036453247\n",
      "Train: Epoch [4], Batch [18/938], Loss: 1.5612621307373047\n",
      "Train: Epoch [4], Batch [19/938], Loss: 1.5423341989517212\n",
      "Train: Epoch [4], Batch [20/938], Loss: 1.544600009918213\n",
      "Train: Epoch [4], Batch [21/938], Loss: 1.7386120557785034\n",
      "Train: Epoch [4], Batch [22/938], Loss: 1.6510003805160522\n",
      "Train: Epoch [4], Batch [23/938], Loss: 1.6032443046569824\n",
      "Train: Epoch [4], Batch [24/938], Loss: 1.4967916011810303\n",
      "Train: Epoch [4], Batch [25/938], Loss: 1.6650761365890503\n",
      "Train: Epoch [4], Batch [26/938], Loss: 1.6130741834640503\n",
      "Train: Epoch [4], Batch [27/938], Loss: 1.5564744472503662\n",
      "Train: Epoch [4], Batch [28/938], Loss: 1.7235411405563354\n",
      "Train: Epoch [4], Batch [29/938], Loss: 1.5985479354858398\n",
      "Train: Epoch [4], Batch [30/938], Loss: 1.7430938482284546\n",
      "Train: Epoch [4], Batch [31/938], Loss: 1.6019341945648193\n",
      "Train: Epoch [4], Batch [32/938], Loss: 1.5732300281524658\n",
      "Train: Epoch [4], Batch [33/938], Loss: 1.6691147089004517\n",
      "Train: Epoch [4], Batch [34/938], Loss: 1.7840181589126587\n",
      "Train: Epoch [4], Batch [35/938], Loss: 1.524100422859192\n",
      "Train: Epoch [4], Batch [36/938], Loss: 1.7480098009109497\n",
      "Train: Epoch [4], Batch [37/938], Loss: 1.6219160556793213\n",
      "Train: Epoch [4], Batch [38/938], Loss: 1.5187631845474243\n",
      "Train: Epoch [4], Batch [39/938], Loss: 1.67169988155365\n",
      "Train: Epoch [4], Batch [40/938], Loss: 1.5384533405303955\n",
      "Train: Epoch [4], Batch [41/938], Loss: 1.4586412906646729\n",
      "Train: Epoch [4], Batch [42/938], Loss: 1.5552740097045898\n",
      "Train: Epoch [4], Batch [43/938], Loss: 1.5990734100341797\n",
      "Train: Epoch [4], Batch [44/938], Loss: 1.6295905113220215\n",
      "Train: Epoch [4], Batch [45/938], Loss: 1.5880481004714966\n",
      "Train: Epoch [4], Batch [46/938], Loss: 1.7150108814239502\n",
      "Train: Epoch [4], Batch [47/938], Loss: 1.7176995277404785\n",
      "Train: Epoch [4], Batch [48/938], Loss: 1.5927385091781616\n",
      "Train: Epoch [4], Batch [49/938], Loss: 1.638627529144287\n",
      "Train: Epoch [4], Batch [50/938], Loss: 1.591624140739441\n",
      "Train: Epoch [4], Batch [51/938], Loss: 1.608378529548645\n",
      "Train: Epoch [4], Batch [52/938], Loss: 1.6444249153137207\n",
      "Train: Epoch [4], Batch [53/938], Loss: 1.715636968612671\n",
      "Train: Epoch [4], Batch [54/938], Loss: 1.7056807279586792\n",
      "Train: Epoch [4], Batch [55/938], Loss: 1.5363109111785889\n",
      "Train: Epoch [4], Batch [56/938], Loss: 1.536589503288269\n",
      "Train: Epoch [4], Batch [57/938], Loss: 1.4678244590759277\n",
      "Train: Epoch [4], Batch [58/938], Loss: 1.6812115907669067\n",
      "Train: Epoch [4], Batch [59/938], Loss: 1.6985090970993042\n",
      "Train: Epoch [4], Batch [60/938], Loss: 1.7588975429534912\n",
      "Train: Epoch [4], Batch [61/938], Loss: 1.6936588287353516\n",
      "Train: Epoch [4], Batch [62/938], Loss: 1.776829481124878\n",
      "Train: Epoch [4], Batch [63/938], Loss: 1.5471513271331787\n",
      "Train: Epoch [4], Batch [64/938], Loss: 1.8235588073730469\n",
      "Train: Epoch [4], Batch [65/938], Loss: 1.68215811252594\n",
      "Train: Epoch [4], Batch [66/938], Loss: 1.500317931175232\n",
      "Train: Epoch [4], Batch [67/938], Loss: 1.6810787916183472\n",
      "Train: Epoch [4], Batch [68/938], Loss: 1.495247721672058\n",
      "Train: Epoch [4], Batch [69/938], Loss: 1.7061641216278076\n",
      "Train: Epoch [4], Batch [70/938], Loss: 1.6241971254348755\n",
      "Train: Epoch [4], Batch [71/938], Loss: 1.694399356842041\n",
      "Train: Epoch [4], Batch [72/938], Loss: 1.5541768074035645\n",
      "Train: Epoch [4], Batch [73/938], Loss: 1.535420536994934\n",
      "Train: Epoch [4], Batch [74/938], Loss: 1.6195741891860962\n",
      "Train: Epoch [4], Batch [75/938], Loss: 1.598132848739624\n",
      "Train: Epoch [4], Batch [76/938], Loss: 1.5466903448104858\n",
      "Train: Epoch [4], Batch [77/938], Loss: 1.6696590185165405\n",
      "Train: Epoch [4], Batch [78/938], Loss: 1.7883093357086182\n",
      "Train: Epoch [4], Batch [79/938], Loss: 1.5197266340255737\n",
      "Train: Epoch [4], Batch [80/938], Loss: 1.641386866569519\n",
      "Train: Epoch [4], Batch [81/938], Loss: 1.7569605112075806\n",
      "Train: Epoch [4], Batch [82/938], Loss: 1.5431559085845947\n",
      "Train: Epoch [4], Batch [83/938], Loss: 1.6084163188934326\n",
      "Train: Epoch [4], Batch [84/938], Loss: 1.655869722366333\n",
      "Train: Epoch [4], Batch [85/938], Loss: 1.63077974319458\n",
      "Train: Epoch [4], Batch [86/938], Loss: 1.5734403133392334\n",
      "Train: Epoch [4], Batch [87/938], Loss: 1.5353938341140747\n",
      "Train: Epoch [4], Batch [88/938], Loss: 1.5561736822128296\n",
      "Train: Epoch [4], Batch [89/938], Loss: 1.489900827407837\n",
      "Train: Epoch [4], Batch [90/938], Loss: 1.5990409851074219\n",
      "Train: Epoch [4], Batch [91/938], Loss: 1.602432131767273\n",
      "Train: Epoch [4], Batch [92/938], Loss: 1.601689100265503\n",
      "Train: Epoch [4], Batch [93/938], Loss: 1.5965195894241333\n",
      "Train: Epoch [4], Batch [94/938], Loss: 1.5774850845336914\n",
      "Train: Epoch [4], Batch [95/938], Loss: 1.613548994064331\n",
      "Train: Epoch [4], Batch [96/938], Loss: 1.7903720140457153\n",
      "Train: Epoch [4], Batch [97/938], Loss: 1.5036059617996216\n",
      "Train: Epoch [4], Batch [98/938], Loss: 1.6221020221710205\n",
      "Train: Epoch [4], Batch [99/938], Loss: 1.5467151403427124\n",
      "Train: Epoch [4], Batch [100/938], Loss: 1.6134276390075684\n",
      "Train: Epoch [4], Batch [101/938], Loss: 1.7144877910614014\n",
      "Train: Epoch [4], Batch [102/938], Loss: 1.3179423809051514\n",
      "Train: Epoch [4], Batch [103/938], Loss: 1.617915153503418\n",
      "Train: Epoch [4], Batch [104/938], Loss: 1.4103469848632812\n",
      "Train: Epoch [4], Batch [105/938], Loss: 1.5890562534332275\n",
      "Train: Epoch [4], Batch [106/938], Loss: 1.6830490827560425\n",
      "Train: Epoch [4], Batch [107/938], Loss: 1.553778886795044\n",
      "Train: Epoch [4], Batch [108/938], Loss: 1.6653802394866943\n",
      "Train: Epoch [4], Batch [109/938], Loss: 1.6982494592666626\n",
      "Train: Epoch [4], Batch [110/938], Loss: 1.7907568216323853\n",
      "Train: Epoch [4], Batch [111/938], Loss: 1.7235435247421265\n",
      "Train: Epoch [4], Batch [112/938], Loss: 1.7438236474990845\n",
      "Train: Epoch [4], Batch [113/938], Loss: 1.6071245670318604\n",
      "Train: Epoch [4], Batch [114/938], Loss: 1.6570470333099365\n",
      "Train: Epoch [4], Batch [115/938], Loss: 1.6435818672180176\n",
      "Train: Epoch [4], Batch [116/938], Loss: 1.5660438537597656\n",
      "Train: Epoch [4], Batch [117/938], Loss: 1.6705577373504639\n",
      "Train: Epoch [4], Batch [118/938], Loss: 1.6771360635757446\n",
      "Train: Epoch [4], Batch [119/938], Loss: 1.4557617902755737\n",
      "Train: Epoch [4], Batch [120/938], Loss: 1.6721458435058594\n",
      "Train: Epoch [4], Batch [121/938], Loss: 1.6024926900863647\n",
      "Train: Epoch [4], Batch [122/938], Loss: 1.5475274324417114\n",
      "Train: Epoch [4], Batch [123/938], Loss: 1.676297664642334\n",
      "Train: Epoch [4], Batch [124/938], Loss: 1.6382699012756348\n",
      "Train: Epoch [4], Batch [125/938], Loss: 1.4419840574264526\n",
      "Train: Epoch [4], Batch [126/938], Loss: 1.6109675168991089\n",
      "Train: Epoch [4], Batch [127/938], Loss: 1.6761890649795532\n",
      "Train: Epoch [4], Batch [128/938], Loss: 1.5911520719528198\n",
      "Train: Epoch [4], Batch [129/938], Loss: 1.620002031326294\n",
      "Train: Epoch [4], Batch [130/938], Loss: 1.8353338241577148\n",
      "Train: Epoch [4], Batch [131/938], Loss: 1.6072218418121338\n",
      "Train: Epoch [4], Batch [132/938], Loss: 1.4502943754196167\n",
      "Train: Epoch [4], Batch [133/938], Loss: 1.6435532569885254\n",
      "Train: Epoch [4], Batch [134/938], Loss: 1.539097785949707\n",
      "Train: Epoch [4], Batch [135/938], Loss: 1.505885362625122\n",
      "Train: Epoch [4], Batch [136/938], Loss: 1.5189918279647827\n",
      "Train: Epoch [4], Batch [137/938], Loss: 1.4163317680358887\n",
      "Train: Epoch [4], Batch [138/938], Loss: 1.6430896520614624\n",
      "Train: Epoch [4], Batch [139/938], Loss: 1.3471415042877197\n",
      "Train: Epoch [4], Batch [140/938], Loss: 1.6137726306915283\n",
      "Train: Epoch [4], Batch [141/938], Loss: 1.5817064046859741\n",
      "Train: Epoch [4], Batch [142/938], Loss: 1.6947641372680664\n",
      "Train: Epoch [4], Batch [143/938], Loss: 1.5935436487197876\n",
      "Train: Epoch [4], Batch [144/938], Loss: 1.7545586824417114\n",
      "Train: Epoch [4], Batch [145/938], Loss: 1.4995834827423096\n",
      "Train: Epoch [4], Batch [146/938], Loss: 1.6031544208526611\n",
      "Train: Epoch [4], Batch [147/938], Loss: 1.32973313331604\n",
      "Train: Epoch [4], Batch [148/938], Loss: 1.4505736827850342\n",
      "Train: Epoch [4], Batch [149/938], Loss: 1.5693851709365845\n",
      "Train: Epoch [4], Batch [150/938], Loss: 1.4483774900436401\n",
      "Train: Epoch [4], Batch [151/938], Loss: 1.6991093158721924\n",
      "Train: Epoch [4], Batch [152/938], Loss: 1.581714153289795\n",
      "Train: Epoch [4], Batch [153/938], Loss: 1.5328325033187866\n",
      "Train: Epoch [4], Batch [154/938], Loss: 1.4995074272155762\n",
      "Train: Epoch [4], Batch [155/938], Loss: 1.59307861328125\n",
      "Train: Epoch [4], Batch [156/938], Loss: 1.6069213151931763\n",
      "Train: Epoch [4], Batch [157/938], Loss: 1.6343121528625488\n",
      "Train: Epoch [4], Batch [158/938], Loss: 1.4233086109161377\n",
      "Train: Epoch [4], Batch [159/938], Loss: 1.4507256746292114\n",
      "Train: Epoch [4], Batch [160/938], Loss: 1.4936989545822144\n",
      "Train: Epoch [4], Batch [161/938], Loss: 1.5244636535644531\n",
      "Train: Epoch [4], Batch [162/938], Loss: 1.4775103330612183\n",
      "Train: Epoch [4], Batch [163/938], Loss: 1.669732928276062\n",
      "Train: Epoch [4], Batch [164/938], Loss: 1.6856188774108887\n",
      "Train: Epoch [4], Batch [165/938], Loss: 1.4261152744293213\n",
      "Train: Epoch [4], Batch [166/938], Loss: 1.490194320678711\n",
      "Train: Epoch [4], Batch [167/938], Loss: 1.521586537361145\n",
      "Train: Epoch [4], Batch [168/938], Loss: 1.4716109037399292\n",
      "Train: Epoch [4], Batch [169/938], Loss: 1.6442644596099854\n",
      "Train: Epoch [4], Batch [170/938], Loss: 1.5368709564208984\n",
      "Train: Epoch [4], Batch [171/938], Loss: 1.4280991554260254\n",
      "Train: Epoch [4], Batch [172/938], Loss: 1.4910134077072144\n",
      "Train: Epoch [4], Batch [173/938], Loss: 1.7622849941253662\n",
      "Train: Epoch [4], Batch [174/938], Loss: 1.5430344343185425\n",
      "Train: Epoch [4], Batch [175/938], Loss: 1.4536774158477783\n",
      "Train: Epoch [4], Batch [176/938], Loss: 1.505742073059082\n",
      "Train: Epoch [4], Batch [177/938], Loss: 1.6026995182037354\n",
      "Train: Epoch [4], Batch [178/938], Loss: 1.5558346509933472\n",
      "Train: Epoch [4], Batch [179/938], Loss: 1.49647855758667\n",
      "Train: Epoch [4], Batch [180/938], Loss: 1.5611838102340698\n",
      "Train: Epoch [4], Batch [181/938], Loss: 1.7116514444351196\n",
      "Train: Epoch [4], Batch [182/938], Loss: 1.623674750328064\n",
      "Train: Epoch [4], Batch [183/938], Loss: 1.43578040599823\n",
      "Train: Epoch [4], Batch [184/938], Loss: 1.2848637104034424\n",
      "Train: Epoch [4], Batch [185/938], Loss: 1.6481690406799316\n",
      "Train: Epoch [4], Batch [186/938], Loss: 1.582298755645752\n",
      "Train: Epoch [4], Batch [187/938], Loss: 1.4552127122879028\n",
      "Train: Epoch [4], Batch [188/938], Loss: 1.4145703315734863\n",
      "Train: Epoch [4], Batch [189/938], Loss: 1.364753007888794\n",
      "Train: Epoch [4], Batch [190/938], Loss: 1.4898825883865356\n",
      "Train: Epoch [4], Batch [191/938], Loss: 1.6012945175170898\n",
      "Train: Epoch [4], Batch [192/938], Loss: 1.3760018348693848\n",
      "Train: Epoch [4], Batch [193/938], Loss: 1.4874926805496216\n",
      "Train: Epoch [4], Batch [194/938], Loss: 1.605437159538269\n",
      "Train: Epoch [4], Batch [195/938], Loss: 1.5522823333740234\n",
      "Train: Epoch [4], Batch [196/938], Loss: 1.377756953239441\n",
      "Train: Epoch [4], Batch [197/938], Loss: 1.5846827030181885\n",
      "Train: Epoch [4], Batch [198/938], Loss: 1.514137864112854\n",
      "Train: Epoch [4], Batch [199/938], Loss: 1.5160372257232666\n",
      "Train: Epoch [4], Batch [200/938], Loss: 1.5108509063720703\n",
      "Train: Epoch [4], Batch [201/938], Loss: 1.5647976398468018\n",
      "Train: Epoch [4], Batch [202/938], Loss: 1.5881749391555786\n",
      "Train: Epoch [4], Batch [203/938], Loss: 1.4393181800842285\n",
      "Train: Epoch [4], Batch [204/938], Loss: 1.6497204303741455\n",
      "Train: Epoch [4], Batch [205/938], Loss: 1.5028027296066284\n",
      "Train: Epoch [4], Batch [206/938], Loss: 1.5567123889923096\n",
      "Train: Epoch [4], Batch [207/938], Loss: 1.5209026336669922\n",
      "Train: Epoch [4], Batch [208/938], Loss: 1.7127668857574463\n",
      "Train: Epoch [4], Batch [209/938], Loss: 1.5221744775772095\n",
      "Train: Epoch [4], Batch [210/938], Loss: 1.4372949600219727\n",
      "Train: Epoch [4], Batch [211/938], Loss: 1.5514823198318481\n",
      "Train: Epoch [4], Batch [212/938], Loss: 1.5398797988891602\n",
      "Train: Epoch [4], Batch [213/938], Loss: 1.5068944692611694\n",
      "Train: Epoch [4], Batch [214/938], Loss: 1.5659890174865723\n",
      "Train: Epoch [4], Batch [215/938], Loss: 1.4328508377075195\n",
      "Train: Epoch [4], Batch [216/938], Loss: 1.694579005241394\n",
      "Train: Epoch [4], Batch [217/938], Loss: 1.6890634298324585\n",
      "Train: Epoch [4], Batch [218/938], Loss: 1.5596370697021484\n",
      "Train: Epoch [4], Batch [219/938], Loss: 1.573120355606079\n",
      "Train: Epoch [4], Batch [220/938], Loss: 1.6142674684524536\n",
      "Train: Epoch [4], Batch [221/938], Loss: 1.609010934829712\n",
      "Train: Epoch [4], Batch [222/938], Loss: 1.5122572183609009\n",
      "Train: Epoch [4], Batch [223/938], Loss: 1.4735653400421143\n",
      "Train: Epoch [4], Batch [224/938], Loss: 1.3484174013137817\n",
      "Train: Epoch [4], Batch [225/938], Loss: 1.4808177947998047\n",
      "Train: Epoch [4], Batch [226/938], Loss: 1.718538761138916\n",
      "Train: Epoch [4], Batch [227/938], Loss: 1.6081962585449219\n",
      "Train: Epoch [4], Batch [228/938], Loss: 1.5458920001983643\n",
      "Train: Epoch [4], Batch [229/938], Loss: 1.4623128175735474\n",
      "Train: Epoch [4], Batch [230/938], Loss: 1.5398383140563965\n",
      "Train: Epoch [4], Batch [231/938], Loss: 1.6199172735214233\n",
      "Train: Epoch [4], Batch [232/938], Loss: 1.593169927597046\n",
      "Train: Epoch [4], Batch [233/938], Loss: 1.5474815368652344\n",
      "Train: Epoch [4], Batch [234/938], Loss: 1.4653964042663574\n",
      "Train: Epoch [4], Batch [235/938], Loss: 1.4358168840408325\n",
      "Train: Epoch [4], Batch [236/938], Loss: 1.5845437049865723\n",
      "Train: Epoch [4], Batch [237/938], Loss: 1.4665629863739014\n",
      "Train: Epoch [4], Batch [238/938], Loss: 1.5850046873092651\n",
      "Train: Epoch [4], Batch [239/938], Loss: 1.4609627723693848\n",
      "Train: Epoch [4], Batch [240/938], Loss: 1.6178160905838013\n",
      "Train: Epoch [4], Batch [241/938], Loss: 1.6684318780899048\n",
      "Train: Epoch [4], Batch [242/938], Loss: 1.5296282768249512\n",
      "Train: Epoch [4], Batch [243/938], Loss: 1.5955448150634766\n",
      "Train: Epoch [4], Batch [244/938], Loss: 1.4538036584854126\n",
      "Train: Epoch [4], Batch [245/938], Loss: 1.568876028060913\n",
      "Train: Epoch [4], Batch [246/938], Loss: 1.5437896251678467\n",
      "Train: Epoch [4], Batch [247/938], Loss: 1.5616661310195923\n",
      "Train: Epoch [4], Batch [248/938], Loss: 1.5062217712402344\n",
      "Train: Epoch [4], Batch [249/938], Loss: 1.5189824104309082\n",
      "Train: Epoch [4], Batch [250/938], Loss: 1.5742913484573364\n",
      "Train: Epoch [4], Batch [251/938], Loss: 1.5471360683441162\n",
      "Train: Epoch [4], Batch [252/938], Loss: 1.4917113780975342\n",
      "Train: Epoch [4], Batch [253/938], Loss: 1.7061563730239868\n",
      "Train: Epoch [4], Batch [254/938], Loss: 1.409729242324829\n",
      "Train: Epoch [4], Batch [255/938], Loss: 1.5434072017669678\n",
      "Train: Epoch [4], Batch [256/938], Loss: 1.4049577713012695\n",
      "Train: Epoch [4], Batch [257/938], Loss: 1.5022437572479248\n",
      "Train: Epoch [4], Batch [258/938], Loss: 1.6544435024261475\n",
      "Train: Epoch [4], Batch [259/938], Loss: 1.474955677986145\n",
      "Train: Epoch [4], Batch [260/938], Loss: 1.4590007066726685\n",
      "Train: Epoch [4], Batch [261/938], Loss: 1.438477873802185\n",
      "Train: Epoch [4], Batch [262/938], Loss: 1.7269833087921143\n",
      "Train: Epoch [4], Batch [263/938], Loss: 1.5067414045333862\n",
      "Train: Epoch [4], Batch [264/938], Loss: 1.472752332687378\n",
      "Train: Epoch [4], Batch [265/938], Loss: 1.5542638301849365\n",
      "Train: Epoch [4], Batch [266/938], Loss: 1.5119141340255737\n",
      "Train: Epoch [4], Batch [267/938], Loss: 1.4453920125961304\n",
      "Train: Epoch [4], Batch [268/938], Loss: 1.3596757650375366\n",
      "Train: Epoch [4], Batch [269/938], Loss: 1.4599554538726807\n",
      "Train: Epoch [4], Batch [270/938], Loss: 1.4189406633377075\n",
      "Train: Epoch [4], Batch [271/938], Loss: 1.4987741708755493\n",
      "Train: Epoch [4], Batch [272/938], Loss: 1.5634233951568604\n",
      "Train: Epoch [4], Batch [273/938], Loss: 1.4279530048370361\n",
      "Train: Epoch [4], Batch [274/938], Loss: 1.6285748481750488\n",
      "Train: Epoch [4], Batch [275/938], Loss: 1.4205526113510132\n",
      "Train: Epoch [4], Batch [276/938], Loss: 1.605376124382019\n",
      "Train: Epoch [4], Batch [277/938], Loss: 1.6378611326217651\n",
      "Train: Epoch [4], Batch [278/938], Loss: 1.4426318407058716\n",
      "Train: Epoch [4], Batch [279/938], Loss: 1.5343997478485107\n",
      "Train: Epoch [4], Batch [280/938], Loss: 1.4320625066757202\n",
      "Train: Epoch [4], Batch [281/938], Loss: 1.3551772832870483\n",
      "Train: Epoch [4], Batch [282/938], Loss: 1.336672067642212\n",
      "Train: Epoch [4], Batch [283/938], Loss: 1.4266612529754639\n",
      "Train: Epoch [4], Batch [284/938], Loss: 1.6566095352172852\n",
      "Train: Epoch [4], Batch [285/938], Loss: 1.573988437652588\n",
      "Train: Epoch [4], Batch [286/938], Loss: 1.504896879196167\n",
      "Train: Epoch [4], Batch [287/938], Loss: 1.5009223222732544\n",
      "Train: Epoch [4], Batch [288/938], Loss: 1.590731143951416\n",
      "Train: Epoch [4], Batch [289/938], Loss: 1.7418588399887085\n",
      "Train: Epoch [4], Batch [290/938], Loss: 1.6162943840026855\n",
      "Train: Epoch [4], Batch [291/938], Loss: 1.3684229850769043\n",
      "Train: Epoch [4], Batch [292/938], Loss: 1.5618151426315308\n",
      "Train: Epoch [4], Batch [293/938], Loss: 1.4578070640563965\n",
      "Train: Epoch [4], Batch [294/938], Loss: 1.4377282857894897\n",
      "Train: Epoch [4], Batch [295/938], Loss: 1.5242713689804077\n",
      "Train: Epoch [4], Batch [296/938], Loss: 1.4992557764053345\n",
      "Train: Epoch [4], Batch [297/938], Loss: 1.469217300415039\n",
      "Train: Epoch [4], Batch [298/938], Loss: 1.4363791942596436\n",
      "Train: Epoch [4], Batch [299/938], Loss: 1.6302497386932373\n",
      "Train: Epoch [4], Batch [300/938], Loss: 1.7260825634002686\n",
      "Train: Epoch [4], Batch [301/938], Loss: 1.3913323879241943\n",
      "Train: Epoch [4], Batch [302/938], Loss: 1.5223431587219238\n",
      "Train: Epoch [4], Batch [303/938], Loss: 1.6906224489212036\n",
      "Train: Epoch [4], Batch [304/938], Loss: 1.567887306213379\n",
      "Train: Epoch [4], Batch [305/938], Loss: 1.3858952522277832\n",
      "Train: Epoch [4], Batch [306/938], Loss: 1.4037086963653564\n",
      "Train: Epoch [4], Batch [307/938], Loss: 1.5645463466644287\n",
      "Train: Epoch [4], Batch [308/938], Loss: 1.6354914903640747\n",
      "Train: Epoch [4], Batch [309/938], Loss: 1.5035147666931152\n",
      "Train: Epoch [4], Batch [310/938], Loss: 1.3642213344573975\n",
      "Train: Epoch [4], Batch [311/938], Loss: 1.7330721616744995\n",
      "Train: Epoch [4], Batch [312/938], Loss: 1.569727897644043\n",
      "Train: Epoch [4], Batch [313/938], Loss: 1.6458042860031128\n",
      "Train: Epoch [4], Batch [314/938], Loss: 1.4953142404556274\n",
      "Train: Epoch [4], Batch [315/938], Loss: 1.4513276815414429\n",
      "Train: Epoch [4], Batch [316/938], Loss: 1.4877092838287354\n",
      "Train: Epoch [4], Batch [317/938], Loss: 1.546538233757019\n",
      "Train: Epoch [4], Batch [318/938], Loss: 1.5060093402862549\n",
      "Train: Epoch [4], Batch [319/938], Loss: 1.582694411277771\n",
      "Train: Epoch [4], Batch [320/938], Loss: 1.411316156387329\n",
      "Train: Epoch [4], Batch [321/938], Loss: 1.303884744644165\n",
      "Train: Epoch [4], Batch [322/938], Loss: 1.7076702117919922\n",
      "Train: Epoch [4], Batch [323/938], Loss: 1.2864809036254883\n",
      "Train: Epoch [4], Batch [324/938], Loss: 1.5903252363204956\n",
      "Train: Epoch [4], Batch [325/938], Loss: 1.3447457551956177\n",
      "Train: Epoch [4], Batch [326/938], Loss: 1.5682235956192017\n",
      "Train: Epoch [4], Batch [327/938], Loss: 1.5069921016693115\n",
      "Train: Epoch [4], Batch [328/938], Loss: 1.6088114976882935\n",
      "Train: Epoch [4], Batch [329/938], Loss: 1.6232349872589111\n",
      "Train: Epoch [4], Batch [330/938], Loss: 1.4972690343856812\n",
      "Train: Epoch [4], Batch [331/938], Loss: 1.579882025718689\n",
      "Train: Epoch [4], Batch [332/938], Loss: 1.5637798309326172\n",
      "Train: Epoch [4], Batch [333/938], Loss: 1.659819483757019\n",
      "Train: Epoch [4], Batch [334/938], Loss: 1.2721203565597534\n",
      "Train: Epoch [4], Batch [335/938], Loss: 1.6550689935684204\n",
      "Train: Epoch [4], Batch [336/938], Loss: 1.5185563564300537\n",
      "Train: Epoch [4], Batch [337/938], Loss: 1.4186376333236694\n",
      "Train: Epoch [4], Batch [338/938], Loss: 1.4429492950439453\n",
      "Train: Epoch [4], Batch [339/938], Loss: 1.5864900350570679\n",
      "Train: Epoch [4], Batch [340/938], Loss: 1.4117668867111206\n",
      "Train: Epoch [4], Batch [341/938], Loss: 1.5150865316390991\n",
      "Train: Epoch [4], Batch [342/938], Loss: 1.5472456216812134\n",
      "Train: Epoch [4], Batch [343/938], Loss: 1.2885762453079224\n",
      "Train: Epoch [4], Batch [344/938], Loss: 1.4816042184829712\n",
      "Train: Epoch [4], Batch [345/938], Loss: 1.6230065822601318\n",
      "Train: Epoch [4], Batch [346/938], Loss: 1.508472204208374\n",
      "Train: Epoch [4], Batch [347/938], Loss: 1.4356870651245117\n",
      "Train: Epoch [4], Batch [348/938], Loss: 1.322927713394165\n",
      "Train: Epoch [4], Batch [349/938], Loss: 1.3424874544143677\n",
      "Train: Epoch [4], Batch [350/938], Loss: 1.3993016481399536\n",
      "Train: Epoch [4], Batch [351/938], Loss: 1.619130253791809\n",
      "Train: Epoch [4], Batch [352/938], Loss: 1.5246078968048096\n",
      "Train: Epoch [4], Batch [353/938], Loss: 1.5552786588668823\n",
      "Train: Epoch [4], Batch [354/938], Loss: 1.6183345317840576\n",
      "Train: Epoch [4], Batch [355/938], Loss: 1.28019380569458\n",
      "Train: Epoch [4], Batch [356/938], Loss: 1.37872314453125\n",
      "Train: Epoch [4], Batch [357/938], Loss: 1.4404120445251465\n",
      "Train: Epoch [4], Batch [358/938], Loss: 1.5793066024780273\n",
      "Train: Epoch [4], Batch [359/938], Loss: 1.4020655155181885\n",
      "Train: Epoch [4], Batch [360/938], Loss: 1.4951636791229248\n",
      "Train: Epoch [4], Batch [361/938], Loss: 1.3885389566421509\n",
      "Train: Epoch [4], Batch [362/938], Loss: 1.4130737781524658\n",
      "Train: Epoch [4], Batch [363/938], Loss: 1.5405769348144531\n",
      "Train: Epoch [4], Batch [364/938], Loss: 1.5654184818267822\n",
      "Train: Epoch [4], Batch [365/938], Loss: 1.4093488454818726\n",
      "Train: Epoch [4], Batch [366/938], Loss: 1.4744198322296143\n",
      "Train: Epoch [4], Batch [367/938], Loss: 1.5477772951126099\n",
      "Train: Epoch [4], Batch [368/938], Loss: 1.595106840133667\n",
      "Train: Epoch [4], Batch [369/938], Loss: 1.5341335535049438\n",
      "Train: Epoch [4], Batch [370/938], Loss: 1.5308167934417725\n",
      "Train: Epoch [4], Batch [371/938], Loss: 1.3341834545135498\n",
      "Train: Epoch [4], Batch [372/938], Loss: 1.409026861190796\n",
      "Train: Epoch [4], Batch [373/938], Loss: 1.355948567390442\n",
      "Train: Epoch [4], Batch [374/938], Loss: 1.4087330102920532\n",
      "Train: Epoch [4], Batch [375/938], Loss: 1.5554391145706177\n",
      "Train: Epoch [4], Batch [376/938], Loss: 1.463539958000183\n",
      "Train: Epoch [4], Batch [377/938], Loss: 1.5652055740356445\n",
      "Train: Epoch [4], Batch [378/938], Loss: 1.5262250900268555\n",
      "Train: Epoch [4], Batch [379/938], Loss: 1.433838129043579\n",
      "Train: Epoch [4], Batch [380/938], Loss: 1.3885644674301147\n",
      "Train: Epoch [4], Batch [381/938], Loss: 1.4759283065795898\n",
      "Train: Epoch [4], Batch [382/938], Loss: 1.4995369911193848\n",
      "Train: Epoch [4], Batch [383/938], Loss: 1.4635252952575684\n",
      "Train: Epoch [4], Batch [384/938], Loss: 1.6210410594940186\n",
      "Train: Epoch [4], Batch [385/938], Loss: 1.6018539667129517\n",
      "Train: Epoch [4], Batch [386/938], Loss: 1.6488211154937744\n",
      "Train: Epoch [4], Batch [387/938], Loss: 1.4581379890441895\n",
      "Train: Epoch [4], Batch [388/938], Loss: 1.3473713397979736\n",
      "Train: Epoch [4], Batch [389/938], Loss: 1.4614590406417847\n",
      "Train: Epoch [4], Batch [390/938], Loss: 1.4399135112762451\n",
      "Train: Epoch [4], Batch [391/938], Loss: 1.47819185256958\n",
      "Train: Epoch [4], Batch [392/938], Loss: 1.5431004762649536\n",
      "Train: Epoch [4], Batch [393/938], Loss: 1.5554468631744385\n",
      "Train: Epoch [4], Batch [394/938], Loss: 1.5767855644226074\n",
      "Train: Epoch [4], Batch [395/938], Loss: 1.4880541563034058\n",
      "Train: Epoch [4], Batch [396/938], Loss: 1.6413116455078125\n",
      "Train: Epoch [4], Batch [397/938], Loss: 1.242576241493225\n",
      "Train: Epoch [4], Batch [398/938], Loss: 1.606716513633728\n",
      "Train: Epoch [4], Batch [399/938], Loss: 1.4644635915756226\n",
      "Train: Epoch [4], Batch [400/938], Loss: 1.4069970846176147\n",
      "Train: Epoch [4], Batch [401/938], Loss: 1.2256765365600586\n",
      "Train: Epoch [4], Batch [402/938], Loss: 1.4570462703704834\n",
      "Train: Epoch [4], Batch [403/938], Loss: 1.5253963470458984\n",
      "Train: Epoch [4], Batch [404/938], Loss: 1.4705073833465576\n",
      "Train: Epoch [4], Batch [405/938], Loss: 1.4393669366836548\n",
      "Train: Epoch [4], Batch [406/938], Loss: 1.6203968524932861\n",
      "Train: Epoch [4], Batch [407/938], Loss: 1.4237685203552246\n",
      "Train: Epoch [4], Batch [408/938], Loss: 1.3273022174835205\n",
      "Train: Epoch [4], Batch [409/938], Loss: 1.5146082639694214\n",
      "Train: Epoch [4], Batch [410/938], Loss: 1.286771535873413\n",
      "Train: Epoch [4], Batch [411/938], Loss: 1.4773321151733398\n",
      "Train: Epoch [4], Batch [412/938], Loss: 1.613051176071167\n",
      "Train: Epoch [4], Batch [413/938], Loss: 1.4099764823913574\n",
      "Train: Epoch [4], Batch [414/938], Loss: 1.4553335905075073\n",
      "Train: Epoch [4], Batch [415/938], Loss: 1.1770780086517334\n",
      "Train: Epoch [4], Batch [416/938], Loss: 1.613155722618103\n",
      "Train: Epoch [4], Batch [417/938], Loss: 1.7429828643798828\n",
      "Train: Epoch [4], Batch [418/938], Loss: 1.367927074432373\n",
      "Train: Epoch [4], Batch [419/938], Loss: 1.4996123313903809\n",
      "Train: Epoch [4], Batch [420/938], Loss: 1.4498608112335205\n",
      "Train: Epoch [4], Batch [421/938], Loss: 1.4661966562271118\n",
      "Train: Epoch [4], Batch [422/938], Loss: 1.442907452583313\n",
      "Train: Epoch [4], Batch [423/938], Loss: 1.3220479488372803\n",
      "Train: Epoch [4], Batch [424/938], Loss: 1.3321983814239502\n",
      "Train: Epoch [4], Batch [425/938], Loss: 1.4555003643035889\n",
      "Train: Epoch [4], Batch [426/938], Loss: 1.5526180267333984\n",
      "Train: Epoch [4], Batch [427/938], Loss: 1.3863139152526855\n",
      "Train: Epoch [4], Batch [428/938], Loss: 1.396926760673523\n",
      "Train: Epoch [4], Batch [429/938], Loss: 1.330915093421936\n",
      "Train: Epoch [4], Batch [430/938], Loss: 1.3864110708236694\n",
      "Train: Epoch [4], Batch [431/938], Loss: 1.3719990253448486\n",
      "Train: Epoch [4], Batch [432/938], Loss: 1.3790138959884644\n",
      "Train: Epoch [4], Batch [433/938], Loss: 1.4492912292480469\n",
      "Train: Epoch [4], Batch [434/938], Loss: 1.5020999908447266\n",
      "Train: Epoch [4], Batch [435/938], Loss: 1.638821005821228\n",
      "Train: Epoch [4], Batch [436/938], Loss: 1.5993006229400635\n",
      "Train: Epoch [4], Batch [437/938], Loss: 1.4415632486343384\n",
      "Train: Epoch [4], Batch [438/938], Loss: 1.293902039527893\n",
      "Train: Epoch [4], Batch [439/938], Loss: 1.4082326889038086\n",
      "Train: Epoch [4], Batch [440/938], Loss: 1.3075220584869385\n",
      "Train: Epoch [4], Batch [441/938], Loss: 1.5068446397781372\n",
      "Train: Epoch [4], Batch [442/938], Loss: 1.3337000608444214\n",
      "Train: Epoch [4], Batch [443/938], Loss: 1.4690604209899902\n",
      "Train: Epoch [4], Batch [444/938], Loss: 1.611619234085083\n",
      "Train: Epoch [4], Batch [445/938], Loss: 1.4237351417541504\n",
      "Train: Epoch [4], Batch [446/938], Loss: 1.69142484664917\n",
      "Train: Epoch [4], Batch [447/938], Loss: 1.3392693996429443\n",
      "Train: Epoch [4], Batch [448/938], Loss: 1.33560049533844\n",
      "Train: Epoch [4], Batch [449/938], Loss: 1.491325855255127\n",
      "Train: Epoch [4], Batch [450/938], Loss: 1.218846082687378\n",
      "Train: Epoch [4], Batch [451/938], Loss: 1.37808358669281\n",
      "Train: Epoch [4], Batch [452/938], Loss: 1.545414686203003\n",
      "Train: Epoch [4], Batch [453/938], Loss: 1.6726154088974\n",
      "Train: Epoch [4], Batch [454/938], Loss: 1.3480908870697021\n",
      "Train: Epoch [4], Batch [455/938], Loss: 1.523346185684204\n",
      "Train: Epoch [4], Batch [456/938], Loss: 1.5390037298202515\n",
      "Train: Epoch [4], Batch [457/938], Loss: 1.5639173984527588\n",
      "Train: Epoch [4], Batch [458/938], Loss: 1.323958158493042\n",
      "Train: Epoch [4], Batch [459/938], Loss: 1.7481716871261597\n",
      "Train: Epoch [4], Batch [460/938], Loss: 1.3407386541366577\n",
      "Train: Epoch [4], Batch [461/938], Loss: 1.3592209815979004\n",
      "Train: Epoch [4], Batch [462/938], Loss: 1.3438209295272827\n",
      "Train: Epoch [4], Batch [463/938], Loss: 1.5188027620315552\n",
      "Train: Epoch [4], Batch [464/938], Loss: 1.5835257768630981\n",
      "Train: Epoch [4], Batch [465/938], Loss: 1.4823827743530273\n",
      "Train: Epoch [4], Batch [466/938], Loss: 1.5157803297042847\n",
      "Train: Epoch [4], Batch [467/938], Loss: 1.2842280864715576\n",
      "Train: Epoch [4], Batch [468/938], Loss: 1.6153322458267212\n",
      "Train: Epoch [4], Batch [469/938], Loss: 1.60506010055542\n",
      "Train: Epoch [4], Batch [470/938], Loss: 1.6702361106872559\n",
      "Train: Epoch [4], Batch [471/938], Loss: 1.6967723369598389\n",
      "Train: Epoch [4], Batch [472/938], Loss: 1.3905959129333496\n",
      "Train: Epoch [4], Batch [473/938], Loss: 1.3791313171386719\n",
      "Train: Epoch [4], Batch [474/938], Loss: 1.2320704460144043\n",
      "Train: Epoch [4], Batch [475/938], Loss: 1.1408419609069824\n",
      "Train: Epoch [4], Batch [476/938], Loss: 1.3505719900131226\n",
      "Train: Epoch [4], Batch [477/938], Loss: 1.4947144985198975\n",
      "Train: Epoch [4], Batch [478/938], Loss: 1.392534613609314\n",
      "Train: Epoch [4], Batch [479/938], Loss: 1.4569774866104126\n",
      "Train: Epoch [4], Batch [480/938], Loss: 1.6017217636108398\n",
      "Train: Epoch [4], Batch [481/938], Loss: 1.3559894561767578\n",
      "Train: Epoch [4], Batch [482/938], Loss: 1.5307297706604004\n",
      "Train: Epoch [4], Batch [483/938], Loss: 1.4781917333602905\n",
      "Train: Epoch [4], Batch [484/938], Loss: 1.4128696918487549\n",
      "Train: Epoch [4], Batch [485/938], Loss: 1.3951311111450195\n",
      "Train: Epoch [4], Batch [486/938], Loss: 1.4818958044052124\n",
      "Train: Epoch [4], Batch [487/938], Loss: 1.3587632179260254\n",
      "Train: Epoch [4], Batch [488/938], Loss: 1.4486984014511108\n",
      "Train: Epoch [4], Batch [489/938], Loss: 1.5122076272964478\n",
      "Train: Epoch [4], Batch [490/938], Loss: 1.2714375257492065\n",
      "Train: Epoch [4], Batch [491/938], Loss: 1.3466331958770752\n",
      "Train: Epoch [4], Batch [492/938], Loss: 1.467822551727295\n",
      "Train: Epoch [4], Batch [493/938], Loss: 1.3468258380889893\n",
      "Train: Epoch [4], Batch [494/938], Loss: 1.489694595336914\n",
      "Train: Epoch [4], Batch [495/938], Loss: 1.3872689008712769\n",
      "Train: Epoch [4], Batch [496/938], Loss: 1.35709547996521\n",
      "Train: Epoch [4], Batch [497/938], Loss: 1.418015956878662\n",
      "Train: Epoch [4], Batch [498/938], Loss: 1.3509571552276611\n",
      "Train: Epoch [4], Batch [499/938], Loss: 1.30177640914917\n",
      "Train: Epoch [4], Batch [500/938], Loss: 1.4749490022659302\n",
      "Train: Epoch [4], Batch [501/938], Loss: 1.4070390462875366\n",
      "Train: Epoch [4], Batch [502/938], Loss: 1.3593358993530273\n",
      "Train: Epoch [4], Batch [503/938], Loss: 1.4928944110870361\n",
      "Train: Epoch [4], Batch [504/938], Loss: 1.5180511474609375\n",
      "Train: Epoch [4], Batch [505/938], Loss: 1.6914809942245483\n",
      "Train: Epoch [4], Batch [506/938], Loss: 1.3932480812072754\n",
      "Train: Epoch [4], Batch [507/938], Loss: 1.3751153945922852\n",
      "Train: Epoch [4], Batch [508/938], Loss: 1.257507085800171\n",
      "Train: Epoch [4], Batch [509/938], Loss: 1.3384698629379272\n",
      "Train: Epoch [4], Batch [510/938], Loss: 1.3864670991897583\n",
      "Train: Epoch [4], Batch [511/938], Loss: 1.4358364343643188\n",
      "Train: Epoch [4], Batch [512/938], Loss: 1.4669569730758667\n",
      "Train: Epoch [4], Batch [513/938], Loss: 1.4327995777130127\n",
      "Train: Epoch [4], Batch [514/938], Loss: 1.478340744972229\n",
      "Train: Epoch [4], Batch [515/938], Loss: 1.4173290729522705\n",
      "Train: Epoch [4], Batch [516/938], Loss: 1.5457165241241455\n",
      "Train: Epoch [4], Batch [517/938], Loss: 1.4694901704788208\n",
      "Train: Epoch [4], Batch [518/938], Loss: 1.3844634294509888\n",
      "Train: Epoch [4], Batch [519/938], Loss: 1.5104196071624756\n",
      "Train: Epoch [4], Batch [520/938], Loss: 1.6210541725158691\n",
      "Train: Epoch [4], Batch [521/938], Loss: 1.4438211917877197\n",
      "Train: Epoch [4], Batch [522/938], Loss: 1.4590811729431152\n",
      "Train: Epoch [4], Batch [523/938], Loss: 1.4394326210021973\n",
      "Train: Epoch [4], Batch [524/938], Loss: 1.4961297512054443\n",
      "Train: Epoch [4], Batch [525/938], Loss: 1.5358915328979492\n",
      "Train: Epoch [4], Batch [526/938], Loss: 1.3786035776138306\n",
      "Train: Epoch [4], Batch [527/938], Loss: 1.3945978879928589\n",
      "Train: Epoch [4], Batch [528/938], Loss: 1.4893798828125\n",
      "Train: Epoch [4], Batch [529/938], Loss: 1.2758852243423462\n",
      "Train: Epoch [4], Batch [530/938], Loss: 1.599064826965332\n",
      "Train: Epoch [4], Batch [531/938], Loss: 1.435672402381897\n",
      "Train: Epoch [4], Batch [532/938], Loss: 1.4425714015960693\n",
      "Train: Epoch [4], Batch [533/938], Loss: 1.484166145324707\n",
      "Train: Epoch [4], Batch [534/938], Loss: 1.4032154083251953\n",
      "Train: Epoch [4], Batch [535/938], Loss: 1.2716084718704224\n",
      "Train: Epoch [4], Batch [536/938], Loss: 1.4302806854248047\n",
      "Train: Epoch [4], Batch [537/938], Loss: 1.2909520864486694\n",
      "Train: Epoch [4], Batch [538/938], Loss: 1.5807615518569946\n",
      "Train: Epoch [4], Batch [539/938], Loss: 1.3808315992355347\n",
      "Train: Epoch [4], Batch [540/938], Loss: 1.4637237787246704\n",
      "Train: Epoch [4], Batch [541/938], Loss: 1.5150094032287598\n",
      "Train: Epoch [4], Batch [542/938], Loss: 1.4201163053512573\n",
      "Train: Epoch [4], Batch [543/938], Loss: 1.2449418306350708\n",
      "Train: Epoch [4], Batch [544/938], Loss: 1.5813326835632324\n",
      "Train: Epoch [4], Batch [545/938], Loss: 1.4103338718414307\n",
      "Train: Epoch [4], Batch [546/938], Loss: 1.4085983037948608\n",
      "Train: Epoch [4], Batch [547/938], Loss: 1.597209095954895\n",
      "Train: Epoch [4], Batch [548/938], Loss: 1.1462639570236206\n",
      "Train: Epoch [4], Batch [549/938], Loss: 1.652897834777832\n",
      "Train: Epoch [4], Batch [550/938], Loss: 1.2929611206054688\n",
      "Train: Epoch [4], Batch [551/938], Loss: 1.4518336057662964\n",
      "Train: Epoch [4], Batch [552/938], Loss: 1.5183507204055786\n",
      "Train: Epoch [4], Batch [553/938], Loss: 1.4530161619186401\n",
      "Train: Epoch [4], Batch [554/938], Loss: 1.360410213470459\n",
      "Train: Epoch [4], Batch [555/938], Loss: 1.3823740482330322\n",
      "Train: Epoch [4], Batch [556/938], Loss: 1.4912875890731812\n",
      "Train: Epoch [4], Batch [557/938], Loss: 1.3281482458114624\n",
      "Train: Epoch [4], Batch [558/938], Loss: 1.3065325021743774\n",
      "Train: Epoch [4], Batch [559/938], Loss: 1.428132176399231\n",
      "Train: Epoch [4], Batch [560/938], Loss: 1.344564437866211\n",
      "Train: Epoch [4], Batch [561/938], Loss: 1.4570891857147217\n",
      "Train: Epoch [4], Batch [562/938], Loss: 1.3612791299819946\n",
      "Train: Epoch [4], Batch [563/938], Loss: 1.5303003787994385\n",
      "Train: Epoch [4], Batch [564/938], Loss: 1.612013816833496\n",
      "Train: Epoch [4], Batch [565/938], Loss: 1.3091377019882202\n",
      "Train: Epoch [4], Batch [566/938], Loss: 1.3351712226867676\n",
      "Train: Epoch [4], Batch [567/938], Loss: 1.2951332330703735\n",
      "Train: Epoch [4], Batch [568/938], Loss: 1.3058934211730957\n",
      "Train: Epoch [4], Batch [569/938], Loss: 1.293674111366272\n",
      "Train: Epoch [4], Batch [570/938], Loss: 1.299453854560852\n",
      "Train: Epoch [4], Batch [571/938], Loss: 1.5493428707122803\n",
      "Train: Epoch [4], Batch [572/938], Loss: 1.2189182043075562\n",
      "Train: Epoch [4], Batch [573/938], Loss: 1.354016900062561\n",
      "Train: Epoch [4], Batch [574/938], Loss: 1.276803731918335\n",
      "Train: Epoch [4], Batch [575/938], Loss: 1.5290700197219849\n",
      "Train: Epoch [4], Batch [576/938], Loss: 1.390547752380371\n",
      "Train: Epoch [4], Batch [577/938], Loss: 1.4380552768707275\n",
      "Train: Epoch [4], Batch [578/938], Loss: 1.1875882148742676\n",
      "Train: Epoch [4], Batch [579/938], Loss: 1.2726243734359741\n",
      "Train: Epoch [4], Batch [580/938], Loss: 1.292293667793274\n",
      "Train: Epoch [4], Batch [581/938], Loss: 1.4748317003250122\n",
      "Train: Epoch [4], Batch [582/938], Loss: 1.2682430744171143\n",
      "Train: Epoch [4], Batch [583/938], Loss: 1.5596897602081299\n",
      "Train: Epoch [4], Batch [584/938], Loss: 1.4693883657455444\n",
      "Train: Epoch [4], Batch [585/938], Loss: 1.5662338733673096\n",
      "Train: Epoch [4], Batch [586/938], Loss: 1.253892421722412\n",
      "Train: Epoch [4], Batch [587/938], Loss: 1.5171741247177124\n",
      "Train: Epoch [4], Batch [588/938], Loss: 1.3307995796203613\n",
      "Train: Epoch [4], Batch [589/938], Loss: 1.2979686260223389\n",
      "Train: Epoch [4], Batch [590/938], Loss: 1.4304159879684448\n",
      "Train: Epoch [4], Batch [591/938], Loss: 1.421401023864746\n",
      "Train: Epoch [4], Batch [592/938], Loss: 1.398324966430664\n",
      "Train: Epoch [4], Batch [593/938], Loss: 1.5157973766326904\n",
      "Train: Epoch [4], Batch [594/938], Loss: 1.50026535987854\n",
      "Train: Epoch [4], Batch [595/938], Loss: 1.3363710641860962\n",
      "Train: Epoch [4], Batch [596/938], Loss: 1.5725889205932617\n",
      "Train: Epoch [4], Batch [597/938], Loss: 1.6944950819015503\n",
      "Train: Epoch [4], Batch [598/938], Loss: 1.3217804431915283\n",
      "Train: Epoch [4], Batch [599/938], Loss: 1.611128807067871\n",
      "Train: Epoch [4], Batch [600/938], Loss: 1.3449947834014893\n",
      "Train: Epoch [4], Batch [601/938], Loss: 1.4179129600524902\n",
      "Train: Epoch [4], Batch [602/938], Loss: 1.4330235719680786\n",
      "Train: Epoch [4], Batch [603/938], Loss: 1.4386610984802246\n",
      "Train: Epoch [4], Batch [604/938], Loss: 1.2854981422424316\n",
      "Train: Epoch [4], Batch [605/938], Loss: 1.4312093257904053\n",
      "Train: Epoch [4], Batch [606/938], Loss: 1.5044116973876953\n",
      "Train: Epoch [4], Batch [607/938], Loss: 1.4244095087051392\n",
      "Train: Epoch [4], Batch [608/938], Loss: 1.2681819200515747\n",
      "Train: Epoch [4], Batch [609/938], Loss: 1.5033360719680786\n",
      "Train: Epoch [4], Batch [610/938], Loss: 1.3355685472488403\n",
      "Train: Epoch [4], Batch [611/938], Loss: 1.5509207248687744\n",
      "Train: Epoch [4], Batch [612/938], Loss: 1.290968894958496\n",
      "Train: Epoch [4], Batch [613/938], Loss: 1.3170995712280273\n",
      "Train: Epoch [4], Batch [614/938], Loss: 1.2544981241226196\n",
      "Train: Epoch [4], Batch [615/938], Loss: 1.393310546875\n",
      "Train: Epoch [4], Batch [616/938], Loss: 1.4235398769378662\n",
      "Train: Epoch [4], Batch [617/938], Loss: 1.4649969339370728\n",
      "Train: Epoch [4], Batch [618/938], Loss: 1.2409042119979858\n",
      "Train: Epoch [4], Batch [619/938], Loss: 1.2766282558441162\n",
      "Train: Epoch [4], Batch [620/938], Loss: 1.3089039325714111\n",
      "Train: Epoch [4], Batch [621/938], Loss: 1.3823133707046509\n",
      "Train: Epoch [4], Batch [622/938], Loss: 1.3814836740493774\n",
      "Train: Epoch [4], Batch [623/938], Loss: 1.4634974002838135\n",
      "Train: Epoch [4], Batch [624/938], Loss: 1.4960297346115112\n",
      "Train: Epoch [4], Batch [625/938], Loss: 1.4154491424560547\n",
      "Train: Epoch [4], Batch [626/938], Loss: 1.3948041200637817\n",
      "Train: Epoch [4], Batch [627/938], Loss: 1.1950299739837646\n",
      "Train: Epoch [4], Batch [628/938], Loss: 1.6302505731582642\n",
      "Train: Epoch [4], Batch [629/938], Loss: 1.3212276697158813\n",
      "Train: Epoch [4], Batch [630/938], Loss: 1.3559070825576782\n",
      "Train: Epoch [4], Batch [631/938], Loss: 1.575796127319336\n",
      "Train: Epoch [4], Batch [632/938], Loss: 1.398714303970337\n",
      "Train: Epoch [4], Batch [633/938], Loss: 1.1785938739776611\n",
      "Train: Epoch [4], Batch [634/938], Loss: 1.4469513893127441\n",
      "Train: Epoch [4], Batch [635/938], Loss: 1.257817268371582\n",
      "Train: Epoch [4], Batch [636/938], Loss: 1.5286877155303955\n",
      "Train: Epoch [4], Batch [637/938], Loss: 1.374665379524231\n",
      "Train: Epoch [4], Batch [638/938], Loss: 1.3805686235427856\n",
      "Train: Epoch [4], Batch [639/938], Loss: 1.3021118640899658\n",
      "Train: Epoch [4], Batch [640/938], Loss: 1.28313410282135\n",
      "Train: Epoch [4], Batch [641/938], Loss: 1.405580759048462\n",
      "Train: Epoch [4], Batch [642/938], Loss: 1.5168745517730713\n",
      "Train: Epoch [4], Batch [643/938], Loss: 1.4298783540725708\n",
      "Train: Epoch [4], Batch [644/938], Loss: 1.3465999364852905\n",
      "Train: Epoch [4], Batch [645/938], Loss: 1.2870932817459106\n",
      "Train: Epoch [4], Batch [646/938], Loss: 1.4073662757873535\n",
      "Train: Epoch [4], Batch [647/938], Loss: 1.292447566986084\n",
      "Train: Epoch [4], Batch [648/938], Loss: 1.336130976676941\n",
      "Train: Epoch [4], Batch [649/938], Loss: 1.3139780759811401\n",
      "Train: Epoch [4], Batch [650/938], Loss: 1.3382946252822876\n",
      "Train: Epoch [4], Batch [651/938], Loss: 1.4553204774856567\n",
      "Train: Epoch [4], Batch [652/938], Loss: 1.3222217559814453\n",
      "Train: Epoch [4], Batch [653/938], Loss: 1.4209531545639038\n",
      "Train: Epoch [4], Batch [654/938], Loss: 1.4808849096298218\n",
      "Train: Epoch [4], Batch [655/938], Loss: 1.2395257949829102\n",
      "Train: Epoch [4], Batch [656/938], Loss: 1.5312001705169678\n",
      "Train: Epoch [4], Batch [657/938], Loss: 1.1931872367858887\n",
      "Train: Epoch [4], Batch [658/938], Loss: 1.2850518226623535\n",
      "Train: Epoch [4], Batch [659/938], Loss: 1.5982487201690674\n",
      "Train: Epoch [4], Batch [660/938], Loss: 1.4308327436447144\n",
      "Train: Epoch [4], Batch [661/938], Loss: 1.337680459022522\n",
      "Train: Epoch [4], Batch [662/938], Loss: 1.3826302289962769\n",
      "Train: Epoch [4], Batch [663/938], Loss: 1.3815113306045532\n",
      "Train: Epoch [4], Batch [664/938], Loss: 1.3092846870422363\n",
      "Train: Epoch [4], Batch [665/938], Loss: 1.305490493774414\n",
      "Train: Epoch [4], Batch [666/938], Loss: 1.393408179283142\n",
      "Train: Epoch [4], Batch [667/938], Loss: 1.3136494159698486\n",
      "Train: Epoch [4], Batch [668/938], Loss: 1.5262784957885742\n",
      "Train: Epoch [4], Batch [669/938], Loss: 1.6233320236206055\n",
      "Train: Epoch [4], Batch [670/938], Loss: 1.532936692237854\n",
      "Train: Epoch [4], Batch [671/938], Loss: 1.3006367683410645\n",
      "Train: Epoch [4], Batch [672/938], Loss: 1.3314690589904785\n",
      "Train: Epoch [4], Batch [673/938], Loss: 1.4466679096221924\n",
      "Train: Epoch [4], Batch [674/938], Loss: 1.440210223197937\n",
      "Train: Epoch [4], Batch [675/938], Loss: 1.5558586120605469\n",
      "Train: Epoch [4], Batch [676/938], Loss: 1.3645120859146118\n",
      "Train: Epoch [4], Batch [677/938], Loss: 1.527604341506958\n",
      "Train: Epoch [4], Batch [678/938], Loss: 1.55049467086792\n",
      "Train: Epoch [4], Batch [679/938], Loss: 1.3891221284866333\n",
      "Train: Epoch [4], Batch [680/938], Loss: 1.3927717208862305\n",
      "Train: Epoch [4], Batch [681/938], Loss: 1.4964655637741089\n",
      "Train: Epoch [4], Batch [682/938], Loss: 1.3788741827011108\n",
      "Train: Epoch [4], Batch [683/938], Loss: 1.3971307277679443\n",
      "Train: Epoch [4], Batch [684/938], Loss: 1.4936189651489258\n",
      "Train: Epoch [4], Batch [685/938], Loss: 1.4969732761383057\n",
      "Train: Epoch [4], Batch [686/938], Loss: 1.2572886943817139\n",
      "Train: Epoch [4], Batch [687/938], Loss: 1.3952243328094482\n",
      "Train: Epoch [4], Batch [688/938], Loss: 1.380787968635559\n",
      "Train: Epoch [4], Batch [689/938], Loss: 1.4000194072723389\n",
      "Train: Epoch [4], Batch [690/938], Loss: 1.4982225894927979\n",
      "Train: Epoch [4], Batch [691/938], Loss: 1.2455264329910278\n",
      "Train: Epoch [4], Batch [692/938], Loss: 1.481784462928772\n",
      "Train: Epoch [4], Batch [693/938], Loss: 1.3741822242736816\n",
      "Train: Epoch [4], Batch [694/938], Loss: 1.416046380996704\n",
      "Train: Epoch [4], Batch [695/938], Loss: 1.468545913696289\n",
      "Train: Epoch [4], Batch [696/938], Loss: 1.3863409757614136\n",
      "Train: Epoch [4], Batch [697/938], Loss: 1.5334597826004028\n",
      "Train: Epoch [4], Batch [698/938], Loss: 1.3420979976654053\n",
      "Train: Epoch [4], Batch [699/938], Loss: 1.348520278930664\n",
      "Train: Epoch [4], Batch [700/938], Loss: 1.4620780944824219\n",
      "Train: Epoch [4], Batch [701/938], Loss: 1.3301575183868408\n",
      "Train: Epoch [4], Batch [702/938], Loss: 1.3107070922851562\n",
      "Train: Epoch [4], Batch [703/938], Loss: 1.1968194246292114\n",
      "Train: Epoch [4], Batch [704/938], Loss: 1.297036051750183\n",
      "Train: Epoch [4], Batch [705/938], Loss: 1.5132782459259033\n",
      "Train: Epoch [4], Batch [706/938], Loss: 1.4676499366760254\n",
      "Train: Epoch [4], Batch [707/938], Loss: 1.3608863353729248\n",
      "Train: Epoch [4], Batch [708/938], Loss: 1.3346796035766602\n",
      "Train: Epoch [4], Batch [709/938], Loss: 1.2381868362426758\n",
      "Train: Epoch [4], Batch [710/938], Loss: 1.2085561752319336\n",
      "Train: Epoch [4], Batch [711/938], Loss: 1.4323341846466064\n",
      "Train: Epoch [4], Batch [712/938], Loss: 1.3732308149337769\n",
      "Train: Epoch [4], Batch [713/938], Loss: 1.241734266281128\n",
      "Train: Epoch [4], Batch [714/938], Loss: 1.4460569620132446\n",
      "Train: Epoch [4], Batch [715/938], Loss: 1.2488030195236206\n",
      "Train: Epoch [4], Batch [716/938], Loss: 1.472645878791809\n",
      "Train: Epoch [4], Batch [717/938], Loss: 1.3774851560592651\n",
      "Train: Epoch [4], Batch [718/938], Loss: 1.328207015991211\n",
      "Train: Epoch [4], Batch [719/938], Loss: 1.4213213920593262\n",
      "Train: Epoch [4], Batch [720/938], Loss: 1.1400667428970337\n",
      "Train: Epoch [4], Batch [721/938], Loss: 1.5543832778930664\n",
      "Train: Epoch [4], Batch [722/938], Loss: 1.4579716920852661\n",
      "Train: Epoch [4], Batch [723/938], Loss: 1.253814697265625\n",
      "Train: Epoch [4], Batch [724/938], Loss: 1.2651230096817017\n",
      "Train: Epoch [4], Batch [725/938], Loss: 1.4828850030899048\n",
      "Train: Epoch [4], Batch [726/938], Loss: 1.3745858669281006\n",
      "Train: Epoch [4], Batch [727/938], Loss: 1.4976098537445068\n",
      "Train: Epoch [4], Batch [728/938], Loss: 1.3934524059295654\n",
      "Train: Epoch [4], Batch [729/938], Loss: 1.406165599822998\n",
      "Train: Epoch [4], Batch [730/938], Loss: 1.3132843971252441\n",
      "Train: Epoch [4], Batch [731/938], Loss: 1.5514315366744995\n",
      "Train: Epoch [4], Batch [732/938], Loss: 1.3952465057373047\n",
      "Train: Epoch [4], Batch [733/938], Loss: 1.198974370956421\n",
      "Train: Epoch [4], Batch [734/938], Loss: 1.381300449371338\n",
      "Train: Epoch [4], Batch [735/938], Loss: 1.1925245523452759\n",
      "Train: Epoch [4], Batch [736/938], Loss: 1.2333489656448364\n",
      "Train: Epoch [4], Batch [737/938], Loss: 1.6036697626113892\n",
      "Train: Epoch [4], Batch [738/938], Loss: 1.2221229076385498\n",
      "Train: Epoch [4], Batch [739/938], Loss: 1.3309147357940674\n",
      "Train: Epoch [4], Batch [740/938], Loss: 1.2557095289230347\n",
      "Train: Epoch [4], Batch [741/938], Loss: 1.3491536378860474\n",
      "Train: Epoch [4], Batch [742/938], Loss: 1.288750410079956\n",
      "Train: Epoch [4], Batch [743/938], Loss: 1.3372634649276733\n",
      "Train: Epoch [4], Batch [744/938], Loss: 1.3525192737579346\n",
      "Train: Epoch [4], Batch [745/938], Loss: 1.4178977012634277\n",
      "Train: Epoch [4], Batch [746/938], Loss: 1.1397473812103271\n",
      "Train: Epoch [4], Batch [747/938], Loss: 1.407138705253601\n",
      "Train: Epoch [4], Batch [748/938], Loss: 1.1679961681365967\n",
      "Train: Epoch [4], Batch [749/938], Loss: 1.109726905822754\n",
      "Train: Epoch [4], Batch [750/938], Loss: 1.411529541015625\n",
      "Train: Epoch [4], Batch [751/938], Loss: 1.3238658905029297\n",
      "Train: Epoch [4], Batch [752/938], Loss: 1.2583922147750854\n",
      "Train: Epoch [4], Batch [753/938], Loss: 1.3664271831512451\n",
      "Train: Epoch [4], Batch [754/938], Loss: 1.44611394405365\n",
      "Train: Epoch [4], Batch [755/938], Loss: 1.1275120973587036\n",
      "Train: Epoch [4], Batch [756/938], Loss: 1.2639721632003784\n",
      "Train: Epoch [4], Batch [757/938], Loss: 1.2348486185073853\n",
      "Train: Epoch [4], Batch [758/938], Loss: 1.3227272033691406\n",
      "Train: Epoch [4], Batch [759/938], Loss: 1.4354588985443115\n",
      "Train: Epoch [4], Batch [760/938], Loss: 1.302620530128479\n",
      "Train: Epoch [4], Batch [761/938], Loss: 1.2668828964233398\n",
      "Train: Epoch [4], Batch [762/938], Loss: 1.4589489698410034\n",
      "Train: Epoch [4], Batch [763/938], Loss: 1.5294054746627808\n",
      "Train: Epoch [4], Batch [764/938], Loss: 1.3321566581726074\n",
      "Train: Epoch [4], Batch [765/938], Loss: 1.380997657775879\n",
      "Train: Epoch [4], Batch [766/938], Loss: 1.3582513332366943\n",
      "Train: Epoch [4], Batch [767/938], Loss: 1.4001774787902832\n",
      "Train: Epoch [4], Batch [768/938], Loss: 1.4356590509414673\n",
      "Train: Epoch [4], Batch [769/938], Loss: 1.346114158630371\n",
      "Train: Epoch [4], Batch [770/938], Loss: 1.2369558811187744\n",
      "Train: Epoch [4], Batch [771/938], Loss: 1.2632877826690674\n",
      "Train: Epoch [4], Batch [772/938], Loss: 1.3387305736541748\n",
      "Train: Epoch [4], Batch [773/938], Loss: 1.413123607635498\n",
      "Train: Epoch [4], Batch [774/938], Loss: 1.222198247909546\n",
      "Train: Epoch [4], Batch [775/938], Loss: 1.2808021306991577\n",
      "Train: Epoch [4], Batch [776/938], Loss: 1.4884436130523682\n",
      "Train: Epoch [4], Batch [777/938], Loss: 1.2237030267715454\n",
      "Train: Epoch [4], Batch [778/938], Loss: 1.4219639301300049\n",
      "Train: Epoch [4], Batch [779/938], Loss: 1.377421498298645\n",
      "Train: Epoch [4], Batch [780/938], Loss: 1.3946242332458496\n",
      "Train: Epoch [4], Batch [781/938], Loss: 1.039199709892273\n",
      "Train: Epoch [4], Batch [782/938], Loss: 1.3366518020629883\n",
      "Train: Epoch [4], Batch [783/938], Loss: 1.3953365087509155\n",
      "Train: Epoch [4], Batch [784/938], Loss: 1.2385456562042236\n",
      "Train: Epoch [4], Batch [785/938], Loss: 1.3268330097198486\n",
      "Train: Epoch [4], Batch [786/938], Loss: 1.1999759674072266\n",
      "Train: Epoch [4], Batch [787/938], Loss: 1.5507171154022217\n",
      "Train: Epoch [4], Batch [788/938], Loss: 1.2815065383911133\n",
      "Train: Epoch [4], Batch [789/938], Loss: 1.510161280632019\n",
      "Train: Epoch [4], Batch [790/938], Loss: 1.441352367401123\n",
      "Train: Epoch [4], Batch [791/938], Loss: 1.4660613536834717\n",
      "Train: Epoch [4], Batch [792/938], Loss: 1.2193422317504883\n",
      "Train: Epoch [4], Batch [793/938], Loss: 1.1169291734695435\n",
      "Train: Epoch [4], Batch [794/938], Loss: 1.2086403369903564\n",
      "Train: Epoch [4], Batch [795/938], Loss: 1.253703236579895\n",
      "Train: Epoch [4], Batch [796/938], Loss: 1.3734415769577026\n",
      "Train: Epoch [4], Batch [797/938], Loss: 1.3538105487823486\n",
      "Train: Epoch [4], Batch [798/938], Loss: 1.3473381996154785\n",
      "Train: Epoch [4], Batch [799/938], Loss: 1.44403874874115\n",
      "Train: Epoch [4], Batch [800/938], Loss: 1.3839539289474487\n",
      "Train: Epoch [4], Batch [801/938], Loss: 1.1333805322647095\n",
      "Train: Epoch [4], Batch [802/938], Loss: 1.5008773803710938\n",
      "Train: Epoch [4], Batch [803/938], Loss: 1.291298508644104\n",
      "Train: Epoch [4], Batch [804/938], Loss: 1.5074143409729004\n",
      "Train: Epoch [4], Batch [805/938], Loss: 1.2963694334030151\n",
      "Train: Epoch [4], Batch [806/938], Loss: 1.4535913467407227\n",
      "Train: Epoch [4], Batch [807/938], Loss: 1.2901126146316528\n",
      "Train: Epoch [4], Batch [808/938], Loss: 1.3066046237945557\n",
      "Train: Epoch [4], Batch [809/938], Loss: 1.2442858219146729\n",
      "Train: Epoch [4], Batch [810/938], Loss: 1.2062370777130127\n",
      "Train: Epoch [4], Batch [811/938], Loss: 1.2717626094818115\n",
      "Train: Epoch [4], Batch [812/938], Loss: 1.3984962701797485\n",
      "Train: Epoch [4], Batch [813/938], Loss: 1.3293695449829102\n",
      "Train: Epoch [4], Batch [814/938], Loss: 1.0988821983337402\n",
      "Train: Epoch [4], Batch [815/938], Loss: 1.291226863861084\n",
      "Train: Epoch [4], Batch [816/938], Loss: 1.363468050956726\n",
      "Train: Epoch [4], Batch [817/938], Loss: 1.1358592510223389\n",
      "Train: Epoch [4], Batch [818/938], Loss: 1.3501428365707397\n",
      "Train: Epoch [4], Batch [819/938], Loss: 1.4008671045303345\n",
      "Train: Epoch [4], Batch [820/938], Loss: 1.3250752687454224\n",
      "Train: Epoch [4], Batch [821/938], Loss: 1.4616539478302002\n",
      "Train: Epoch [4], Batch [822/938], Loss: 1.6057262420654297\n",
      "Train: Epoch [4], Batch [823/938], Loss: 1.2895464897155762\n",
      "Train: Epoch [4], Batch [824/938], Loss: 1.2385179996490479\n",
      "Train: Epoch [4], Batch [825/938], Loss: 1.2884063720703125\n",
      "Train: Epoch [4], Batch [826/938], Loss: 1.274059772491455\n",
      "Train: Epoch [4], Batch [827/938], Loss: 1.329118251800537\n",
      "Train: Epoch [4], Batch [828/938], Loss: 1.2243026494979858\n",
      "Train: Epoch [4], Batch [829/938], Loss: 1.254523515701294\n",
      "Train: Epoch [4], Batch [830/938], Loss: 1.4506738185882568\n",
      "Train: Epoch [4], Batch [831/938], Loss: 1.2620784044265747\n",
      "Train: Epoch [4], Batch [832/938], Loss: 1.1598869562149048\n",
      "Train: Epoch [4], Batch [833/938], Loss: 1.482717514038086\n",
      "Train: Epoch [4], Batch [834/938], Loss: 1.300478458404541\n",
      "Train: Epoch [4], Batch [835/938], Loss: 1.3745394945144653\n",
      "Train: Epoch [4], Batch [836/938], Loss: 1.2282160520553589\n",
      "Train: Epoch [4], Batch [837/938], Loss: 1.2988086938858032\n",
      "Train: Epoch [4], Batch [838/938], Loss: 1.1764088869094849\n",
      "Train: Epoch [4], Batch [839/938], Loss: 1.48172926902771\n",
      "Train: Epoch [4], Batch [840/938], Loss: 1.3196251392364502\n",
      "Train: Epoch [4], Batch [841/938], Loss: 1.2886735200881958\n",
      "Train: Epoch [4], Batch [842/938], Loss: 1.288866639137268\n",
      "Train: Epoch [4], Batch [843/938], Loss: 1.1520016193389893\n",
      "Train: Epoch [4], Batch [844/938], Loss: 1.3016393184661865\n",
      "Train: Epoch [4], Batch [845/938], Loss: 1.285951852798462\n",
      "Train: Epoch [4], Batch [846/938], Loss: 1.2456940412521362\n",
      "Train: Epoch [4], Batch [847/938], Loss: 1.474816083908081\n",
      "Train: Epoch [4], Batch [848/938], Loss: 1.0992600917816162\n",
      "Train: Epoch [4], Batch [849/938], Loss: 1.1451835632324219\n",
      "Train: Epoch [4], Batch [850/938], Loss: 1.2132034301757812\n",
      "Train: Epoch [4], Batch [851/938], Loss: 1.2975395917892456\n",
      "Train: Epoch [4], Batch [852/938], Loss: 1.421897053718567\n",
      "Train: Epoch [4], Batch [853/938], Loss: 1.2622005939483643\n",
      "Train: Epoch [4], Batch [854/938], Loss: 1.3547563552856445\n",
      "Train: Epoch [4], Batch [855/938], Loss: 1.5381819009780884\n",
      "Train: Epoch [4], Batch [856/938], Loss: 1.2976175546646118\n",
      "Train: Epoch [4], Batch [857/938], Loss: 1.5495500564575195\n",
      "Train: Epoch [4], Batch [858/938], Loss: 1.113453984260559\n",
      "Train: Epoch [4], Batch [859/938], Loss: 1.283333659172058\n",
      "Train: Epoch [4], Batch [860/938], Loss: 1.2512977123260498\n",
      "Train: Epoch [4], Batch [861/938], Loss: 1.0898985862731934\n",
      "Train: Epoch [4], Batch [862/938], Loss: 1.3774336576461792\n",
      "Train: Epoch [4], Batch [863/938], Loss: 1.0382769107818604\n",
      "Train: Epoch [4], Batch [864/938], Loss: 1.375076174736023\n",
      "Train: Epoch [4], Batch [865/938], Loss: 1.4029358625411987\n",
      "Train: Epoch [4], Batch [866/938], Loss: 1.3017194271087646\n",
      "Train: Epoch [4], Batch [867/938], Loss: 1.1832642555236816\n",
      "Train: Epoch [4], Batch [868/938], Loss: 1.2446471452713013\n",
      "Train: Epoch [4], Batch [869/938], Loss: 1.471825361251831\n",
      "Train: Epoch [4], Batch [870/938], Loss: 1.1870498657226562\n",
      "Train: Epoch [4], Batch [871/938], Loss: 1.3782697916030884\n",
      "Train: Epoch [4], Batch [872/938], Loss: 1.2595932483673096\n",
      "Train: Epoch [4], Batch [873/938], Loss: 1.3224356174468994\n",
      "Train: Epoch [4], Batch [874/938], Loss: 1.560719609260559\n",
      "Train: Epoch [4], Batch [875/938], Loss: 1.5324872732162476\n",
      "Train: Epoch [4], Batch [876/938], Loss: 1.199347734451294\n",
      "Train: Epoch [4], Batch [877/938], Loss: 1.430253028869629\n",
      "Train: Epoch [4], Batch [878/938], Loss: 1.2621822357177734\n",
      "Train: Epoch [4], Batch [879/938], Loss: 1.153572916984558\n",
      "Train: Epoch [4], Batch [880/938], Loss: 1.450458288192749\n",
      "Train: Epoch [4], Batch [881/938], Loss: 1.413650631904602\n",
      "Train: Epoch [4], Batch [882/938], Loss: 1.2922947406768799\n",
      "Train: Epoch [4], Batch [883/938], Loss: 1.249650239944458\n",
      "Train: Epoch [4], Batch [884/938], Loss: 1.4630522727966309\n",
      "Train: Epoch [4], Batch [885/938], Loss: 1.303939700126648\n",
      "Train: Epoch [4], Batch [886/938], Loss: 1.2122719287872314\n",
      "Train: Epoch [4], Batch [887/938], Loss: 1.365729570388794\n",
      "Train: Epoch [4], Batch [888/938], Loss: 1.22218918800354\n",
      "Train: Epoch [4], Batch [889/938], Loss: 1.3767025470733643\n",
      "Train: Epoch [4], Batch [890/938], Loss: 1.484886646270752\n",
      "Train: Epoch [4], Batch [891/938], Loss: 1.4198956489562988\n",
      "Train: Epoch [4], Batch [892/938], Loss: 1.5032882690429688\n",
      "Train: Epoch [4], Batch [893/938], Loss: 1.2304728031158447\n",
      "Train: Epoch [4], Batch [894/938], Loss: 1.4320318698883057\n",
      "Train: Epoch [4], Batch [895/938], Loss: 1.2054072618484497\n",
      "Train: Epoch [4], Batch [896/938], Loss: 1.328588604927063\n",
      "Train: Epoch [4], Batch [897/938], Loss: 1.4095911979675293\n",
      "Train: Epoch [4], Batch [898/938], Loss: 1.279396414756775\n",
      "Train: Epoch [4], Batch [899/938], Loss: 1.4622278213500977\n",
      "Train: Epoch [4], Batch [900/938], Loss: 1.1128144264221191\n",
      "Train: Epoch [4], Batch [901/938], Loss: 1.3328967094421387\n",
      "Train: Epoch [4], Batch [902/938], Loss: 1.332176923751831\n",
      "Train: Epoch [4], Batch [903/938], Loss: 1.3632720708847046\n",
      "Train: Epoch [4], Batch [904/938], Loss: 1.351369857788086\n",
      "Train: Epoch [4], Batch [905/938], Loss: 1.1497894525527954\n",
      "Train: Epoch [4], Batch [906/938], Loss: 1.3287034034729004\n",
      "Train: Epoch [4], Batch [907/938], Loss: 1.2345491647720337\n",
      "Train: Epoch [4], Batch [908/938], Loss: 1.5389559268951416\n",
      "Train: Epoch [4], Batch [909/938], Loss: 1.2681766748428345\n",
      "Train: Epoch [4], Batch [910/938], Loss: 1.2567131519317627\n",
      "Train: Epoch [4], Batch [911/938], Loss: 1.1830188035964966\n",
      "Train: Epoch [4], Batch [912/938], Loss: 1.0203396081924438\n",
      "Train: Epoch [4], Batch [913/938], Loss: 1.144951581954956\n",
      "Train: Epoch [4], Batch [914/938], Loss: 1.2551624774932861\n",
      "Train: Epoch [4], Batch [915/938], Loss: 1.1505351066589355\n",
      "Train: Epoch [4], Batch [916/938], Loss: 1.5129235982894897\n",
      "Train: Epoch [4], Batch [917/938], Loss: 1.466676950454712\n",
      "Train: Epoch [4], Batch [918/938], Loss: 1.3718624114990234\n",
      "Train: Epoch [4], Batch [919/938], Loss: 1.3684959411621094\n",
      "Train: Epoch [4], Batch [920/938], Loss: 1.4745278358459473\n",
      "Train: Epoch [4], Batch [921/938], Loss: 1.2806298732757568\n",
      "Train: Epoch [4], Batch [922/938], Loss: 1.1949034929275513\n",
      "Train: Epoch [4], Batch [923/938], Loss: 1.361437439918518\n",
      "Train: Epoch [4], Batch [924/938], Loss: 1.3539386987686157\n",
      "Train: Epoch [4], Batch [925/938], Loss: 1.361814260482788\n",
      "Train: Epoch [4], Batch [926/938], Loss: 1.3058550357818604\n",
      "Train: Epoch [4], Batch [927/938], Loss: 1.2776843309402466\n",
      "Train: Epoch [4], Batch [928/938], Loss: 1.2272148132324219\n",
      "Train: Epoch [4], Batch [929/938], Loss: 1.2712072134017944\n",
      "Train: Epoch [4], Batch [930/938], Loss: 1.2385486364364624\n",
      "Train: Epoch [4], Batch [931/938], Loss: 1.2394773960113525\n",
      "Train: Epoch [4], Batch [932/938], Loss: 1.1880507469177246\n",
      "Train: Epoch [4], Batch [933/938], Loss: 1.4569954872131348\n",
      "Train: Epoch [4], Batch [934/938], Loss: 1.2509324550628662\n",
      "Train: Epoch [4], Batch [935/938], Loss: 1.3183141946792603\n",
      "Train: Epoch [4], Batch [936/938], Loss: 1.0697803497314453\n",
      "Train: Epoch [4], Batch [937/938], Loss: 1.4075312614440918\n",
      "Train: Epoch [4], Batch [938/938], Loss: 1.3064548969268799\n",
      "Accuracy of train set: 0.51245\n",
      "Validation: Epoch [4], Batch [1/938], Loss: 1.1065279245376587\n",
      "Validation: Epoch [4], Batch [2/938], Loss: 1.3889803886413574\n",
      "Validation: Epoch [4], Batch [3/938], Loss: 1.406147837638855\n",
      "Validation: Epoch [4], Batch [4/938], Loss: 1.2546099424362183\n",
      "Validation: Epoch [4], Batch [5/938], Loss: 1.3310964107513428\n",
      "Validation: Epoch [4], Batch [6/938], Loss: 1.252122402191162\n",
      "Validation: Epoch [4], Batch [7/938], Loss: 1.6403138637542725\n",
      "Validation: Epoch [4], Batch [8/938], Loss: 1.3699177503585815\n",
      "Validation: Epoch [4], Batch [9/938], Loss: 1.3037478923797607\n",
      "Validation: Epoch [4], Batch [10/938], Loss: 1.2289345264434814\n",
      "Validation: Epoch [4], Batch [11/938], Loss: 1.1993647813796997\n",
      "Validation: Epoch [4], Batch [12/938], Loss: 1.1818257570266724\n",
      "Validation: Epoch [4], Batch [13/938], Loss: 1.273625135421753\n",
      "Validation: Epoch [4], Batch [14/938], Loss: 1.4384739398956299\n",
      "Validation: Epoch [4], Batch [15/938], Loss: 1.1453272104263306\n",
      "Validation: Epoch [4], Batch [16/938], Loss: 1.161454677581787\n",
      "Validation: Epoch [4], Batch [17/938], Loss: 1.3660216331481934\n",
      "Validation: Epoch [4], Batch [18/938], Loss: 1.5564241409301758\n",
      "Validation: Epoch [4], Batch [19/938], Loss: 1.3630878925323486\n",
      "Validation: Epoch [4], Batch [20/938], Loss: 1.290448546409607\n",
      "Validation: Epoch [4], Batch [21/938], Loss: 1.134918451309204\n",
      "Validation: Epoch [4], Batch [22/938], Loss: 1.233160376548767\n",
      "Validation: Epoch [4], Batch [23/938], Loss: 1.2240146398544312\n",
      "Validation: Epoch [4], Batch [24/938], Loss: 1.5348613262176514\n",
      "Validation: Epoch [4], Batch [25/938], Loss: 1.4934264421463013\n",
      "Validation: Epoch [4], Batch [26/938], Loss: 1.2743186950683594\n",
      "Validation: Epoch [4], Batch [27/938], Loss: 1.2906135320663452\n",
      "Validation: Epoch [4], Batch [28/938], Loss: 1.3068710565567017\n",
      "Validation: Epoch [4], Batch [29/938], Loss: 1.3434239625930786\n",
      "Validation: Epoch [4], Batch [30/938], Loss: 1.6546733379364014\n",
      "Validation: Epoch [4], Batch [31/938], Loss: 1.4405198097229004\n",
      "Validation: Epoch [4], Batch [32/938], Loss: 1.5228554010391235\n",
      "Validation: Epoch [4], Batch [33/938], Loss: 1.3725272417068481\n",
      "Validation: Epoch [4], Batch [34/938], Loss: 1.3351718187332153\n",
      "Validation: Epoch [4], Batch [35/938], Loss: 1.3331739902496338\n",
      "Validation: Epoch [4], Batch [36/938], Loss: 1.2514784336090088\n",
      "Validation: Epoch [4], Batch [37/938], Loss: 1.43673574924469\n",
      "Validation: Epoch [4], Batch [38/938], Loss: 1.1262221336364746\n",
      "Validation: Epoch [4], Batch [39/938], Loss: 1.4709548950195312\n",
      "Validation: Epoch [4], Batch [40/938], Loss: 1.3073595762252808\n",
      "Validation: Epoch [4], Batch [41/938], Loss: 1.3646531105041504\n",
      "Validation: Epoch [4], Batch [42/938], Loss: 1.2733211517333984\n",
      "Validation: Epoch [4], Batch [43/938], Loss: 1.3038501739501953\n",
      "Validation: Epoch [4], Batch [44/938], Loss: 1.2045955657958984\n",
      "Validation: Epoch [4], Batch [45/938], Loss: 1.106142520904541\n",
      "Validation: Epoch [4], Batch [46/938], Loss: 1.5028724670410156\n",
      "Validation: Epoch [4], Batch [47/938], Loss: 1.285672664642334\n",
      "Validation: Epoch [4], Batch [48/938], Loss: 1.5128567218780518\n",
      "Validation: Epoch [4], Batch [49/938], Loss: 1.4660810232162476\n",
      "Validation: Epoch [4], Batch [50/938], Loss: 1.1591367721557617\n",
      "Validation: Epoch [4], Batch [51/938], Loss: 1.240610122680664\n",
      "Validation: Epoch [4], Batch [52/938], Loss: 1.457192063331604\n",
      "Validation: Epoch [4], Batch [53/938], Loss: 1.192264437675476\n",
      "Validation: Epoch [4], Batch [54/938], Loss: 1.3475067615509033\n",
      "Validation: Epoch [4], Batch [55/938], Loss: 1.4853687286376953\n",
      "Validation: Epoch [4], Batch [56/938], Loss: 1.1057868003845215\n",
      "Validation: Epoch [4], Batch [57/938], Loss: 1.2640702724456787\n",
      "Validation: Epoch [4], Batch [58/938], Loss: 1.413825511932373\n",
      "Validation: Epoch [4], Batch [59/938], Loss: 1.2686933279037476\n",
      "Validation: Epoch [4], Batch [60/938], Loss: 1.4538136720657349\n",
      "Validation: Epoch [4], Batch [61/938], Loss: 1.1702346801757812\n",
      "Validation: Epoch [4], Batch [62/938], Loss: 1.3929716348648071\n",
      "Validation: Epoch [4], Batch [63/938], Loss: 1.257220983505249\n",
      "Validation: Epoch [4], Batch [64/938], Loss: 1.3763904571533203\n",
      "Validation: Epoch [4], Batch [65/938], Loss: 1.4285086393356323\n",
      "Validation: Epoch [4], Batch [66/938], Loss: 1.3011225461959839\n",
      "Validation: Epoch [4], Batch [67/938], Loss: 1.2135794162750244\n",
      "Validation: Epoch [4], Batch [68/938], Loss: 1.3570616245269775\n",
      "Validation: Epoch [4], Batch [69/938], Loss: 1.233688473701477\n",
      "Validation: Epoch [4], Batch [70/938], Loss: 1.1345593929290771\n",
      "Validation: Epoch [4], Batch [71/938], Loss: 1.264814853668213\n",
      "Validation: Epoch [4], Batch [72/938], Loss: 1.2863601446151733\n",
      "Validation: Epoch [4], Batch [73/938], Loss: 1.4425933361053467\n",
      "Validation: Epoch [4], Batch [74/938], Loss: 1.149416446685791\n",
      "Validation: Epoch [4], Batch [75/938], Loss: 1.2564990520477295\n",
      "Validation: Epoch [4], Batch [76/938], Loss: 1.1757409572601318\n",
      "Validation: Epoch [4], Batch [77/938], Loss: 1.190431833267212\n",
      "Validation: Epoch [4], Batch [78/938], Loss: 1.3205817937850952\n",
      "Validation: Epoch [4], Batch [79/938], Loss: 1.3911988735198975\n",
      "Validation: Epoch [4], Batch [80/938], Loss: 1.4368596076965332\n",
      "Validation: Epoch [4], Batch [81/938], Loss: 1.3975611925125122\n",
      "Validation: Epoch [4], Batch [82/938], Loss: 1.439285159111023\n",
      "Validation: Epoch [4], Batch [83/938], Loss: 1.2700741291046143\n",
      "Validation: Epoch [4], Batch [84/938], Loss: 1.2693215608596802\n",
      "Validation: Epoch [4], Batch [85/938], Loss: 1.5373539924621582\n",
      "Validation: Epoch [4], Batch [86/938], Loss: 1.3831700086593628\n",
      "Validation: Epoch [4], Batch [87/938], Loss: 1.216205358505249\n",
      "Validation: Epoch [4], Batch [88/938], Loss: 1.3363667726516724\n",
      "Validation: Epoch [4], Batch [89/938], Loss: 1.3129245042800903\n",
      "Validation: Epoch [4], Batch [90/938], Loss: 1.3770397901535034\n",
      "Validation: Epoch [4], Batch [91/938], Loss: 1.392574667930603\n",
      "Validation: Epoch [4], Batch [92/938], Loss: 1.3624757528305054\n",
      "Validation: Epoch [4], Batch [93/938], Loss: 1.419126272201538\n",
      "Validation: Epoch [4], Batch [94/938], Loss: 1.4110854864120483\n",
      "Validation: Epoch [4], Batch [95/938], Loss: 1.0639568567276\n",
      "Validation: Epoch [4], Batch [96/938], Loss: 1.2701139450073242\n",
      "Validation: Epoch [4], Batch [97/938], Loss: 1.3571301698684692\n",
      "Validation: Epoch [4], Batch [98/938], Loss: 1.1623423099517822\n",
      "Validation: Epoch [4], Batch [99/938], Loss: 1.3762059211730957\n",
      "Validation: Epoch [4], Batch [100/938], Loss: 1.3289903402328491\n",
      "Validation: Epoch [4], Batch [101/938], Loss: 1.3384407758712769\n",
      "Validation: Epoch [4], Batch [102/938], Loss: 1.5599242448806763\n",
      "Validation: Epoch [4], Batch [103/938], Loss: 1.255635380744934\n",
      "Validation: Epoch [4], Batch [104/938], Loss: 1.2396960258483887\n",
      "Validation: Epoch [4], Batch [105/938], Loss: 1.4103692770004272\n",
      "Validation: Epoch [4], Batch [106/938], Loss: 1.3169080018997192\n",
      "Validation: Epoch [4], Batch [107/938], Loss: 1.4172210693359375\n",
      "Validation: Epoch [4], Batch [108/938], Loss: 1.0410174131393433\n",
      "Validation: Epoch [4], Batch [109/938], Loss: 1.3330953121185303\n",
      "Validation: Epoch [4], Batch [110/938], Loss: 1.3443561792373657\n",
      "Validation: Epoch [4], Batch [111/938], Loss: 1.4677462577819824\n",
      "Validation: Epoch [4], Batch [112/938], Loss: 1.2571911811828613\n",
      "Validation: Epoch [4], Batch [113/938], Loss: 1.2405734062194824\n",
      "Validation: Epoch [4], Batch [114/938], Loss: 1.153900146484375\n",
      "Validation: Epoch [4], Batch [115/938], Loss: 1.3084560632705688\n",
      "Validation: Epoch [4], Batch [116/938], Loss: 1.1148790121078491\n",
      "Validation: Epoch [4], Batch [117/938], Loss: 1.3604912757873535\n",
      "Validation: Epoch [4], Batch [118/938], Loss: 1.400048017501831\n",
      "Validation: Epoch [4], Batch [119/938], Loss: 1.175307035446167\n",
      "Validation: Epoch [4], Batch [120/938], Loss: 1.2231841087341309\n",
      "Validation: Epoch [4], Batch [121/938], Loss: 1.2750475406646729\n",
      "Validation: Epoch [4], Batch [122/938], Loss: 1.2557597160339355\n",
      "Validation: Epoch [4], Batch [123/938], Loss: 1.4309005737304688\n",
      "Validation: Epoch [4], Batch [124/938], Loss: 1.5998592376708984\n",
      "Validation: Epoch [4], Batch [125/938], Loss: 1.4875459671020508\n",
      "Validation: Epoch [4], Batch [126/938], Loss: 1.1810569763183594\n",
      "Validation: Epoch [4], Batch [127/938], Loss: 1.3034788370132446\n",
      "Validation: Epoch [4], Batch [128/938], Loss: 1.222990870475769\n",
      "Validation: Epoch [4], Batch [129/938], Loss: 1.4269740581512451\n",
      "Validation: Epoch [4], Batch [130/938], Loss: 1.3502633571624756\n",
      "Validation: Epoch [4], Batch [131/938], Loss: 1.1330156326293945\n",
      "Validation: Epoch [4], Batch [132/938], Loss: 1.3296639919281006\n",
      "Validation: Epoch [4], Batch [133/938], Loss: 1.3099560737609863\n",
      "Validation: Epoch [4], Batch [134/938], Loss: 1.5372726917266846\n",
      "Validation: Epoch [4], Batch [135/938], Loss: 1.4582027196884155\n",
      "Validation: Epoch [4], Batch [136/938], Loss: 1.339557409286499\n",
      "Validation: Epoch [4], Batch [137/938], Loss: 1.2584424018859863\n",
      "Validation: Epoch [4], Batch [138/938], Loss: 1.3371343612670898\n",
      "Validation: Epoch [4], Batch [139/938], Loss: 1.4081374406814575\n",
      "Validation: Epoch [4], Batch [140/938], Loss: 1.2642784118652344\n",
      "Validation: Epoch [4], Batch [141/938], Loss: 1.4234753847122192\n",
      "Validation: Epoch [4], Batch [142/938], Loss: 1.4501113891601562\n",
      "Validation: Epoch [4], Batch [143/938], Loss: 1.3940858840942383\n",
      "Validation: Epoch [4], Batch [144/938], Loss: 1.2408733367919922\n",
      "Validation: Epoch [4], Batch [145/938], Loss: 1.2901376485824585\n",
      "Validation: Epoch [4], Batch [146/938], Loss: 1.3878782987594604\n",
      "Validation: Epoch [4], Batch [147/938], Loss: 1.3761403560638428\n",
      "Validation: Epoch [4], Batch [148/938], Loss: 1.2957520484924316\n",
      "Validation: Epoch [4], Batch [149/938], Loss: 1.3498165607452393\n",
      "Validation: Epoch [4], Batch [150/938], Loss: 1.185636043548584\n",
      "Validation: Epoch [4], Batch [151/938], Loss: 1.4733606576919556\n",
      "Validation: Epoch [4], Batch [152/938], Loss: 1.312504529953003\n",
      "Validation: Epoch [4], Batch [153/938], Loss: 1.2289081811904907\n",
      "Validation: Epoch [4], Batch [154/938], Loss: 1.0444239377975464\n",
      "Validation: Epoch [4], Batch [155/938], Loss: 1.3781437873840332\n",
      "Validation: Epoch [4], Batch [156/938], Loss: 1.3043674230575562\n",
      "Validation: Epoch [4], Batch [157/938], Loss: 1.4482781887054443\n",
      "Validation: Epoch [4], Batch [158/938], Loss: 1.3274376392364502\n",
      "Validation: Epoch [4], Batch [159/938], Loss: 1.427181601524353\n",
      "Validation: Epoch [4], Batch [160/938], Loss: 1.3916549682617188\n",
      "Validation: Epoch [4], Batch [161/938], Loss: 1.3034238815307617\n",
      "Validation: Epoch [4], Batch [162/938], Loss: 1.2972025871276855\n",
      "Validation: Epoch [4], Batch [163/938], Loss: 1.2108010053634644\n",
      "Validation: Epoch [4], Batch [164/938], Loss: 1.280855655670166\n",
      "Validation: Epoch [4], Batch [165/938], Loss: 1.4272232055664062\n",
      "Validation: Epoch [4], Batch [166/938], Loss: 1.2173787355422974\n",
      "Validation: Epoch [4], Batch [167/938], Loss: 1.2924792766571045\n",
      "Validation: Epoch [4], Batch [168/938], Loss: 1.047263264656067\n",
      "Validation: Epoch [4], Batch [169/938], Loss: 1.1377043724060059\n",
      "Validation: Epoch [4], Batch [170/938], Loss: 1.3167433738708496\n",
      "Validation: Epoch [4], Batch [171/938], Loss: 1.2627477645874023\n",
      "Validation: Epoch [4], Batch [172/938], Loss: 1.239719271659851\n",
      "Validation: Epoch [4], Batch [173/938], Loss: 1.3689812421798706\n",
      "Validation: Epoch [4], Batch [174/938], Loss: 1.312551498413086\n",
      "Validation: Epoch [4], Batch [175/938], Loss: 1.250365972518921\n",
      "Validation: Epoch [4], Batch [176/938], Loss: 1.2703869342803955\n",
      "Validation: Epoch [4], Batch [177/938], Loss: 1.3082395792007446\n",
      "Validation: Epoch [4], Batch [178/938], Loss: 1.4844969511032104\n",
      "Validation: Epoch [4], Batch [179/938], Loss: 1.1737375259399414\n",
      "Validation: Epoch [4], Batch [180/938], Loss: 1.6203594207763672\n",
      "Validation: Epoch [4], Batch [181/938], Loss: 1.320892095565796\n",
      "Validation: Epoch [4], Batch [182/938], Loss: 1.1373445987701416\n",
      "Validation: Epoch [4], Batch [183/938], Loss: 1.4863277673721313\n",
      "Validation: Epoch [4], Batch [184/938], Loss: 1.4347212314605713\n",
      "Validation: Epoch [4], Batch [185/938], Loss: 1.1292214393615723\n",
      "Validation: Epoch [4], Batch [186/938], Loss: 1.581176996231079\n",
      "Validation: Epoch [4], Batch [187/938], Loss: 1.425748586654663\n",
      "Validation: Epoch [4], Batch [188/938], Loss: 1.3385323286056519\n",
      "Validation: Epoch [4], Batch [189/938], Loss: 1.5911674499511719\n",
      "Validation: Epoch [4], Batch [190/938], Loss: 1.2205417156219482\n",
      "Validation: Epoch [4], Batch [191/938], Loss: 1.2674503326416016\n",
      "Validation: Epoch [4], Batch [192/938], Loss: 1.274073839187622\n",
      "Validation: Epoch [4], Batch [193/938], Loss: 1.3937623500823975\n",
      "Validation: Epoch [4], Batch [194/938], Loss: 1.3993793725967407\n",
      "Validation: Epoch [4], Batch [195/938], Loss: 0.985416829586029\n",
      "Validation: Epoch [4], Batch [196/938], Loss: 1.3769726753234863\n",
      "Validation: Epoch [4], Batch [197/938], Loss: 1.1937073469161987\n",
      "Validation: Epoch [4], Batch [198/938], Loss: 1.3003965616226196\n",
      "Validation: Epoch [4], Batch [199/938], Loss: 1.3357534408569336\n",
      "Validation: Epoch [4], Batch [200/938], Loss: 1.2628122568130493\n",
      "Validation: Epoch [4], Batch [201/938], Loss: 1.1818091869354248\n",
      "Validation: Epoch [4], Batch [202/938], Loss: 1.4061717987060547\n",
      "Validation: Epoch [4], Batch [203/938], Loss: 1.3388187885284424\n",
      "Validation: Epoch [4], Batch [204/938], Loss: 1.1922118663787842\n",
      "Validation: Epoch [4], Batch [205/938], Loss: 1.190335988998413\n",
      "Validation: Epoch [4], Batch [206/938], Loss: 1.1885367631912231\n",
      "Validation: Epoch [4], Batch [207/938], Loss: 1.4298593997955322\n",
      "Validation: Epoch [4], Batch [208/938], Loss: 1.2881982326507568\n",
      "Validation: Epoch [4], Batch [209/938], Loss: 1.190968632698059\n",
      "Validation: Epoch [4], Batch [210/938], Loss: 1.481342077255249\n",
      "Validation: Epoch [4], Batch [211/938], Loss: 1.1866366863250732\n",
      "Validation: Epoch [4], Batch [212/938], Loss: 1.3868992328643799\n",
      "Validation: Epoch [4], Batch [213/938], Loss: 1.247214674949646\n",
      "Validation: Epoch [4], Batch [214/938], Loss: 1.042843222618103\n",
      "Validation: Epoch [4], Batch [215/938], Loss: 1.1012274026870728\n",
      "Validation: Epoch [4], Batch [216/938], Loss: 1.1143531799316406\n",
      "Validation: Epoch [4], Batch [217/938], Loss: 1.2658708095550537\n",
      "Validation: Epoch [4], Batch [218/938], Loss: 1.2593730688095093\n",
      "Validation: Epoch [4], Batch [219/938], Loss: 1.1024724245071411\n",
      "Validation: Epoch [4], Batch [220/938], Loss: 1.3946797847747803\n",
      "Validation: Epoch [4], Batch [221/938], Loss: 1.458528757095337\n",
      "Validation: Epoch [4], Batch [222/938], Loss: 1.3721624612808228\n",
      "Validation: Epoch [4], Batch [223/938], Loss: 1.4855393171310425\n",
      "Validation: Epoch [4], Batch [224/938], Loss: 1.2793958187103271\n",
      "Validation: Epoch [4], Batch [225/938], Loss: 1.2514387369155884\n",
      "Validation: Epoch [4], Batch [226/938], Loss: 1.4781297445297241\n",
      "Validation: Epoch [4], Batch [227/938], Loss: 1.459290862083435\n",
      "Validation: Epoch [4], Batch [228/938], Loss: 1.3454351425170898\n",
      "Validation: Epoch [4], Batch [229/938], Loss: 1.3448671102523804\n",
      "Validation: Epoch [4], Batch [230/938], Loss: 1.4614591598510742\n",
      "Validation: Epoch [4], Batch [231/938], Loss: 1.5185450315475464\n",
      "Validation: Epoch [4], Batch [232/938], Loss: 1.2240204811096191\n",
      "Validation: Epoch [4], Batch [233/938], Loss: 1.0915532112121582\n",
      "Validation: Epoch [4], Batch [234/938], Loss: 1.4824466705322266\n",
      "Validation: Epoch [4], Batch [235/938], Loss: 1.220641851425171\n",
      "Validation: Epoch [4], Batch [236/938], Loss: 1.336285948753357\n",
      "Validation: Epoch [4], Batch [237/938], Loss: 1.3383967876434326\n",
      "Validation: Epoch [4], Batch [238/938], Loss: 1.2870863676071167\n",
      "Validation: Epoch [4], Batch [239/938], Loss: 1.3508758544921875\n",
      "Validation: Epoch [4], Batch [240/938], Loss: 1.566501259803772\n",
      "Validation: Epoch [4], Batch [241/938], Loss: 1.1934020519256592\n",
      "Validation: Epoch [4], Batch [242/938], Loss: 1.1837247610092163\n",
      "Validation: Epoch [4], Batch [243/938], Loss: 1.3853117227554321\n",
      "Validation: Epoch [4], Batch [244/938], Loss: 1.4117199182510376\n",
      "Validation: Epoch [4], Batch [245/938], Loss: 1.2925150394439697\n",
      "Validation: Epoch [4], Batch [246/938], Loss: 1.4546847343444824\n",
      "Validation: Epoch [4], Batch [247/938], Loss: 1.3829371929168701\n",
      "Validation: Epoch [4], Batch [248/938], Loss: 1.3694599866867065\n",
      "Validation: Epoch [4], Batch [249/938], Loss: 1.302889108657837\n",
      "Validation: Epoch [4], Batch [250/938], Loss: 1.4593892097473145\n",
      "Validation: Epoch [4], Batch [251/938], Loss: 1.2115508317947388\n",
      "Validation: Epoch [4], Batch [252/938], Loss: 1.324302315711975\n",
      "Validation: Epoch [4], Batch [253/938], Loss: 1.2515381574630737\n",
      "Validation: Epoch [4], Batch [254/938], Loss: 1.333442211151123\n",
      "Validation: Epoch [4], Batch [255/938], Loss: 1.2646896839141846\n",
      "Validation: Epoch [4], Batch [256/938], Loss: 1.2957210540771484\n",
      "Validation: Epoch [4], Batch [257/938], Loss: 1.2657146453857422\n",
      "Validation: Epoch [4], Batch [258/938], Loss: 1.4313040971755981\n",
      "Validation: Epoch [4], Batch [259/938], Loss: 1.0940607786178589\n",
      "Validation: Epoch [4], Batch [260/938], Loss: 1.2907520532608032\n",
      "Validation: Epoch [4], Batch [261/938], Loss: 1.166666030883789\n",
      "Validation: Epoch [4], Batch [262/938], Loss: 1.482864499092102\n",
      "Validation: Epoch [4], Batch [263/938], Loss: 1.114794135093689\n",
      "Validation: Epoch [4], Batch [264/938], Loss: 1.1074382066726685\n",
      "Validation: Epoch [4], Batch [265/938], Loss: 1.1433494091033936\n",
      "Validation: Epoch [4], Batch [266/938], Loss: 1.1059919595718384\n",
      "Validation: Epoch [4], Batch [267/938], Loss: 1.2098033428192139\n",
      "Validation: Epoch [4], Batch [268/938], Loss: 1.2878553867340088\n",
      "Validation: Epoch [4], Batch [269/938], Loss: 1.272597312927246\n",
      "Validation: Epoch [4], Batch [270/938], Loss: 1.5238546133041382\n",
      "Validation: Epoch [4], Batch [271/938], Loss: 1.3100528717041016\n",
      "Validation: Epoch [4], Batch [272/938], Loss: 1.2679895162582397\n",
      "Validation: Epoch [4], Batch [273/938], Loss: 1.4792509078979492\n",
      "Validation: Epoch [4], Batch [274/938], Loss: 1.3231877088546753\n",
      "Validation: Epoch [4], Batch [275/938], Loss: 1.5447076559066772\n",
      "Validation: Epoch [4], Batch [276/938], Loss: 1.1154494285583496\n",
      "Validation: Epoch [4], Batch [277/938], Loss: 1.579489827156067\n",
      "Validation: Epoch [4], Batch [278/938], Loss: 1.0406335592269897\n",
      "Validation: Epoch [4], Batch [279/938], Loss: 1.204857349395752\n",
      "Validation: Epoch [4], Batch [280/938], Loss: 1.1011059284210205\n",
      "Validation: Epoch [4], Batch [281/938], Loss: 1.3861217498779297\n",
      "Validation: Epoch [4], Batch [282/938], Loss: 1.2250475883483887\n",
      "Validation: Epoch [4], Batch [283/938], Loss: 1.4033613204956055\n",
      "Validation: Epoch [4], Batch [284/938], Loss: 1.2693191766738892\n",
      "Validation: Epoch [4], Batch [285/938], Loss: 1.476523756980896\n",
      "Validation: Epoch [4], Batch [286/938], Loss: 1.353493571281433\n",
      "Validation: Epoch [4], Batch [287/938], Loss: 1.3290668725967407\n",
      "Validation: Epoch [4], Batch [288/938], Loss: 1.2706307172775269\n",
      "Validation: Epoch [4], Batch [289/938], Loss: 1.3154774904251099\n",
      "Validation: Epoch [4], Batch [290/938], Loss: 1.2942495346069336\n",
      "Validation: Epoch [4], Batch [291/938], Loss: 1.4906229972839355\n",
      "Validation: Epoch [4], Batch [292/938], Loss: 1.4506208896636963\n",
      "Validation: Epoch [4], Batch [293/938], Loss: 1.3262759447097778\n",
      "Validation: Epoch [4], Batch [294/938], Loss: 1.2339258193969727\n",
      "Validation: Epoch [4], Batch [295/938], Loss: 1.4426615238189697\n",
      "Validation: Epoch [4], Batch [296/938], Loss: 1.4193031787872314\n",
      "Validation: Epoch [4], Batch [297/938], Loss: 1.1450886726379395\n",
      "Validation: Epoch [4], Batch [298/938], Loss: 1.4875006675720215\n",
      "Validation: Epoch [4], Batch [299/938], Loss: 1.122541904449463\n",
      "Validation: Epoch [4], Batch [300/938], Loss: 1.3214704990386963\n",
      "Validation: Epoch [4], Batch [301/938], Loss: 1.3397369384765625\n",
      "Validation: Epoch [4], Batch [302/938], Loss: 1.4780057668685913\n",
      "Validation: Epoch [4], Batch [303/938], Loss: 1.4157333374023438\n",
      "Validation: Epoch [4], Batch [304/938], Loss: 1.1111191511154175\n",
      "Validation: Epoch [4], Batch [305/938], Loss: 1.3577899932861328\n",
      "Validation: Epoch [4], Batch [306/938], Loss: 1.2443227767944336\n",
      "Validation: Epoch [4], Batch [307/938], Loss: 1.1505663394927979\n",
      "Validation: Epoch [4], Batch [308/938], Loss: 1.4031392335891724\n",
      "Validation: Epoch [4], Batch [309/938], Loss: 1.3170052766799927\n",
      "Validation: Epoch [4], Batch [310/938], Loss: 1.4464974403381348\n",
      "Validation: Epoch [4], Batch [311/938], Loss: 1.3478657007217407\n",
      "Validation: Epoch [4], Batch [312/938], Loss: 1.4545438289642334\n",
      "Validation: Epoch [4], Batch [313/938], Loss: 1.2801156044006348\n",
      "Validation: Epoch [4], Batch [314/938], Loss: 1.1519358158111572\n",
      "Validation: Epoch [4], Batch [315/938], Loss: 1.069109320640564\n",
      "Validation: Epoch [4], Batch [316/938], Loss: 1.222651720046997\n",
      "Validation: Epoch [4], Batch [317/938], Loss: 1.4173356294631958\n",
      "Validation: Epoch [4], Batch [318/938], Loss: 1.5136972665786743\n",
      "Validation: Epoch [4], Batch [319/938], Loss: 1.1558098793029785\n",
      "Validation: Epoch [4], Batch [320/938], Loss: 1.2044072151184082\n",
      "Validation: Epoch [4], Batch [321/938], Loss: 1.0482122898101807\n",
      "Validation: Epoch [4], Batch [322/938], Loss: 1.4006595611572266\n",
      "Validation: Epoch [4], Batch [323/938], Loss: 1.4543782472610474\n",
      "Validation: Epoch [4], Batch [324/938], Loss: 1.2300525903701782\n",
      "Validation: Epoch [4], Batch [325/938], Loss: 1.1213692426681519\n",
      "Validation: Epoch [4], Batch [326/938], Loss: 1.3993420600891113\n",
      "Validation: Epoch [4], Batch [327/938], Loss: 1.2404807806015015\n",
      "Validation: Epoch [4], Batch [328/938], Loss: 1.2933193445205688\n",
      "Validation: Epoch [4], Batch [329/938], Loss: 1.2026070356369019\n",
      "Validation: Epoch [4], Batch [330/938], Loss: 1.09627366065979\n",
      "Validation: Epoch [4], Batch [331/938], Loss: 1.3717931509017944\n",
      "Validation: Epoch [4], Batch [332/938], Loss: 1.393381953239441\n",
      "Validation: Epoch [4], Batch [333/938], Loss: 1.3676044940948486\n",
      "Validation: Epoch [4], Batch [334/938], Loss: 1.2668061256408691\n",
      "Validation: Epoch [4], Batch [335/938], Loss: 1.2157009840011597\n",
      "Validation: Epoch [4], Batch [336/938], Loss: 1.1662685871124268\n",
      "Validation: Epoch [4], Batch [337/938], Loss: 1.1992602348327637\n",
      "Validation: Epoch [4], Batch [338/938], Loss: 1.4813483953475952\n",
      "Validation: Epoch [4], Batch [339/938], Loss: 1.3338607549667358\n",
      "Validation: Epoch [4], Batch [340/938], Loss: 1.3826181888580322\n",
      "Validation: Epoch [4], Batch [341/938], Loss: 1.3253778219223022\n",
      "Validation: Epoch [4], Batch [342/938], Loss: 1.2495747804641724\n",
      "Validation: Epoch [4], Batch [343/938], Loss: 1.2419072389602661\n",
      "Validation: Epoch [4], Batch [344/938], Loss: 1.4351351261138916\n",
      "Validation: Epoch [4], Batch [345/938], Loss: 1.1518850326538086\n",
      "Validation: Epoch [4], Batch [346/938], Loss: 1.4155737161636353\n",
      "Validation: Epoch [4], Batch [347/938], Loss: 1.4404901266098022\n",
      "Validation: Epoch [4], Batch [348/938], Loss: 1.3895827531814575\n",
      "Validation: Epoch [4], Batch [349/938], Loss: 1.3653197288513184\n",
      "Validation: Epoch [4], Batch [350/938], Loss: 1.3479161262512207\n",
      "Validation: Epoch [4], Batch [351/938], Loss: 1.4311792850494385\n",
      "Validation: Epoch [4], Batch [352/938], Loss: 1.5128893852233887\n",
      "Validation: Epoch [4], Batch [353/938], Loss: 1.4482537508010864\n",
      "Validation: Epoch [4], Batch [354/938], Loss: 1.1756967306137085\n",
      "Validation: Epoch [4], Batch [355/938], Loss: 1.4774982929229736\n",
      "Validation: Epoch [4], Batch [356/938], Loss: 1.307985544204712\n",
      "Validation: Epoch [4], Batch [357/938], Loss: 1.2487269639968872\n",
      "Validation: Epoch [4], Batch [358/938], Loss: 1.282256841659546\n",
      "Validation: Epoch [4], Batch [359/938], Loss: 1.0201492309570312\n",
      "Validation: Epoch [4], Batch [360/938], Loss: 1.2138638496398926\n",
      "Validation: Epoch [4], Batch [361/938], Loss: 1.208369255065918\n",
      "Validation: Epoch [4], Batch [362/938], Loss: 1.4071732759475708\n",
      "Validation: Epoch [4], Batch [363/938], Loss: 1.4021210670471191\n",
      "Validation: Epoch [4], Batch [364/938], Loss: 1.246036410331726\n",
      "Validation: Epoch [4], Batch [365/938], Loss: 1.4836671352386475\n",
      "Validation: Epoch [4], Batch [366/938], Loss: 1.3166923522949219\n",
      "Validation: Epoch [4], Batch [367/938], Loss: 1.2150967121124268\n",
      "Validation: Epoch [4], Batch [368/938], Loss: 1.438589334487915\n",
      "Validation: Epoch [4], Batch [369/938], Loss: 1.1440274715423584\n",
      "Validation: Epoch [4], Batch [370/938], Loss: 1.530836820602417\n",
      "Validation: Epoch [4], Batch [371/938], Loss: 1.3385077714920044\n",
      "Validation: Epoch [4], Batch [372/938], Loss: 1.1622881889343262\n",
      "Validation: Epoch [4], Batch [373/938], Loss: 1.1923909187316895\n",
      "Validation: Epoch [4], Batch [374/938], Loss: 1.1062533855438232\n",
      "Validation: Epoch [4], Batch [375/938], Loss: 1.2939420938491821\n",
      "Validation: Epoch [4], Batch [376/938], Loss: 1.3913267850875854\n",
      "Validation: Epoch [4], Batch [377/938], Loss: 1.1607948541641235\n",
      "Validation: Epoch [4], Batch [378/938], Loss: 1.4498939514160156\n",
      "Validation: Epoch [4], Batch [379/938], Loss: 1.117424488067627\n",
      "Validation: Epoch [4], Batch [380/938], Loss: 1.2459107637405396\n",
      "Validation: Epoch [4], Batch [381/938], Loss: 1.2069318294525146\n",
      "Validation: Epoch [4], Batch [382/938], Loss: 1.2585382461547852\n",
      "Validation: Epoch [4], Batch [383/938], Loss: 1.4568686485290527\n",
      "Validation: Epoch [4], Batch [384/938], Loss: 1.0471460819244385\n",
      "Validation: Epoch [4], Batch [385/938], Loss: 1.3103867769241333\n",
      "Validation: Epoch [4], Batch [386/938], Loss: 1.5375206470489502\n",
      "Validation: Epoch [4], Batch [387/938], Loss: 1.243693232536316\n",
      "Validation: Epoch [4], Batch [388/938], Loss: 1.2244600057601929\n",
      "Validation: Epoch [4], Batch [389/938], Loss: 1.1751307249069214\n",
      "Validation: Epoch [4], Batch [390/938], Loss: 1.4935131072998047\n",
      "Validation: Epoch [4], Batch [391/938], Loss: 1.085132360458374\n",
      "Validation: Epoch [4], Batch [392/938], Loss: 1.2926766872406006\n",
      "Validation: Epoch [4], Batch [393/938], Loss: 1.032252311706543\n",
      "Validation: Epoch [4], Batch [394/938], Loss: 1.4918768405914307\n",
      "Validation: Epoch [4], Batch [395/938], Loss: 1.5488678216934204\n",
      "Validation: Epoch [4], Batch [396/938], Loss: 1.4478020668029785\n",
      "Validation: Epoch [4], Batch [397/938], Loss: 1.3908270597457886\n",
      "Validation: Epoch [4], Batch [398/938], Loss: 1.1149356365203857\n",
      "Validation: Epoch [4], Batch [399/938], Loss: 1.6383026838302612\n",
      "Validation: Epoch [4], Batch [400/938], Loss: 1.4048705101013184\n",
      "Validation: Epoch [4], Batch [401/938], Loss: 1.3375136852264404\n",
      "Validation: Epoch [4], Batch [402/938], Loss: 1.503769874572754\n",
      "Validation: Epoch [4], Batch [403/938], Loss: 1.3719245195388794\n",
      "Validation: Epoch [4], Batch [404/938], Loss: 1.265444040298462\n",
      "Validation: Epoch [4], Batch [405/938], Loss: 1.2839674949645996\n",
      "Validation: Epoch [4], Batch [406/938], Loss: 1.3973740339279175\n",
      "Validation: Epoch [4], Batch [407/938], Loss: 1.3194838762283325\n",
      "Validation: Epoch [4], Batch [408/938], Loss: 1.3776309490203857\n",
      "Validation: Epoch [4], Batch [409/938], Loss: 1.1302814483642578\n",
      "Validation: Epoch [4], Batch [410/938], Loss: 1.1599626541137695\n",
      "Validation: Epoch [4], Batch [411/938], Loss: 1.199436068534851\n",
      "Validation: Epoch [4], Batch [412/938], Loss: 1.0694748163223267\n",
      "Validation: Epoch [4], Batch [413/938], Loss: 1.065483570098877\n",
      "Validation: Epoch [4], Batch [414/938], Loss: 1.3225016593933105\n",
      "Validation: Epoch [4], Batch [415/938], Loss: 1.3690763711929321\n",
      "Validation: Epoch [4], Batch [416/938], Loss: 1.3350694179534912\n",
      "Validation: Epoch [4], Batch [417/938], Loss: 1.3895471096038818\n",
      "Validation: Epoch [4], Batch [418/938], Loss: 1.3394486904144287\n",
      "Validation: Epoch [4], Batch [419/938], Loss: 1.4157558679580688\n",
      "Validation: Epoch [4], Batch [420/938], Loss: 0.9937775731086731\n",
      "Validation: Epoch [4], Batch [421/938], Loss: 1.2257413864135742\n",
      "Validation: Epoch [4], Batch [422/938], Loss: 1.3022758960723877\n",
      "Validation: Epoch [4], Batch [423/938], Loss: 1.2260960340499878\n",
      "Validation: Epoch [4], Batch [424/938], Loss: 1.2844562530517578\n",
      "Validation: Epoch [4], Batch [425/938], Loss: 1.3869445323944092\n",
      "Validation: Epoch [4], Batch [426/938], Loss: 1.3852444887161255\n",
      "Validation: Epoch [4], Batch [427/938], Loss: 1.327148675918579\n",
      "Validation: Epoch [4], Batch [428/938], Loss: 1.4771969318389893\n",
      "Validation: Epoch [4], Batch [429/938], Loss: 1.4603338241577148\n",
      "Validation: Epoch [4], Batch [430/938], Loss: 1.441184639930725\n",
      "Validation: Epoch [4], Batch [431/938], Loss: 1.3272074460983276\n",
      "Validation: Epoch [4], Batch [432/938], Loss: 1.3469001054763794\n",
      "Validation: Epoch [4], Batch [433/938], Loss: 1.2932595014572144\n",
      "Validation: Epoch [4], Batch [434/938], Loss: 1.331883430480957\n",
      "Validation: Epoch [4], Batch [435/938], Loss: 1.2832640409469604\n",
      "Validation: Epoch [4], Batch [436/938], Loss: 1.4233675003051758\n",
      "Validation: Epoch [4], Batch [437/938], Loss: 1.708400011062622\n",
      "Validation: Epoch [4], Batch [438/938], Loss: 1.372180700302124\n",
      "Validation: Epoch [4], Batch [439/938], Loss: 1.1735025644302368\n",
      "Validation: Epoch [4], Batch [440/938], Loss: 1.2171051502227783\n",
      "Validation: Epoch [4], Batch [441/938], Loss: 1.3660987615585327\n",
      "Validation: Epoch [4], Batch [442/938], Loss: 1.3033661842346191\n",
      "Validation: Epoch [4], Batch [443/938], Loss: 1.2246273756027222\n",
      "Validation: Epoch [4], Batch [444/938], Loss: 1.4833146333694458\n",
      "Validation: Epoch [4], Batch [445/938], Loss: 1.4465006589889526\n",
      "Validation: Epoch [4], Batch [446/938], Loss: 1.2354103326797485\n",
      "Validation: Epoch [4], Batch [447/938], Loss: 1.1820710897445679\n",
      "Validation: Epoch [4], Batch [448/938], Loss: 1.2380505800247192\n",
      "Validation: Epoch [4], Batch [449/938], Loss: 1.2938036918640137\n",
      "Validation: Epoch [4], Batch [450/938], Loss: 1.1022121906280518\n",
      "Validation: Epoch [4], Batch [451/938], Loss: 1.3000051975250244\n",
      "Validation: Epoch [4], Batch [452/938], Loss: 1.4127790927886963\n",
      "Validation: Epoch [4], Batch [453/938], Loss: 1.397752046585083\n",
      "Validation: Epoch [4], Batch [454/938], Loss: 1.58194100856781\n",
      "Validation: Epoch [4], Batch [455/938], Loss: 1.1621900796890259\n",
      "Validation: Epoch [4], Batch [456/938], Loss: 1.224505066871643\n",
      "Validation: Epoch [4], Batch [457/938], Loss: 1.2703449726104736\n",
      "Validation: Epoch [4], Batch [458/938], Loss: 1.2602183818817139\n",
      "Validation: Epoch [4], Batch [459/938], Loss: 1.2636747360229492\n",
      "Validation: Epoch [4], Batch [460/938], Loss: 1.2886571884155273\n",
      "Validation: Epoch [4], Batch [461/938], Loss: 1.2423789501190186\n",
      "Validation: Epoch [4], Batch [462/938], Loss: 1.3491054773330688\n",
      "Validation: Epoch [4], Batch [463/938], Loss: 1.193904161453247\n",
      "Validation: Epoch [4], Batch [464/938], Loss: 1.3316559791564941\n",
      "Validation: Epoch [4], Batch [465/938], Loss: 1.3676764965057373\n",
      "Validation: Epoch [4], Batch [466/938], Loss: 1.3445327281951904\n",
      "Validation: Epoch [4], Batch [467/938], Loss: 1.4202117919921875\n",
      "Validation: Epoch [4], Batch [468/938], Loss: 1.5828769207000732\n",
      "Validation: Epoch [4], Batch [469/938], Loss: 1.316004991531372\n",
      "Validation: Epoch [4], Batch [470/938], Loss: 1.557981252670288\n",
      "Validation: Epoch [4], Batch [471/938], Loss: 1.324802041053772\n",
      "Validation: Epoch [4], Batch [472/938], Loss: 1.4161438941955566\n",
      "Validation: Epoch [4], Batch [473/938], Loss: 1.3167601823806763\n",
      "Validation: Epoch [4], Batch [474/938], Loss: 1.4108061790466309\n",
      "Validation: Epoch [4], Batch [475/938], Loss: 1.444278359413147\n",
      "Validation: Epoch [4], Batch [476/938], Loss: 1.395890474319458\n",
      "Validation: Epoch [4], Batch [477/938], Loss: 1.37004816532135\n",
      "Validation: Epoch [4], Batch [478/938], Loss: 1.2775360345840454\n",
      "Validation: Epoch [4], Batch [479/938], Loss: 1.2735486030578613\n",
      "Validation: Epoch [4], Batch [480/938], Loss: 1.168058156967163\n",
      "Validation: Epoch [4], Batch [481/938], Loss: 1.1587648391723633\n",
      "Validation: Epoch [4], Batch [482/938], Loss: 1.438855528831482\n",
      "Validation: Epoch [4], Batch [483/938], Loss: 1.0907630920410156\n",
      "Validation: Epoch [4], Batch [484/938], Loss: 1.0558677911758423\n",
      "Validation: Epoch [4], Batch [485/938], Loss: 1.1795737743377686\n",
      "Validation: Epoch [4], Batch [486/938], Loss: 1.3938262462615967\n",
      "Validation: Epoch [4], Batch [487/938], Loss: 1.360852599143982\n",
      "Validation: Epoch [4], Batch [488/938], Loss: 1.3731017112731934\n",
      "Validation: Epoch [4], Batch [489/938], Loss: 1.2671167850494385\n",
      "Validation: Epoch [4], Batch [490/938], Loss: 1.0260921716690063\n",
      "Validation: Epoch [4], Batch [491/938], Loss: 1.3242065906524658\n",
      "Validation: Epoch [4], Batch [492/938], Loss: 1.4791873693466187\n",
      "Validation: Epoch [4], Batch [493/938], Loss: 1.3546453714370728\n",
      "Validation: Epoch [4], Batch [494/938], Loss: 1.384871006011963\n",
      "Validation: Epoch [4], Batch [495/938], Loss: 1.166202187538147\n",
      "Validation: Epoch [4], Batch [496/938], Loss: 1.364743709564209\n",
      "Validation: Epoch [4], Batch [497/938], Loss: 1.1995328664779663\n",
      "Validation: Epoch [4], Batch [498/938], Loss: 1.4985990524291992\n",
      "Validation: Epoch [4], Batch [499/938], Loss: 1.3467669486999512\n",
      "Validation: Epoch [4], Batch [500/938], Loss: 1.3695507049560547\n",
      "Validation: Epoch [4], Batch [501/938], Loss: 1.2050268650054932\n",
      "Validation: Epoch [4], Batch [502/938], Loss: 1.417441964149475\n",
      "Validation: Epoch [4], Batch [503/938], Loss: 1.4083784818649292\n",
      "Validation: Epoch [4], Batch [504/938], Loss: 1.1677849292755127\n",
      "Validation: Epoch [4], Batch [505/938], Loss: 1.1532174348831177\n",
      "Validation: Epoch [4], Batch [506/938], Loss: 1.2798488140106201\n",
      "Validation: Epoch [4], Batch [507/938], Loss: 1.4269664287567139\n",
      "Validation: Epoch [4], Batch [508/938], Loss: 1.3047053813934326\n",
      "Validation: Epoch [4], Batch [509/938], Loss: 1.2228469848632812\n",
      "Validation: Epoch [4], Batch [510/938], Loss: 1.2516605854034424\n",
      "Validation: Epoch [4], Batch [511/938], Loss: 1.2258021831512451\n",
      "Validation: Epoch [4], Batch [512/938], Loss: 1.306508183479309\n",
      "Validation: Epoch [4], Batch [513/938], Loss: 1.3870234489440918\n",
      "Validation: Epoch [4], Batch [514/938], Loss: 1.3217828273773193\n",
      "Validation: Epoch [4], Batch [515/938], Loss: 1.2209583520889282\n",
      "Validation: Epoch [4], Batch [516/938], Loss: 1.2295955419540405\n",
      "Validation: Epoch [4], Batch [517/938], Loss: 1.337493896484375\n",
      "Validation: Epoch [4], Batch [518/938], Loss: 1.5056438446044922\n",
      "Validation: Epoch [4], Batch [519/938], Loss: 1.2941679954528809\n",
      "Validation: Epoch [4], Batch [520/938], Loss: 1.4187978506088257\n",
      "Validation: Epoch [4], Batch [521/938], Loss: 1.2445225715637207\n",
      "Validation: Epoch [4], Batch [522/938], Loss: 1.4763368368148804\n",
      "Validation: Epoch [4], Batch [523/938], Loss: 1.4011367559432983\n",
      "Validation: Epoch [4], Batch [524/938], Loss: 1.3956536054611206\n",
      "Validation: Epoch [4], Batch [525/938], Loss: 1.2663084268569946\n",
      "Validation: Epoch [4], Batch [526/938], Loss: 1.3222012519836426\n",
      "Validation: Epoch [4], Batch [527/938], Loss: 1.263633370399475\n",
      "Validation: Epoch [4], Batch [528/938], Loss: 1.4534497261047363\n",
      "Validation: Epoch [4], Batch [529/938], Loss: 1.1641566753387451\n",
      "Validation: Epoch [4], Batch [530/938], Loss: 1.2579066753387451\n",
      "Validation: Epoch [4], Batch [531/938], Loss: 1.3289580345153809\n",
      "Validation: Epoch [4], Batch [532/938], Loss: 1.3938682079315186\n",
      "Validation: Epoch [4], Batch [533/938], Loss: 1.1120914220809937\n",
      "Validation: Epoch [4], Batch [534/938], Loss: 1.1625360250473022\n",
      "Validation: Epoch [4], Batch [535/938], Loss: 1.246401309967041\n",
      "Validation: Epoch [4], Batch [536/938], Loss: 1.1576926708221436\n",
      "Validation: Epoch [4], Batch [537/938], Loss: 1.3198096752166748\n",
      "Validation: Epoch [4], Batch [538/938], Loss: 1.3281610012054443\n",
      "Validation: Epoch [4], Batch [539/938], Loss: 1.3499770164489746\n",
      "Validation: Epoch [4], Batch [540/938], Loss: 1.2519234418869019\n",
      "Validation: Epoch [4], Batch [541/938], Loss: 1.5020053386688232\n",
      "Validation: Epoch [4], Batch [542/938], Loss: 1.2870302200317383\n",
      "Validation: Epoch [4], Batch [543/938], Loss: 1.1452230215072632\n",
      "Validation: Epoch [4], Batch [544/938], Loss: 1.393777847290039\n",
      "Validation: Epoch [4], Batch [545/938], Loss: 1.334902286529541\n",
      "Validation: Epoch [4], Batch [546/938], Loss: 1.2116574048995972\n",
      "Validation: Epoch [4], Batch [547/938], Loss: 1.222412347793579\n",
      "Validation: Epoch [4], Batch [548/938], Loss: 1.1679582595825195\n",
      "Validation: Epoch [4], Batch [549/938], Loss: 1.2462880611419678\n",
      "Validation: Epoch [4], Batch [550/938], Loss: 1.1094105243682861\n",
      "Validation: Epoch [4], Batch [551/938], Loss: 1.319431185722351\n",
      "Validation: Epoch [4], Batch [552/938], Loss: 1.0269497632980347\n",
      "Validation: Epoch [4], Batch [553/938], Loss: 1.4110918045043945\n",
      "Validation: Epoch [4], Batch [554/938], Loss: 1.4072189331054688\n",
      "Validation: Epoch [4], Batch [555/938], Loss: 1.351635456085205\n",
      "Validation: Epoch [4], Batch [556/938], Loss: 1.0877119302749634\n",
      "Validation: Epoch [4], Batch [557/938], Loss: 1.4960072040557861\n",
      "Validation: Epoch [4], Batch [558/938], Loss: 1.183295488357544\n",
      "Validation: Epoch [4], Batch [559/938], Loss: 1.342642068862915\n",
      "Validation: Epoch [4], Batch [560/938], Loss: 1.2394417524337769\n",
      "Validation: Epoch [4], Batch [561/938], Loss: 1.634385108947754\n",
      "Validation: Epoch [4], Batch [562/938], Loss: 1.4109594821929932\n",
      "Validation: Epoch [4], Batch [563/938], Loss: 1.486945629119873\n",
      "Validation: Epoch [4], Batch [564/938], Loss: 1.2641246318817139\n",
      "Validation: Epoch [4], Batch [565/938], Loss: 1.110692024230957\n",
      "Validation: Epoch [4], Batch [566/938], Loss: 1.1835637092590332\n",
      "Validation: Epoch [4], Batch [567/938], Loss: 1.2790402173995972\n",
      "Validation: Epoch [4], Batch [568/938], Loss: 1.180414080619812\n",
      "Validation: Epoch [4], Batch [569/938], Loss: 1.280264139175415\n",
      "Validation: Epoch [4], Batch [570/938], Loss: 1.243351936340332\n",
      "Validation: Epoch [4], Batch [571/938], Loss: 1.4049873352050781\n",
      "Validation: Epoch [4], Batch [572/938], Loss: 1.530383825302124\n",
      "Validation: Epoch [4], Batch [573/938], Loss: 1.193104863166809\n",
      "Validation: Epoch [4], Batch [574/938], Loss: 1.4629161357879639\n",
      "Validation: Epoch [4], Batch [575/938], Loss: 1.487608790397644\n",
      "Validation: Epoch [4], Batch [576/938], Loss: 1.1759495735168457\n",
      "Validation: Epoch [4], Batch [577/938], Loss: 1.391209602355957\n",
      "Validation: Epoch [4], Batch [578/938], Loss: 1.5840851068496704\n",
      "Validation: Epoch [4], Batch [579/938], Loss: 1.3988759517669678\n",
      "Validation: Epoch [4], Batch [580/938], Loss: 1.236172080039978\n",
      "Validation: Epoch [4], Batch [581/938], Loss: 1.153459906578064\n",
      "Validation: Epoch [4], Batch [582/938], Loss: 1.0796855688095093\n",
      "Validation: Epoch [4], Batch [583/938], Loss: 1.089972972869873\n",
      "Validation: Epoch [4], Batch [584/938], Loss: 1.4364272356033325\n",
      "Validation: Epoch [4], Batch [585/938], Loss: 1.3157148361206055\n",
      "Validation: Epoch [4], Batch [586/938], Loss: 1.4081995487213135\n",
      "Validation: Epoch [4], Batch [587/938], Loss: 1.2927969694137573\n",
      "Validation: Epoch [4], Batch [588/938], Loss: 1.2002320289611816\n",
      "Validation: Epoch [4], Batch [589/938], Loss: 1.2003506422042847\n",
      "Validation: Epoch [4], Batch [590/938], Loss: 1.3218141794204712\n",
      "Validation: Epoch [4], Batch [591/938], Loss: 1.374776840209961\n",
      "Validation: Epoch [4], Batch [592/938], Loss: 1.2547343969345093\n",
      "Validation: Epoch [4], Batch [593/938], Loss: 1.3572015762329102\n",
      "Validation: Epoch [4], Batch [594/938], Loss: 1.3216402530670166\n",
      "Validation: Epoch [4], Batch [595/938], Loss: 1.2813355922698975\n",
      "Validation: Epoch [4], Batch [596/938], Loss: 1.3937878608703613\n",
      "Validation: Epoch [4], Batch [597/938], Loss: 1.3066658973693848\n",
      "Validation: Epoch [4], Batch [598/938], Loss: 1.329456090927124\n",
      "Validation: Epoch [4], Batch [599/938], Loss: 1.4539281129837036\n",
      "Validation: Epoch [4], Batch [600/938], Loss: 1.2551631927490234\n",
      "Validation: Epoch [4], Batch [601/938], Loss: 1.3566417694091797\n",
      "Validation: Epoch [4], Batch [602/938], Loss: 1.438768744468689\n",
      "Validation: Epoch [4], Batch [603/938], Loss: 1.5384209156036377\n",
      "Validation: Epoch [4], Batch [604/938], Loss: 1.126809000968933\n",
      "Validation: Epoch [4], Batch [605/938], Loss: 1.5087950229644775\n",
      "Validation: Epoch [4], Batch [606/938], Loss: 1.3859920501708984\n",
      "Validation: Epoch [4], Batch [607/938], Loss: 1.3004066944122314\n",
      "Validation: Epoch [4], Batch [608/938], Loss: 1.018064260482788\n",
      "Validation: Epoch [4], Batch [609/938], Loss: 1.2383784055709839\n",
      "Validation: Epoch [4], Batch [610/938], Loss: 1.1993507146835327\n",
      "Validation: Epoch [4], Batch [611/938], Loss: 1.1303696632385254\n",
      "Validation: Epoch [4], Batch [612/938], Loss: 1.3217602968215942\n",
      "Validation: Epoch [4], Batch [613/938], Loss: 1.2029225826263428\n",
      "Validation: Epoch [4], Batch [614/938], Loss: 1.3150392770767212\n",
      "Validation: Epoch [4], Batch [615/938], Loss: 1.492293119430542\n",
      "Validation: Epoch [4], Batch [616/938], Loss: 1.12449049949646\n",
      "Validation: Epoch [4], Batch [617/938], Loss: 1.364891767501831\n",
      "Validation: Epoch [4], Batch [618/938], Loss: 1.123071312904358\n",
      "Validation: Epoch [4], Batch [619/938], Loss: 1.185225009918213\n",
      "Validation: Epoch [4], Batch [620/938], Loss: 1.5155457258224487\n",
      "Validation: Epoch [4], Batch [621/938], Loss: 1.2225959300994873\n",
      "Validation: Epoch [4], Batch [622/938], Loss: 1.4388114213943481\n",
      "Validation: Epoch [4], Batch [623/938], Loss: 1.4465652704238892\n",
      "Validation: Epoch [4], Batch [624/938], Loss: 1.1113877296447754\n",
      "Validation: Epoch [4], Batch [625/938], Loss: 1.2786598205566406\n",
      "Validation: Epoch [4], Batch [626/938], Loss: 1.3215242624282837\n",
      "Validation: Epoch [4], Batch [627/938], Loss: 1.1515581607818604\n",
      "Validation: Epoch [4], Batch [628/938], Loss: 1.0806794166564941\n",
      "Validation: Epoch [4], Batch [629/938], Loss: 1.2430458068847656\n",
      "Validation: Epoch [4], Batch [630/938], Loss: 1.4365860223770142\n",
      "Validation: Epoch [4], Batch [631/938], Loss: 1.3791847229003906\n",
      "Validation: Epoch [4], Batch [632/938], Loss: 1.5115898847579956\n",
      "Validation: Epoch [4], Batch [633/938], Loss: 1.2524757385253906\n",
      "Validation: Epoch [4], Batch [634/938], Loss: 1.2190320491790771\n",
      "Validation: Epoch [4], Batch [635/938], Loss: 1.2261821031570435\n",
      "Validation: Epoch [4], Batch [636/938], Loss: 1.1167439222335815\n",
      "Validation: Epoch [4], Batch [637/938], Loss: 1.2015870809555054\n",
      "Validation: Epoch [4], Batch [638/938], Loss: 1.3196052312850952\n",
      "Validation: Epoch [4], Batch [639/938], Loss: 1.208364725112915\n",
      "Validation: Epoch [4], Batch [640/938], Loss: 1.330142617225647\n",
      "Validation: Epoch [4], Batch [641/938], Loss: 1.385684847831726\n",
      "Validation: Epoch [4], Batch [642/938], Loss: 1.3559781312942505\n",
      "Validation: Epoch [4], Batch [643/938], Loss: 1.2361664772033691\n",
      "Validation: Epoch [4], Batch [644/938], Loss: 1.0453170537948608\n",
      "Validation: Epoch [4], Batch [645/938], Loss: 1.4216164350509644\n",
      "Validation: Epoch [4], Batch [646/938], Loss: 1.2873550653457642\n",
      "Validation: Epoch [4], Batch [647/938], Loss: 1.2941482067108154\n",
      "Validation: Epoch [4], Batch [648/938], Loss: 1.3720316886901855\n",
      "Validation: Epoch [4], Batch [649/938], Loss: 1.2987587451934814\n",
      "Validation: Epoch [4], Batch [650/938], Loss: 1.446366786956787\n",
      "Validation: Epoch [4], Batch [651/938], Loss: 1.3006322383880615\n",
      "Validation: Epoch [4], Batch [652/938], Loss: 1.1322884559631348\n",
      "Validation: Epoch [4], Batch [653/938], Loss: 1.2502729892730713\n",
      "Validation: Epoch [4], Batch [654/938], Loss: 1.2358458042144775\n",
      "Validation: Epoch [4], Batch [655/938], Loss: 1.3418763875961304\n",
      "Validation: Epoch [4], Batch [656/938], Loss: 1.4057871103286743\n",
      "Validation: Epoch [4], Batch [657/938], Loss: 1.2205228805541992\n",
      "Validation: Epoch [4], Batch [658/938], Loss: 1.260330319404602\n",
      "Validation: Epoch [4], Batch [659/938], Loss: 1.337019920349121\n",
      "Validation: Epoch [4], Batch [660/938], Loss: 1.3149373531341553\n",
      "Validation: Epoch [4], Batch [661/938], Loss: 1.4104249477386475\n",
      "Validation: Epoch [4], Batch [662/938], Loss: 1.3613181114196777\n",
      "Validation: Epoch [4], Batch [663/938], Loss: 1.2425373792648315\n",
      "Validation: Epoch [4], Batch [664/938], Loss: 1.113532304763794\n",
      "Validation: Epoch [4], Batch [665/938], Loss: 1.3561031818389893\n",
      "Validation: Epoch [4], Batch [666/938], Loss: 1.4241219758987427\n",
      "Validation: Epoch [4], Batch [667/938], Loss: 1.4477789402008057\n",
      "Validation: Epoch [4], Batch [668/938], Loss: 1.2741690874099731\n",
      "Validation: Epoch [4], Batch [669/938], Loss: 1.2958273887634277\n",
      "Validation: Epoch [4], Batch [670/938], Loss: 1.5571351051330566\n",
      "Validation: Epoch [4], Batch [671/938], Loss: 1.284597396850586\n",
      "Validation: Epoch [4], Batch [672/938], Loss: 1.2634605169296265\n",
      "Validation: Epoch [4], Batch [673/938], Loss: 1.2207973003387451\n",
      "Validation: Epoch [4], Batch [674/938], Loss: 1.3053216934204102\n",
      "Validation: Epoch [4], Batch [675/938], Loss: 1.3678725957870483\n",
      "Validation: Epoch [4], Batch [676/938], Loss: 1.368795394897461\n",
      "Validation: Epoch [4], Batch [677/938], Loss: 1.0806348323822021\n",
      "Validation: Epoch [4], Batch [678/938], Loss: 1.2947885990142822\n",
      "Validation: Epoch [4], Batch [679/938], Loss: 1.4230024814605713\n",
      "Validation: Epoch [4], Batch [680/938], Loss: 1.1979420185089111\n",
      "Validation: Epoch [4], Batch [681/938], Loss: 1.1280884742736816\n",
      "Validation: Epoch [4], Batch [682/938], Loss: 1.291430115699768\n",
      "Validation: Epoch [4], Batch [683/938], Loss: 1.2545368671417236\n",
      "Validation: Epoch [4], Batch [684/938], Loss: 1.3593289852142334\n",
      "Validation: Epoch [4], Batch [685/938], Loss: 1.4253838062286377\n",
      "Validation: Epoch [4], Batch [686/938], Loss: 1.3107128143310547\n",
      "Validation: Epoch [4], Batch [687/938], Loss: 1.493834376335144\n",
      "Validation: Epoch [4], Batch [688/938], Loss: 1.3386276960372925\n",
      "Validation: Epoch [4], Batch [689/938], Loss: 1.1639395952224731\n",
      "Validation: Epoch [4], Batch [690/938], Loss: 1.1457598209381104\n",
      "Validation: Epoch [4], Batch [691/938], Loss: 1.2525415420532227\n",
      "Validation: Epoch [4], Batch [692/938], Loss: 1.388992428779602\n",
      "Validation: Epoch [4], Batch [693/938], Loss: 1.3934366703033447\n",
      "Validation: Epoch [4], Batch [694/938], Loss: 1.4628605842590332\n",
      "Validation: Epoch [4], Batch [695/938], Loss: 1.260219693183899\n",
      "Validation: Epoch [4], Batch [696/938], Loss: 1.4090583324432373\n",
      "Validation: Epoch [4], Batch [697/938], Loss: 1.2773752212524414\n",
      "Validation: Epoch [4], Batch [698/938], Loss: 1.4820201396942139\n",
      "Validation: Epoch [4], Batch [699/938], Loss: 1.3308981657028198\n",
      "Validation: Epoch [4], Batch [700/938], Loss: 1.2031687498092651\n",
      "Validation: Epoch [4], Batch [701/938], Loss: 1.147730827331543\n",
      "Validation: Epoch [4], Batch [702/938], Loss: 1.2844457626342773\n",
      "Validation: Epoch [4], Batch [703/938], Loss: 1.3724089860916138\n",
      "Validation: Epoch [4], Batch [704/938], Loss: 1.2729010581970215\n",
      "Validation: Epoch [4], Batch [705/938], Loss: 1.152614951133728\n",
      "Validation: Epoch [4], Batch [706/938], Loss: 1.1617931127548218\n",
      "Validation: Epoch [4], Batch [707/938], Loss: 1.3177845478057861\n",
      "Validation: Epoch [4], Batch [708/938], Loss: 1.342976450920105\n",
      "Validation: Epoch [4], Batch [709/938], Loss: 1.311611294746399\n",
      "Validation: Epoch [4], Batch [710/938], Loss: 1.1549046039581299\n",
      "Validation: Epoch [4], Batch [711/938], Loss: 1.3376805782318115\n",
      "Validation: Epoch [4], Batch [712/938], Loss: 1.379011631011963\n",
      "Validation: Epoch [4], Batch [713/938], Loss: 1.4364954233169556\n",
      "Validation: Epoch [4], Batch [714/938], Loss: 1.498473048210144\n",
      "Validation: Epoch [4], Batch [715/938], Loss: 1.1866588592529297\n",
      "Validation: Epoch [4], Batch [716/938], Loss: 1.3044867515563965\n",
      "Validation: Epoch [4], Batch [717/938], Loss: 1.2382798194885254\n",
      "Validation: Epoch [4], Batch [718/938], Loss: 1.3809599876403809\n",
      "Validation: Epoch [4], Batch [719/938], Loss: 1.305949330329895\n",
      "Validation: Epoch [4], Batch [720/938], Loss: 1.2293570041656494\n",
      "Validation: Epoch [4], Batch [721/938], Loss: 1.2928106784820557\n",
      "Validation: Epoch [4], Batch [722/938], Loss: 1.2694603204727173\n",
      "Validation: Epoch [4], Batch [723/938], Loss: 1.0716828107833862\n",
      "Validation: Epoch [4], Batch [724/938], Loss: 1.1129679679870605\n",
      "Validation: Epoch [4], Batch [725/938], Loss: 1.3959852457046509\n",
      "Validation: Epoch [4], Batch [726/938], Loss: 1.4182604551315308\n",
      "Validation: Epoch [4], Batch [727/938], Loss: 1.3442652225494385\n",
      "Validation: Epoch [4], Batch [728/938], Loss: 1.3578908443450928\n",
      "Validation: Epoch [4], Batch [729/938], Loss: 1.282431721687317\n",
      "Validation: Epoch [4], Batch [730/938], Loss: 1.2451740503311157\n",
      "Validation: Epoch [4], Batch [731/938], Loss: 1.222898244857788\n",
      "Validation: Epoch [4], Batch [732/938], Loss: 1.419667363166809\n",
      "Validation: Epoch [4], Batch [733/938], Loss: 1.172784686088562\n",
      "Validation: Epoch [4], Batch [734/938], Loss: 1.2780983448028564\n",
      "Validation: Epoch [4], Batch [735/938], Loss: 1.406317114830017\n",
      "Validation: Epoch [4], Batch [736/938], Loss: 1.345119833946228\n",
      "Validation: Epoch [4], Batch [737/938], Loss: 1.3152835369110107\n",
      "Validation: Epoch [4], Batch [738/938], Loss: 1.1293764114379883\n",
      "Validation: Epoch [4], Batch [739/938], Loss: 1.3281680345535278\n",
      "Validation: Epoch [4], Batch [740/938], Loss: 1.2119255065917969\n",
      "Validation: Epoch [4], Batch [741/938], Loss: 1.1469783782958984\n",
      "Validation: Epoch [4], Batch [742/938], Loss: 1.3807493448257446\n",
      "Validation: Epoch [4], Batch [743/938], Loss: 1.1665176153182983\n",
      "Validation: Epoch [4], Batch [744/938], Loss: 1.3678605556488037\n",
      "Validation: Epoch [4], Batch [745/938], Loss: 1.100656270980835\n",
      "Validation: Epoch [4], Batch [746/938], Loss: 1.2633697986602783\n",
      "Validation: Epoch [4], Batch [747/938], Loss: 1.2698321342468262\n",
      "Validation: Epoch [4], Batch [748/938], Loss: 1.29768705368042\n",
      "Validation: Epoch [4], Batch [749/938], Loss: 1.2216651439666748\n",
      "Validation: Epoch [4], Batch [750/938], Loss: 1.4221439361572266\n",
      "Validation: Epoch [4], Batch [751/938], Loss: 1.3880854845046997\n",
      "Validation: Epoch [4], Batch [752/938], Loss: 1.2831836938858032\n",
      "Validation: Epoch [4], Batch [753/938], Loss: 1.5286264419555664\n",
      "Validation: Epoch [4], Batch [754/938], Loss: 1.3888013362884521\n",
      "Validation: Epoch [4], Batch [755/938], Loss: 1.3074545860290527\n",
      "Validation: Epoch [4], Batch [756/938], Loss: 1.2578034400939941\n",
      "Validation: Epoch [4], Batch [757/938], Loss: 1.591191291809082\n",
      "Validation: Epoch [4], Batch [758/938], Loss: 1.3954508304595947\n",
      "Validation: Epoch [4], Batch [759/938], Loss: 1.3460651636123657\n",
      "Validation: Epoch [4], Batch [760/938], Loss: 1.254802942276001\n",
      "Validation: Epoch [4], Batch [761/938], Loss: 1.2746542692184448\n",
      "Validation: Epoch [4], Batch [762/938], Loss: 1.334761381149292\n",
      "Validation: Epoch [4], Batch [763/938], Loss: 1.5219151973724365\n",
      "Validation: Epoch [4], Batch [764/938], Loss: 1.432824730873108\n",
      "Validation: Epoch [4], Batch [765/938], Loss: 1.0574047565460205\n",
      "Validation: Epoch [4], Batch [766/938], Loss: 1.2973668575286865\n",
      "Validation: Epoch [4], Batch [767/938], Loss: 1.2172729969024658\n",
      "Validation: Epoch [4], Batch [768/938], Loss: 1.2480905055999756\n",
      "Validation: Epoch [4], Batch [769/938], Loss: 1.198926329612732\n",
      "Validation: Epoch [4], Batch [770/938], Loss: 1.4815784692764282\n",
      "Validation: Epoch [4], Batch [771/938], Loss: 1.3152501583099365\n",
      "Validation: Epoch [4], Batch [772/938], Loss: 1.0571593046188354\n",
      "Validation: Epoch [4], Batch [773/938], Loss: 1.288494348526001\n",
      "Validation: Epoch [4], Batch [774/938], Loss: 1.3920239210128784\n",
      "Validation: Epoch [4], Batch [775/938], Loss: 1.5210189819335938\n",
      "Validation: Epoch [4], Batch [776/938], Loss: 1.247467041015625\n",
      "Validation: Epoch [4], Batch [777/938], Loss: 1.136581540107727\n",
      "Validation: Epoch [4], Batch [778/938], Loss: 1.2904632091522217\n",
      "Validation: Epoch [4], Batch [779/938], Loss: 1.2000389099121094\n",
      "Validation: Epoch [4], Batch [780/938], Loss: 1.2307374477386475\n",
      "Validation: Epoch [4], Batch [781/938], Loss: 1.3861829042434692\n",
      "Validation: Epoch [4], Batch [782/938], Loss: 1.2291667461395264\n",
      "Validation: Epoch [4], Batch [783/938], Loss: 1.366692304611206\n",
      "Validation: Epoch [4], Batch [784/938], Loss: 1.3140125274658203\n",
      "Validation: Epoch [4], Batch [785/938], Loss: 1.402703046798706\n",
      "Validation: Epoch [4], Batch [786/938], Loss: 1.0750161409378052\n",
      "Validation: Epoch [4], Batch [787/938], Loss: 1.3712409734725952\n",
      "Validation: Epoch [4], Batch [788/938], Loss: 1.6995993852615356\n",
      "Validation: Epoch [4], Batch [789/938], Loss: 1.1668627262115479\n",
      "Validation: Epoch [4], Batch [790/938], Loss: 1.0586297512054443\n",
      "Validation: Epoch [4], Batch [791/938], Loss: 1.3236563205718994\n",
      "Validation: Epoch [4], Batch [792/938], Loss: 1.1999154090881348\n",
      "Validation: Epoch [4], Batch [793/938], Loss: 1.3668386936187744\n",
      "Validation: Epoch [4], Batch [794/938], Loss: 1.2877275943756104\n",
      "Validation: Epoch [4], Batch [795/938], Loss: 1.2036244869232178\n",
      "Validation: Epoch [4], Batch [796/938], Loss: 1.4715054035186768\n",
      "Validation: Epoch [4], Batch [797/938], Loss: 1.3318203687667847\n",
      "Validation: Epoch [4], Batch [798/938], Loss: 1.3514072895050049\n",
      "Validation: Epoch [4], Batch [799/938], Loss: 1.4140852689743042\n",
      "Validation: Epoch [4], Batch [800/938], Loss: 1.4583642482757568\n",
      "Validation: Epoch [4], Batch [801/938], Loss: 1.3225170373916626\n",
      "Validation: Epoch [4], Batch [802/938], Loss: 1.343526005744934\n",
      "Validation: Epoch [4], Batch [803/938], Loss: 1.55686354637146\n",
      "Validation: Epoch [4], Batch [804/938], Loss: 1.261296272277832\n",
      "Validation: Epoch [4], Batch [805/938], Loss: 1.3281867504119873\n",
      "Validation: Epoch [4], Batch [806/938], Loss: 1.2317657470703125\n",
      "Validation: Epoch [4], Batch [807/938], Loss: 1.2969956398010254\n",
      "Validation: Epoch [4], Batch [808/938], Loss: 1.4042515754699707\n",
      "Validation: Epoch [4], Batch [809/938], Loss: 1.4023079872131348\n",
      "Validation: Epoch [4], Batch [810/938], Loss: 1.1191937923431396\n",
      "Validation: Epoch [4], Batch [811/938], Loss: 1.4104280471801758\n",
      "Validation: Epoch [4], Batch [812/938], Loss: 1.2870643138885498\n",
      "Validation: Epoch [4], Batch [813/938], Loss: 1.1208257675170898\n",
      "Validation: Epoch [4], Batch [814/938], Loss: 1.4669806957244873\n",
      "Validation: Epoch [4], Batch [815/938], Loss: 1.1636152267456055\n",
      "Validation: Epoch [4], Batch [816/938], Loss: 1.1731469631195068\n",
      "Validation: Epoch [4], Batch [817/938], Loss: 1.452649474143982\n",
      "Validation: Epoch [4], Batch [818/938], Loss: 1.0341240167617798\n",
      "Validation: Epoch [4], Batch [819/938], Loss: 1.267026424407959\n",
      "Validation: Epoch [4], Batch [820/938], Loss: 1.19886314868927\n",
      "Validation: Epoch [4], Batch [821/938], Loss: 1.557579517364502\n",
      "Validation: Epoch [4], Batch [822/938], Loss: 1.4198240041732788\n",
      "Validation: Epoch [4], Batch [823/938], Loss: 1.2631207704544067\n",
      "Validation: Epoch [4], Batch [824/938], Loss: 1.249780535697937\n",
      "Validation: Epoch [4], Batch [825/938], Loss: 1.1465176343917847\n",
      "Validation: Epoch [4], Batch [826/938], Loss: 1.3866827487945557\n",
      "Validation: Epoch [4], Batch [827/938], Loss: 1.388422966003418\n",
      "Validation: Epoch [4], Batch [828/938], Loss: 1.344857931137085\n",
      "Validation: Epoch [4], Batch [829/938], Loss: 1.1388471126556396\n",
      "Validation: Epoch [4], Batch [830/938], Loss: 1.2149345874786377\n",
      "Validation: Epoch [4], Batch [831/938], Loss: 1.137974739074707\n",
      "Validation: Epoch [4], Batch [832/938], Loss: 1.2883213758468628\n",
      "Validation: Epoch [4], Batch [833/938], Loss: 1.1869274377822876\n",
      "Validation: Epoch [4], Batch [834/938], Loss: 1.2074904441833496\n",
      "Validation: Epoch [4], Batch [835/938], Loss: 1.1141570806503296\n",
      "Validation: Epoch [4], Batch [836/938], Loss: 1.1165306568145752\n",
      "Validation: Epoch [4], Batch [837/938], Loss: 1.2890901565551758\n",
      "Validation: Epoch [4], Batch [838/938], Loss: 1.148924708366394\n",
      "Validation: Epoch [4], Batch [839/938], Loss: 1.3124957084655762\n",
      "Validation: Epoch [4], Batch [840/938], Loss: 1.2546526193618774\n",
      "Validation: Epoch [4], Batch [841/938], Loss: 1.1537971496582031\n",
      "Validation: Epoch [4], Batch [842/938], Loss: 1.1671984195709229\n",
      "Validation: Epoch [4], Batch [843/938], Loss: 1.5131707191467285\n",
      "Validation: Epoch [4], Batch [844/938], Loss: 1.3052724599838257\n",
      "Validation: Epoch [4], Batch [845/938], Loss: 1.187878131866455\n",
      "Validation: Epoch [4], Batch [846/938], Loss: 1.2929471731185913\n",
      "Validation: Epoch [4], Batch [847/938], Loss: 1.4395546913146973\n",
      "Validation: Epoch [4], Batch [848/938], Loss: 1.305403709411621\n",
      "Validation: Epoch [4], Batch [849/938], Loss: 1.393994688987732\n",
      "Validation: Epoch [4], Batch [850/938], Loss: 1.3267775774002075\n",
      "Validation: Epoch [4], Batch [851/938], Loss: 1.2364736795425415\n",
      "Validation: Epoch [4], Batch [852/938], Loss: 1.2650434970855713\n",
      "Validation: Epoch [4], Batch [853/938], Loss: 1.276136040687561\n",
      "Validation: Epoch [4], Batch [854/938], Loss: 1.2670230865478516\n",
      "Validation: Epoch [4], Batch [855/938], Loss: 1.2952187061309814\n",
      "Validation: Epoch [4], Batch [856/938], Loss: 1.1803092956542969\n",
      "Validation: Epoch [4], Batch [857/938], Loss: 1.3721551895141602\n",
      "Validation: Epoch [4], Batch [858/938], Loss: 1.2218890190124512\n",
      "Validation: Epoch [4], Batch [859/938], Loss: 1.3270114660263062\n",
      "Validation: Epoch [4], Batch [860/938], Loss: 1.3361765146255493\n",
      "Validation: Epoch [4], Batch [861/938], Loss: 1.2980726957321167\n",
      "Validation: Epoch [4], Batch [862/938], Loss: 1.244776725769043\n",
      "Validation: Epoch [4], Batch [863/938], Loss: 1.437111258506775\n",
      "Validation: Epoch [4], Batch [864/938], Loss: 1.3771253824234009\n",
      "Validation: Epoch [4], Batch [865/938], Loss: 1.3500486612319946\n",
      "Validation: Epoch [4], Batch [866/938], Loss: 1.3073678016662598\n",
      "Validation: Epoch [4], Batch [867/938], Loss: 1.4001872539520264\n",
      "Validation: Epoch [4], Batch [868/938], Loss: 1.2219982147216797\n",
      "Validation: Epoch [4], Batch [869/938], Loss: 1.5030694007873535\n",
      "Validation: Epoch [4], Batch [870/938], Loss: 1.249385952949524\n",
      "Validation: Epoch [4], Batch [871/938], Loss: 1.392519235610962\n",
      "Validation: Epoch [4], Batch [872/938], Loss: 1.0625710487365723\n",
      "Validation: Epoch [4], Batch [873/938], Loss: 1.0885252952575684\n",
      "Validation: Epoch [4], Batch [874/938], Loss: 1.583123803138733\n",
      "Validation: Epoch [4], Batch [875/938], Loss: 1.4971801042556763\n",
      "Validation: Epoch [4], Batch [876/938], Loss: 1.2991113662719727\n",
      "Validation: Epoch [4], Batch [877/938], Loss: 1.2826380729675293\n",
      "Validation: Epoch [4], Batch [878/938], Loss: 1.2285901308059692\n",
      "Validation: Epoch [4], Batch [879/938], Loss: 1.2448734045028687\n",
      "Validation: Epoch [4], Batch [880/938], Loss: 1.3282302618026733\n",
      "Validation: Epoch [4], Batch [881/938], Loss: 1.322282314300537\n",
      "Validation: Epoch [4], Batch [882/938], Loss: 1.3158601522445679\n",
      "Validation: Epoch [4], Batch [883/938], Loss: 1.182235836982727\n",
      "Validation: Epoch [4], Batch [884/938], Loss: 1.4347074031829834\n",
      "Validation: Epoch [4], Batch [885/938], Loss: 1.2830227613449097\n",
      "Validation: Epoch [4], Batch [886/938], Loss: 1.0971852540969849\n",
      "Validation: Epoch [4], Batch [887/938], Loss: 1.1611709594726562\n",
      "Validation: Epoch [4], Batch [888/938], Loss: 1.2227555513381958\n",
      "Validation: Epoch [4], Batch [889/938], Loss: 1.079080581665039\n",
      "Validation: Epoch [4], Batch [890/938], Loss: 1.1238726377487183\n",
      "Validation: Epoch [4], Batch [891/938], Loss: 1.3419032096862793\n",
      "Validation: Epoch [4], Batch [892/938], Loss: 1.197651743888855\n",
      "Validation: Epoch [4], Batch [893/938], Loss: 1.2677093744277954\n",
      "Validation: Epoch [4], Batch [894/938], Loss: 1.012943983078003\n",
      "Validation: Epoch [4], Batch [895/938], Loss: 1.158423900604248\n",
      "Validation: Epoch [4], Batch [896/938], Loss: 1.1362966299057007\n",
      "Validation: Epoch [4], Batch [897/938], Loss: 1.3639881610870361\n",
      "Validation: Epoch [4], Batch [898/938], Loss: 1.3797073364257812\n",
      "Validation: Epoch [4], Batch [899/938], Loss: 1.165260672569275\n",
      "Validation: Epoch [4], Batch [900/938], Loss: 1.3348619937896729\n",
      "Validation: Epoch [4], Batch [901/938], Loss: 1.3603371381759644\n",
      "Validation: Epoch [4], Batch [902/938], Loss: 1.21156907081604\n",
      "Validation: Epoch [4], Batch [903/938], Loss: 1.171667456626892\n",
      "Validation: Epoch [4], Batch [904/938], Loss: 1.3894487619400024\n",
      "Validation: Epoch [4], Batch [905/938], Loss: 1.3143473863601685\n",
      "Validation: Epoch [4], Batch [906/938], Loss: 1.282547950744629\n",
      "Validation: Epoch [4], Batch [907/938], Loss: 1.322422742843628\n",
      "Validation: Epoch [4], Batch [908/938], Loss: 1.2686529159545898\n",
      "Validation: Epoch [4], Batch [909/938], Loss: 1.4404443502426147\n",
      "Validation: Epoch [4], Batch [910/938], Loss: 1.327756643295288\n",
      "Validation: Epoch [4], Batch [911/938], Loss: 1.2338038682937622\n",
      "Validation: Epoch [4], Batch [912/938], Loss: 1.4286631345748901\n",
      "Validation: Epoch [4], Batch [913/938], Loss: 1.2866554260253906\n",
      "Validation: Epoch [4], Batch [914/938], Loss: 1.3135526180267334\n",
      "Validation: Epoch [4], Batch [915/938], Loss: 1.2055590152740479\n",
      "Validation: Epoch [4], Batch [916/938], Loss: 1.274014949798584\n",
      "Validation: Epoch [4], Batch [917/938], Loss: 1.3695262670516968\n",
      "Validation: Epoch [4], Batch [918/938], Loss: 1.3877304792404175\n",
      "Validation: Epoch [4], Batch [919/938], Loss: 1.206108808517456\n",
      "Validation: Epoch [4], Batch [920/938], Loss: 1.341912031173706\n",
      "Validation: Epoch [4], Batch [921/938], Loss: 1.3200924396514893\n",
      "Validation: Epoch [4], Batch [922/938], Loss: 1.1973193883895874\n",
      "Validation: Epoch [4], Batch [923/938], Loss: 1.2896100282669067\n",
      "Validation: Epoch [4], Batch [924/938], Loss: 1.3218836784362793\n",
      "Validation: Epoch [4], Batch [925/938], Loss: 1.1112525463104248\n",
      "Validation: Epoch [4], Batch [926/938], Loss: 1.3278220891952515\n",
      "Validation: Epoch [4], Batch [927/938], Loss: 1.3601850271224976\n",
      "Validation: Epoch [4], Batch [928/938], Loss: 1.443528413772583\n",
      "Validation: Epoch [4], Batch [929/938], Loss: 1.2149419784545898\n",
      "Validation: Epoch [4], Batch [930/938], Loss: 1.405542016029358\n",
      "Validation: Epoch [4], Batch [931/938], Loss: 1.3051166534423828\n",
      "Validation: Epoch [4], Batch [932/938], Loss: 1.2945281267166138\n",
      "Validation: Epoch [4], Batch [933/938], Loss: 1.2618143558502197\n",
      "Validation: Epoch [4], Batch [934/938], Loss: 1.405794620513916\n",
      "Validation: Epoch [4], Batch [935/938], Loss: 1.2315285205841064\n",
      "Validation: Epoch [4], Batch [936/938], Loss: 1.562595248222351\n",
      "Validation: Epoch [4], Batch [937/938], Loss: 1.2435317039489746\n",
      "Validation: Epoch [4], Batch [938/938], Loss: 1.323492407798767\n",
      "Accuracy of test set: 0.5596333333333333\n",
      "Train: Epoch [5], Batch [1/938], Loss: 1.3125673532485962\n",
      "Train: Epoch [5], Batch [2/938], Loss: 1.4634764194488525\n",
      "Train: Epoch [5], Batch [3/938], Loss: 1.431984543800354\n",
      "Train: Epoch [5], Batch [4/938], Loss: 1.281383991241455\n",
      "Train: Epoch [5], Batch [5/938], Loss: 1.0879931449890137\n",
      "Train: Epoch [5], Batch [6/938], Loss: 1.3436697721481323\n",
      "Train: Epoch [5], Batch [7/938], Loss: 1.2540820837020874\n",
      "Train: Epoch [5], Batch [8/938], Loss: 1.3841195106506348\n",
      "Train: Epoch [5], Batch [9/938], Loss: 1.3136587142944336\n",
      "Train: Epoch [5], Batch [10/938], Loss: 1.2642873525619507\n",
      "Train: Epoch [5], Batch [11/938], Loss: 1.2517859935760498\n",
      "Train: Epoch [5], Batch [12/938], Loss: 1.1235592365264893\n",
      "Train: Epoch [5], Batch [13/938], Loss: 1.2914620637893677\n",
      "Train: Epoch [5], Batch [14/938], Loss: 1.198991060256958\n",
      "Train: Epoch [5], Batch [15/938], Loss: 1.2195523977279663\n",
      "Train: Epoch [5], Batch [16/938], Loss: 1.050380825996399\n",
      "Train: Epoch [5], Batch [17/938], Loss: 1.4826829433441162\n",
      "Train: Epoch [5], Batch [18/938], Loss: 1.2652478218078613\n",
      "Train: Epoch [5], Batch [19/938], Loss: 1.3746274709701538\n",
      "Train: Epoch [5], Batch [20/938], Loss: 1.2579591274261475\n",
      "Train: Epoch [5], Batch [21/938], Loss: 1.28291654586792\n",
      "Train: Epoch [5], Batch [22/938], Loss: 1.5472567081451416\n",
      "Train: Epoch [5], Batch [23/938], Loss: 1.513201355934143\n",
      "Train: Epoch [5], Batch [24/938], Loss: 1.1612826585769653\n",
      "Train: Epoch [5], Batch [25/938], Loss: 1.4210683107376099\n",
      "Train: Epoch [5], Batch [26/938], Loss: 1.4778177738189697\n",
      "Train: Epoch [5], Batch [27/938], Loss: 1.3360722064971924\n",
      "Train: Epoch [5], Batch [28/938], Loss: 1.2082328796386719\n",
      "Train: Epoch [5], Batch [29/938], Loss: 1.4414912462234497\n",
      "Train: Epoch [5], Batch [30/938], Loss: 1.1553900241851807\n",
      "Train: Epoch [5], Batch [31/938], Loss: 1.1765117645263672\n",
      "Train: Epoch [5], Batch [32/938], Loss: 1.3630656003952026\n",
      "Train: Epoch [5], Batch [33/938], Loss: 1.2301908731460571\n",
      "Train: Epoch [5], Batch [34/938], Loss: 1.3928009271621704\n",
      "Train: Epoch [5], Batch [35/938], Loss: 1.256058692932129\n",
      "Train: Epoch [5], Batch [36/938], Loss: 1.0038418769836426\n",
      "Train: Epoch [5], Batch [37/938], Loss: 1.5563068389892578\n",
      "Train: Epoch [5], Batch [38/938], Loss: 1.164944052696228\n",
      "Train: Epoch [5], Batch [39/938], Loss: 1.446732759475708\n",
      "Train: Epoch [5], Batch [40/938], Loss: 1.3574798107147217\n",
      "Train: Epoch [5], Batch [41/938], Loss: 1.452823519706726\n",
      "Train: Epoch [5], Batch [42/938], Loss: 1.106648325920105\n",
      "Train: Epoch [5], Batch [43/938], Loss: 1.2869760990142822\n",
      "Train: Epoch [5], Batch [44/938], Loss: 1.262765884399414\n",
      "Train: Epoch [5], Batch [45/938], Loss: 1.12061607837677\n",
      "Train: Epoch [5], Batch [46/938], Loss: 1.4135301113128662\n",
      "Train: Epoch [5], Batch [47/938], Loss: 1.1189086437225342\n",
      "Train: Epoch [5], Batch [48/938], Loss: 1.3173511028289795\n",
      "Train: Epoch [5], Batch [49/938], Loss: 1.3042892217636108\n",
      "Train: Epoch [5], Batch [50/938], Loss: 1.304343819618225\n",
      "Train: Epoch [5], Batch [51/938], Loss: 1.477848768234253\n",
      "Train: Epoch [5], Batch [52/938], Loss: 1.098554253578186\n",
      "Train: Epoch [5], Batch [53/938], Loss: 1.2665693759918213\n",
      "Train: Epoch [5], Batch [54/938], Loss: 1.1724590063095093\n",
      "Train: Epoch [5], Batch [55/938], Loss: 1.5141135454177856\n",
      "Train: Epoch [5], Batch [56/938], Loss: 1.2926875352859497\n",
      "Train: Epoch [5], Batch [57/938], Loss: 1.2283403873443604\n",
      "Train: Epoch [5], Batch [58/938], Loss: 1.160541296005249\n",
      "Train: Epoch [5], Batch [59/938], Loss: 1.3103275299072266\n",
      "Train: Epoch [5], Batch [60/938], Loss: 1.1168795824050903\n",
      "Train: Epoch [5], Batch [61/938], Loss: 1.3077386617660522\n",
      "Train: Epoch [5], Batch [62/938], Loss: 1.3835082054138184\n",
      "Train: Epoch [5], Batch [63/938], Loss: 1.5310180187225342\n",
      "Train: Epoch [5], Batch [64/938], Loss: 1.099224328994751\n",
      "Train: Epoch [5], Batch [65/938], Loss: 1.4410488605499268\n",
      "Train: Epoch [5], Batch [66/938], Loss: 1.3412750959396362\n",
      "Train: Epoch [5], Batch [67/938], Loss: 1.2171885967254639\n",
      "Train: Epoch [5], Batch [68/938], Loss: 1.0243737697601318\n",
      "Train: Epoch [5], Batch [69/938], Loss: 1.3820185661315918\n",
      "Train: Epoch [5], Batch [70/938], Loss: 1.238986611366272\n",
      "Train: Epoch [5], Batch [71/938], Loss: 1.4496829509735107\n",
      "Train: Epoch [5], Batch [72/938], Loss: 1.363525390625\n",
      "Train: Epoch [5], Batch [73/938], Loss: 1.265085220336914\n",
      "Train: Epoch [5], Batch [74/938], Loss: 1.292738437652588\n",
      "Train: Epoch [5], Batch [75/938], Loss: 1.2211545705795288\n",
      "Train: Epoch [5], Batch [76/938], Loss: 1.1423659324645996\n",
      "Train: Epoch [5], Batch [77/938], Loss: 1.0529391765594482\n",
      "Train: Epoch [5], Batch [78/938], Loss: 1.21278715133667\n",
      "Train: Epoch [5], Batch [79/938], Loss: 1.53944730758667\n",
      "Train: Epoch [5], Batch [80/938], Loss: 1.3676420450210571\n",
      "Train: Epoch [5], Batch [81/938], Loss: 1.07998526096344\n",
      "Train: Epoch [5], Batch [82/938], Loss: 1.1452749967575073\n",
      "Train: Epoch [5], Batch [83/938], Loss: 1.2437946796417236\n",
      "Train: Epoch [5], Batch [84/938], Loss: 1.4650264978408813\n",
      "Train: Epoch [5], Batch [85/938], Loss: 1.2744213342666626\n",
      "Train: Epoch [5], Batch [86/938], Loss: 1.4804246425628662\n",
      "Train: Epoch [5], Batch [87/938], Loss: 1.1844056844711304\n",
      "Train: Epoch [5], Batch [88/938], Loss: 1.3884036540985107\n",
      "Train: Epoch [5], Batch [89/938], Loss: 1.0990649461746216\n",
      "Train: Epoch [5], Batch [90/938], Loss: 1.1173162460327148\n",
      "Train: Epoch [5], Batch [91/938], Loss: 1.3475719690322876\n",
      "Train: Epoch [5], Batch [92/938], Loss: 1.155135154724121\n",
      "Train: Epoch [5], Batch [93/938], Loss: 1.1512165069580078\n",
      "Train: Epoch [5], Batch [94/938], Loss: 1.220363736152649\n",
      "Train: Epoch [5], Batch [95/938], Loss: 1.4546949863433838\n",
      "Train: Epoch [5], Batch [96/938], Loss: 1.2265737056732178\n",
      "Train: Epoch [5], Batch [97/938], Loss: 1.2434847354888916\n",
      "Train: Epoch [5], Batch [98/938], Loss: 1.2378989458084106\n",
      "Train: Epoch [5], Batch [99/938], Loss: 1.1295380592346191\n",
      "Train: Epoch [5], Batch [100/938], Loss: 1.3247401714324951\n",
      "Train: Epoch [5], Batch [101/938], Loss: 1.2983760833740234\n",
      "Train: Epoch [5], Batch [102/938], Loss: 1.0583058595657349\n",
      "Train: Epoch [5], Batch [103/938], Loss: 1.4783742427825928\n",
      "Train: Epoch [5], Batch [104/938], Loss: 1.2416963577270508\n",
      "Train: Epoch [5], Batch [105/938], Loss: 1.1743628978729248\n",
      "Train: Epoch [5], Batch [106/938], Loss: 0.9475462436676025\n",
      "Train: Epoch [5], Batch [107/938], Loss: 1.3076047897338867\n",
      "Train: Epoch [5], Batch [108/938], Loss: 1.1529033184051514\n",
      "Train: Epoch [5], Batch [109/938], Loss: 1.015061378479004\n",
      "Train: Epoch [5], Batch [110/938], Loss: 1.1995911598205566\n",
      "Train: Epoch [5], Batch [111/938], Loss: 1.295853614807129\n",
      "Train: Epoch [5], Batch [112/938], Loss: 1.2372945547103882\n",
      "Train: Epoch [5], Batch [113/938], Loss: 1.0531625747680664\n",
      "Train: Epoch [5], Batch [114/938], Loss: 1.3457484245300293\n",
      "Train: Epoch [5], Batch [115/938], Loss: 1.471435785293579\n",
      "Train: Epoch [5], Batch [116/938], Loss: 1.202528953552246\n",
      "Train: Epoch [5], Batch [117/938], Loss: 1.2375272512435913\n",
      "Train: Epoch [5], Batch [118/938], Loss: 1.2532137632369995\n",
      "Train: Epoch [5], Batch [119/938], Loss: 1.1737788915634155\n",
      "Train: Epoch [5], Batch [120/938], Loss: 1.0353541374206543\n",
      "Train: Epoch [5], Batch [121/938], Loss: 1.2477912902832031\n",
      "Train: Epoch [5], Batch [122/938], Loss: 1.2187408208847046\n",
      "Train: Epoch [5], Batch [123/938], Loss: 1.4070848226547241\n",
      "Train: Epoch [5], Batch [124/938], Loss: 1.233389973640442\n",
      "Train: Epoch [5], Batch [125/938], Loss: 1.288455605506897\n",
      "Train: Epoch [5], Batch [126/938], Loss: 1.0438600778579712\n",
      "Train: Epoch [5], Batch [127/938], Loss: 1.1321039199829102\n",
      "Train: Epoch [5], Batch [128/938], Loss: 1.2253437042236328\n",
      "Train: Epoch [5], Batch [129/938], Loss: 1.1832072734832764\n",
      "Train: Epoch [5], Batch [130/938], Loss: 1.1770689487457275\n",
      "Train: Epoch [5], Batch [131/938], Loss: 1.2299115657806396\n",
      "Train: Epoch [5], Batch [132/938], Loss: 1.2187455892562866\n",
      "Train: Epoch [5], Batch [133/938], Loss: 1.02923583984375\n",
      "Train: Epoch [5], Batch [134/938], Loss: 1.1940340995788574\n",
      "Train: Epoch [5], Batch [135/938], Loss: 1.2663631439208984\n",
      "Train: Epoch [5], Batch [136/938], Loss: 1.1894457340240479\n",
      "Train: Epoch [5], Batch [137/938], Loss: 1.2124313116073608\n",
      "Train: Epoch [5], Batch [138/938], Loss: 1.1217600107192993\n",
      "Train: Epoch [5], Batch [139/938], Loss: 1.1979988813400269\n",
      "Train: Epoch [5], Batch [140/938], Loss: 1.2758060693740845\n",
      "Train: Epoch [5], Batch [141/938], Loss: 1.2132903337478638\n",
      "Train: Epoch [5], Batch [142/938], Loss: 1.3870937824249268\n",
      "Train: Epoch [5], Batch [143/938], Loss: 1.0973340272903442\n",
      "Train: Epoch [5], Batch [144/938], Loss: 1.3635921478271484\n",
      "Train: Epoch [5], Batch [145/938], Loss: 1.2866382598876953\n",
      "Train: Epoch [5], Batch [146/938], Loss: 1.021566390991211\n",
      "Train: Epoch [5], Batch [147/938], Loss: 1.137002944946289\n",
      "Train: Epoch [5], Batch [148/938], Loss: 1.3240374326705933\n",
      "Train: Epoch [5], Batch [149/938], Loss: 1.358185052871704\n",
      "Train: Epoch [5], Batch [150/938], Loss: 1.2589718103408813\n",
      "Train: Epoch [5], Batch [151/938], Loss: 1.2142888307571411\n",
      "Train: Epoch [5], Batch [152/938], Loss: 1.117343783378601\n",
      "Train: Epoch [5], Batch [153/938], Loss: 0.9992662668228149\n",
      "Train: Epoch [5], Batch [154/938], Loss: 1.1087441444396973\n",
      "Train: Epoch [5], Batch [155/938], Loss: 1.1636533737182617\n",
      "Train: Epoch [5], Batch [156/938], Loss: 1.1198124885559082\n",
      "Train: Epoch [5], Batch [157/938], Loss: 1.1869038343429565\n",
      "Train: Epoch [5], Batch [158/938], Loss: 1.0600855350494385\n",
      "Train: Epoch [5], Batch [159/938], Loss: 1.222778081893921\n",
      "Train: Epoch [5], Batch [160/938], Loss: 1.1086900234222412\n",
      "Train: Epoch [5], Batch [161/938], Loss: 1.1156930923461914\n",
      "Train: Epoch [5], Batch [162/938], Loss: 1.1800750494003296\n",
      "Train: Epoch [5], Batch [163/938], Loss: 1.1056954860687256\n",
      "Train: Epoch [5], Batch [164/938], Loss: 1.187257170677185\n",
      "Train: Epoch [5], Batch [165/938], Loss: 1.1523094177246094\n",
      "Train: Epoch [5], Batch [166/938], Loss: 1.1715906858444214\n",
      "Train: Epoch [5], Batch [167/938], Loss: 1.1042934656143188\n",
      "Train: Epoch [5], Batch [168/938], Loss: 0.9909393787384033\n",
      "Train: Epoch [5], Batch [169/938], Loss: 1.3482860326766968\n",
      "Train: Epoch [5], Batch [170/938], Loss: 1.216169834136963\n",
      "Train: Epoch [5], Batch [171/938], Loss: 1.0192254781723022\n",
      "Train: Epoch [5], Batch [172/938], Loss: 1.1530803442001343\n",
      "Train: Epoch [5], Batch [173/938], Loss: 1.2415828704833984\n",
      "Train: Epoch [5], Batch [174/938], Loss: 1.1634540557861328\n",
      "Train: Epoch [5], Batch [175/938], Loss: 1.2245783805847168\n",
      "Train: Epoch [5], Batch [176/938], Loss: 1.042216420173645\n",
      "Train: Epoch [5], Batch [177/938], Loss: 1.2104326486587524\n",
      "Train: Epoch [5], Batch [178/938], Loss: 1.110230565071106\n",
      "Train: Epoch [5], Batch [179/938], Loss: 1.3651368618011475\n",
      "Train: Epoch [5], Batch [180/938], Loss: 1.2002933025360107\n",
      "Train: Epoch [5], Batch [181/938], Loss: 1.125203013420105\n",
      "Train: Epoch [5], Batch [182/938], Loss: 1.0545710325241089\n",
      "Train: Epoch [5], Batch [183/938], Loss: 1.0577750205993652\n",
      "Train: Epoch [5], Batch [184/938], Loss: 1.1419771909713745\n",
      "Train: Epoch [5], Batch [185/938], Loss: 1.143883466720581\n",
      "Train: Epoch [5], Batch [186/938], Loss: 1.2444044351577759\n",
      "Train: Epoch [5], Batch [187/938], Loss: 1.1142783164978027\n",
      "Train: Epoch [5], Batch [188/938], Loss: 1.2157869338989258\n",
      "Train: Epoch [5], Batch [189/938], Loss: 1.2341798543930054\n",
      "Train: Epoch [5], Batch [190/938], Loss: 1.3332889080047607\n",
      "Train: Epoch [5], Batch [191/938], Loss: 1.2131390571594238\n",
      "Train: Epoch [5], Batch [192/938], Loss: 1.0115588903427124\n",
      "Train: Epoch [5], Batch [193/938], Loss: 1.292944312095642\n",
      "Train: Epoch [5], Batch [194/938], Loss: 1.11650812625885\n",
      "Train: Epoch [5], Batch [195/938], Loss: 1.1815764904022217\n",
      "Train: Epoch [5], Batch [196/938], Loss: 1.0550963878631592\n",
      "Train: Epoch [5], Batch [197/938], Loss: 0.9318668842315674\n",
      "Train: Epoch [5], Batch [198/938], Loss: 0.9433698654174805\n",
      "Train: Epoch [5], Batch [199/938], Loss: 1.4483591318130493\n",
      "Train: Epoch [5], Batch [200/938], Loss: 1.2047803401947021\n",
      "Train: Epoch [5], Batch [201/938], Loss: 1.1802165508270264\n",
      "Train: Epoch [5], Batch [202/938], Loss: 1.2495816946029663\n",
      "Train: Epoch [5], Batch [203/938], Loss: 1.005864143371582\n",
      "Train: Epoch [5], Batch [204/938], Loss: 1.4891208410263062\n",
      "Train: Epoch [5], Batch [205/938], Loss: 1.2169690132141113\n",
      "Train: Epoch [5], Batch [206/938], Loss: 1.0546685457229614\n",
      "Train: Epoch [5], Batch [207/938], Loss: 1.2059955596923828\n",
      "Train: Epoch [5], Batch [208/938], Loss: 1.1319667100906372\n",
      "Train: Epoch [5], Batch [209/938], Loss: 1.2293013334274292\n",
      "Train: Epoch [5], Batch [210/938], Loss: 1.1683300733566284\n",
      "Train: Epoch [5], Batch [211/938], Loss: 1.1836029291152954\n",
      "Train: Epoch [5], Batch [212/938], Loss: 1.1980379819869995\n",
      "Train: Epoch [5], Batch [213/938], Loss: 1.0079423189163208\n",
      "Train: Epoch [5], Batch [214/938], Loss: 1.1292649507522583\n",
      "Train: Epoch [5], Batch [215/938], Loss: 1.2805297374725342\n",
      "Train: Epoch [5], Batch [216/938], Loss: 1.10989511013031\n",
      "Train: Epoch [5], Batch [217/938], Loss: 1.2242249250411987\n",
      "Train: Epoch [5], Batch [218/938], Loss: 1.419402837753296\n",
      "Train: Epoch [5], Batch [219/938], Loss: 1.2328526973724365\n",
      "Train: Epoch [5], Batch [220/938], Loss: 1.150215983390808\n",
      "Train: Epoch [5], Batch [221/938], Loss: 1.2083033323287964\n",
      "Train: Epoch [5], Batch [222/938], Loss: 1.3302901983261108\n",
      "Train: Epoch [5], Batch [223/938], Loss: 1.1476175785064697\n",
      "Train: Epoch [5], Batch [224/938], Loss: 1.2828556299209595\n",
      "Train: Epoch [5], Batch [225/938], Loss: 1.4077448844909668\n",
      "Train: Epoch [5], Batch [226/938], Loss: 1.1081042289733887\n",
      "Train: Epoch [5], Batch [227/938], Loss: 1.0940219163894653\n",
      "Train: Epoch [5], Batch [228/938], Loss: 1.1271140575408936\n",
      "Train: Epoch [5], Batch [229/938], Loss: 1.0391581058502197\n",
      "Train: Epoch [5], Batch [230/938], Loss: 1.1326267719268799\n",
      "Train: Epoch [5], Batch [231/938], Loss: 1.1347675323486328\n",
      "Train: Epoch [5], Batch [232/938], Loss: 1.1486313343048096\n",
      "Train: Epoch [5], Batch [233/938], Loss: 1.3339109420776367\n",
      "Train: Epoch [5], Batch [234/938], Loss: 0.9818558096885681\n",
      "Train: Epoch [5], Batch [235/938], Loss: 1.0834141969680786\n",
      "Train: Epoch [5], Batch [236/938], Loss: 1.0062024593353271\n",
      "Train: Epoch [5], Batch [237/938], Loss: 1.0730788707733154\n",
      "Train: Epoch [5], Batch [238/938], Loss: 1.4432002305984497\n",
      "Train: Epoch [5], Batch [239/938], Loss: 0.9449425935745239\n",
      "Train: Epoch [5], Batch [240/938], Loss: 1.0103328227996826\n",
      "Train: Epoch [5], Batch [241/938], Loss: 1.168267011642456\n",
      "Train: Epoch [5], Batch [242/938], Loss: 1.2570301294326782\n",
      "Train: Epoch [5], Batch [243/938], Loss: 1.1745383739471436\n",
      "Train: Epoch [5], Batch [244/938], Loss: 1.374910593032837\n",
      "Train: Epoch [5], Batch [245/938], Loss: 1.2561973333358765\n",
      "Train: Epoch [5], Batch [246/938], Loss: 1.304500699043274\n",
      "Train: Epoch [5], Batch [247/938], Loss: 1.092779517173767\n",
      "Train: Epoch [5], Batch [248/938], Loss: 1.0843322277069092\n",
      "Train: Epoch [5], Batch [249/938], Loss: 1.1459286212921143\n",
      "Train: Epoch [5], Batch [250/938], Loss: 1.2447489500045776\n",
      "Train: Epoch [5], Batch [251/938], Loss: 1.1410343647003174\n",
      "Train: Epoch [5], Batch [252/938], Loss: 1.1293854713439941\n",
      "Train: Epoch [5], Batch [253/938], Loss: 1.1853456497192383\n",
      "Train: Epoch [5], Batch [254/938], Loss: 1.2374417781829834\n",
      "Train: Epoch [5], Batch [255/938], Loss: 1.12099289894104\n",
      "Train: Epoch [5], Batch [256/938], Loss: 0.9688477516174316\n",
      "Train: Epoch [5], Batch [257/938], Loss: 0.9778456091880798\n",
      "Train: Epoch [5], Batch [258/938], Loss: 1.1046687364578247\n",
      "Train: Epoch [5], Batch [259/938], Loss: 1.1359508037567139\n",
      "Train: Epoch [5], Batch [260/938], Loss: 1.1330623626708984\n",
      "Train: Epoch [5], Batch [261/938], Loss: 1.0878592729568481\n",
      "Train: Epoch [5], Batch [262/938], Loss: 1.1933256387710571\n",
      "Train: Epoch [5], Batch [263/938], Loss: 1.1330870389938354\n",
      "Train: Epoch [5], Batch [264/938], Loss: 1.175531268119812\n",
      "Train: Epoch [5], Batch [265/938], Loss: 1.101380467414856\n",
      "Train: Epoch [5], Batch [266/938], Loss: 1.07780921459198\n",
      "Train: Epoch [5], Batch [267/938], Loss: 1.0972234010696411\n",
      "Train: Epoch [5], Batch [268/938], Loss: 1.1145306825637817\n",
      "Train: Epoch [5], Batch [269/938], Loss: 1.2287214994430542\n",
      "Train: Epoch [5], Batch [270/938], Loss: 1.2667646408081055\n",
      "Train: Epoch [5], Batch [271/938], Loss: 1.0133028030395508\n",
      "Train: Epoch [5], Batch [272/938], Loss: 1.2472586631774902\n",
      "Train: Epoch [5], Batch [273/938], Loss: 1.1685419082641602\n",
      "Train: Epoch [5], Batch [274/938], Loss: 1.072787880897522\n",
      "Train: Epoch [5], Batch [275/938], Loss: 0.9980296492576599\n",
      "Train: Epoch [5], Batch [276/938], Loss: 1.205981969833374\n",
      "Train: Epoch [5], Batch [277/938], Loss: 1.031865119934082\n",
      "Train: Epoch [5], Batch [278/938], Loss: 1.1623568534851074\n",
      "Train: Epoch [5], Batch [279/938], Loss: 1.282924771308899\n",
      "Train: Epoch [5], Batch [280/938], Loss: 1.1312310695648193\n",
      "Train: Epoch [5], Batch [281/938], Loss: 1.2794196605682373\n",
      "Train: Epoch [5], Batch [282/938], Loss: 1.1673320531845093\n",
      "Train: Epoch [5], Batch [283/938], Loss: 1.1971385478973389\n",
      "Train: Epoch [5], Batch [284/938], Loss: 1.0610978603363037\n",
      "Train: Epoch [5], Batch [285/938], Loss: 1.0376907587051392\n",
      "Train: Epoch [5], Batch [286/938], Loss: 1.2465085983276367\n",
      "Train: Epoch [5], Batch [287/938], Loss: 1.0688238143920898\n",
      "Train: Epoch [5], Batch [288/938], Loss: 1.1864383220672607\n",
      "Train: Epoch [5], Batch [289/938], Loss: 1.122086763381958\n",
      "Train: Epoch [5], Batch [290/938], Loss: 1.206375002861023\n",
      "Train: Epoch [5], Batch [291/938], Loss: 0.952662467956543\n",
      "Train: Epoch [5], Batch [292/938], Loss: 1.1076304912567139\n",
      "Train: Epoch [5], Batch [293/938], Loss: 1.1586581468582153\n",
      "Train: Epoch [5], Batch [294/938], Loss: 1.2248303890228271\n",
      "Train: Epoch [5], Batch [295/938], Loss: 1.0810985565185547\n",
      "Train: Epoch [5], Batch [296/938], Loss: 1.0980783700942993\n",
      "Train: Epoch [5], Batch [297/938], Loss: 0.9982846975326538\n",
      "Train: Epoch [5], Batch [298/938], Loss: 1.0218554735183716\n",
      "Train: Epoch [5], Batch [299/938], Loss: 0.8455347418785095\n",
      "Train: Epoch [5], Batch [300/938], Loss: 1.124129295349121\n",
      "Train: Epoch [5], Batch [301/938], Loss: 0.9777284264564514\n",
      "Train: Epoch [5], Batch [302/938], Loss: 1.0907459259033203\n",
      "Train: Epoch [5], Batch [303/938], Loss: 1.2551121711730957\n",
      "Train: Epoch [5], Batch [304/938], Loss: 1.237152099609375\n",
      "Train: Epoch [5], Batch [305/938], Loss: 1.1345372200012207\n",
      "Train: Epoch [5], Batch [306/938], Loss: 0.9744855165481567\n",
      "Train: Epoch [5], Batch [307/938], Loss: 1.2095835208892822\n",
      "Train: Epoch [5], Batch [308/938], Loss: 1.1423126459121704\n",
      "Train: Epoch [5], Batch [309/938], Loss: 1.2186264991760254\n",
      "Train: Epoch [5], Batch [310/938], Loss: 1.087187647819519\n",
      "Train: Epoch [5], Batch [311/938], Loss: 1.0791165828704834\n",
      "Train: Epoch [5], Batch [312/938], Loss: 1.017886996269226\n",
      "Train: Epoch [5], Batch [313/938], Loss: 1.1437698602676392\n",
      "Train: Epoch [5], Batch [314/938], Loss: 1.2495975494384766\n",
      "Train: Epoch [5], Batch [315/938], Loss: 0.9605462551116943\n",
      "Train: Epoch [5], Batch [316/938], Loss: 1.0956205129623413\n",
      "Train: Epoch [5], Batch [317/938], Loss: 1.2000941038131714\n",
      "Train: Epoch [5], Batch [318/938], Loss: 1.4012244939804077\n",
      "Train: Epoch [5], Batch [319/938], Loss: 1.0779895782470703\n",
      "Train: Epoch [5], Batch [320/938], Loss: 1.3612186908721924\n",
      "Train: Epoch [5], Batch [321/938], Loss: 1.174538016319275\n",
      "Train: Epoch [5], Batch [322/938], Loss: 1.154880166053772\n",
      "Train: Epoch [5], Batch [323/938], Loss: 0.9195264577865601\n",
      "Train: Epoch [5], Batch [324/938], Loss: 1.193568468093872\n",
      "Train: Epoch [5], Batch [325/938], Loss: 1.1492286920547485\n",
      "Train: Epoch [5], Batch [326/938], Loss: 1.0471163988113403\n",
      "Train: Epoch [5], Batch [327/938], Loss: 1.329827904701233\n",
      "Train: Epoch [5], Batch [328/938], Loss: 1.2903701066970825\n",
      "Train: Epoch [5], Batch [329/938], Loss: 0.9404295086860657\n",
      "Train: Epoch [5], Batch [330/938], Loss: 1.0816302299499512\n",
      "Train: Epoch [5], Batch [331/938], Loss: 1.041882038116455\n",
      "Train: Epoch [5], Batch [332/938], Loss: 1.0402963161468506\n",
      "Train: Epoch [5], Batch [333/938], Loss: 1.0430892705917358\n",
      "Train: Epoch [5], Batch [334/938], Loss: 1.2068215608596802\n",
      "Train: Epoch [5], Batch [335/938], Loss: 1.0496710538864136\n",
      "Train: Epoch [5], Batch [336/938], Loss: 1.0224989652633667\n",
      "Train: Epoch [5], Batch [337/938], Loss: 1.4884312152862549\n",
      "Train: Epoch [5], Batch [338/938], Loss: 1.073655605316162\n",
      "Train: Epoch [5], Batch [339/938], Loss: 1.041472315788269\n",
      "Train: Epoch [5], Batch [340/938], Loss: 1.061902403831482\n",
      "Train: Epoch [5], Batch [341/938], Loss: 1.2782737016677856\n",
      "Train: Epoch [5], Batch [342/938], Loss: 1.2091923952102661\n",
      "Train: Epoch [5], Batch [343/938], Loss: 0.8149403929710388\n",
      "Train: Epoch [5], Batch [344/938], Loss: 1.0556459426879883\n",
      "Train: Epoch [5], Batch [345/938], Loss: 1.1589765548706055\n",
      "Train: Epoch [5], Batch [346/938], Loss: 1.0764667987823486\n",
      "Train: Epoch [5], Batch [347/938], Loss: 1.3074901103973389\n",
      "Train: Epoch [5], Batch [348/938], Loss: 1.0679407119750977\n",
      "Train: Epoch [5], Batch [349/938], Loss: 1.1540722846984863\n",
      "Train: Epoch [5], Batch [350/938], Loss: 1.1792725324630737\n",
      "Train: Epoch [5], Batch [351/938], Loss: 0.975597083568573\n",
      "Train: Epoch [5], Batch [352/938], Loss: 1.0614821910858154\n",
      "Train: Epoch [5], Batch [353/938], Loss: 1.0561907291412354\n",
      "Train: Epoch [5], Batch [354/938], Loss: 1.2513220310211182\n",
      "Train: Epoch [5], Batch [355/938], Loss: 0.9603787660598755\n",
      "Train: Epoch [5], Batch [356/938], Loss: 1.078046202659607\n",
      "Train: Epoch [5], Batch [357/938], Loss: 1.068224549293518\n",
      "Train: Epoch [5], Batch [358/938], Loss: 1.0666905641555786\n",
      "Train: Epoch [5], Batch [359/938], Loss: 1.371764063835144\n",
      "Train: Epoch [5], Batch [360/938], Loss: 1.0881083011627197\n",
      "Train: Epoch [5], Batch [361/938], Loss: 1.20309317111969\n",
      "Train: Epoch [5], Batch [362/938], Loss: 1.2040576934814453\n",
      "Train: Epoch [5], Batch [363/938], Loss: 1.032869577407837\n",
      "Train: Epoch [5], Batch [364/938], Loss: 1.0120303630828857\n",
      "Train: Epoch [5], Batch [365/938], Loss: 1.1929718255996704\n",
      "Train: Epoch [5], Batch [366/938], Loss: 1.035215139389038\n",
      "Train: Epoch [5], Batch [367/938], Loss: 1.4007964134216309\n",
      "Train: Epoch [5], Batch [368/938], Loss: 1.159959316253662\n",
      "Train: Epoch [5], Batch [369/938], Loss: 1.1296790838241577\n",
      "Train: Epoch [5], Batch [370/938], Loss: 1.0645463466644287\n",
      "Train: Epoch [5], Batch [371/938], Loss: 1.0106172561645508\n",
      "Train: Epoch [5], Batch [372/938], Loss: 1.2253782749176025\n",
      "Train: Epoch [5], Batch [373/938], Loss: 1.0758384466171265\n",
      "Train: Epoch [5], Batch [374/938], Loss: 1.1757556200027466\n",
      "Train: Epoch [5], Batch [375/938], Loss: 1.1738033294677734\n",
      "Train: Epoch [5], Batch [376/938], Loss: 1.145708680152893\n",
      "Train: Epoch [5], Batch [377/938], Loss: 0.7233788371086121\n",
      "Train: Epoch [5], Batch [378/938], Loss: 1.1387335062026978\n",
      "Train: Epoch [5], Batch [379/938], Loss: 1.1430401802062988\n",
      "Train: Epoch [5], Batch [380/938], Loss: 1.0295660495758057\n",
      "Train: Epoch [5], Batch [381/938], Loss: 1.1753703355789185\n",
      "Train: Epoch [5], Batch [382/938], Loss: 1.2443227767944336\n",
      "Train: Epoch [5], Batch [383/938], Loss: 1.2320908308029175\n",
      "Train: Epoch [5], Batch [384/938], Loss: 0.8619335889816284\n",
      "Train: Epoch [5], Batch [385/938], Loss: 1.0631611347198486\n",
      "Train: Epoch [5], Batch [386/938], Loss: 1.1129562854766846\n",
      "Train: Epoch [5], Batch [387/938], Loss: 1.06373929977417\n",
      "Train: Epoch [5], Batch [388/938], Loss: 1.1207756996154785\n",
      "Train: Epoch [5], Batch [389/938], Loss: 1.0052626132965088\n",
      "Train: Epoch [5], Batch [390/938], Loss: 1.0730726718902588\n",
      "Train: Epoch [5], Batch [391/938], Loss: 1.0006680488586426\n",
      "Train: Epoch [5], Batch [392/938], Loss: 1.0389177799224854\n",
      "Train: Epoch [5], Batch [393/938], Loss: 1.1323323249816895\n",
      "Train: Epoch [5], Batch [394/938], Loss: 1.2649965286254883\n",
      "Train: Epoch [5], Batch [395/938], Loss: 1.3245956897735596\n",
      "Train: Epoch [5], Batch [396/938], Loss: 0.9945639967918396\n",
      "Train: Epoch [5], Batch [397/938], Loss: 1.090299367904663\n",
      "Train: Epoch [5], Batch [398/938], Loss: 0.9622976779937744\n",
      "Train: Epoch [5], Batch [399/938], Loss: 1.055687665939331\n",
      "Train: Epoch [5], Batch [400/938], Loss: 1.245733618736267\n",
      "Train: Epoch [5], Batch [401/938], Loss: 1.1937541961669922\n",
      "Train: Epoch [5], Batch [402/938], Loss: 1.1926759481430054\n",
      "Train: Epoch [5], Batch [403/938], Loss: 1.3099184036254883\n",
      "Train: Epoch [5], Batch [404/938], Loss: 1.2845044136047363\n",
      "Train: Epoch [5], Batch [405/938], Loss: 1.1398276090621948\n",
      "Train: Epoch [5], Batch [406/938], Loss: 1.0787094831466675\n",
      "Train: Epoch [5], Batch [407/938], Loss: 1.0844142436981201\n",
      "Train: Epoch [5], Batch [408/938], Loss: 0.9461073279380798\n",
      "Train: Epoch [5], Batch [409/938], Loss: 1.1681866645812988\n",
      "Train: Epoch [5], Batch [410/938], Loss: 1.243005394935608\n",
      "Train: Epoch [5], Batch [411/938], Loss: 0.834009051322937\n",
      "Train: Epoch [5], Batch [412/938], Loss: 1.090421199798584\n",
      "Train: Epoch [5], Batch [413/938], Loss: 1.2270939350128174\n",
      "Train: Epoch [5], Batch [414/938], Loss: 0.9278988838195801\n",
      "Train: Epoch [5], Batch [415/938], Loss: 1.237907886505127\n",
      "Train: Epoch [5], Batch [416/938], Loss: 1.26521635055542\n",
      "Train: Epoch [5], Batch [417/938], Loss: 0.9958100914955139\n",
      "Train: Epoch [5], Batch [418/938], Loss: 1.0448200702667236\n",
      "Train: Epoch [5], Batch [419/938], Loss: 1.2944873571395874\n",
      "Train: Epoch [5], Batch [420/938], Loss: 1.1153433322906494\n",
      "Train: Epoch [5], Batch [421/938], Loss: 1.0555177927017212\n",
      "Train: Epoch [5], Batch [422/938], Loss: 1.2075303792953491\n",
      "Train: Epoch [5], Batch [423/938], Loss: 1.055000901222229\n",
      "Train: Epoch [5], Batch [424/938], Loss: 0.901180624961853\n",
      "Train: Epoch [5], Batch [425/938], Loss: 1.0504542589187622\n",
      "Train: Epoch [5], Batch [426/938], Loss: 0.9263648986816406\n",
      "Train: Epoch [5], Batch [427/938], Loss: 1.1071314811706543\n",
      "Train: Epoch [5], Batch [428/938], Loss: 1.025507926940918\n",
      "Train: Epoch [5], Batch [429/938], Loss: 1.357680082321167\n",
      "Train: Epoch [5], Batch [430/938], Loss: 1.137819528579712\n",
      "Train: Epoch [5], Batch [431/938], Loss: 1.025754690170288\n",
      "Train: Epoch [5], Batch [432/938], Loss: 1.03623366355896\n",
      "Train: Epoch [5], Batch [433/938], Loss: 1.17450749874115\n",
      "Train: Epoch [5], Batch [434/938], Loss: 1.0860185623168945\n",
      "Train: Epoch [5], Batch [435/938], Loss: 0.8898494243621826\n",
      "Train: Epoch [5], Batch [436/938], Loss: 1.2637863159179688\n",
      "Train: Epoch [5], Batch [437/938], Loss: 1.1504782438278198\n",
      "Train: Epoch [5], Batch [438/938], Loss: 1.150460124015808\n",
      "Train: Epoch [5], Batch [439/938], Loss: 0.9611358642578125\n",
      "Train: Epoch [5], Batch [440/938], Loss: 1.4134202003479004\n",
      "Train: Epoch [5], Batch [441/938], Loss: 1.0556936264038086\n",
      "Train: Epoch [5], Batch [442/938], Loss: 1.1307615041732788\n",
      "Train: Epoch [5], Batch [443/938], Loss: 1.0289183855056763\n",
      "Train: Epoch [5], Batch [444/938], Loss: 1.0866515636444092\n",
      "Train: Epoch [5], Batch [445/938], Loss: 1.0983213186264038\n",
      "Train: Epoch [5], Batch [446/938], Loss: 1.1496403217315674\n",
      "Train: Epoch [5], Batch [447/938], Loss: 1.3873597383499146\n",
      "Train: Epoch [5], Batch [448/938], Loss: 1.1432137489318848\n",
      "Train: Epoch [5], Batch [449/938], Loss: 1.1440147161483765\n",
      "Train: Epoch [5], Batch [450/938], Loss: 0.9315165281295776\n",
      "Train: Epoch [5], Batch [451/938], Loss: 1.1593726873397827\n",
      "Train: Epoch [5], Batch [452/938], Loss: 1.0400525331497192\n",
      "Train: Epoch [5], Batch [453/938], Loss: 1.168208122253418\n",
      "Train: Epoch [5], Batch [454/938], Loss: 1.0995066165924072\n",
      "Train: Epoch [5], Batch [455/938], Loss: 1.0065338611602783\n",
      "Train: Epoch [5], Batch [456/938], Loss: 1.0835700035095215\n",
      "Train: Epoch [5], Batch [457/938], Loss: 1.3046413660049438\n",
      "Train: Epoch [5], Batch [458/938], Loss: 1.2389194965362549\n",
      "Train: Epoch [5], Batch [459/938], Loss: 1.100534200668335\n",
      "Train: Epoch [5], Batch [460/938], Loss: 1.1195635795593262\n",
      "Train: Epoch [5], Batch [461/938], Loss: 1.0717427730560303\n",
      "Train: Epoch [5], Batch [462/938], Loss: 1.0709807872772217\n",
      "Train: Epoch [5], Batch [463/938], Loss: 1.1329307556152344\n",
      "Train: Epoch [5], Batch [464/938], Loss: 1.090693473815918\n",
      "Train: Epoch [5], Batch [465/938], Loss: 1.170900821685791\n",
      "Train: Epoch [5], Batch [466/938], Loss: 1.0191701650619507\n",
      "Train: Epoch [5], Batch [467/938], Loss: 1.0643030405044556\n",
      "Train: Epoch [5], Batch [468/938], Loss: 1.0325270891189575\n",
      "Train: Epoch [5], Batch [469/938], Loss: 0.9424827694892883\n",
      "Train: Epoch [5], Batch [470/938], Loss: 1.0019218921661377\n",
      "Train: Epoch [5], Batch [471/938], Loss: 1.0976109504699707\n",
      "Train: Epoch [5], Batch [472/938], Loss: 1.2807573080062866\n",
      "Train: Epoch [5], Batch [473/938], Loss: 1.0118235349655151\n",
      "Train: Epoch [5], Batch [474/938], Loss: 0.9452998638153076\n",
      "Train: Epoch [5], Batch [475/938], Loss: 0.9192513823509216\n",
      "Train: Epoch [5], Batch [476/938], Loss: 1.0661054849624634\n",
      "Train: Epoch [5], Batch [477/938], Loss: 1.1054335832595825\n",
      "Train: Epoch [5], Batch [478/938], Loss: 0.9775881171226501\n",
      "Train: Epoch [5], Batch [479/938], Loss: 1.0877434015274048\n",
      "Train: Epoch [5], Batch [480/938], Loss: 0.8923380374908447\n",
      "Train: Epoch [5], Batch [481/938], Loss: 1.030991792678833\n",
      "Train: Epoch [5], Batch [482/938], Loss: 1.145322322845459\n",
      "Train: Epoch [5], Batch [483/938], Loss: 0.9376221895217896\n",
      "Train: Epoch [5], Batch [484/938], Loss: 0.941028356552124\n",
      "Train: Epoch [5], Batch [485/938], Loss: 1.2065558433532715\n",
      "Train: Epoch [5], Batch [486/938], Loss: 1.1023895740509033\n",
      "Train: Epoch [5], Batch [487/938], Loss: 1.2752139568328857\n",
      "Train: Epoch [5], Batch [488/938], Loss: 1.0893902778625488\n",
      "Train: Epoch [5], Batch [489/938], Loss: 1.1156216859817505\n",
      "Train: Epoch [5], Batch [490/938], Loss: 1.03455650806427\n",
      "Train: Epoch [5], Batch [491/938], Loss: 1.0654563903808594\n",
      "Train: Epoch [5], Batch [492/938], Loss: 1.2439923286437988\n",
      "Train: Epoch [5], Batch [493/938], Loss: 1.0630862712860107\n",
      "Train: Epoch [5], Batch [494/938], Loss: 1.3116743564605713\n",
      "Train: Epoch [5], Batch [495/938], Loss: 0.937325119972229\n",
      "Train: Epoch [5], Batch [496/938], Loss: 0.913672924041748\n",
      "Train: Epoch [5], Batch [497/938], Loss: 1.0195311307907104\n",
      "Train: Epoch [5], Batch [498/938], Loss: 1.1051853895187378\n",
      "Train: Epoch [5], Batch [499/938], Loss: 1.0648542642593384\n",
      "Train: Epoch [5], Batch [500/938], Loss: 0.9180938601493835\n",
      "Train: Epoch [5], Batch [501/938], Loss: 1.204224705696106\n",
      "Train: Epoch [5], Batch [502/938], Loss: 0.94291090965271\n",
      "Train: Epoch [5], Batch [503/938], Loss: 1.0363210439682007\n",
      "Train: Epoch [5], Batch [504/938], Loss: 1.1305146217346191\n",
      "Train: Epoch [5], Batch [505/938], Loss: 1.0855730772018433\n",
      "Train: Epoch [5], Batch [506/938], Loss: 1.0265984535217285\n",
      "Train: Epoch [5], Batch [507/938], Loss: 1.0656734704971313\n",
      "Train: Epoch [5], Batch [508/938], Loss: 1.1054078340530396\n",
      "Train: Epoch [5], Batch [509/938], Loss: 1.280887484550476\n",
      "Train: Epoch [5], Batch [510/938], Loss: 1.1674320697784424\n",
      "Train: Epoch [5], Batch [511/938], Loss: 1.1238230466842651\n",
      "Train: Epoch [5], Batch [512/938], Loss: 1.2834521532058716\n",
      "Train: Epoch [5], Batch [513/938], Loss: 1.1209676265716553\n",
      "Train: Epoch [5], Batch [514/938], Loss: 1.3773239850997925\n",
      "Train: Epoch [5], Batch [515/938], Loss: 0.9398614168167114\n",
      "Train: Epoch [5], Batch [516/938], Loss: 1.0277642011642456\n",
      "Train: Epoch [5], Batch [517/938], Loss: 1.0935652256011963\n",
      "Train: Epoch [5], Batch [518/938], Loss: 1.086016297340393\n",
      "Train: Epoch [5], Batch [519/938], Loss: 0.8598610162734985\n",
      "Train: Epoch [5], Batch [520/938], Loss: 1.1739293336868286\n",
      "Train: Epoch [5], Batch [521/938], Loss: 0.9011863470077515\n",
      "Train: Epoch [5], Batch [522/938], Loss: 1.401225209236145\n",
      "Train: Epoch [5], Batch [523/938], Loss: 1.3207179307937622\n",
      "Train: Epoch [5], Batch [524/938], Loss: 1.437308430671692\n",
      "Train: Epoch [5], Batch [525/938], Loss: 1.1234450340270996\n",
      "Train: Epoch [5], Batch [526/938], Loss: 0.9243600368499756\n",
      "Train: Epoch [5], Batch [527/938], Loss: 0.9083810448646545\n",
      "Train: Epoch [5], Batch [528/938], Loss: 1.1943320035934448\n",
      "Train: Epoch [5], Batch [529/938], Loss: 1.0736631155014038\n",
      "Train: Epoch [5], Batch [530/938], Loss: 1.0609142780303955\n",
      "Train: Epoch [5], Batch [531/938], Loss: 0.9500068426132202\n",
      "Train: Epoch [5], Batch [532/938], Loss: 1.302578091621399\n",
      "Train: Epoch [5], Batch [533/938], Loss: 1.0916980504989624\n",
      "Train: Epoch [5], Batch [534/938], Loss: 1.0401180982589722\n",
      "Train: Epoch [5], Batch [535/938], Loss: 1.088045358657837\n",
      "Train: Epoch [5], Batch [536/938], Loss: 0.9984520673751831\n",
      "Train: Epoch [5], Batch [537/938], Loss: 1.1721673011779785\n",
      "Train: Epoch [5], Batch [538/938], Loss: 1.0843875408172607\n",
      "Train: Epoch [5], Batch [539/938], Loss: 1.074721336364746\n",
      "Train: Epoch [5], Batch [540/938], Loss: 1.286970853805542\n",
      "Train: Epoch [5], Batch [541/938], Loss: 1.0127906799316406\n",
      "Train: Epoch [5], Batch [542/938], Loss: 1.1287362575531006\n",
      "Train: Epoch [5], Batch [543/938], Loss: 1.0908859968185425\n",
      "Train: Epoch [5], Batch [544/938], Loss: 1.1727856397628784\n",
      "Train: Epoch [5], Batch [545/938], Loss: 1.0974516868591309\n",
      "Train: Epoch [5], Batch [546/938], Loss: 0.9984195232391357\n",
      "Train: Epoch [5], Batch [547/938], Loss: 1.1736178398132324\n",
      "Train: Epoch [5], Batch [548/938], Loss: 1.0540449619293213\n",
      "Train: Epoch [5], Batch [549/938], Loss: 0.7627360224723816\n",
      "Train: Epoch [5], Batch [550/938], Loss: 1.2170648574829102\n",
      "Train: Epoch [5], Batch [551/938], Loss: 0.9359932541847229\n",
      "Train: Epoch [5], Batch [552/938], Loss: 1.0252662897109985\n",
      "Train: Epoch [5], Batch [553/938], Loss: 1.1443822383880615\n",
      "Train: Epoch [5], Batch [554/938], Loss: 1.0033053159713745\n",
      "Train: Epoch [5], Batch [555/938], Loss: 1.1039289236068726\n",
      "Train: Epoch [5], Batch [556/938], Loss: 1.1230218410491943\n",
      "Train: Epoch [5], Batch [557/938], Loss: 1.0230036973953247\n",
      "Train: Epoch [5], Batch [558/938], Loss: 1.0685694217681885\n",
      "Train: Epoch [5], Batch [559/938], Loss: 1.1608854532241821\n",
      "Train: Epoch [5], Batch [560/938], Loss: 1.0583522319793701\n",
      "Train: Epoch [5], Batch [561/938], Loss: 1.0750155448913574\n",
      "Train: Epoch [5], Batch [562/938], Loss: 1.2250664234161377\n",
      "Train: Epoch [5], Batch [563/938], Loss: 1.1639275550842285\n",
      "Train: Epoch [5], Batch [564/938], Loss: 1.0438506603240967\n",
      "Train: Epoch [5], Batch [565/938], Loss: 1.0030086040496826\n",
      "Train: Epoch [5], Batch [566/938], Loss: 0.9076545238494873\n",
      "Train: Epoch [5], Batch [567/938], Loss: 1.1653286218643188\n",
      "Train: Epoch [5], Batch [568/938], Loss: 1.1150070428848267\n",
      "Train: Epoch [5], Batch [569/938], Loss: 1.1861144304275513\n",
      "Train: Epoch [5], Batch [570/938], Loss: 1.0003753900527954\n",
      "Train: Epoch [5], Batch [571/938], Loss: 0.8808570504188538\n",
      "Train: Epoch [5], Batch [572/938], Loss: 0.9539366960525513\n",
      "Train: Epoch [5], Batch [573/938], Loss: 1.2255859375\n",
      "Train: Epoch [5], Batch [574/938], Loss: 1.0944348573684692\n",
      "Train: Epoch [5], Batch [575/938], Loss: 1.0644302368164062\n",
      "Train: Epoch [5], Batch [576/938], Loss: 0.8786669373512268\n",
      "Train: Epoch [5], Batch [577/938], Loss: 1.252461314201355\n",
      "Train: Epoch [5], Batch [578/938], Loss: 1.1154717206954956\n",
      "Train: Epoch [5], Batch [579/938], Loss: 1.0244359970092773\n",
      "Train: Epoch [5], Batch [580/938], Loss: 1.441053032875061\n",
      "Train: Epoch [5], Batch [581/938], Loss: 0.9461231231689453\n",
      "Train: Epoch [5], Batch [582/938], Loss: 0.8428579568862915\n",
      "Train: Epoch [5], Batch [583/938], Loss: 1.0438649654388428\n",
      "Train: Epoch [5], Batch [584/938], Loss: 0.9407070279121399\n",
      "Train: Epoch [5], Batch [585/938], Loss: 0.9875407218933105\n",
      "Train: Epoch [5], Batch [586/938], Loss: 1.201128363609314\n",
      "Train: Epoch [5], Batch [587/938], Loss: 1.1191455125808716\n",
      "Train: Epoch [5], Batch [588/938], Loss: 1.045285701751709\n",
      "Train: Epoch [5], Batch [589/938], Loss: 1.119300127029419\n",
      "Train: Epoch [5], Batch [590/938], Loss: 1.088990569114685\n",
      "Train: Epoch [5], Batch [591/938], Loss: 0.8902816772460938\n",
      "Train: Epoch [5], Batch [592/938], Loss: 0.9547382593154907\n",
      "Train: Epoch [5], Batch [593/938], Loss: 1.0907429456710815\n",
      "Train: Epoch [5], Batch [594/938], Loss: 1.0170543193817139\n",
      "Train: Epoch [5], Batch [595/938], Loss: 1.059996485710144\n",
      "Train: Epoch [5], Batch [596/938], Loss: 1.0050524473190308\n",
      "Train: Epoch [5], Batch [597/938], Loss: 1.2231782674789429\n",
      "Train: Epoch [5], Batch [598/938], Loss: 0.9968582987785339\n",
      "Train: Epoch [5], Batch [599/938], Loss: 0.9502072930335999\n",
      "Train: Epoch [5], Batch [600/938], Loss: 1.0822290182113647\n",
      "Train: Epoch [5], Batch [601/938], Loss: 0.8047388792037964\n",
      "Train: Epoch [5], Batch [602/938], Loss: 1.1273738145828247\n",
      "Train: Epoch [5], Batch [603/938], Loss: 1.0848139524459839\n",
      "Train: Epoch [5], Batch [604/938], Loss: 1.1574525833129883\n",
      "Train: Epoch [5], Batch [605/938], Loss: 1.2442631721496582\n",
      "Train: Epoch [5], Batch [606/938], Loss: 1.0156513452529907\n",
      "Train: Epoch [5], Batch [607/938], Loss: 1.007851004600525\n",
      "Train: Epoch [5], Batch [608/938], Loss: 0.9321960210800171\n",
      "Train: Epoch [5], Batch [609/938], Loss: 1.1859745979309082\n",
      "Train: Epoch [5], Batch [610/938], Loss: 1.1959526538848877\n",
      "Train: Epoch [5], Batch [611/938], Loss: 1.27996826171875\n",
      "Train: Epoch [5], Batch [612/938], Loss: 0.8773598670959473\n",
      "Train: Epoch [5], Batch [613/938], Loss: 1.1954716444015503\n",
      "Train: Epoch [5], Batch [614/938], Loss: 1.008465051651001\n",
      "Train: Epoch [5], Batch [615/938], Loss: 1.1606101989746094\n",
      "Train: Epoch [5], Batch [616/938], Loss: 1.0208293199539185\n",
      "Train: Epoch [5], Batch [617/938], Loss: 1.0641173124313354\n",
      "Train: Epoch [5], Batch [618/938], Loss: 1.0462545156478882\n",
      "Train: Epoch [5], Batch [619/938], Loss: 1.0859966278076172\n",
      "Train: Epoch [5], Batch [620/938], Loss: 1.1014602184295654\n",
      "Train: Epoch [5], Batch [621/938], Loss: 1.075150966644287\n",
      "Train: Epoch [5], Batch [622/938], Loss: 0.9587263464927673\n",
      "Train: Epoch [5], Batch [623/938], Loss: 0.9028597474098206\n",
      "Train: Epoch [5], Batch [624/938], Loss: 1.2318859100341797\n",
      "Train: Epoch [5], Batch [625/938], Loss: 1.1356431245803833\n",
      "Train: Epoch [5], Batch [626/938], Loss: 0.8599433302879333\n",
      "Train: Epoch [5], Batch [627/938], Loss: 1.1057244539260864\n",
      "Train: Epoch [5], Batch [628/938], Loss: 1.1067469120025635\n",
      "Train: Epoch [5], Batch [629/938], Loss: 0.9970482587814331\n",
      "Train: Epoch [5], Batch [630/938], Loss: 1.0902307033538818\n",
      "Train: Epoch [5], Batch [631/938], Loss: 0.9014667272567749\n",
      "Train: Epoch [5], Batch [632/938], Loss: 1.0516853332519531\n",
      "Train: Epoch [5], Batch [633/938], Loss: 0.9725878238677979\n",
      "Train: Epoch [5], Batch [634/938], Loss: 1.320200800895691\n",
      "Train: Epoch [5], Batch [635/938], Loss: 0.91480553150177\n",
      "Train: Epoch [5], Batch [636/938], Loss: 0.9342800974845886\n",
      "Train: Epoch [5], Batch [637/938], Loss: 1.1014578342437744\n",
      "Train: Epoch [5], Batch [638/938], Loss: 0.8476120829582214\n",
      "Train: Epoch [5], Batch [639/938], Loss: 1.0833733081817627\n",
      "Train: Epoch [5], Batch [640/938], Loss: 1.2119084596633911\n",
      "Train: Epoch [5], Batch [641/938], Loss: 0.9056532979011536\n",
      "Train: Epoch [5], Batch [642/938], Loss: 1.0999600887298584\n",
      "Train: Epoch [5], Batch [643/938], Loss: 1.2251062393188477\n",
      "Train: Epoch [5], Batch [644/938], Loss: 1.0342528820037842\n",
      "Train: Epoch [5], Batch [645/938], Loss: 1.1418825387954712\n",
      "Train: Epoch [5], Batch [646/938], Loss: 1.051289677619934\n",
      "Train: Epoch [5], Batch [647/938], Loss: 0.9479106664657593\n",
      "Train: Epoch [5], Batch [648/938], Loss: 1.0102882385253906\n",
      "Train: Epoch [5], Batch [649/938], Loss: 1.2906920909881592\n",
      "Train: Epoch [5], Batch [650/938], Loss: 1.062620997428894\n",
      "Train: Epoch [5], Batch [651/938], Loss: 0.9425179958343506\n",
      "Train: Epoch [5], Batch [652/938], Loss: 1.0559512376785278\n",
      "Train: Epoch [5], Batch [653/938], Loss: 1.1425522565841675\n",
      "Train: Epoch [5], Batch [654/938], Loss: 0.8950450420379639\n",
      "Train: Epoch [5], Batch [655/938], Loss: 0.8320631384849548\n",
      "Train: Epoch [5], Batch [656/938], Loss: 0.9432418346405029\n",
      "Train: Epoch [5], Batch [657/938], Loss: 0.9939556121826172\n",
      "Train: Epoch [5], Batch [658/938], Loss: 1.087628722190857\n",
      "Train: Epoch [5], Batch [659/938], Loss: 1.084247350692749\n",
      "Train: Epoch [5], Batch [660/938], Loss: 1.029960036277771\n",
      "Train: Epoch [5], Batch [661/938], Loss: 1.2096554040908813\n",
      "Train: Epoch [5], Batch [662/938], Loss: 1.1977767944335938\n",
      "Train: Epoch [5], Batch [663/938], Loss: 1.14723539352417\n",
      "Train: Epoch [5], Batch [664/938], Loss: 0.9248335361480713\n",
      "Train: Epoch [5], Batch [665/938], Loss: 1.0397273302078247\n",
      "Train: Epoch [5], Batch [666/938], Loss: 1.1096333265304565\n",
      "Train: Epoch [5], Batch [667/938], Loss: 1.377787709236145\n",
      "Train: Epoch [5], Batch [668/938], Loss: 0.9426491260528564\n",
      "Train: Epoch [5], Batch [669/938], Loss: 0.8441173434257507\n",
      "Train: Epoch [5], Batch [670/938], Loss: 1.1519581079483032\n",
      "Train: Epoch [5], Batch [671/938], Loss: 1.0692352056503296\n",
      "Train: Epoch [5], Batch [672/938], Loss: 1.0988616943359375\n",
      "Train: Epoch [5], Batch [673/938], Loss: 1.03530752658844\n",
      "Train: Epoch [5], Batch [674/938], Loss: 1.1434906721115112\n",
      "Train: Epoch [5], Batch [675/938], Loss: 1.2553802728652954\n",
      "Train: Epoch [5], Batch [676/938], Loss: 1.062788724899292\n",
      "Train: Epoch [5], Batch [677/938], Loss: 0.9516994953155518\n",
      "Train: Epoch [5], Batch [678/938], Loss: 1.0928436517715454\n",
      "Train: Epoch [5], Batch [679/938], Loss: 1.0602571964263916\n",
      "Train: Epoch [5], Batch [680/938], Loss: 0.9471381902694702\n",
      "Train: Epoch [5], Batch [681/938], Loss: 1.0448652505874634\n",
      "Train: Epoch [5], Batch [682/938], Loss: 1.1597721576690674\n",
      "Train: Epoch [5], Batch [683/938], Loss: 1.1470998525619507\n",
      "Train: Epoch [5], Batch [684/938], Loss: 1.233583688735962\n",
      "Train: Epoch [5], Batch [685/938], Loss: 0.9318757057189941\n",
      "Train: Epoch [5], Batch [686/938], Loss: 1.1144065856933594\n",
      "Train: Epoch [5], Batch [687/938], Loss: 1.0407813787460327\n",
      "Train: Epoch [5], Batch [688/938], Loss: 0.9037688374519348\n",
      "Train: Epoch [5], Batch [689/938], Loss: 1.1361043453216553\n",
      "Train: Epoch [5], Batch [690/938], Loss: 1.0116090774536133\n",
      "Train: Epoch [5], Batch [691/938], Loss: 0.9944877028465271\n",
      "Train: Epoch [5], Batch [692/938], Loss: 1.0589679479599\n",
      "Train: Epoch [5], Batch [693/938], Loss: 1.1415300369262695\n",
      "Train: Epoch [5], Batch [694/938], Loss: 1.0660697221755981\n",
      "Train: Epoch [5], Batch [695/938], Loss: 0.9817764759063721\n",
      "Train: Epoch [5], Batch [696/938], Loss: 1.2129416465759277\n",
      "Train: Epoch [5], Batch [697/938], Loss: 1.1593271493911743\n",
      "Train: Epoch [5], Batch [698/938], Loss: 0.9661323428153992\n",
      "Train: Epoch [5], Batch [699/938], Loss: 1.2887322902679443\n",
      "Train: Epoch [5], Batch [700/938], Loss: 1.108932375907898\n",
      "Train: Epoch [5], Batch [701/938], Loss: 1.2284470796585083\n",
      "Train: Epoch [5], Batch [702/938], Loss: 0.9723541736602783\n",
      "Train: Epoch [5], Batch [703/938], Loss: 0.9975848197937012\n",
      "Train: Epoch [5], Batch [704/938], Loss: 1.0261945724487305\n",
      "Train: Epoch [5], Batch [705/938], Loss: 1.0902223587036133\n",
      "Train: Epoch [5], Batch [706/938], Loss: 1.032641887664795\n",
      "Train: Epoch [5], Batch [707/938], Loss: 0.8576539158821106\n",
      "Train: Epoch [5], Batch [708/938], Loss: 1.013071060180664\n",
      "Train: Epoch [5], Batch [709/938], Loss: 1.1643128395080566\n",
      "Train: Epoch [5], Batch [710/938], Loss: 1.090314269065857\n",
      "Train: Epoch [5], Batch [711/938], Loss: 0.97170090675354\n",
      "Train: Epoch [5], Batch [712/938], Loss: 0.8244606852531433\n",
      "Train: Epoch [5], Batch [713/938], Loss: 0.9933805465698242\n",
      "Train: Epoch [5], Batch [714/938], Loss: 0.9907032251358032\n",
      "Train: Epoch [5], Batch [715/938], Loss: 1.0973689556121826\n",
      "Train: Epoch [5], Batch [716/938], Loss: 0.9801166653633118\n",
      "Train: Epoch [5], Batch [717/938], Loss: 0.9097012281417847\n",
      "Train: Epoch [5], Batch [718/938], Loss: 1.001004934310913\n",
      "Train: Epoch [5], Batch [719/938], Loss: 0.9925826787948608\n",
      "Train: Epoch [5], Batch [720/938], Loss: 1.1885906457901\n",
      "Train: Epoch [5], Batch [721/938], Loss: 1.3107410669326782\n",
      "Train: Epoch [5], Batch [722/938], Loss: 0.9090890288352966\n",
      "Train: Epoch [5], Batch [723/938], Loss: 1.1201481819152832\n",
      "Train: Epoch [5], Batch [724/938], Loss: 1.0233476161956787\n",
      "Train: Epoch [5], Batch [725/938], Loss: 1.024917721748352\n",
      "Train: Epoch [5], Batch [726/938], Loss: 1.1076747179031372\n",
      "Train: Epoch [5], Batch [727/938], Loss: 1.1049425601959229\n",
      "Train: Epoch [5], Batch [728/938], Loss: 1.0228713750839233\n",
      "Train: Epoch [5], Batch [729/938], Loss: 1.2051903009414673\n",
      "Train: Epoch [5], Batch [730/938], Loss: 1.3527987003326416\n",
      "Train: Epoch [5], Batch [731/938], Loss: 0.9711331725120544\n",
      "Train: Epoch [5], Batch [732/938], Loss: 0.9134870767593384\n",
      "Train: Epoch [5], Batch [733/938], Loss: 0.8332416415214539\n",
      "Train: Epoch [5], Batch [734/938], Loss: 1.0720294713974\n",
      "Train: Epoch [5], Batch [735/938], Loss: 0.9361663460731506\n",
      "Train: Epoch [5], Batch [736/938], Loss: 1.0165961980819702\n",
      "Train: Epoch [5], Batch [737/938], Loss: 0.8418328762054443\n",
      "Train: Epoch [5], Batch [738/938], Loss: 0.9448792338371277\n",
      "Train: Epoch [5], Batch [739/938], Loss: 1.051571011543274\n",
      "Train: Epoch [5], Batch [740/938], Loss: 0.9698712825775146\n",
      "Train: Epoch [5], Batch [741/938], Loss: 1.2246239185333252\n",
      "Train: Epoch [5], Batch [742/938], Loss: 1.1571149826049805\n",
      "Train: Epoch [5], Batch [743/938], Loss: 1.0930986404418945\n",
      "Train: Epoch [5], Batch [744/938], Loss: 1.2772085666656494\n",
      "Train: Epoch [5], Batch [745/938], Loss: 1.2566945552825928\n",
      "Train: Epoch [5], Batch [746/938], Loss: 1.036872148513794\n",
      "Train: Epoch [5], Batch [747/938], Loss: 1.008420467376709\n",
      "Train: Epoch [5], Batch [748/938], Loss: 1.3590043783187866\n",
      "Train: Epoch [5], Batch [749/938], Loss: 0.9923495650291443\n",
      "Train: Epoch [5], Batch [750/938], Loss: 1.136945366859436\n",
      "Train: Epoch [5], Batch [751/938], Loss: 1.0046643018722534\n",
      "Train: Epoch [5], Batch [752/938], Loss: 0.8831126093864441\n",
      "Train: Epoch [5], Batch [753/938], Loss: 1.1301525831222534\n",
      "Train: Epoch [5], Batch [754/938], Loss: 0.924933671951294\n",
      "Train: Epoch [5], Batch [755/938], Loss: 1.0520678758621216\n",
      "Train: Epoch [5], Batch [756/938], Loss: 1.0947341918945312\n",
      "Train: Epoch [5], Batch [757/938], Loss: 1.1728073358535767\n",
      "Train: Epoch [5], Batch [758/938], Loss: 0.9300280809402466\n",
      "Train: Epoch [5], Batch [759/938], Loss: 1.0898956060409546\n",
      "Train: Epoch [5], Batch [760/938], Loss: 1.1844463348388672\n",
      "Train: Epoch [5], Batch [761/938], Loss: 1.0788960456848145\n",
      "Train: Epoch [5], Batch [762/938], Loss: 1.0564295053482056\n",
      "Train: Epoch [5], Batch [763/938], Loss: 1.1703321933746338\n",
      "Train: Epoch [5], Batch [764/938], Loss: 1.1152547597885132\n",
      "Train: Epoch [5], Batch [765/938], Loss: 1.1567273139953613\n",
      "Train: Epoch [5], Batch [766/938], Loss: 1.1800721883773804\n",
      "Train: Epoch [5], Batch [767/938], Loss: 0.9024841785430908\n",
      "Train: Epoch [5], Batch [768/938], Loss: 1.0189474821090698\n",
      "Train: Epoch [5], Batch [769/938], Loss: 0.9876275062561035\n",
      "Train: Epoch [5], Batch [770/938], Loss: 1.025800347328186\n",
      "Train: Epoch [5], Batch [771/938], Loss: 1.118977665901184\n",
      "Train: Epoch [5], Batch [772/938], Loss: 1.034335732460022\n",
      "Train: Epoch [5], Batch [773/938], Loss: 1.163222074508667\n",
      "Train: Epoch [5], Batch [774/938], Loss: 1.0004518032073975\n",
      "Train: Epoch [5], Batch [775/938], Loss: 0.8432940244674683\n",
      "Train: Epoch [5], Batch [776/938], Loss: 1.0769366025924683\n",
      "Train: Epoch [5], Batch [777/938], Loss: 0.9421606063842773\n",
      "Train: Epoch [5], Batch [778/938], Loss: 0.9039319753646851\n",
      "Train: Epoch [5], Batch [779/938], Loss: 1.0423049926757812\n",
      "Train: Epoch [5], Batch [780/938], Loss: 1.087349534034729\n",
      "Train: Epoch [5], Batch [781/938], Loss: 0.8204944729804993\n",
      "Train: Epoch [5], Batch [782/938], Loss: 0.9935604333877563\n",
      "Train: Epoch [5], Batch [783/938], Loss: 1.1789451837539673\n",
      "Train: Epoch [5], Batch [784/938], Loss: 1.2785700559616089\n",
      "Train: Epoch [5], Batch [785/938], Loss: 0.8052839040756226\n",
      "Train: Epoch [5], Batch [786/938], Loss: 0.8669376373291016\n",
      "Train: Epoch [5], Batch [787/938], Loss: 0.9856536984443665\n",
      "Train: Epoch [5], Batch [788/938], Loss: 1.4755735397338867\n",
      "Train: Epoch [5], Batch [789/938], Loss: 0.9209733605384827\n",
      "Train: Epoch [5], Batch [790/938], Loss: 1.054822564125061\n",
      "Train: Epoch [5], Batch [791/938], Loss: 1.0500513315200806\n",
      "Train: Epoch [5], Batch [792/938], Loss: 0.9775905609130859\n",
      "Train: Epoch [5], Batch [793/938], Loss: 1.0435295104980469\n",
      "Train: Epoch [5], Batch [794/938], Loss: 1.0381293296813965\n",
      "Train: Epoch [5], Batch [795/938], Loss: 1.1095401048660278\n",
      "Train: Epoch [5], Batch [796/938], Loss: 1.0389878749847412\n",
      "Train: Epoch [5], Batch [797/938], Loss: 0.9669317603111267\n",
      "Train: Epoch [5], Batch [798/938], Loss: 1.228519082069397\n",
      "Train: Epoch [5], Batch [799/938], Loss: 0.9565593600273132\n",
      "Train: Epoch [5], Batch [800/938], Loss: 0.9902894496917725\n",
      "Train: Epoch [5], Batch [801/938], Loss: 1.262624979019165\n",
      "Train: Epoch [5], Batch [802/938], Loss: 0.9131367802619934\n",
      "Train: Epoch [5], Batch [803/938], Loss: 0.9667208194732666\n",
      "Train: Epoch [5], Batch [804/938], Loss: 1.0648224353790283\n",
      "Train: Epoch [5], Batch [805/938], Loss: 0.7810962200164795\n",
      "Train: Epoch [5], Batch [806/938], Loss: 1.0679326057434082\n",
      "Train: Epoch [5], Batch [807/938], Loss: 1.1374425888061523\n",
      "Train: Epoch [5], Batch [808/938], Loss: 0.8644470572471619\n",
      "Train: Epoch [5], Batch [809/938], Loss: 0.9593998789787292\n",
      "Train: Epoch [5], Batch [810/938], Loss: 1.021303415298462\n",
      "Train: Epoch [5], Batch [811/938], Loss: 1.1398249864578247\n",
      "Train: Epoch [5], Batch [812/938], Loss: 0.954064130783081\n",
      "Train: Epoch [5], Batch [813/938], Loss: 1.1171189546585083\n",
      "Train: Epoch [5], Batch [814/938], Loss: 1.108722448348999\n",
      "Train: Epoch [5], Batch [815/938], Loss: 1.1028729677200317\n",
      "Train: Epoch [5], Batch [816/938], Loss: 0.7817935347557068\n",
      "Train: Epoch [5], Batch [817/938], Loss: 1.0710911750793457\n",
      "Train: Epoch [5], Batch [818/938], Loss: 1.0247881412506104\n",
      "Train: Epoch [5], Batch [819/938], Loss: 0.8702632188796997\n",
      "Train: Epoch [5], Batch [820/938], Loss: 1.1335521936416626\n",
      "Train: Epoch [5], Batch [821/938], Loss: 0.8033095002174377\n",
      "Train: Epoch [5], Batch [822/938], Loss: 0.9796406030654907\n",
      "Train: Epoch [5], Batch [823/938], Loss: 0.7598854899406433\n",
      "Train: Epoch [5], Batch [824/938], Loss: 0.9705222845077515\n",
      "Train: Epoch [5], Batch [825/938], Loss: 1.051428198814392\n",
      "Train: Epoch [5], Batch [826/938], Loss: 0.8479893207550049\n",
      "Train: Epoch [5], Batch [827/938], Loss: 0.9260637760162354\n",
      "Train: Epoch [5], Batch [828/938], Loss: 1.0604454278945923\n",
      "Train: Epoch [5], Batch [829/938], Loss: 0.9371511936187744\n",
      "Train: Epoch [5], Batch [830/938], Loss: 0.9575738906860352\n",
      "Train: Epoch [5], Batch [831/938], Loss: 0.9683272838592529\n",
      "Train: Epoch [5], Batch [832/938], Loss: 1.0129822492599487\n",
      "Train: Epoch [5], Batch [833/938], Loss: 1.1111000776290894\n",
      "Train: Epoch [5], Batch [834/938], Loss: 0.8948662281036377\n",
      "Train: Epoch [5], Batch [835/938], Loss: 0.8198493123054504\n",
      "Train: Epoch [5], Batch [836/938], Loss: 0.8816028833389282\n",
      "Train: Epoch [5], Batch [837/938], Loss: 1.0423667430877686\n",
      "Train: Epoch [5], Batch [838/938], Loss: 0.8931083679199219\n",
      "Train: Epoch [5], Batch [839/938], Loss: 1.287132978439331\n",
      "Train: Epoch [5], Batch [840/938], Loss: 0.8807327747344971\n",
      "Train: Epoch [5], Batch [841/938], Loss: 0.9622552990913391\n",
      "Train: Epoch [5], Batch [842/938], Loss: 1.22428560256958\n",
      "Train: Epoch [5], Batch [843/938], Loss: 0.9905585646629333\n",
      "Train: Epoch [5], Batch [844/938], Loss: 1.283043384552002\n",
      "Train: Epoch [5], Batch [845/938], Loss: 0.9000226259231567\n",
      "Train: Epoch [5], Batch [846/938], Loss: 0.999660074710846\n",
      "Train: Epoch [5], Batch [847/938], Loss: 0.9210748672485352\n",
      "Train: Epoch [5], Batch [848/938], Loss: 0.9698547720909119\n",
      "Train: Epoch [5], Batch [849/938], Loss: 0.9969579577445984\n",
      "Train: Epoch [5], Batch [850/938], Loss: 1.0305614471435547\n",
      "Train: Epoch [5], Batch [851/938], Loss: 0.9634352326393127\n",
      "Train: Epoch [5], Batch [852/938], Loss: 0.9305211305618286\n",
      "Train: Epoch [5], Batch [853/938], Loss: 0.8803492784500122\n",
      "Train: Epoch [5], Batch [854/938], Loss: 0.8741335272789001\n",
      "Train: Epoch [5], Batch [855/938], Loss: 1.0787334442138672\n",
      "Train: Epoch [5], Batch [856/938], Loss: 0.9979458451271057\n",
      "Train: Epoch [5], Batch [857/938], Loss: 1.0467495918273926\n",
      "Train: Epoch [5], Batch [858/938], Loss: 1.1653996706008911\n",
      "Train: Epoch [5], Batch [859/938], Loss: 0.9054993391036987\n",
      "Train: Epoch [5], Batch [860/938], Loss: 0.9925742149353027\n",
      "Train: Epoch [5], Batch [861/938], Loss: 0.9910740852355957\n",
      "Train: Epoch [5], Batch [862/938], Loss: 1.0712647438049316\n",
      "Train: Epoch [5], Batch [863/938], Loss: 1.2414164543151855\n",
      "Train: Epoch [5], Batch [864/938], Loss: 1.0182480812072754\n",
      "Train: Epoch [5], Batch [865/938], Loss: 0.8443209528923035\n",
      "Train: Epoch [5], Batch [866/938], Loss: 1.022359013557434\n",
      "Train: Epoch [5], Batch [867/938], Loss: 1.0598046779632568\n",
      "Train: Epoch [5], Batch [868/938], Loss: 1.144286870956421\n",
      "Train: Epoch [5], Batch [869/938], Loss: 0.9292606115341187\n",
      "Train: Epoch [5], Batch [870/938], Loss: 1.1955528259277344\n",
      "Train: Epoch [5], Batch [871/938], Loss: 1.0986182689666748\n",
      "Train: Epoch [5], Batch [872/938], Loss: 1.042569875717163\n",
      "Train: Epoch [5], Batch [873/938], Loss: 0.8863791823387146\n",
      "Train: Epoch [5], Batch [874/938], Loss: 0.718852698802948\n",
      "Train: Epoch [5], Batch [875/938], Loss: 0.9591670036315918\n",
      "Train: Epoch [5], Batch [876/938], Loss: 1.1162282228469849\n",
      "Train: Epoch [5], Batch [877/938], Loss: 1.1577064990997314\n",
      "Train: Epoch [5], Batch [878/938], Loss: 0.8775020837783813\n",
      "Train: Epoch [5], Batch [879/938], Loss: 0.7992061376571655\n",
      "Train: Epoch [5], Batch [880/938], Loss: 1.0613279342651367\n",
      "Train: Epoch [5], Batch [881/938], Loss: 1.064421534538269\n",
      "Train: Epoch [5], Batch [882/938], Loss: 1.0366504192352295\n",
      "Train: Epoch [5], Batch [883/938], Loss: 1.071845531463623\n",
      "Train: Epoch [5], Batch [884/938], Loss: 0.9898403286933899\n",
      "Train: Epoch [5], Batch [885/938], Loss: 0.930323600769043\n",
      "Train: Epoch [5], Batch [886/938], Loss: 1.1377487182617188\n",
      "Train: Epoch [5], Batch [887/938], Loss: 1.2370532751083374\n",
      "Train: Epoch [5], Batch [888/938], Loss: 1.089930534362793\n",
      "Train: Epoch [5], Batch [889/938], Loss: 0.8784292340278625\n",
      "Train: Epoch [5], Batch [890/938], Loss: 1.0106514692306519\n",
      "Train: Epoch [5], Batch [891/938], Loss: 1.0512160062789917\n",
      "Train: Epoch [5], Batch [892/938], Loss: 1.066449761390686\n",
      "Train: Epoch [5], Batch [893/938], Loss: 1.1864680051803589\n",
      "Train: Epoch [5], Batch [894/938], Loss: 0.9094263911247253\n",
      "Train: Epoch [5], Batch [895/938], Loss: 1.14547598361969\n",
      "Train: Epoch [5], Batch [896/938], Loss: 1.0945537090301514\n",
      "Train: Epoch [5], Batch [897/938], Loss: 0.9778072834014893\n",
      "Train: Epoch [5], Batch [898/938], Loss: 1.1466962099075317\n",
      "Train: Epoch [5], Batch [899/938], Loss: 0.9381767511367798\n",
      "Train: Epoch [5], Batch [900/938], Loss: 1.125731348991394\n",
      "Train: Epoch [5], Batch [901/938], Loss: 0.9897311329841614\n",
      "Train: Epoch [5], Batch [902/938], Loss: 1.0095776319503784\n",
      "Train: Epoch [5], Batch [903/938], Loss: 1.2261815071105957\n",
      "Train: Epoch [5], Batch [904/938], Loss: 0.9941325187683105\n",
      "Train: Epoch [5], Batch [905/938], Loss: 0.872270941734314\n",
      "Train: Epoch [5], Batch [906/938], Loss: 0.8466490507125854\n",
      "Train: Epoch [5], Batch [907/938], Loss: 0.9567516446113586\n",
      "Train: Epoch [5], Batch [908/938], Loss: 1.09903883934021\n",
      "Train: Epoch [5], Batch [909/938], Loss: 1.0825668573379517\n",
      "Train: Epoch [5], Batch [910/938], Loss: 0.9739689826965332\n",
      "Train: Epoch [5], Batch [911/938], Loss: 1.0195926427841187\n",
      "Train: Epoch [5], Batch [912/938], Loss: 1.0977519750595093\n",
      "Train: Epoch [5], Batch [913/938], Loss: 0.908355712890625\n",
      "Train: Epoch [5], Batch [914/938], Loss: 1.0415071249008179\n",
      "Train: Epoch [5], Batch [915/938], Loss: 1.1757006645202637\n",
      "Train: Epoch [5], Batch [916/938], Loss: 1.0561285018920898\n",
      "Train: Epoch [5], Batch [917/938], Loss: 1.029813528060913\n",
      "Train: Epoch [5], Batch [918/938], Loss: 1.1050854921340942\n",
      "Train: Epoch [5], Batch [919/938], Loss: 1.2251362800598145\n",
      "Train: Epoch [5], Batch [920/938], Loss: 1.142499566078186\n",
      "Train: Epoch [5], Batch [921/938], Loss: 1.2013578414916992\n",
      "Train: Epoch [5], Batch [922/938], Loss: 0.8896140456199646\n",
      "Train: Epoch [5], Batch [923/938], Loss: 0.8226040601730347\n",
      "Train: Epoch [5], Batch [924/938], Loss: 1.0154439210891724\n",
      "Train: Epoch [5], Batch [925/938], Loss: 0.8861149549484253\n",
      "Train: Epoch [5], Batch [926/938], Loss: 1.3308192491531372\n",
      "Train: Epoch [5], Batch [927/938], Loss: 0.8783932328224182\n",
      "Train: Epoch [5], Batch [928/938], Loss: 0.8503550887107849\n",
      "Train: Epoch [5], Batch [929/938], Loss: 1.096326470375061\n",
      "Train: Epoch [5], Batch [930/938], Loss: 1.1211206912994385\n",
      "Train: Epoch [5], Batch [931/938], Loss: 1.0933783054351807\n",
      "Train: Epoch [5], Batch [932/938], Loss: 1.0401428937911987\n",
      "Train: Epoch [5], Batch [933/938], Loss: 1.0824354887008667\n",
      "Train: Epoch [5], Batch [934/938], Loss: 1.2629876136779785\n",
      "Train: Epoch [5], Batch [935/938], Loss: 1.0063457489013672\n",
      "Train: Epoch [5], Batch [936/938], Loss: 0.8793331980705261\n",
      "Train: Epoch [5], Batch [937/938], Loss: 0.8111543655395508\n",
      "Train: Epoch [5], Batch [938/938], Loss: 1.1406550407409668\n",
      "Accuracy of train set: 0.58975\n",
      "Validation: Epoch [5], Batch [1/938], Loss: 1.094303846359253\n",
      "Validation: Epoch [5], Batch [2/938], Loss: 1.0026516914367676\n",
      "Validation: Epoch [5], Batch [3/938], Loss: 1.02791428565979\n",
      "Validation: Epoch [5], Batch [4/938], Loss: 0.7355592250823975\n",
      "Validation: Epoch [5], Batch [5/938], Loss: 1.1111960411071777\n",
      "Validation: Epoch [5], Batch [6/938], Loss: 1.0064467191696167\n",
      "Validation: Epoch [5], Batch [7/938], Loss: 1.0558730363845825\n",
      "Validation: Epoch [5], Batch [8/938], Loss: 1.1727170944213867\n",
      "Validation: Epoch [5], Batch [9/938], Loss: 0.9505109190940857\n",
      "Validation: Epoch [5], Batch [10/938], Loss: 1.058246374130249\n",
      "Validation: Epoch [5], Batch [11/938], Loss: 1.1154247522354126\n",
      "Validation: Epoch [5], Batch [12/938], Loss: 1.0328541994094849\n",
      "Validation: Epoch [5], Batch [13/938], Loss: 1.033672571182251\n",
      "Validation: Epoch [5], Batch [14/938], Loss: 0.953147292137146\n",
      "Validation: Epoch [5], Batch [15/938], Loss: 0.9917929768562317\n",
      "Validation: Epoch [5], Batch [16/938], Loss: 1.02910315990448\n",
      "Validation: Epoch [5], Batch [17/938], Loss: 0.9487501382827759\n",
      "Validation: Epoch [5], Batch [18/938], Loss: 1.0917085409164429\n",
      "Validation: Epoch [5], Batch [19/938], Loss: 1.3952921628952026\n",
      "Validation: Epoch [5], Batch [20/938], Loss: 1.009306788444519\n",
      "Validation: Epoch [5], Batch [21/938], Loss: 1.033972144126892\n",
      "Validation: Epoch [5], Batch [22/938], Loss: 1.0723721981048584\n",
      "Validation: Epoch [5], Batch [23/938], Loss: 0.7731664180755615\n",
      "Validation: Epoch [5], Batch [24/938], Loss: 1.016442894935608\n",
      "Validation: Epoch [5], Batch [25/938], Loss: 1.1390814781188965\n",
      "Validation: Epoch [5], Batch [26/938], Loss: 0.9634566307067871\n",
      "Validation: Epoch [5], Batch [27/938], Loss: 0.906013011932373\n",
      "Validation: Epoch [5], Batch [28/938], Loss: 1.1149919033050537\n",
      "Validation: Epoch [5], Batch [29/938], Loss: 0.9452148675918579\n",
      "Validation: Epoch [5], Batch [30/938], Loss: 1.0389515161514282\n",
      "Validation: Epoch [5], Batch [31/938], Loss: 1.1013710498809814\n",
      "Validation: Epoch [5], Batch [32/938], Loss: 0.8709815740585327\n",
      "Validation: Epoch [5], Batch [33/938], Loss: 0.888634979724884\n",
      "Validation: Epoch [5], Batch [34/938], Loss: 1.2033122777938843\n",
      "Validation: Epoch [5], Batch [35/938], Loss: 1.0054004192352295\n",
      "Validation: Epoch [5], Batch [36/938], Loss: 1.26629638671875\n",
      "Validation: Epoch [5], Batch [37/938], Loss: 1.0375044345855713\n",
      "Validation: Epoch [5], Batch [38/938], Loss: 1.0773842334747314\n",
      "Validation: Epoch [5], Batch [39/938], Loss: 0.9177408814430237\n",
      "Validation: Epoch [5], Batch [40/938], Loss: 0.9789104461669922\n",
      "Validation: Epoch [5], Batch [41/938], Loss: 1.0677433013916016\n",
      "Validation: Epoch [5], Batch [42/938], Loss: 1.1205012798309326\n",
      "Validation: Epoch [5], Batch [43/938], Loss: 0.9431463479995728\n",
      "Validation: Epoch [5], Batch [44/938], Loss: 1.2330496311187744\n",
      "Validation: Epoch [5], Batch [45/938], Loss: 1.0019304752349854\n",
      "Validation: Epoch [5], Batch [46/938], Loss: 1.1616919040679932\n",
      "Validation: Epoch [5], Batch [47/938], Loss: 1.0887451171875\n",
      "Validation: Epoch [5], Batch [48/938], Loss: 1.0540357828140259\n",
      "Validation: Epoch [5], Batch [49/938], Loss: 1.109104871749878\n",
      "Validation: Epoch [5], Batch [50/938], Loss: 0.9284591674804688\n",
      "Validation: Epoch [5], Batch [51/938], Loss: 0.9848593473434448\n",
      "Validation: Epoch [5], Batch [52/938], Loss: 1.1048097610473633\n",
      "Validation: Epoch [5], Batch [53/938], Loss: 1.1290782690048218\n",
      "Validation: Epoch [5], Batch [54/938], Loss: 0.8176775574684143\n",
      "Validation: Epoch [5], Batch [55/938], Loss: 0.89061039686203\n",
      "Validation: Epoch [5], Batch [56/938], Loss: 0.899707555770874\n",
      "Validation: Epoch [5], Batch [57/938], Loss: 0.7975877523422241\n",
      "Validation: Epoch [5], Batch [58/938], Loss: 0.9535462856292725\n",
      "Validation: Epoch [5], Batch [59/938], Loss: 0.9679625034332275\n",
      "Validation: Epoch [5], Batch [60/938], Loss: 1.2120496034622192\n",
      "Validation: Epoch [5], Batch [61/938], Loss: 0.9891505837440491\n",
      "Validation: Epoch [5], Batch [62/938], Loss: 0.9202725887298584\n",
      "Validation: Epoch [5], Batch [63/938], Loss: 1.0199435949325562\n",
      "Validation: Epoch [5], Batch [64/938], Loss: 1.0323083400726318\n",
      "Validation: Epoch [5], Batch [65/938], Loss: 1.051498293876648\n",
      "Validation: Epoch [5], Batch [66/938], Loss: 1.1352888345718384\n",
      "Validation: Epoch [5], Batch [67/938], Loss: 0.9358229637145996\n",
      "Validation: Epoch [5], Batch [68/938], Loss: 1.1157112121582031\n",
      "Validation: Epoch [5], Batch [69/938], Loss: 1.1033737659454346\n",
      "Validation: Epoch [5], Batch [70/938], Loss: 1.0447410345077515\n",
      "Validation: Epoch [5], Batch [71/938], Loss: 0.9449904561042786\n",
      "Validation: Epoch [5], Batch [72/938], Loss: 1.0545614957809448\n",
      "Validation: Epoch [5], Batch [73/938], Loss: 1.1879932880401611\n",
      "Validation: Epoch [5], Batch [74/938], Loss: 1.0459016561508179\n",
      "Validation: Epoch [5], Batch [75/938], Loss: 1.1817927360534668\n",
      "Validation: Epoch [5], Batch [76/938], Loss: 0.9222696423530579\n",
      "Validation: Epoch [5], Batch [77/938], Loss: 0.9634992480278015\n",
      "Validation: Epoch [5], Batch [78/938], Loss: 0.9834002256393433\n",
      "Validation: Epoch [5], Batch [79/938], Loss: 1.1542178392410278\n",
      "Validation: Epoch [5], Batch [80/938], Loss: 1.033488392829895\n",
      "Validation: Epoch [5], Batch [81/938], Loss: 1.1957224607467651\n",
      "Validation: Epoch [5], Batch [82/938], Loss: 1.0901914834976196\n",
      "Validation: Epoch [5], Batch [83/938], Loss: 1.055372714996338\n",
      "Validation: Epoch [5], Batch [84/938], Loss: 0.979579746723175\n",
      "Validation: Epoch [5], Batch [85/938], Loss: 1.0295771360397339\n",
      "Validation: Epoch [5], Batch [86/938], Loss: 0.9001996517181396\n",
      "Validation: Epoch [5], Batch [87/938], Loss: 1.0207929611206055\n",
      "Validation: Epoch [5], Batch [88/938], Loss: 0.9301921725273132\n",
      "Validation: Epoch [5], Batch [89/938], Loss: 1.1614124774932861\n",
      "Validation: Epoch [5], Batch [90/938], Loss: 0.9720668196678162\n",
      "Validation: Epoch [5], Batch [91/938], Loss: 0.9437952041625977\n",
      "Validation: Epoch [5], Batch [92/938], Loss: 0.9688682556152344\n",
      "Validation: Epoch [5], Batch [93/938], Loss: 1.2065743207931519\n",
      "Validation: Epoch [5], Batch [94/938], Loss: 0.9024314284324646\n",
      "Validation: Epoch [5], Batch [95/938], Loss: 0.9188767075538635\n",
      "Validation: Epoch [5], Batch [96/938], Loss: 1.0443801879882812\n",
      "Validation: Epoch [5], Batch [97/938], Loss: 0.9182065725326538\n",
      "Validation: Epoch [5], Batch [98/938], Loss: 1.286385416984558\n",
      "Validation: Epoch [5], Batch [99/938], Loss: 1.0613925457000732\n",
      "Validation: Epoch [5], Batch [100/938], Loss: 1.0431787967681885\n",
      "Validation: Epoch [5], Batch [101/938], Loss: 1.2296429872512817\n",
      "Validation: Epoch [5], Batch [102/938], Loss: 1.1232893466949463\n",
      "Validation: Epoch [5], Batch [103/938], Loss: 1.0397673845291138\n",
      "Validation: Epoch [5], Batch [104/938], Loss: 0.9222319722175598\n",
      "Validation: Epoch [5], Batch [105/938], Loss: 1.0313310623168945\n",
      "Validation: Epoch [5], Batch [106/938], Loss: 1.1588304042816162\n",
      "Validation: Epoch [5], Batch [107/938], Loss: 0.9105424284934998\n",
      "Validation: Epoch [5], Batch [108/938], Loss: 1.0119537115097046\n",
      "Validation: Epoch [5], Batch [109/938], Loss: 1.043672800064087\n",
      "Validation: Epoch [5], Batch [110/938], Loss: 0.9122264385223389\n",
      "Validation: Epoch [5], Batch [111/938], Loss: 1.1707441806793213\n",
      "Validation: Epoch [5], Batch [112/938], Loss: 1.1128469705581665\n",
      "Validation: Epoch [5], Batch [113/938], Loss: 1.2384376525878906\n",
      "Validation: Epoch [5], Batch [114/938], Loss: 1.020287275314331\n",
      "Validation: Epoch [5], Batch [115/938], Loss: 1.2292225360870361\n",
      "Validation: Epoch [5], Batch [116/938], Loss: 1.1842529773712158\n",
      "Validation: Epoch [5], Batch [117/938], Loss: 0.9287858009338379\n",
      "Validation: Epoch [5], Batch [118/938], Loss: 1.1962201595306396\n",
      "Validation: Epoch [5], Batch [119/938], Loss: 0.9338362216949463\n",
      "Validation: Epoch [5], Batch [120/938], Loss: 1.026146650314331\n",
      "Validation: Epoch [5], Batch [121/938], Loss: 1.0699830055236816\n",
      "Validation: Epoch [5], Batch [122/938], Loss: 1.0768646001815796\n",
      "Validation: Epoch [5], Batch [123/938], Loss: 1.088498592376709\n",
      "Validation: Epoch [5], Batch [124/938], Loss: 0.9197892546653748\n",
      "Validation: Epoch [5], Batch [125/938], Loss: 1.0138614177703857\n",
      "Validation: Epoch [5], Batch [126/938], Loss: 1.0230813026428223\n",
      "Validation: Epoch [5], Batch [127/938], Loss: 1.221818208694458\n",
      "Validation: Epoch [5], Batch [128/938], Loss: 0.8418185710906982\n",
      "Validation: Epoch [5], Batch [129/938], Loss: 1.1052578687667847\n",
      "Validation: Epoch [5], Batch [130/938], Loss: 1.2128264904022217\n",
      "Validation: Epoch [5], Batch [131/938], Loss: 1.066414475440979\n",
      "Validation: Epoch [5], Batch [132/938], Loss: 1.2614259719848633\n",
      "Validation: Epoch [5], Batch [133/938], Loss: 0.9126852750778198\n",
      "Validation: Epoch [5], Batch [134/938], Loss: 1.0482879877090454\n",
      "Validation: Epoch [5], Batch [135/938], Loss: 0.9926193952560425\n",
      "Validation: Epoch [5], Batch [136/938], Loss: 1.046589732170105\n",
      "Validation: Epoch [5], Batch [137/938], Loss: 1.058703899383545\n",
      "Validation: Epoch [5], Batch [138/938], Loss: 1.0352791547775269\n",
      "Validation: Epoch [5], Batch [139/938], Loss: 1.0030823945999146\n",
      "Validation: Epoch [5], Batch [140/938], Loss: 1.1680840253829956\n",
      "Validation: Epoch [5], Batch [141/938], Loss: 1.268970012664795\n",
      "Validation: Epoch [5], Batch [142/938], Loss: 1.1199347972869873\n",
      "Validation: Epoch [5], Batch [143/938], Loss: 0.9163154363632202\n",
      "Validation: Epoch [5], Batch [144/938], Loss: 0.8981227874755859\n",
      "Validation: Epoch [5], Batch [145/938], Loss: 0.9509270787239075\n",
      "Validation: Epoch [5], Batch [146/938], Loss: 1.4218885898590088\n",
      "Validation: Epoch [5], Batch [147/938], Loss: 1.0547295808792114\n",
      "Validation: Epoch [5], Batch [148/938], Loss: 1.0886540412902832\n",
      "Validation: Epoch [5], Batch [149/938], Loss: 1.0856231451034546\n",
      "Validation: Epoch [5], Batch [150/938], Loss: 1.0119065046310425\n",
      "Validation: Epoch [5], Batch [151/938], Loss: 0.9994932413101196\n",
      "Validation: Epoch [5], Batch [152/938], Loss: 0.9890748262405396\n",
      "Validation: Epoch [5], Batch [153/938], Loss: 0.725331723690033\n",
      "Validation: Epoch [5], Batch [154/938], Loss: 1.100938320159912\n",
      "Validation: Epoch [5], Batch [155/938], Loss: 1.0823577642440796\n",
      "Validation: Epoch [5], Batch [156/938], Loss: 1.2683888673782349\n",
      "Validation: Epoch [5], Batch [157/938], Loss: 1.024747610092163\n",
      "Validation: Epoch [5], Batch [158/938], Loss: 0.9976878762245178\n",
      "Validation: Epoch [5], Batch [159/938], Loss: 0.9139717817306519\n",
      "Validation: Epoch [5], Batch [160/938], Loss: 0.9794202446937561\n",
      "Validation: Epoch [5], Batch [161/938], Loss: 0.8544559478759766\n",
      "Validation: Epoch [5], Batch [162/938], Loss: 0.937666654586792\n",
      "Validation: Epoch [5], Batch [163/938], Loss: 1.1743876934051514\n",
      "Validation: Epoch [5], Batch [164/938], Loss: 1.025108814239502\n",
      "Validation: Epoch [5], Batch [165/938], Loss: 1.0920594930648804\n",
      "Validation: Epoch [5], Batch [166/938], Loss: 0.9659165740013123\n",
      "Validation: Epoch [5], Batch [167/938], Loss: 1.206052541732788\n",
      "Validation: Epoch [5], Batch [168/938], Loss: 1.357446551322937\n",
      "Validation: Epoch [5], Batch [169/938], Loss: 1.1851962804794312\n",
      "Validation: Epoch [5], Batch [170/938], Loss: 0.9017974734306335\n",
      "Validation: Epoch [5], Batch [171/938], Loss: 0.9124337434768677\n",
      "Validation: Epoch [5], Batch [172/938], Loss: 0.9941385984420776\n",
      "Validation: Epoch [5], Batch [173/938], Loss: 1.0343453884124756\n",
      "Validation: Epoch [5], Batch [174/938], Loss: 0.9534711241722107\n",
      "Validation: Epoch [5], Batch [175/938], Loss: 0.968622088432312\n",
      "Validation: Epoch [5], Batch [176/938], Loss: 1.0600436925888062\n",
      "Validation: Epoch [5], Batch [177/938], Loss: 1.0989367961883545\n",
      "Validation: Epoch [5], Batch [178/938], Loss: 1.0772496461868286\n",
      "Validation: Epoch [5], Batch [179/938], Loss: 0.9968079328536987\n",
      "Validation: Epoch [5], Batch [180/938], Loss: 1.066645622253418\n",
      "Validation: Epoch [5], Batch [181/938], Loss: 0.9855825901031494\n",
      "Validation: Epoch [5], Batch [182/938], Loss: 0.9415656328201294\n",
      "Validation: Epoch [5], Batch [183/938], Loss: 0.8963345289230347\n",
      "Validation: Epoch [5], Batch [184/938], Loss: 1.0212048292160034\n",
      "Validation: Epoch [5], Batch [185/938], Loss: 1.1311044692993164\n",
      "Validation: Epoch [5], Batch [186/938], Loss: 0.9985547065734863\n",
      "Validation: Epoch [5], Batch [187/938], Loss: 0.9342412948608398\n",
      "Validation: Epoch [5], Batch [188/938], Loss: 0.9446191787719727\n",
      "Validation: Epoch [5], Batch [189/938], Loss: 1.2546614408493042\n",
      "Validation: Epoch [5], Batch [190/938], Loss: 0.9072985649108887\n",
      "Validation: Epoch [5], Batch [191/938], Loss: 1.02751624584198\n",
      "Validation: Epoch [5], Batch [192/938], Loss: 0.9791638851165771\n",
      "Validation: Epoch [5], Batch [193/938], Loss: 0.9553605914115906\n",
      "Validation: Epoch [5], Batch [194/938], Loss: 1.0188754796981812\n",
      "Validation: Epoch [5], Batch [195/938], Loss: 1.1854212284088135\n",
      "Validation: Epoch [5], Batch [196/938], Loss: 1.090128779411316\n",
      "Validation: Epoch [5], Batch [197/938], Loss: 1.1090641021728516\n",
      "Validation: Epoch [5], Batch [198/938], Loss: 1.06830632686615\n",
      "Validation: Epoch [5], Batch [199/938], Loss: 0.9400801658630371\n",
      "Validation: Epoch [5], Batch [200/938], Loss: 0.9723109006881714\n",
      "Validation: Epoch [5], Batch [201/938], Loss: 0.7396689653396606\n",
      "Validation: Epoch [5], Batch [202/938], Loss: 1.1578420400619507\n",
      "Validation: Epoch [5], Batch [203/938], Loss: 1.0571949481964111\n",
      "Validation: Epoch [5], Batch [204/938], Loss: 0.9823787212371826\n",
      "Validation: Epoch [5], Batch [205/938], Loss: 0.8103095293045044\n",
      "Validation: Epoch [5], Batch [206/938], Loss: 1.0305325984954834\n",
      "Validation: Epoch [5], Batch [207/938], Loss: 1.2283103466033936\n",
      "Validation: Epoch [5], Batch [208/938], Loss: 0.8063634037971497\n",
      "Validation: Epoch [5], Batch [209/938], Loss: 1.0541715621948242\n",
      "Validation: Epoch [5], Batch [210/938], Loss: 1.206404447555542\n",
      "Validation: Epoch [5], Batch [211/938], Loss: 1.034315824508667\n",
      "Validation: Epoch [5], Batch [212/938], Loss: 1.140363097190857\n",
      "Validation: Epoch [5], Batch [213/938], Loss: 1.0220279693603516\n",
      "Validation: Epoch [5], Batch [214/938], Loss: 1.1029317378997803\n",
      "Validation: Epoch [5], Batch [215/938], Loss: 0.9437476396560669\n",
      "Validation: Epoch [5], Batch [216/938], Loss: 1.0793569087982178\n",
      "Validation: Epoch [5], Batch [217/938], Loss: 1.150078535079956\n",
      "Validation: Epoch [5], Batch [218/938], Loss: 1.1735625267028809\n",
      "Validation: Epoch [5], Batch [219/938], Loss: 1.2496100664138794\n",
      "Validation: Epoch [5], Batch [220/938], Loss: 0.8168929815292358\n",
      "Validation: Epoch [5], Batch [221/938], Loss: 1.126743197441101\n",
      "Validation: Epoch [5], Batch [222/938], Loss: 0.8522517681121826\n",
      "Validation: Epoch [5], Batch [223/938], Loss: 1.0532554388046265\n",
      "Validation: Epoch [5], Batch [224/938], Loss: 1.2925792932510376\n",
      "Validation: Epoch [5], Batch [225/938], Loss: 0.9443634152412415\n",
      "Validation: Epoch [5], Batch [226/938], Loss: 1.1230885982513428\n",
      "Validation: Epoch [5], Batch [227/938], Loss: 1.260511875152588\n",
      "Validation: Epoch [5], Batch [228/938], Loss: 1.2347053289413452\n",
      "Validation: Epoch [5], Batch [229/938], Loss: 1.153706669807434\n",
      "Validation: Epoch [5], Batch [230/938], Loss: 1.1038025617599487\n",
      "Validation: Epoch [5], Batch [231/938], Loss: 0.9637431502342224\n",
      "Validation: Epoch [5], Batch [232/938], Loss: 1.031874418258667\n",
      "Validation: Epoch [5], Batch [233/938], Loss: 0.9657863974571228\n",
      "Validation: Epoch [5], Batch [234/938], Loss: 0.9771921634674072\n",
      "Validation: Epoch [5], Batch [235/938], Loss: 0.9654459953308105\n",
      "Validation: Epoch [5], Batch [236/938], Loss: 1.066874384880066\n",
      "Validation: Epoch [5], Batch [237/938], Loss: 1.1500775814056396\n",
      "Validation: Epoch [5], Batch [238/938], Loss: 1.1412369012832642\n",
      "Validation: Epoch [5], Batch [239/938], Loss: 0.9423708915710449\n",
      "Validation: Epoch [5], Batch [240/938], Loss: 1.0134531259536743\n",
      "Validation: Epoch [5], Batch [241/938], Loss: 1.3019516468048096\n",
      "Validation: Epoch [5], Batch [242/938], Loss: 0.949044406414032\n",
      "Validation: Epoch [5], Batch [243/938], Loss: 0.9833725690841675\n",
      "Validation: Epoch [5], Batch [244/938], Loss: 1.245545744895935\n",
      "Validation: Epoch [5], Batch [245/938], Loss: 0.9392216801643372\n",
      "Validation: Epoch [5], Batch [246/938], Loss: 1.051517367362976\n",
      "Validation: Epoch [5], Batch [247/938], Loss: 0.8525394201278687\n",
      "Validation: Epoch [5], Batch [248/938], Loss: 1.0122966766357422\n",
      "Validation: Epoch [5], Batch [249/938], Loss: 0.9273673295974731\n",
      "Validation: Epoch [5], Batch [250/938], Loss: 0.970602810382843\n",
      "Validation: Epoch [5], Batch [251/938], Loss: 1.2622754573822021\n",
      "Validation: Epoch [5], Batch [252/938], Loss: 0.8537969589233398\n",
      "Validation: Epoch [5], Batch [253/938], Loss: 0.8902993202209473\n",
      "Validation: Epoch [5], Batch [254/938], Loss: 1.1533381938934326\n",
      "Validation: Epoch [5], Batch [255/938], Loss: 0.915189802646637\n",
      "Validation: Epoch [5], Batch [256/938], Loss: 0.9544693827629089\n",
      "Validation: Epoch [5], Batch [257/938], Loss: 1.0833054780960083\n",
      "Validation: Epoch [5], Batch [258/938], Loss: 1.07168710231781\n",
      "Validation: Epoch [5], Batch [259/938], Loss: 0.9150615930557251\n",
      "Validation: Epoch [5], Batch [260/938], Loss: 1.04623544216156\n",
      "Validation: Epoch [5], Batch [261/938], Loss: 0.9951542615890503\n",
      "Validation: Epoch [5], Batch [262/938], Loss: 0.9055238962173462\n",
      "Validation: Epoch [5], Batch [263/938], Loss: 1.1044551134109497\n",
      "Validation: Epoch [5], Batch [264/938], Loss: 1.1636524200439453\n",
      "Validation: Epoch [5], Batch [265/938], Loss: 1.1532233953475952\n",
      "Validation: Epoch [5], Batch [266/938], Loss: 0.8731144666671753\n",
      "Validation: Epoch [5], Batch [267/938], Loss: 1.0890142917633057\n",
      "Validation: Epoch [5], Batch [268/938], Loss: 1.0435190200805664\n",
      "Validation: Epoch [5], Batch [269/938], Loss: 1.1843369007110596\n",
      "Validation: Epoch [5], Batch [270/938], Loss: 1.183415412902832\n",
      "Validation: Epoch [5], Batch [271/938], Loss: 0.9103205800056458\n",
      "Validation: Epoch [5], Batch [272/938], Loss: 1.434016466140747\n",
      "Validation: Epoch [5], Batch [273/938], Loss: 1.020716667175293\n",
      "Validation: Epoch [5], Batch [274/938], Loss: 1.1156538724899292\n",
      "Validation: Epoch [5], Batch [275/938], Loss: 0.9490654468536377\n",
      "Validation: Epoch [5], Batch [276/938], Loss: 1.0166451930999756\n",
      "Validation: Epoch [5], Batch [277/938], Loss: 1.060640573501587\n",
      "Validation: Epoch [5], Batch [278/938], Loss: 1.2149521112442017\n",
      "Validation: Epoch [5], Batch [279/938], Loss: 1.1329492330551147\n",
      "Validation: Epoch [5], Batch [280/938], Loss: 0.9705888032913208\n",
      "Validation: Epoch [5], Batch [281/938], Loss: 1.0238412618637085\n",
      "Validation: Epoch [5], Batch [282/938], Loss: 0.9679552912712097\n",
      "Validation: Epoch [5], Batch [283/938], Loss: 0.9361807703971863\n",
      "Validation: Epoch [5], Batch [284/938], Loss: 0.7781323790550232\n",
      "Validation: Epoch [5], Batch [285/938], Loss: 1.2348541021347046\n",
      "Validation: Epoch [5], Batch [286/938], Loss: 0.896428108215332\n",
      "Validation: Epoch [5], Batch [287/938], Loss: 0.796409010887146\n",
      "Validation: Epoch [5], Batch [288/938], Loss: 1.0613598823547363\n",
      "Validation: Epoch [5], Batch [289/938], Loss: 0.9093384146690369\n",
      "Validation: Epoch [5], Batch [290/938], Loss: 0.9267248511314392\n",
      "Validation: Epoch [5], Batch [291/938], Loss: 0.905441164970398\n",
      "Validation: Epoch [5], Batch [292/938], Loss: 1.0078115463256836\n",
      "Validation: Epoch [5], Batch [293/938], Loss: 0.9888089895248413\n",
      "Validation: Epoch [5], Batch [294/938], Loss: 0.9982708692550659\n",
      "Validation: Epoch [5], Batch [295/938], Loss: 1.0621317625045776\n",
      "Validation: Epoch [5], Batch [296/938], Loss: 0.9740824699401855\n",
      "Validation: Epoch [5], Batch [297/938], Loss: 1.0670562982559204\n",
      "Validation: Epoch [5], Batch [298/938], Loss: 0.8613402843475342\n",
      "Validation: Epoch [5], Batch [299/938], Loss: 1.0098227262496948\n",
      "Validation: Epoch [5], Batch [300/938], Loss: 1.0455033779144287\n",
      "Validation: Epoch [5], Batch [301/938], Loss: 1.307373046875\n",
      "Validation: Epoch [5], Batch [302/938], Loss: 0.9090917110443115\n",
      "Validation: Epoch [5], Batch [303/938], Loss: 1.0228004455566406\n",
      "Validation: Epoch [5], Batch [304/938], Loss: 0.9913270473480225\n",
      "Validation: Epoch [5], Batch [305/938], Loss: 0.8479339480400085\n",
      "Validation: Epoch [5], Batch [306/938], Loss: 0.9864050149917603\n",
      "Validation: Epoch [5], Batch [307/938], Loss: 0.8685271739959717\n",
      "Validation: Epoch [5], Batch [308/938], Loss: 1.0146507024765015\n",
      "Validation: Epoch [5], Batch [309/938], Loss: 1.057887315750122\n",
      "Validation: Epoch [5], Batch [310/938], Loss: 0.9014928936958313\n",
      "Validation: Epoch [5], Batch [311/938], Loss: 1.1150654554367065\n",
      "Validation: Epoch [5], Batch [312/938], Loss: 1.0458347797393799\n",
      "Validation: Epoch [5], Batch [313/938], Loss: 1.011894702911377\n",
      "Validation: Epoch [5], Batch [314/938], Loss: 0.9257858991622925\n",
      "Validation: Epoch [5], Batch [315/938], Loss: 0.8263182640075684\n",
      "Validation: Epoch [5], Batch [316/938], Loss: 0.9518662095069885\n",
      "Validation: Epoch [5], Batch [317/938], Loss: 1.1441653966903687\n",
      "Validation: Epoch [5], Batch [318/938], Loss: 1.0299739837646484\n",
      "Validation: Epoch [5], Batch [319/938], Loss: 1.2127666473388672\n",
      "Validation: Epoch [5], Batch [320/938], Loss: 1.2383499145507812\n",
      "Validation: Epoch [5], Batch [321/938], Loss: 0.8733362555503845\n",
      "Validation: Epoch [5], Batch [322/938], Loss: 1.0390691757202148\n",
      "Validation: Epoch [5], Batch [323/938], Loss: 1.0717822313308716\n",
      "Validation: Epoch [5], Batch [324/938], Loss: 0.8949341773986816\n",
      "Validation: Epoch [5], Batch [325/938], Loss: 1.1213467121124268\n",
      "Validation: Epoch [5], Batch [326/938], Loss: 0.9761968851089478\n",
      "Validation: Epoch [5], Batch [327/938], Loss: 0.9953950047492981\n",
      "Validation: Epoch [5], Batch [328/938], Loss: 1.0156710147857666\n",
      "Validation: Epoch [5], Batch [329/938], Loss: 0.9841733574867249\n",
      "Validation: Epoch [5], Batch [330/938], Loss: 1.2412041425704956\n",
      "Validation: Epoch [5], Batch [331/938], Loss: 1.0870410203933716\n",
      "Validation: Epoch [5], Batch [332/938], Loss: 1.0537258386611938\n",
      "Validation: Epoch [5], Batch [333/938], Loss: 1.0933867692947388\n",
      "Validation: Epoch [5], Batch [334/938], Loss: 0.9983896613121033\n",
      "Validation: Epoch [5], Batch [335/938], Loss: 1.1821985244750977\n",
      "Validation: Epoch [5], Batch [336/938], Loss: 1.020107626914978\n",
      "Validation: Epoch [5], Batch [337/938], Loss: 1.0348111391067505\n",
      "Validation: Epoch [5], Batch [338/938], Loss: 0.7819551229476929\n",
      "Validation: Epoch [5], Batch [339/938], Loss: 0.9005458354949951\n",
      "Validation: Epoch [5], Batch [340/938], Loss: 1.0606939792633057\n",
      "Validation: Epoch [5], Batch [341/938], Loss: 1.0818392038345337\n",
      "Validation: Epoch [5], Batch [342/938], Loss: 0.7569196820259094\n",
      "Validation: Epoch [5], Batch [343/938], Loss: 0.9298492670059204\n",
      "Validation: Epoch [5], Batch [344/938], Loss: 0.9977149963378906\n",
      "Validation: Epoch [5], Batch [345/938], Loss: 0.8809043169021606\n",
      "Validation: Epoch [5], Batch [346/938], Loss: 0.918654203414917\n",
      "Validation: Epoch [5], Batch [347/938], Loss: 0.9616051316261292\n",
      "Validation: Epoch [5], Batch [348/938], Loss: 0.8870872259140015\n",
      "Validation: Epoch [5], Batch [349/938], Loss: 0.8015977144241333\n",
      "Validation: Epoch [5], Batch [350/938], Loss: 1.1912106275558472\n",
      "Validation: Epoch [5], Batch [351/938], Loss: 1.0157729387283325\n",
      "Validation: Epoch [5], Batch [352/938], Loss: 0.9659870862960815\n",
      "Validation: Epoch [5], Batch [353/938], Loss: 1.0597131252288818\n",
      "Validation: Epoch [5], Batch [354/938], Loss: 1.0904433727264404\n",
      "Validation: Epoch [5], Batch [355/938], Loss: 1.0408552885055542\n",
      "Validation: Epoch [5], Batch [356/938], Loss: 1.0328270196914673\n",
      "Validation: Epoch [5], Batch [357/938], Loss: 0.9185715913772583\n",
      "Validation: Epoch [5], Batch [358/938], Loss: 1.1958141326904297\n",
      "Validation: Epoch [5], Batch [359/938], Loss: 1.000760555267334\n",
      "Validation: Epoch [5], Batch [360/938], Loss: 1.0815520286560059\n",
      "Validation: Epoch [5], Batch [361/938], Loss: 0.9948329329490662\n",
      "Validation: Epoch [5], Batch [362/938], Loss: 1.1382046937942505\n",
      "Validation: Epoch [5], Batch [363/938], Loss: 1.0253515243530273\n",
      "Validation: Epoch [5], Batch [364/938], Loss: 1.006752610206604\n",
      "Validation: Epoch [5], Batch [365/938], Loss: 0.973560631275177\n",
      "Validation: Epoch [5], Batch [366/938], Loss: 1.0785455703735352\n",
      "Validation: Epoch [5], Batch [367/938], Loss: 0.9088169932365417\n",
      "Validation: Epoch [5], Batch [368/938], Loss: 1.14655601978302\n",
      "Validation: Epoch [5], Batch [369/938], Loss: 0.9730466604232788\n",
      "Validation: Epoch [5], Batch [370/938], Loss: 1.0865892171859741\n",
      "Validation: Epoch [5], Batch [371/938], Loss: 1.0071910619735718\n",
      "Validation: Epoch [5], Batch [372/938], Loss: 1.2237112522125244\n",
      "Validation: Epoch [5], Batch [373/938], Loss: 1.2419729232788086\n",
      "Validation: Epoch [5], Batch [374/938], Loss: 1.1802678108215332\n",
      "Validation: Epoch [5], Batch [375/938], Loss: 1.192899227142334\n",
      "Validation: Epoch [5], Batch [376/938], Loss: 1.1099584102630615\n",
      "Validation: Epoch [5], Batch [377/938], Loss: 1.372846245765686\n",
      "Validation: Epoch [5], Batch [378/938], Loss: 1.1364177465438843\n",
      "Validation: Epoch [5], Batch [379/938], Loss: 1.06484055519104\n",
      "Validation: Epoch [5], Batch [380/938], Loss: 1.1004327535629272\n",
      "Validation: Epoch [5], Batch [381/938], Loss: 1.017876386642456\n",
      "Validation: Epoch [5], Batch [382/938], Loss: 1.0699758529663086\n",
      "Validation: Epoch [5], Batch [383/938], Loss: 0.9561468362808228\n",
      "Validation: Epoch [5], Batch [384/938], Loss: 0.8669628500938416\n",
      "Validation: Epoch [5], Batch [385/938], Loss: 1.0685627460479736\n",
      "Validation: Epoch [5], Batch [386/938], Loss: 1.0109894275665283\n",
      "Validation: Epoch [5], Batch [387/938], Loss: 1.0550462007522583\n",
      "Validation: Epoch [5], Batch [388/938], Loss: 0.9071399569511414\n",
      "Validation: Epoch [5], Batch [389/938], Loss: 1.007887840270996\n",
      "Validation: Epoch [5], Batch [390/938], Loss: 1.1005969047546387\n",
      "Validation: Epoch [5], Batch [391/938], Loss: 1.2667683362960815\n",
      "Validation: Epoch [5], Batch [392/938], Loss: 0.9526763558387756\n",
      "Validation: Epoch [5], Batch [393/938], Loss: 1.1845020055770874\n",
      "Validation: Epoch [5], Batch [394/938], Loss: 1.0698498487472534\n",
      "Validation: Epoch [5], Batch [395/938], Loss: 1.0428518056869507\n",
      "Validation: Epoch [5], Batch [396/938], Loss: 0.973494827747345\n",
      "Validation: Epoch [5], Batch [397/938], Loss: 0.9382398128509521\n",
      "Validation: Epoch [5], Batch [398/938], Loss: 0.9910041093826294\n",
      "Validation: Epoch [5], Batch [399/938], Loss: 1.0844979286193848\n",
      "Validation: Epoch [5], Batch [400/938], Loss: 1.0066804885864258\n",
      "Validation: Epoch [5], Batch [401/938], Loss: 1.116209626197815\n",
      "Validation: Epoch [5], Batch [402/938], Loss: 1.1660759449005127\n",
      "Validation: Epoch [5], Batch [403/938], Loss: 1.1393053531646729\n",
      "Validation: Epoch [5], Batch [404/938], Loss: 1.1054307222366333\n",
      "Validation: Epoch [5], Batch [405/938], Loss: 1.0239267349243164\n",
      "Validation: Epoch [5], Batch [406/938], Loss: 0.9866321086883545\n",
      "Validation: Epoch [5], Batch [407/938], Loss: 1.1821818351745605\n",
      "Validation: Epoch [5], Batch [408/938], Loss: 1.1259294748306274\n",
      "Validation: Epoch [5], Batch [409/938], Loss: 1.2503383159637451\n",
      "Validation: Epoch [5], Batch [410/938], Loss: 0.9513007402420044\n",
      "Validation: Epoch [5], Batch [411/938], Loss: 0.9502736330032349\n",
      "Validation: Epoch [5], Batch [412/938], Loss: 0.8993156552314758\n",
      "Validation: Epoch [5], Batch [413/938], Loss: 1.0601974725723267\n",
      "Validation: Epoch [5], Batch [414/938], Loss: 0.984784722328186\n",
      "Validation: Epoch [5], Batch [415/938], Loss: 1.0896587371826172\n",
      "Validation: Epoch [5], Batch [416/938], Loss: 0.9507607221603394\n",
      "Validation: Epoch [5], Batch [417/938], Loss: 1.1515228748321533\n",
      "Validation: Epoch [5], Batch [418/938], Loss: 0.7968215942382812\n",
      "Validation: Epoch [5], Batch [419/938], Loss: 1.1629835367202759\n",
      "Validation: Epoch [5], Batch [420/938], Loss: 0.8377511501312256\n",
      "Validation: Epoch [5], Batch [421/938], Loss: 0.9740663170814514\n",
      "Validation: Epoch [5], Batch [422/938], Loss: 1.049917221069336\n",
      "Validation: Epoch [5], Batch [423/938], Loss: 1.2209224700927734\n",
      "Validation: Epoch [5], Batch [424/938], Loss: 1.0833008289337158\n",
      "Validation: Epoch [5], Batch [425/938], Loss: 0.9347318410873413\n",
      "Validation: Epoch [5], Batch [426/938], Loss: 1.216774582862854\n",
      "Validation: Epoch [5], Batch [427/938], Loss: 1.1453330516815186\n",
      "Validation: Epoch [5], Batch [428/938], Loss: 0.9247764348983765\n",
      "Validation: Epoch [5], Batch [429/938], Loss: 1.1852272748947144\n",
      "Validation: Epoch [5], Batch [430/938], Loss: 0.9497534036636353\n",
      "Validation: Epoch [5], Batch [431/938], Loss: 0.8567403554916382\n",
      "Validation: Epoch [5], Batch [432/938], Loss: 1.0910613536834717\n",
      "Validation: Epoch [5], Batch [433/938], Loss: 1.086047887802124\n",
      "Validation: Epoch [5], Batch [434/938], Loss: 1.057510256767273\n",
      "Validation: Epoch [5], Batch [435/938], Loss: 0.9784210920333862\n",
      "Validation: Epoch [5], Batch [436/938], Loss: 1.067487359046936\n",
      "Validation: Epoch [5], Batch [437/938], Loss: 1.017449140548706\n",
      "Validation: Epoch [5], Batch [438/938], Loss: 0.8587602376937866\n",
      "Validation: Epoch [5], Batch [439/938], Loss: 1.2452771663665771\n",
      "Validation: Epoch [5], Batch [440/938], Loss: 1.029536247253418\n",
      "Validation: Epoch [5], Batch [441/938], Loss: 1.177109956741333\n",
      "Validation: Epoch [5], Batch [442/938], Loss: 1.1973676681518555\n",
      "Validation: Epoch [5], Batch [443/938], Loss: 0.8693976998329163\n",
      "Validation: Epoch [5], Batch [444/938], Loss: 0.9882974624633789\n",
      "Validation: Epoch [5], Batch [445/938], Loss: 1.2917828559875488\n",
      "Validation: Epoch [5], Batch [446/938], Loss: 1.1023521423339844\n",
      "Validation: Epoch [5], Batch [447/938], Loss: 1.0690137147903442\n",
      "Validation: Epoch [5], Batch [448/938], Loss: 1.1878386735916138\n",
      "Validation: Epoch [5], Batch [449/938], Loss: 0.8748815059661865\n",
      "Validation: Epoch [5], Batch [450/938], Loss: 1.0479246377944946\n",
      "Validation: Epoch [5], Batch [451/938], Loss: 0.9520948529243469\n",
      "Validation: Epoch [5], Batch [452/938], Loss: 0.9672876596450806\n",
      "Validation: Epoch [5], Batch [453/938], Loss: 0.9428303241729736\n",
      "Validation: Epoch [5], Batch [454/938], Loss: 1.079411506652832\n",
      "Validation: Epoch [5], Batch [455/938], Loss: 0.9116235971450806\n",
      "Validation: Epoch [5], Batch [456/938], Loss: 1.003267765045166\n",
      "Validation: Epoch [5], Batch [457/938], Loss: 1.1317908763885498\n",
      "Validation: Epoch [5], Batch [458/938], Loss: 0.8303287625312805\n",
      "Validation: Epoch [5], Batch [459/938], Loss: 0.9899201989173889\n",
      "Validation: Epoch [5], Batch [460/938], Loss: 1.1054152250289917\n",
      "Validation: Epoch [5], Batch [461/938], Loss: 1.0905365943908691\n",
      "Validation: Epoch [5], Batch [462/938], Loss: 1.1367701292037964\n",
      "Validation: Epoch [5], Batch [463/938], Loss: 1.0278937816619873\n",
      "Validation: Epoch [5], Batch [464/938], Loss: 1.0599006414413452\n",
      "Validation: Epoch [5], Batch [465/938], Loss: 1.0147281885147095\n",
      "Validation: Epoch [5], Batch [466/938], Loss: 1.021337628364563\n",
      "Validation: Epoch [5], Batch [467/938], Loss: 1.0011476278305054\n",
      "Validation: Epoch [5], Batch [468/938], Loss: 1.0701792240142822\n",
      "Validation: Epoch [5], Batch [469/938], Loss: 1.105540156364441\n",
      "Validation: Epoch [5], Batch [470/938], Loss: 0.8882656097412109\n",
      "Validation: Epoch [5], Batch [471/938], Loss: 0.9784342646598816\n",
      "Validation: Epoch [5], Batch [472/938], Loss: 1.0721327066421509\n",
      "Validation: Epoch [5], Batch [473/938], Loss: 1.050202488899231\n",
      "Validation: Epoch [5], Batch [474/938], Loss: 1.1168936491012573\n",
      "Validation: Epoch [5], Batch [475/938], Loss: 1.0066261291503906\n",
      "Validation: Epoch [5], Batch [476/938], Loss: 1.114863634109497\n",
      "Validation: Epoch [5], Batch [477/938], Loss: 1.1602613925933838\n",
      "Validation: Epoch [5], Batch [478/938], Loss: 1.0639958381652832\n",
      "Validation: Epoch [5], Batch [479/938], Loss: 1.1023552417755127\n",
      "Validation: Epoch [5], Batch [480/938], Loss: 1.1315009593963623\n",
      "Validation: Epoch [5], Batch [481/938], Loss: 1.0905591249465942\n",
      "Validation: Epoch [5], Batch [482/938], Loss: 1.1387012004852295\n",
      "Validation: Epoch [5], Batch [483/938], Loss: 1.0500798225402832\n",
      "Validation: Epoch [5], Batch [484/938], Loss: 1.0039117336273193\n",
      "Validation: Epoch [5], Batch [485/938], Loss: 1.0120478868484497\n",
      "Validation: Epoch [5], Batch [486/938], Loss: 0.9940207004547119\n",
      "Validation: Epoch [5], Batch [487/938], Loss: 0.9845876693725586\n",
      "Validation: Epoch [5], Batch [488/938], Loss: 0.9538894295692444\n",
      "Validation: Epoch [5], Batch [489/938], Loss: 1.0690367221832275\n",
      "Validation: Epoch [5], Batch [490/938], Loss: 0.8934077024459839\n",
      "Validation: Epoch [5], Batch [491/938], Loss: 1.0091272592544556\n",
      "Validation: Epoch [5], Batch [492/938], Loss: 1.1473885774612427\n",
      "Validation: Epoch [5], Batch [493/938], Loss: 1.2018139362335205\n",
      "Validation: Epoch [5], Batch [494/938], Loss: 0.9454001188278198\n",
      "Validation: Epoch [5], Batch [495/938], Loss: 0.9408305883407593\n",
      "Validation: Epoch [5], Batch [496/938], Loss: 0.8934888243675232\n",
      "Validation: Epoch [5], Batch [497/938], Loss: 0.8726228475570679\n",
      "Validation: Epoch [5], Batch [498/938], Loss: 1.0081974267959595\n",
      "Validation: Epoch [5], Batch [499/938], Loss: 1.183147668838501\n",
      "Validation: Epoch [5], Batch [500/938], Loss: 1.1838184595108032\n",
      "Validation: Epoch [5], Batch [501/938], Loss: 1.0388048887252808\n",
      "Validation: Epoch [5], Batch [502/938], Loss: 1.033433198928833\n",
      "Validation: Epoch [5], Batch [503/938], Loss: 1.0777157545089722\n",
      "Validation: Epoch [5], Batch [504/938], Loss: 0.878781259059906\n",
      "Validation: Epoch [5], Batch [505/938], Loss: 1.0233393907546997\n",
      "Validation: Epoch [5], Batch [506/938], Loss: 1.1146143674850464\n",
      "Validation: Epoch [5], Batch [507/938], Loss: 0.7969662547111511\n",
      "Validation: Epoch [5], Batch [508/938], Loss: 1.1564981937408447\n",
      "Validation: Epoch [5], Batch [509/938], Loss: 1.0928823947906494\n",
      "Validation: Epoch [5], Batch [510/938], Loss: 1.0287599563598633\n",
      "Validation: Epoch [5], Batch [511/938], Loss: 1.1671310663223267\n",
      "Validation: Epoch [5], Batch [512/938], Loss: 0.9735708832740784\n",
      "Validation: Epoch [5], Batch [513/938], Loss: 1.175171136856079\n",
      "Validation: Epoch [5], Batch [514/938], Loss: 0.9136999845504761\n",
      "Validation: Epoch [5], Batch [515/938], Loss: 1.153329610824585\n",
      "Validation: Epoch [5], Batch [516/938], Loss: 0.7924919128417969\n",
      "Validation: Epoch [5], Batch [517/938], Loss: 1.257028341293335\n",
      "Validation: Epoch [5], Batch [518/938], Loss: 0.817252516746521\n",
      "Validation: Epoch [5], Batch [519/938], Loss: 0.9011995196342468\n",
      "Validation: Epoch [5], Batch [520/938], Loss: 1.0188977718353271\n",
      "Validation: Epoch [5], Batch [521/938], Loss: 0.8397634029388428\n",
      "Validation: Epoch [5], Batch [522/938], Loss: 1.0570333003997803\n",
      "Validation: Epoch [5], Batch [523/938], Loss: 0.964554488658905\n",
      "Validation: Epoch [5], Batch [524/938], Loss: 1.0222035646438599\n",
      "Validation: Epoch [5], Batch [525/938], Loss: 0.9876444339752197\n",
      "Validation: Epoch [5], Batch [526/938], Loss: 1.1339366436004639\n",
      "Validation: Epoch [5], Batch [527/938], Loss: 0.9179134368896484\n",
      "Validation: Epoch [5], Batch [528/938], Loss: 1.217853307723999\n",
      "Validation: Epoch [5], Batch [529/938], Loss: 0.9320011734962463\n",
      "Validation: Epoch [5], Batch [530/938], Loss: 1.1182829141616821\n",
      "Validation: Epoch [5], Batch [531/938], Loss: 1.0253212451934814\n",
      "Validation: Epoch [5], Batch [532/938], Loss: 0.9977313280105591\n",
      "Validation: Epoch [5], Batch [533/938], Loss: 0.9913654923439026\n",
      "Validation: Epoch [5], Batch [534/938], Loss: 1.0491822957992554\n",
      "Validation: Epoch [5], Batch [535/938], Loss: 1.1024407148361206\n",
      "Validation: Epoch [5], Batch [536/938], Loss: 1.1841903924942017\n",
      "Validation: Epoch [5], Batch [537/938], Loss: 0.8718543648719788\n",
      "Validation: Epoch [5], Batch [538/938], Loss: 1.0012112855911255\n",
      "Validation: Epoch [5], Batch [539/938], Loss: 1.0404165983200073\n",
      "Validation: Epoch [5], Batch [540/938], Loss: 0.9056286215782166\n",
      "Validation: Epoch [5], Batch [541/938], Loss: 1.1196776628494263\n",
      "Validation: Epoch [5], Batch [542/938], Loss: 1.0413380861282349\n",
      "Validation: Epoch [5], Batch [543/938], Loss: 0.9908934235572815\n",
      "Validation: Epoch [5], Batch [544/938], Loss: 1.072627067565918\n",
      "Validation: Epoch [5], Batch [545/938], Loss: 0.8127771019935608\n",
      "Validation: Epoch [5], Batch [546/938], Loss: 0.984524667263031\n",
      "Validation: Epoch [5], Batch [547/938], Loss: 1.187456488609314\n",
      "Validation: Epoch [5], Batch [548/938], Loss: 0.9583282470703125\n",
      "Validation: Epoch [5], Batch [549/938], Loss: 1.1564314365386963\n",
      "Validation: Epoch [5], Batch [550/938], Loss: 1.2526609897613525\n",
      "Validation: Epoch [5], Batch [551/938], Loss: 1.0775362253189087\n",
      "Validation: Epoch [5], Batch [552/938], Loss: 1.0864191055297852\n",
      "Validation: Epoch [5], Batch [553/938], Loss: 0.9569398760795593\n",
      "Validation: Epoch [5], Batch [554/938], Loss: 0.9055227041244507\n",
      "Validation: Epoch [5], Batch [555/938], Loss: 0.8874144554138184\n",
      "Validation: Epoch [5], Batch [556/938], Loss: 1.0615853071212769\n",
      "Validation: Epoch [5], Batch [557/938], Loss: 1.099520206451416\n",
      "Validation: Epoch [5], Batch [558/938], Loss: 1.0784902572631836\n",
      "Validation: Epoch [5], Batch [559/938], Loss: 0.9285724759101868\n",
      "Validation: Epoch [5], Batch [560/938], Loss: 0.96295166015625\n",
      "Validation: Epoch [5], Batch [561/938], Loss: 1.1061545610427856\n",
      "Validation: Epoch [5], Batch [562/938], Loss: 0.9187721014022827\n",
      "Validation: Epoch [5], Batch [563/938], Loss: 0.9847233295440674\n",
      "Validation: Epoch [5], Batch [564/938], Loss: 0.9338850378990173\n",
      "Validation: Epoch [5], Batch [565/938], Loss: 0.9195587038993835\n",
      "Validation: Epoch [5], Batch [566/938], Loss: 1.054824948310852\n",
      "Validation: Epoch [5], Batch [567/938], Loss: 1.310474157333374\n",
      "Validation: Epoch [5], Batch [568/938], Loss: 1.1523396968841553\n",
      "Validation: Epoch [5], Batch [569/938], Loss: 0.8960533738136292\n",
      "Validation: Epoch [5], Batch [570/938], Loss: 0.90565425157547\n",
      "Validation: Epoch [5], Batch [571/938], Loss: 1.114464282989502\n",
      "Validation: Epoch [5], Batch [572/938], Loss: 0.813002347946167\n",
      "Validation: Epoch [5], Batch [573/938], Loss: 1.020355224609375\n",
      "Validation: Epoch [5], Batch [574/938], Loss: 0.9924301505088806\n",
      "Validation: Epoch [5], Batch [575/938], Loss: 1.0980249643325806\n",
      "Validation: Epoch [5], Batch [576/938], Loss: 1.0797380208969116\n",
      "Validation: Epoch [5], Batch [577/938], Loss: 0.8891947865486145\n",
      "Validation: Epoch [5], Batch [578/938], Loss: 1.1089205741882324\n",
      "Validation: Epoch [5], Batch [579/938], Loss: 0.896246075630188\n",
      "Validation: Epoch [5], Batch [580/938], Loss: 1.0607315301895142\n",
      "Validation: Epoch [5], Batch [581/938], Loss: 0.931961178779602\n",
      "Validation: Epoch [5], Batch [582/938], Loss: 0.9686100482940674\n",
      "Validation: Epoch [5], Batch [583/938], Loss: 0.9614188075065613\n",
      "Validation: Epoch [5], Batch [584/938], Loss: 0.9759100079536438\n",
      "Validation: Epoch [5], Batch [585/938], Loss: 0.9962949156761169\n",
      "Validation: Epoch [5], Batch [586/938], Loss: 1.0418808460235596\n",
      "Validation: Epoch [5], Batch [587/938], Loss: 1.1667065620422363\n",
      "Validation: Epoch [5], Batch [588/938], Loss: 0.869516134262085\n",
      "Validation: Epoch [5], Batch [589/938], Loss: 1.172868251800537\n",
      "Validation: Epoch [5], Batch [590/938], Loss: 0.9655006527900696\n",
      "Validation: Epoch [5], Batch [591/938], Loss: 0.9905792474746704\n",
      "Validation: Epoch [5], Batch [592/938], Loss: 1.1052703857421875\n",
      "Validation: Epoch [5], Batch [593/938], Loss: 0.9245097041130066\n",
      "Validation: Epoch [5], Batch [594/938], Loss: 1.134049892425537\n",
      "Validation: Epoch [5], Batch [595/938], Loss: 1.2600384950637817\n",
      "Validation: Epoch [5], Batch [596/938], Loss: 0.9576062560081482\n",
      "Validation: Epoch [5], Batch [597/938], Loss: 0.829975962638855\n",
      "Validation: Epoch [5], Batch [598/938], Loss: 1.0517873764038086\n",
      "Validation: Epoch [5], Batch [599/938], Loss: 1.0659273862838745\n",
      "Validation: Epoch [5], Batch [600/938], Loss: 0.9847427606582642\n",
      "Validation: Epoch [5], Batch [601/938], Loss: 0.9662512540817261\n",
      "Validation: Epoch [5], Batch [602/938], Loss: 0.9971705079078674\n",
      "Validation: Epoch [5], Batch [603/938], Loss: 1.0765259265899658\n",
      "Validation: Epoch [5], Batch [604/938], Loss: 1.1890950202941895\n",
      "Validation: Epoch [5], Batch [605/938], Loss: 1.1840561628341675\n",
      "Validation: Epoch [5], Batch [606/938], Loss: 0.9313750863075256\n",
      "Validation: Epoch [5], Batch [607/938], Loss: 0.9604992270469666\n",
      "Validation: Epoch [5], Batch [608/938], Loss: 1.0079928636550903\n",
      "Validation: Epoch [5], Batch [609/938], Loss: 1.035388708114624\n",
      "Validation: Epoch [5], Batch [610/938], Loss: 1.0372072458267212\n",
      "Validation: Epoch [5], Batch [611/938], Loss: 0.9747974276542664\n",
      "Validation: Epoch [5], Batch [612/938], Loss: 1.2018383741378784\n",
      "Validation: Epoch [5], Batch [613/938], Loss: 1.0178683996200562\n",
      "Validation: Epoch [5], Batch [614/938], Loss: 0.8670478463172913\n",
      "Validation: Epoch [5], Batch [615/938], Loss: 0.918035626411438\n",
      "Validation: Epoch [5], Batch [616/938], Loss: 1.002553105354309\n",
      "Validation: Epoch [5], Batch [617/938], Loss: 0.8453163504600525\n",
      "Validation: Epoch [5], Batch [618/938], Loss: 1.085669755935669\n",
      "Validation: Epoch [5], Batch [619/938], Loss: 1.1674644947052002\n",
      "Validation: Epoch [5], Batch [620/938], Loss: 0.9579228162765503\n",
      "Validation: Epoch [5], Batch [621/938], Loss: 1.0864781141281128\n",
      "Validation: Epoch [5], Batch [622/938], Loss: 0.9164575934410095\n",
      "Validation: Epoch [5], Batch [623/938], Loss: 0.9369933009147644\n",
      "Validation: Epoch [5], Batch [624/938], Loss: 1.0057628154754639\n",
      "Validation: Epoch [5], Batch [625/938], Loss: 1.1267778873443604\n",
      "Validation: Epoch [5], Batch [626/938], Loss: 1.027856707572937\n",
      "Validation: Epoch [5], Batch [627/938], Loss: 0.9998522996902466\n",
      "Validation: Epoch [5], Batch [628/938], Loss: 0.9555999040603638\n",
      "Validation: Epoch [5], Batch [629/938], Loss: 1.219586968421936\n",
      "Validation: Epoch [5], Batch [630/938], Loss: 1.086937427520752\n",
      "Validation: Epoch [5], Batch [631/938], Loss: 1.080417513847351\n",
      "Validation: Epoch [5], Batch [632/938], Loss: 0.9043668508529663\n",
      "Validation: Epoch [5], Batch [633/938], Loss: 1.0450925827026367\n",
      "Validation: Epoch [5], Batch [634/938], Loss: 1.1563401222229004\n",
      "Validation: Epoch [5], Batch [635/938], Loss: 1.3231627941131592\n",
      "Validation: Epoch [5], Batch [636/938], Loss: 0.9774428606033325\n",
      "Validation: Epoch [5], Batch [637/938], Loss: 1.0855954885482788\n",
      "Validation: Epoch [5], Batch [638/938], Loss: 1.022457242012024\n",
      "Validation: Epoch [5], Batch [639/938], Loss: 0.930656909942627\n",
      "Validation: Epoch [5], Batch [640/938], Loss: 1.0019255876541138\n",
      "Validation: Epoch [5], Batch [641/938], Loss: 0.8856004476547241\n",
      "Validation: Epoch [5], Batch [642/938], Loss: 0.9811469912528992\n",
      "Validation: Epoch [5], Batch [643/938], Loss: 1.1756079196929932\n",
      "Validation: Epoch [5], Batch [644/938], Loss: 1.2057645320892334\n",
      "Validation: Epoch [5], Batch [645/938], Loss: 1.1642084121704102\n",
      "Validation: Epoch [5], Batch [646/938], Loss: 0.9281821250915527\n",
      "Validation: Epoch [5], Batch [647/938], Loss: 1.3290916681289673\n",
      "Validation: Epoch [5], Batch [648/938], Loss: 0.952947199344635\n",
      "Validation: Epoch [5], Batch [649/938], Loss: 0.9424490928649902\n",
      "Validation: Epoch [5], Batch [650/938], Loss: 1.158360242843628\n",
      "Validation: Epoch [5], Batch [651/938], Loss: 0.8940410017967224\n",
      "Validation: Epoch [5], Batch [652/938], Loss: 0.9861670136451721\n",
      "Validation: Epoch [5], Batch [653/938], Loss: 0.9677797555923462\n",
      "Validation: Epoch [5], Batch [654/938], Loss: 0.8356627225875854\n",
      "Validation: Epoch [5], Batch [655/938], Loss: 0.9162136316299438\n",
      "Validation: Epoch [5], Batch [656/938], Loss: 1.2516695261001587\n",
      "Validation: Epoch [5], Batch [657/938], Loss: 1.1031724214553833\n",
      "Validation: Epoch [5], Batch [658/938], Loss: 0.9637795686721802\n",
      "Validation: Epoch [5], Batch [659/938], Loss: 1.1708338260650635\n",
      "Validation: Epoch [5], Batch [660/938], Loss: 1.0882179737091064\n",
      "Validation: Epoch [5], Batch [661/938], Loss: 1.0538078546524048\n",
      "Validation: Epoch [5], Batch [662/938], Loss: 0.9284225106239319\n",
      "Validation: Epoch [5], Batch [663/938], Loss: 1.546484351158142\n",
      "Validation: Epoch [5], Batch [664/938], Loss: 1.0033107995986938\n",
      "Validation: Epoch [5], Batch [665/938], Loss: 1.0658988952636719\n",
      "Validation: Epoch [5], Batch [666/938], Loss: 1.1369805335998535\n",
      "Validation: Epoch [5], Batch [667/938], Loss: 0.984974205493927\n",
      "Validation: Epoch [5], Batch [668/938], Loss: 1.014818787574768\n",
      "Validation: Epoch [5], Batch [669/938], Loss: 1.0520178079605103\n",
      "Validation: Epoch [5], Batch [670/938], Loss: 1.1038028001785278\n",
      "Validation: Epoch [5], Batch [671/938], Loss: 1.0816855430603027\n",
      "Validation: Epoch [5], Batch [672/938], Loss: 1.1172280311584473\n",
      "Validation: Epoch [5], Batch [673/938], Loss: 0.9971503019332886\n",
      "Validation: Epoch [5], Batch [674/938], Loss: 0.9522833228111267\n",
      "Validation: Epoch [5], Batch [675/938], Loss: 1.0882914066314697\n",
      "Validation: Epoch [5], Batch [676/938], Loss: 0.9757254719734192\n",
      "Validation: Epoch [5], Batch [677/938], Loss: 1.1863551139831543\n",
      "Validation: Epoch [5], Batch [678/938], Loss: 1.02249276638031\n",
      "Validation: Epoch [5], Batch [679/938], Loss: 1.0597039461135864\n",
      "Validation: Epoch [5], Batch [680/938], Loss: 0.8826025128364563\n",
      "Validation: Epoch [5], Batch [681/938], Loss: 1.1082367897033691\n",
      "Validation: Epoch [5], Batch [682/938], Loss: 1.0127414464950562\n",
      "Validation: Epoch [5], Batch [683/938], Loss: 1.0227797031402588\n",
      "Validation: Epoch [5], Batch [684/938], Loss: 0.9052566289901733\n",
      "Validation: Epoch [5], Batch [685/938], Loss: 1.217078447341919\n",
      "Validation: Epoch [5], Batch [686/938], Loss: 0.8941500186920166\n",
      "Validation: Epoch [5], Batch [687/938], Loss: 0.9757745265960693\n",
      "Validation: Epoch [5], Batch [688/938], Loss: 0.94273841381073\n",
      "Validation: Epoch [5], Batch [689/938], Loss: 1.0135844945907593\n",
      "Validation: Epoch [5], Batch [690/938], Loss: 1.1078944206237793\n",
      "Validation: Epoch [5], Batch [691/938], Loss: 1.0305445194244385\n",
      "Validation: Epoch [5], Batch [692/938], Loss: 1.0268157720565796\n",
      "Validation: Epoch [5], Batch [693/938], Loss: 1.1287717819213867\n",
      "Validation: Epoch [5], Batch [694/938], Loss: 1.051612377166748\n",
      "Validation: Epoch [5], Batch [695/938], Loss: 0.9038935899734497\n",
      "Validation: Epoch [5], Batch [696/938], Loss: 0.8644676208496094\n",
      "Validation: Epoch [5], Batch [697/938], Loss: 1.178054690361023\n",
      "Validation: Epoch [5], Batch [698/938], Loss: 1.166884422302246\n",
      "Validation: Epoch [5], Batch [699/938], Loss: 1.0868219137191772\n",
      "Validation: Epoch [5], Batch [700/938], Loss: 1.1961524486541748\n",
      "Validation: Epoch [5], Batch [701/938], Loss: 1.0754119157791138\n",
      "Validation: Epoch [5], Batch [702/938], Loss: 0.9681816697120667\n",
      "Validation: Epoch [5], Batch [703/938], Loss: 0.9736459255218506\n",
      "Validation: Epoch [5], Batch [704/938], Loss: 1.146299123764038\n",
      "Validation: Epoch [5], Batch [705/938], Loss: 1.014269232749939\n",
      "Validation: Epoch [5], Batch [706/938], Loss: 0.9347835779190063\n",
      "Validation: Epoch [5], Batch [707/938], Loss: 0.9651880860328674\n",
      "Validation: Epoch [5], Batch [708/938], Loss: 0.9575113654136658\n",
      "Validation: Epoch [5], Batch [709/938], Loss: 1.0040215253829956\n",
      "Validation: Epoch [5], Batch [710/938], Loss: 1.2657525539398193\n",
      "Validation: Epoch [5], Batch [711/938], Loss: 1.042312741279602\n",
      "Validation: Epoch [5], Batch [712/938], Loss: 1.0367131233215332\n",
      "Validation: Epoch [5], Batch [713/938], Loss: 1.1188699007034302\n",
      "Validation: Epoch [5], Batch [714/938], Loss: 1.021735429763794\n",
      "Validation: Epoch [5], Batch [715/938], Loss: 1.234231948852539\n",
      "Validation: Epoch [5], Batch [716/938], Loss: 1.2162024974822998\n",
      "Validation: Epoch [5], Batch [717/938], Loss: 0.9445757865905762\n",
      "Validation: Epoch [5], Batch [718/938], Loss: 1.0123652219772339\n",
      "Validation: Epoch [5], Batch [719/938], Loss: 0.957309365272522\n",
      "Validation: Epoch [5], Batch [720/938], Loss: 1.071323037147522\n",
      "Validation: Epoch [5], Batch [721/938], Loss: 1.1334640979766846\n",
      "Validation: Epoch [5], Batch [722/938], Loss: 1.007635235786438\n",
      "Validation: Epoch [5], Batch [723/938], Loss: 1.2810142040252686\n",
      "Validation: Epoch [5], Batch [724/938], Loss: 1.085760235786438\n",
      "Validation: Epoch [5], Batch [725/938], Loss: 0.8732864856719971\n",
      "Validation: Epoch [5], Batch [726/938], Loss: 0.927653968334198\n",
      "Validation: Epoch [5], Batch [727/938], Loss: 0.7996649742126465\n",
      "Validation: Epoch [5], Batch [728/938], Loss: 1.1714255809783936\n",
      "Validation: Epoch [5], Batch [729/938], Loss: 1.0274016857147217\n",
      "Validation: Epoch [5], Batch [730/938], Loss: 1.0890637636184692\n",
      "Validation: Epoch [5], Batch [731/938], Loss: 0.8839002251625061\n",
      "Validation: Epoch [5], Batch [732/938], Loss: 0.9637606143951416\n",
      "Validation: Epoch [5], Batch [733/938], Loss: 0.9459767937660217\n",
      "Validation: Epoch [5], Batch [734/938], Loss: 1.044562816619873\n",
      "Validation: Epoch [5], Batch [735/938], Loss: 1.1355055570602417\n",
      "Validation: Epoch [5], Batch [736/938], Loss: 1.1149576902389526\n",
      "Validation: Epoch [5], Batch [737/938], Loss: 1.010802984237671\n",
      "Validation: Epoch [5], Batch [738/938], Loss: 0.8640888929367065\n",
      "Validation: Epoch [5], Batch [739/938], Loss: 0.9947604537010193\n",
      "Validation: Epoch [5], Batch [740/938], Loss: 1.218079686164856\n",
      "Validation: Epoch [5], Batch [741/938], Loss: 0.8568349480628967\n",
      "Validation: Epoch [5], Batch [742/938], Loss: 1.1019548177719116\n",
      "Validation: Epoch [5], Batch [743/938], Loss: 1.4448469877243042\n",
      "Validation: Epoch [5], Batch [744/938], Loss: 1.0796494483947754\n",
      "Validation: Epoch [5], Batch [745/938], Loss: 1.241251826286316\n",
      "Validation: Epoch [5], Batch [746/938], Loss: 1.1535007953643799\n",
      "Validation: Epoch [5], Batch [747/938], Loss: 0.9865404367446899\n",
      "Validation: Epoch [5], Batch [748/938], Loss: 1.079338550567627\n",
      "Validation: Epoch [5], Batch [749/938], Loss: 0.9666059017181396\n",
      "Validation: Epoch [5], Batch [750/938], Loss: 1.0692517757415771\n",
      "Validation: Epoch [5], Batch [751/938], Loss: 0.9609279632568359\n",
      "Validation: Epoch [5], Batch [752/938], Loss: 1.0868417024612427\n",
      "Validation: Epoch [5], Batch [753/938], Loss: 0.9745134115219116\n",
      "Validation: Epoch [5], Batch [754/938], Loss: 0.9231546521186829\n",
      "Validation: Epoch [5], Batch [755/938], Loss: 0.9531631469726562\n",
      "Validation: Epoch [5], Batch [756/938], Loss: 1.0917969942092896\n",
      "Validation: Epoch [5], Batch [757/938], Loss: 1.124619483947754\n",
      "Validation: Epoch [5], Batch [758/938], Loss: 0.9819570779800415\n",
      "Validation: Epoch [5], Batch [759/938], Loss: 0.9252521991729736\n",
      "Validation: Epoch [5], Batch [760/938], Loss: 1.1057567596435547\n",
      "Validation: Epoch [5], Batch [761/938], Loss: 1.0196282863616943\n",
      "Validation: Epoch [5], Batch [762/938], Loss: 1.1436108350753784\n",
      "Validation: Epoch [5], Batch [763/938], Loss: 1.2164897918701172\n",
      "Validation: Epoch [5], Batch [764/938], Loss: 1.13039231300354\n",
      "Validation: Epoch [5], Batch [765/938], Loss: 1.0147889852523804\n",
      "Validation: Epoch [5], Batch [766/938], Loss: 1.134377360343933\n",
      "Validation: Epoch [5], Batch [767/938], Loss: 0.9480021595954895\n",
      "Validation: Epoch [5], Batch [768/938], Loss: 0.9900088906288147\n",
      "Validation: Epoch [5], Batch [769/938], Loss: 1.0447218418121338\n",
      "Validation: Epoch [5], Batch [770/938], Loss: 1.220738410949707\n",
      "Validation: Epoch [5], Batch [771/938], Loss: 1.2143422365188599\n",
      "Validation: Epoch [5], Batch [772/938], Loss: 0.9901145100593567\n",
      "Validation: Epoch [5], Batch [773/938], Loss: 0.8248606324195862\n",
      "Validation: Epoch [5], Batch [774/938], Loss: 0.8929282426834106\n",
      "Validation: Epoch [5], Batch [775/938], Loss: 1.135319471359253\n",
      "Validation: Epoch [5], Batch [776/938], Loss: 0.9857136011123657\n",
      "Validation: Epoch [5], Batch [777/938], Loss: 0.7781742811203003\n",
      "Validation: Epoch [5], Batch [778/938], Loss: 0.9160058498382568\n",
      "Validation: Epoch [5], Batch [779/938], Loss: 1.0799126625061035\n",
      "Validation: Epoch [5], Batch [780/938], Loss: 1.0266071557998657\n",
      "Validation: Epoch [5], Batch [781/938], Loss: 0.9638720750808716\n",
      "Validation: Epoch [5], Batch [782/938], Loss: 0.9783259630203247\n",
      "Validation: Epoch [5], Batch [783/938], Loss: 0.8636850118637085\n",
      "Validation: Epoch [5], Batch [784/938], Loss: 1.000534176826477\n",
      "Validation: Epoch [5], Batch [785/938], Loss: 0.8708151578903198\n",
      "Validation: Epoch [5], Batch [786/938], Loss: 1.1697673797607422\n",
      "Validation: Epoch [5], Batch [787/938], Loss: 1.1457278728485107\n",
      "Validation: Epoch [5], Batch [788/938], Loss: 0.8422133326530457\n",
      "Validation: Epoch [5], Batch [789/938], Loss: 1.087904453277588\n",
      "Validation: Epoch [5], Batch [790/938], Loss: 1.0391323566436768\n",
      "Validation: Epoch [5], Batch [791/938], Loss: 0.8452960848808289\n",
      "Validation: Epoch [5], Batch [792/938], Loss: 1.1598718166351318\n",
      "Validation: Epoch [5], Batch [793/938], Loss: 0.9294995069503784\n",
      "Validation: Epoch [5], Batch [794/938], Loss: 0.9723377227783203\n",
      "Validation: Epoch [5], Batch [795/938], Loss: 0.9570795297622681\n",
      "Validation: Epoch [5], Batch [796/938], Loss: 1.070222020149231\n",
      "Validation: Epoch [5], Batch [797/938], Loss: 1.1860747337341309\n",
      "Validation: Epoch [5], Batch [798/938], Loss: 0.875454306602478\n",
      "Validation: Epoch [5], Batch [799/938], Loss: 1.0097652673721313\n",
      "Validation: Epoch [5], Batch [800/938], Loss: 1.137601613998413\n",
      "Validation: Epoch [5], Batch [801/938], Loss: 1.193880558013916\n",
      "Validation: Epoch [5], Batch [802/938], Loss: 1.206786870956421\n",
      "Validation: Epoch [5], Batch [803/938], Loss: 0.9114060401916504\n",
      "Validation: Epoch [5], Batch [804/938], Loss: 1.0137391090393066\n",
      "Validation: Epoch [5], Batch [805/938], Loss: 0.9928426146507263\n",
      "Validation: Epoch [5], Batch [806/938], Loss: 1.038820743560791\n",
      "Validation: Epoch [5], Batch [807/938], Loss: 1.0895296335220337\n",
      "Validation: Epoch [5], Batch [808/938], Loss: 1.1043405532836914\n",
      "Validation: Epoch [5], Batch [809/938], Loss: 1.0353599786758423\n",
      "Validation: Epoch [5], Batch [810/938], Loss: 0.9094341397285461\n",
      "Validation: Epoch [5], Batch [811/938], Loss: 1.111886739730835\n",
      "Validation: Epoch [5], Batch [812/938], Loss: 1.0451622009277344\n",
      "Validation: Epoch [5], Batch [813/938], Loss: 1.0974113941192627\n",
      "Validation: Epoch [5], Batch [814/938], Loss: 1.1063841581344604\n",
      "Validation: Epoch [5], Batch [815/938], Loss: 1.0969372987747192\n",
      "Validation: Epoch [5], Batch [816/938], Loss: 0.9166581034660339\n",
      "Validation: Epoch [5], Batch [817/938], Loss: 1.1316865682601929\n",
      "Validation: Epoch [5], Batch [818/938], Loss: 1.1243855953216553\n",
      "Validation: Epoch [5], Batch [819/938], Loss: 1.055379867553711\n",
      "Validation: Epoch [5], Batch [820/938], Loss: 0.8946914076805115\n",
      "Validation: Epoch [5], Batch [821/938], Loss: 0.9070684313774109\n",
      "Validation: Epoch [5], Batch [822/938], Loss: 0.925384521484375\n",
      "Validation: Epoch [5], Batch [823/938], Loss: 1.036584734916687\n",
      "Validation: Epoch [5], Batch [824/938], Loss: 1.1020913124084473\n",
      "Validation: Epoch [5], Batch [825/938], Loss: 1.1491776704788208\n",
      "Validation: Epoch [5], Batch [826/938], Loss: 1.2233164310455322\n",
      "Validation: Epoch [5], Batch [827/938], Loss: 0.9786985516548157\n",
      "Validation: Epoch [5], Batch [828/938], Loss: 0.9524064064025879\n",
      "Validation: Epoch [5], Batch [829/938], Loss: 1.1346200704574585\n",
      "Validation: Epoch [5], Batch [830/938], Loss: 1.1754114627838135\n",
      "Validation: Epoch [5], Batch [831/938], Loss: 1.1925047636032104\n",
      "Validation: Epoch [5], Batch [832/938], Loss: 0.8779864311218262\n",
      "Validation: Epoch [5], Batch [833/938], Loss: 1.0829778909683228\n",
      "Validation: Epoch [5], Batch [834/938], Loss: 0.809238076210022\n",
      "Validation: Epoch [5], Batch [835/938], Loss: 1.2652854919433594\n",
      "Validation: Epoch [5], Batch [836/938], Loss: 1.1494390964508057\n",
      "Validation: Epoch [5], Batch [837/938], Loss: 1.1299664974212646\n",
      "Validation: Epoch [5], Batch [838/938], Loss: 0.9373171329498291\n",
      "Validation: Epoch [5], Batch [839/938], Loss: 1.1399332284927368\n",
      "Validation: Epoch [5], Batch [840/938], Loss: 1.092409372329712\n",
      "Validation: Epoch [5], Batch [841/938], Loss: 0.7418621778488159\n",
      "Validation: Epoch [5], Batch [842/938], Loss: 0.9584368467330933\n",
      "Validation: Epoch [5], Batch [843/938], Loss: 0.9348187446594238\n",
      "Validation: Epoch [5], Batch [844/938], Loss: 0.9705532193183899\n",
      "Validation: Epoch [5], Batch [845/938], Loss: 1.0215598344802856\n",
      "Validation: Epoch [5], Batch [846/938], Loss: 1.0805608034133911\n",
      "Validation: Epoch [5], Batch [847/938], Loss: 1.0271425247192383\n",
      "Validation: Epoch [5], Batch [848/938], Loss: 0.9503991007804871\n",
      "Validation: Epoch [5], Batch [849/938], Loss: 1.2930107116699219\n",
      "Validation: Epoch [5], Batch [850/938], Loss: 1.0709500312805176\n",
      "Validation: Epoch [5], Batch [851/938], Loss: 0.9666686654090881\n",
      "Validation: Epoch [5], Batch [852/938], Loss: 0.9144103527069092\n",
      "Validation: Epoch [5], Batch [853/938], Loss: 1.0001144409179688\n",
      "Validation: Epoch [5], Batch [854/938], Loss: 1.426609992980957\n",
      "Validation: Epoch [5], Batch [855/938], Loss: 1.001035213470459\n",
      "Validation: Epoch [5], Batch [856/938], Loss: 0.9875342845916748\n",
      "Validation: Epoch [5], Batch [857/938], Loss: 0.9817704558372498\n",
      "Validation: Epoch [5], Batch [858/938], Loss: 0.9602313041687012\n",
      "Validation: Epoch [5], Batch [859/938], Loss: 1.1449445486068726\n",
      "Validation: Epoch [5], Batch [860/938], Loss: 1.1630373001098633\n",
      "Validation: Epoch [5], Batch [861/938], Loss: 1.0595340728759766\n",
      "Validation: Epoch [5], Batch [862/938], Loss: 1.027590036392212\n",
      "Validation: Epoch [5], Batch [863/938], Loss: 1.0269389152526855\n",
      "Validation: Epoch [5], Batch [864/938], Loss: 0.9879761934280396\n",
      "Validation: Epoch [5], Batch [865/938], Loss: 1.1048738956451416\n",
      "Validation: Epoch [5], Batch [866/938], Loss: 0.8880478143692017\n",
      "Validation: Epoch [5], Batch [867/938], Loss: 1.0209705829620361\n",
      "Validation: Epoch [5], Batch [868/938], Loss: 1.3336379528045654\n",
      "Validation: Epoch [5], Batch [869/938], Loss: 1.0736290216445923\n",
      "Validation: Epoch [5], Batch [870/938], Loss: 0.948409378528595\n",
      "Validation: Epoch [5], Batch [871/938], Loss: 0.9667712450027466\n",
      "Validation: Epoch [5], Batch [872/938], Loss: 1.110629916191101\n",
      "Validation: Epoch [5], Batch [873/938], Loss: 0.6841306686401367\n",
      "Validation: Epoch [5], Batch [874/938], Loss: 0.8701235055923462\n",
      "Validation: Epoch [5], Batch [875/938], Loss: 1.308699369430542\n",
      "Validation: Epoch [5], Batch [876/938], Loss: 1.156975507736206\n",
      "Validation: Epoch [5], Batch [877/938], Loss: 1.0604047775268555\n",
      "Validation: Epoch [5], Batch [878/938], Loss: 1.2826616764068604\n",
      "Validation: Epoch [5], Batch [879/938], Loss: 0.9586058855056763\n",
      "Validation: Epoch [5], Batch [880/938], Loss: 1.0695137977600098\n",
      "Validation: Epoch [5], Batch [881/938], Loss: 1.0096865892410278\n",
      "Validation: Epoch [5], Batch [882/938], Loss: 0.9132642149925232\n",
      "Validation: Epoch [5], Batch [883/938], Loss: 0.8837828636169434\n",
      "Validation: Epoch [5], Batch [884/938], Loss: 1.2155694961547852\n",
      "Validation: Epoch [5], Batch [885/938], Loss: 1.0248744487762451\n",
      "Validation: Epoch [5], Batch [886/938], Loss: 0.9586817026138306\n",
      "Validation: Epoch [5], Batch [887/938], Loss: 1.118720293045044\n",
      "Validation: Epoch [5], Batch [888/938], Loss: 1.092921257019043\n",
      "Validation: Epoch [5], Batch [889/938], Loss: 1.099321722984314\n",
      "Validation: Epoch [5], Batch [890/938], Loss: 1.121174931526184\n",
      "Validation: Epoch [5], Batch [891/938], Loss: 1.0630035400390625\n",
      "Validation: Epoch [5], Batch [892/938], Loss: 1.0235745906829834\n",
      "Validation: Epoch [5], Batch [893/938], Loss: 1.1099944114685059\n",
      "Validation: Epoch [5], Batch [894/938], Loss: 1.062857985496521\n",
      "Validation: Epoch [5], Batch [895/938], Loss: 0.9066041707992554\n",
      "Validation: Epoch [5], Batch [896/938], Loss: 0.9500148892402649\n",
      "Validation: Epoch [5], Batch [897/938], Loss: 1.1607595682144165\n",
      "Validation: Epoch [5], Batch [898/938], Loss: 1.0280863046646118\n",
      "Validation: Epoch [5], Batch [899/938], Loss: 0.8665685057640076\n",
      "Validation: Epoch [5], Batch [900/938], Loss: 1.251375675201416\n",
      "Validation: Epoch [5], Batch [901/938], Loss: 1.2187731266021729\n",
      "Validation: Epoch [5], Batch [902/938], Loss: 0.9912788271903992\n",
      "Validation: Epoch [5], Batch [903/938], Loss: 0.9164005517959595\n",
      "Validation: Epoch [5], Batch [904/938], Loss: 1.117630124092102\n",
      "Validation: Epoch [5], Batch [905/938], Loss: 1.0066871643066406\n",
      "Validation: Epoch [5], Batch [906/938], Loss: 0.9610570073127747\n",
      "Validation: Epoch [5], Batch [907/938], Loss: 1.172407865524292\n",
      "Validation: Epoch [5], Batch [908/938], Loss: 0.9418554902076721\n",
      "Validation: Epoch [5], Batch [909/938], Loss: 1.0666567087173462\n",
      "Validation: Epoch [5], Batch [910/938], Loss: 0.8902130126953125\n",
      "Validation: Epoch [5], Batch [911/938], Loss: 0.8671368956565857\n",
      "Validation: Epoch [5], Batch [912/938], Loss: 0.8534343242645264\n",
      "Validation: Epoch [5], Batch [913/938], Loss: 1.0477986335754395\n",
      "Validation: Epoch [5], Batch [914/938], Loss: 1.0985052585601807\n",
      "Validation: Epoch [5], Batch [915/938], Loss: 0.8955230712890625\n",
      "Validation: Epoch [5], Batch [916/938], Loss: 1.0536935329437256\n",
      "Validation: Epoch [5], Batch [917/938], Loss: 1.166394829750061\n",
      "Validation: Epoch [5], Batch [918/938], Loss: 1.1552571058273315\n",
      "Validation: Epoch [5], Batch [919/938], Loss: 1.002519130706787\n",
      "Validation: Epoch [5], Batch [920/938], Loss: 1.1165504455566406\n",
      "Validation: Epoch [5], Batch [921/938], Loss: 1.0712244510650635\n",
      "Validation: Epoch [5], Batch [922/938], Loss: 0.9966354370117188\n",
      "Validation: Epoch [5], Batch [923/938], Loss: 1.155677080154419\n",
      "Validation: Epoch [5], Batch [924/938], Loss: 0.830897867679596\n",
      "Validation: Epoch [5], Batch [925/938], Loss: 1.182538628578186\n",
      "Validation: Epoch [5], Batch [926/938], Loss: 1.1550779342651367\n",
      "Validation: Epoch [5], Batch [927/938], Loss: 1.4157648086547852\n",
      "Validation: Epoch [5], Batch [928/938], Loss: 0.9932156801223755\n",
      "Validation: Epoch [5], Batch [929/938], Loss: 1.0636255741119385\n",
      "Validation: Epoch [5], Batch [930/938], Loss: 1.3039056062698364\n",
      "Validation: Epoch [5], Batch [931/938], Loss: 0.8203726410865784\n",
      "Validation: Epoch [5], Batch [932/938], Loss: 0.9408291578292847\n",
      "Validation: Epoch [5], Batch [933/938], Loss: 0.8977319002151489\n",
      "Validation: Epoch [5], Batch [934/938], Loss: 0.9519593119621277\n",
      "Validation: Epoch [5], Batch [935/938], Loss: 1.036954641342163\n",
      "Validation: Epoch [5], Batch [936/938], Loss: 1.266564965248108\n",
      "Validation: Epoch [5], Batch [937/938], Loss: 0.9901781678199768\n",
      "Validation: Epoch [5], Batch [938/938], Loss: 0.9230048656463623\n",
      "Accuracy of test set: 0.5977\n",
      "Train: Epoch [6], Batch [1/938], Loss: 0.9200206995010376\n",
      "Train: Epoch [6], Batch [2/938], Loss: 1.0193021297454834\n",
      "Train: Epoch [6], Batch [3/938], Loss: 0.9799867272377014\n",
      "Train: Epoch [6], Batch [4/938], Loss: 1.0614304542541504\n",
      "Train: Epoch [6], Batch [5/938], Loss: 1.1081706285476685\n",
      "Train: Epoch [6], Batch [6/938], Loss: 1.0086239576339722\n",
      "Train: Epoch [6], Batch [7/938], Loss: 0.7395498156547546\n",
      "Train: Epoch [6], Batch [8/938], Loss: 1.1487640142440796\n",
      "Train: Epoch [6], Batch [9/938], Loss: 1.223466157913208\n",
      "Train: Epoch [6], Batch [10/938], Loss: 0.8760002255439758\n",
      "Train: Epoch [6], Batch [11/938], Loss: 0.9865236282348633\n",
      "Train: Epoch [6], Batch [12/938], Loss: 0.9659606218338013\n",
      "Train: Epoch [6], Batch [13/938], Loss: 0.8844629526138306\n",
      "Train: Epoch [6], Batch [14/938], Loss: 1.0999040603637695\n",
      "Train: Epoch [6], Batch [15/938], Loss: 0.9023538827896118\n",
      "Train: Epoch [6], Batch [16/938], Loss: 0.7641329169273376\n",
      "Train: Epoch [6], Batch [17/938], Loss: 0.9685730934143066\n",
      "Train: Epoch [6], Batch [18/938], Loss: 0.8095009326934814\n",
      "Train: Epoch [6], Batch [19/938], Loss: 1.1172837018966675\n",
      "Train: Epoch [6], Batch [20/938], Loss: 0.9379301071166992\n",
      "Train: Epoch [6], Batch [21/938], Loss: 1.2174243927001953\n",
      "Train: Epoch [6], Batch [22/938], Loss: 1.2610161304473877\n",
      "Train: Epoch [6], Batch [23/938], Loss: 0.826111912727356\n",
      "Train: Epoch [6], Batch [24/938], Loss: 0.9699616432189941\n",
      "Train: Epoch [6], Batch [25/938], Loss: 1.0554285049438477\n",
      "Train: Epoch [6], Batch [26/938], Loss: 0.8817740082740784\n",
      "Train: Epoch [6], Batch [27/938], Loss: 0.8659219145774841\n",
      "Train: Epoch [6], Batch [28/938], Loss: 0.8916380405426025\n",
      "Train: Epoch [6], Batch [29/938], Loss: 0.7960079908370972\n",
      "Train: Epoch [6], Batch [30/938], Loss: 0.9289522767066956\n",
      "Train: Epoch [6], Batch [31/938], Loss: 0.878608763217926\n",
      "Train: Epoch [6], Batch [32/938], Loss: 1.0512207746505737\n",
      "Train: Epoch [6], Batch [33/938], Loss: 1.0800977945327759\n",
      "Train: Epoch [6], Batch [34/938], Loss: 1.1582473516464233\n",
      "Train: Epoch [6], Batch [35/938], Loss: 1.023107647895813\n",
      "Train: Epoch [6], Batch [36/938], Loss: 1.0006670951843262\n",
      "Train: Epoch [6], Batch [37/938], Loss: 1.0520225763320923\n",
      "Train: Epoch [6], Batch [38/938], Loss: 1.0040451288223267\n",
      "Train: Epoch [6], Batch [39/938], Loss: 0.9368606805801392\n",
      "Train: Epoch [6], Batch [40/938], Loss: 1.0132930278778076\n",
      "Train: Epoch [6], Batch [41/938], Loss: 1.0746978521347046\n",
      "Train: Epoch [6], Batch [42/938], Loss: 0.8432139754295349\n",
      "Train: Epoch [6], Batch [43/938], Loss: 1.1043505668640137\n",
      "Train: Epoch [6], Batch [44/938], Loss: 0.9657646417617798\n",
      "Train: Epoch [6], Batch [45/938], Loss: 0.8853328824043274\n",
      "Train: Epoch [6], Batch [46/938], Loss: 0.9577811360359192\n",
      "Train: Epoch [6], Batch [47/938], Loss: 1.1131467819213867\n",
      "Train: Epoch [6], Batch [48/938], Loss: 1.148219108581543\n",
      "Train: Epoch [6], Batch [49/938], Loss: 1.256365180015564\n",
      "Train: Epoch [6], Batch [50/938], Loss: 1.1877565383911133\n",
      "Train: Epoch [6], Batch [51/938], Loss: 1.0395370721817017\n",
      "Train: Epoch [6], Batch [52/938], Loss: 0.9354989528656006\n",
      "Train: Epoch [6], Batch [53/938], Loss: 0.9348041415214539\n",
      "Train: Epoch [6], Batch [54/938], Loss: 0.926601231098175\n",
      "Train: Epoch [6], Batch [55/938], Loss: 0.8236123919487\n",
      "Train: Epoch [6], Batch [56/938], Loss: 0.945375919342041\n",
      "Train: Epoch [6], Batch [57/938], Loss: 0.6991616487503052\n",
      "Train: Epoch [6], Batch [58/938], Loss: 0.9489040374755859\n",
      "Train: Epoch [6], Batch [59/938], Loss: 0.9266061186790466\n",
      "Train: Epoch [6], Batch [60/938], Loss: 1.0538969039916992\n",
      "Train: Epoch [6], Batch [61/938], Loss: 0.9935261011123657\n",
      "Train: Epoch [6], Batch [62/938], Loss: 0.9424748420715332\n",
      "Train: Epoch [6], Batch [63/938], Loss: 0.9448422789573669\n",
      "Train: Epoch [6], Batch [64/938], Loss: 1.0857291221618652\n",
      "Train: Epoch [6], Batch [65/938], Loss: 0.8047456741333008\n",
      "Train: Epoch [6], Batch [66/938], Loss: 0.8246994614601135\n",
      "Train: Epoch [6], Batch [67/938], Loss: 1.0318882465362549\n",
      "Train: Epoch [6], Batch [68/938], Loss: 1.1655465364456177\n",
      "Train: Epoch [6], Batch [69/938], Loss: 1.2258769273757935\n",
      "Train: Epoch [6], Batch [70/938], Loss: 0.8975633978843689\n",
      "Train: Epoch [6], Batch [71/938], Loss: 1.0886919498443604\n",
      "Train: Epoch [6], Batch [72/938], Loss: 0.9595988392829895\n",
      "Train: Epoch [6], Batch [73/938], Loss: 1.0048471689224243\n",
      "Train: Epoch [6], Batch [74/938], Loss: 1.023699402809143\n",
      "Train: Epoch [6], Batch [75/938], Loss: 0.9882882237434387\n",
      "Train: Epoch [6], Batch [76/938], Loss: 1.2718313932418823\n",
      "Train: Epoch [6], Batch [77/938], Loss: 0.9253520965576172\n",
      "Train: Epoch [6], Batch [78/938], Loss: 1.0467263460159302\n",
      "Train: Epoch [6], Batch [79/938], Loss: 0.9780203700065613\n",
      "Train: Epoch [6], Batch [80/938], Loss: 0.9934443235397339\n",
      "Train: Epoch [6], Batch [81/938], Loss: 0.9948817491531372\n",
      "Train: Epoch [6], Batch [82/938], Loss: 0.8818490505218506\n",
      "Train: Epoch [6], Batch [83/938], Loss: 0.9286412000656128\n",
      "Train: Epoch [6], Batch [84/938], Loss: 0.9913005828857422\n",
      "Train: Epoch [6], Batch [85/938], Loss: 1.0319454669952393\n",
      "Train: Epoch [6], Batch [86/938], Loss: 1.0321823358535767\n",
      "Train: Epoch [6], Batch [87/938], Loss: 0.8609133958816528\n",
      "Train: Epoch [6], Batch [88/938], Loss: 1.043102502822876\n",
      "Train: Epoch [6], Batch [89/938], Loss: 1.0463467836380005\n",
      "Train: Epoch [6], Batch [90/938], Loss: 0.9035941362380981\n",
      "Train: Epoch [6], Batch [91/938], Loss: 0.9794859886169434\n",
      "Train: Epoch [6], Batch [92/938], Loss: 0.8915347456932068\n",
      "Train: Epoch [6], Batch [93/938], Loss: 1.176067590713501\n",
      "Train: Epoch [6], Batch [94/938], Loss: 0.9570654630661011\n",
      "Train: Epoch [6], Batch [95/938], Loss: 1.0696849822998047\n",
      "Train: Epoch [6], Batch [96/938], Loss: 1.1325268745422363\n",
      "Train: Epoch [6], Batch [97/938], Loss: 0.9807983636856079\n",
      "Train: Epoch [6], Batch [98/938], Loss: 1.0169398784637451\n",
      "Train: Epoch [6], Batch [99/938], Loss: 1.1748977899551392\n",
      "Train: Epoch [6], Batch [100/938], Loss: 0.8427436351776123\n",
      "Train: Epoch [6], Batch [101/938], Loss: 0.8858350515365601\n",
      "Train: Epoch [6], Batch [102/938], Loss: 1.1346427202224731\n",
      "Train: Epoch [6], Batch [103/938], Loss: 0.8318604826927185\n",
      "Train: Epoch [6], Batch [104/938], Loss: 1.0419570207595825\n",
      "Train: Epoch [6], Batch [105/938], Loss: 0.8453400135040283\n",
      "Train: Epoch [6], Batch [106/938], Loss: 0.9572864174842834\n",
      "Train: Epoch [6], Batch [107/938], Loss: 1.041357159614563\n",
      "Train: Epoch [6], Batch [108/938], Loss: 1.0476932525634766\n",
      "Train: Epoch [6], Batch [109/938], Loss: 1.1825207471847534\n",
      "Train: Epoch [6], Batch [110/938], Loss: 1.023164987564087\n",
      "Train: Epoch [6], Batch [111/938], Loss: 1.0636329650878906\n",
      "Train: Epoch [6], Batch [112/938], Loss: 0.9991879463195801\n",
      "Train: Epoch [6], Batch [113/938], Loss: 0.9179906845092773\n",
      "Train: Epoch [6], Batch [114/938], Loss: 1.1188137531280518\n",
      "Train: Epoch [6], Batch [115/938], Loss: 0.9709807634353638\n",
      "Train: Epoch [6], Batch [116/938], Loss: 0.8777299523353577\n",
      "Train: Epoch [6], Batch [117/938], Loss: 0.8893507719039917\n",
      "Train: Epoch [6], Batch [118/938], Loss: 0.9618512392044067\n",
      "Train: Epoch [6], Batch [119/938], Loss: 0.9249581098556519\n",
      "Train: Epoch [6], Batch [120/938], Loss: 1.1170024871826172\n",
      "Train: Epoch [6], Batch [121/938], Loss: 1.0248744487762451\n",
      "Train: Epoch [6], Batch [122/938], Loss: 1.1320552825927734\n",
      "Train: Epoch [6], Batch [123/938], Loss: 0.9723231792449951\n",
      "Train: Epoch [6], Batch [124/938], Loss: 0.9255150556564331\n",
      "Train: Epoch [6], Batch [125/938], Loss: 0.9770653247833252\n",
      "Train: Epoch [6], Batch [126/938], Loss: 0.9765084385871887\n",
      "Train: Epoch [6], Batch [127/938], Loss: 0.7895075678825378\n",
      "Train: Epoch [6], Batch [128/938], Loss: 0.8727768063545227\n",
      "Train: Epoch [6], Batch [129/938], Loss: 1.0647096633911133\n",
      "Train: Epoch [6], Batch [130/938], Loss: 1.0677815675735474\n",
      "Train: Epoch [6], Batch [131/938], Loss: 1.0614694356918335\n",
      "Train: Epoch [6], Batch [132/938], Loss: 1.0357933044433594\n",
      "Train: Epoch [6], Batch [133/938], Loss: 0.8739781379699707\n",
      "Train: Epoch [6], Batch [134/938], Loss: 0.9254648685455322\n",
      "Train: Epoch [6], Batch [135/938], Loss: 0.8097206354141235\n",
      "Train: Epoch [6], Batch [136/938], Loss: 0.8815488219261169\n",
      "Train: Epoch [6], Batch [137/938], Loss: 1.0417324304580688\n",
      "Train: Epoch [6], Batch [138/938], Loss: 1.1396188735961914\n",
      "Train: Epoch [6], Batch [139/938], Loss: 0.8860710263252258\n",
      "Train: Epoch [6], Batch [140/938], Loss: 0.993257999420166\n",
      "Train: Epoch [6], Batch [141/938], Loss: 1.0357000827789307\n",
      "Train: Epoch [6], Batch [142/938], Loss: 0.899165689945221\n",
      "Train: Epoch [6], Batch [143/938], Loss: 1.107138991355896\n",
      "Train: Epoch [6], Batch [144/938], Loss: 1.2467281818389893\n",
      "Train: Epoch [6], Batch [145/938], Loss: 1.0372860431671143\n",
      "Train: Epoch [6], Batch [146/938], Loss: 0.9068567156791687\n",
      "Train: Epoch [6], Batch [147/938], Loss: 0.99215167760849\n",
      "Train: Epoch [6], Batch [148/938], Loss: 1.1456396579742432\n",
      "Train: Epoch [6], Batch [149/938], Loss: 0.9617998600006104\n",
      "Train: Epoch [6], Batch [150/938], Loss: 0.9511221051216125\n",
      "Train: Epoch [6], Batch [151/938], Loss: 1.0002282857894897\n",
      "Train: Epoch [6], Batch [152/938], Loss: 0.8793666362762451\n",
      "Train: Epoch [6], Batch [153/938], Loss: 0.9494028687477112\n",
      "Train: Epoch [6], Batch [154/938], Loss: 0.9029496908187866\n",
      "Train: Epoch [6], Batch [155/938], Loss: 1.134558916091919\n",
      "Train: Epoch [6], Batch [156/938], Loss: 1.1950583457946777\n",
      "Train: Epoch [6], Batch [157/938], Loss: 1.0142215490341187\n",
      "Train: Epoch [6], Batch [158/938], Loss: 1.042346477508545\n",
      "Train: Epoch [6], Batch [159/938], Loss: 0.995725691318512\n",
      "Train: Epoch [6], Batch [160/938], Loss: 0.943595290184021\n",
      "Train: Epoch [6], Batch [161/938], Loss: 1.0946778059005737\n",
      "Train: Epoch [6], Batch [162/938], Loss: 0.9188662767410278\n",
      "Train: Epoch [6], Batch [163/938], Loss: 1.059517502784729\n",
      "Train: Epoch [6], Batch [164/938], Loss: 1.0437625646591187\n",
      "Train: Epoch [6], Batch [165/938], Loss: 1.0685091018676758\n",
      "Train: Epoch [6], Batch [166/938], Loss: 1.0323452949523926\n",
      "Train: Epoch [6], Batch [167/938], Loss: 0.7957280278205872\n",
      "Train: Epoch [6], Batch [168/938], Loss: 1.121483325958252\n",
      "Train: Epoch [6], Batch [169/938], Loss: 1.0786596536636353\n",
      "Train: Epoch [6], Batch [170/938], Loss: 1.194486141204834\n",
      "Train: Epoch [6], Batch [171/938], Loss: 1.0065973997116089\n",
      "Train: Epoch [6], Batch [172/938], Loss: 1.0741004943847656\n",
      "Train: Epoch [6], Batch [173/938], Loss: 0.9538255929946899\n",
      "Train: Epoch [6], Batch [174/938], Loss: 0.9539729356765747\n",
      "Train: Epoch [6], Batch [175/938], Loss: 0.9210558533668518\n",
      "Train: Epoch [6], Batch [176/938], Loss: 0.8914608359336853\n",
      "Train: Epoch [6], Batch [177/938], Loss: 0.8405560851097107\n",
      "Train: Epoch [6], Batch [178/938], Loss: 1.295629620552063\n",
      "Train: Epoch [6], Batch [179/938], Loss: 0.9162254929542542\n",
      "Train: Epoch [6], Batch [180/938], Loss: 0.8495495319366455\n",
      "Train: Epoch [6], Batch [181/938], Loss: 0.8618519306182861\n",
      "Train: Epoch [6], Batch [182/938], Loss: 1.0645318031311035\n",
      "Train: Epoch [6], Batch [183/938], Loss: 0.9434106349945068\n",
      "Train: Epoch [6], Batch [184/938], Loss: 0.9859952330589294\n",
      "Train: Epoch [6], Batch [185/938], Loss: 1.1475154161453247\n",
      "Train: Epoch [6], Batch [186/938], Loss: 0.8265358209609985\n",
      "Train: Epoch [6], Batch [187/938], Loss: 0.8325145244598389\n",
      "Train: Epoch [6], Batch [188/938], Loss: 0.8920283913612366\n",
      "Train: Epoch [6], Batch [189/938], Loss: 1.1137447357177734\n",
      "Train: Epoch [6], Batch [190/938], Loss: 0.8543087244033813\n",
      "Train: Epoch [6], Batch [191/938], Loss: 0.9823459386825562\n",
      "Train: Epoch [6], Batch [192/938], Loss: 0.90000981092453\n",
      "Train: Epoch [6], Batch [193/938], Loss: 0.9674391746520996\n",
      "Train: Epoch [6], Batch [194/938], Loss: 0.9273327589035034\n",
      "Train: Epoch [6], Batch [195/938], Loss: 1.1903761625289917\n",
      "Train: Epoch [6], Batch [196/938], Loss: 0.8633434772491455\n",
      "Train: Epoch [6], Batch [197/938], Loss: 1.1182233095169067\n",
      "Train: Epoch [6], Batch [198/938], Loss: 0.9177529811859131\n",
      "Train: Epoch [6], Batch [199/938], Loss: 1.2668468952178955\n",
      "Train: Epoch [6], Batch [200/938], Loss: 0.9126994013786316\n",
      "Train: Epoch [6], Batch [201/938], Loss: 0.9441269636154175\n",
      "Train: Epoch [6], Batch [202/938], Loss: 0.8786255717277527\n",
      "Train: Epoch [6], Batch [203/938], Loss: 1.2356106042861938\n",
      "Train: Epoch [6], Batch [204/938], Loss: 1.0056846141815186\n",
      "Train: Epoch [6], Batch [205/938], Loss: 0.9247956275939941\n",
      "Train: Epoch [6], Batch [206/938], Loss: 1.1137287616729736\n",
      "Train: Epoch [6], Batch [207/938], Loss: 0.869715690612793\n",
      "Train: Epoch [6], Batch [208/938], Loss: 1.0605512857437134\n",
      "Train: Epoch [6], Batch [209/938], Loss: 1.1469058990478516\n",
      "Train: Epoch [6], Batch [210/938], Loss: 0.8522962927818298\n",
      "Train: Epoch [6], Batch [211/938], Loss: 0.953220784664154\n",
      "Train: Epoch [6], Batch [212/938], Loss: 1.1489168405532837\n",
      "Train: Epoch [6], Batch [213/938], Loss: 0.9116397500038147\n",
      "Train: Epoch [6], Batch [214/938], Loss: 1.1234581470489502\n",
      "Train: Epoch [6], Batch [215/938], Loss: 0.8459081649780273\n",
      "Train: Epoch [6], Batch [216/938], Loss: 0.9730490446090698\n",
      "Train: Epoch [6], Batch [217/938], Loss: 1.2141013145446777\n",
      "Train: Epoch [6], Batch [218/938], Loss: 0.920647382736206\n",
      "Train: Epoch [6], Batch [219/938], Loss: 1.0789457559585571\n",
      "Train: Epoch [6], Batch [220/938], Loss: 0.9418900012969971\n",
      "Train: Epoch [6], Batch [221/938], Loss: 1.1284552812576294\n",
      "Train: Epoch [6], Batch [222/938], Loss: 0.9324605464935303\n",
      "Train: Epoch [6], Batch [223/938], Loss: 0.9947072863578796\n",
      "Train: Epoch [6], Batch [224/938], Loss: 0.9262261986732483\n",
      "Train: Epoch [6], Batch [225/938], Loss: 0.8471854329109192\n",
      "Train: Epoch [6], Batch [226/938], Loss: 0.9353823661804199\n",
      "Train: Epoch [6], Batch [227/938], Loss: 0.8152093291282654\n",
      "Train: Epoch [6], Batch [228/938], Loss: 1.136443853378296\n",
      "Train: Epoch [6], Batch [229/938], Loss: 1.0100951194763184\n",
      "Train: Epoch [6], Batch [230/938], Loss: 0.9273452758789062\n",
      "Train: Epoch [6], Batch [231/938], Loss: 1.1354782581329346\n",
      "Train: Epoch [6], Batch [232/938], Loss: 0.984976053237915\n",
      "Train: Epoch [6], Batch [233/938], Loss: 0.9604395031929016\n",
      "Train: Epoch [6], Batch [234/938], Loss: 1.1994826793670654\n",
      "Train: Epoch [6], Batch [235/938], Loss: 0.9328195452690125\n",
      "Train: Epoch [6], Batch [236/938], Loss: 1.0822570323944092\n",
      "Train: Epoch [6], Batch [237/938], Loss: 1.025578260421753\n",
      "Train: Epoch [6], Batch [238/938], Loss: 0.9421821236610413\n",
      "Train: Epoch [6], Batch [239/938], Loss: 0.9811034202575684\n",
      "Train: Epoch [6], Batch [240/938], Loss: 1.1320878267288208\n",
      "Train: Epoch [6], Batch [241/938], Loss: 0.7468262314796448\n",
      "Train: Epoch [6], Batch [242/938], Loss: 1.078656792640686\n",
      "Train: Epoch [6], Batch [243/938], Loss: 0.8832942843437195\n",
      "Train: Epoch [6], Batch [244/938], Loss: 0.9277014136314392\n",
      "Train: Epoch [6], Batch [245/938], Loss: 0.9845178723335266\n",
      "Train: Epoch [6], Batch [246/938], Loss: 1.0621862411499023\n",
      "Train: Epoch [6], Batch [247/938], Loss: 1.0870436429977417\n",
      "Train: Epoch [6], Batch [248/938], Loss: 1.2623118162155151\n",
      "Train: Epoch [6], Batch [249/938], Loss: 1.1400604248046875\n",
      "Train: Epoch [6], Batch [250/938], Loss: 1.0756999254226685\n",
      "Train: Epoch [6], Batch [251/938], Loss: 0.9296207427978516\n",
      "Train: Epoch [6], Batch [252/938], Loss: 0.8736667633056641\n",
      "Train: Epoch [6], Batch [253/938], Loss: 1.170174241065979\n",
      "Train: Epoch [6], Batch [254/938], Loss: 1.011430263519287\n",
      "Train: Epoch [6], Batch [255/938], Loss: 1.0328258275985718\n",
      "Train: Epoch [6], Batch [256/938], Loss: 1.0102384090423584\n",
      "Train: Epoch [6], Batch [257/938], Loss: 1.0099409818649292\n",
      "Train: Epoch [6], Batch [258/938], Loss: 0.9595534801483154\n",
      "Train: Epoch [6], Batch [259/938], Loss: 0.962378978729248\n",
      "Train: Epoch [6], Batch [260/938], Loss: 0.9001154899597168\n",
      "Train: Epoch [6], Batch [261/938], Loss: 0.9555208086967468\n",
      "Train: Epoch [6], Batch [262/938], Loss: 0.9376235604286194\n",
      "Train: Epoch [6], Batch [263/938], Loss: 1.041344165802002\n",
      "Train: Epoch [6], Batch [264/938], Loss: 0.9378121495246887\n",
      "Train: Epoch [6], Batch [265/938], Loss: 0.8330832123756409\n",
      "Train: Epoch [6], Batch [266/938], Loss: 0.889241099357605\n",
      "Train: Epoch [6], Batch [267/938], Loss: 0.8300415277481079\n",
      "Train: Epoch [6], Batch [268/938], Loss: 0.9718929529190063\n",
      "Train: Epoch [6], Batch [269/938], Loss: 0.9187121391296387\n",
      "Train: Epoch [6], Batch [270/938], Loss: 0.9392406344413757\n",
      "Train: Epoch [6], Batch [271/938], Loss: 1.0208715200424194\n",
      "Train: Epoch [6], Batch [272/938], Loss: 1.0774835348129272\n",
      "Train: Epoch [6], Batch [273/938], Loss: 0.9892268180847168\n",
      "Train: Epoch [6], Batch [274/938], Loss: 1.0508983135223389\n",
      "Train: Epoch [6], Batch [275/938], Loss: 0.9866361618041992\n",
      "Train: Epoch [6], Batch [276/938], Loss: 0.944581925868988\n",
      "Train: Epoch [6], Batch [277/938], Loss: 1.066713571548462\n",
      "Train: Epoch [6], Batch [278/938], Loss: 0.7725013494491577\n",
      "Train: Epoch [6], Batch [279/938], Loss: 1.1792998313903809\n",
      "Train: Epoch [6], Batch [280/938], Loss: 1.0400886535644531\n",
      "Train: Epoch [6], Batch [281/938], Loss: 1.140866756439209\n",
      "Train: Epoch [6], Batch [282/938], Loss: 1.0560208559036255\n",
      "Train: Epoch [6], Batch [283/938], Loss: 1.031858205795288\n",
      "Train: Epoch [6], Batch [284/938], Loss: 1.1867414712905884\n",
      "Train: Epoch [6], Batch [285/938], Loss: 0.8409874439239502\n",
      "Train: Epoch [6], Batch [286/938], Loss: 1.0954877138137817\n",
      "Train: Epoch [6], Batch [287/938], Loss: 1.244596242904663\n",
      "Train: Epoch [6], Batch [288/938], Loss: 1.0966742038726807\n",
      "Train: Epoch [6], Batch [289/938], Loss: 1.0475130081176758\n",
      "Train: Epoch [6], Batch [290/938], Loss: 1.2745691537857056\n",
      "Train: Epoch [6], Batch [291/938], Loss: 0.9116448163986206\n",
      "Train: Epoch [6], Batch [292/938], Loss: 1.1209295988082886\n",
      "Train: Epoch [6], Batch [293/938], Loss: 0.9866251945495605\n",
      "Train: Epoch [6], Batch [294/938], Loss: 0.9081140160560608\n",
      "Train: Epoch [6], Batch [295/938], Loss: 1.0979983806610107\n",
      "Train: Epoch [6], Batch [296/938], Loss: 0.9485623240470886\n",
      "Train: Epoch [6], Batch [297/938], Loss: 1.0028276443481445\n",
      "Train: Epoch [6], Batch [298/938], Loss: 0.9393829107284546\n",
      "Train: Epoch [6], Batch [299/938], Loss: 1.0438188314437866\n",
      "Train: Epoch [6], Batch [300/938], Loss: 0.9749701619148254\n",
      "Train: Epoch [6], Batch [301/938], Loss: 0.8504352569580078\n",
      "Train: Epoch [6], Batch [302/938], Loss: 0.9814135432243347\n",
      "Train: Epoch [6], Batch [303/938], Loss: 1.0966616868972778\n",
      "Train: Epoch [6], Batch [304/938], Loss: 0.9097592830657959\n",
      "Train: Epoch [6], Batch [305/938], Loss: 0.9078036546707153\n",
      "Train: Epoch [6], Batch [306/938], Loss: 1.0988472700119019\n",
      "Train: Epoch [6], Batch [307/938], Loss: 1.207715392112732\n",
      "Train: Epoch [6], Batch [308/938], Loss: 0.8571550250053406\n",
      "Train: Epoch [6], Batch [309/938], Loss: 0.9968307018280029\n",
      "Train: Epoch [6], Batch [310/938], Loss: 0.8057964444160461\n",
      "Train: Epoch [6], Batch [311/938], Loss: 0.9069557189941406\n",
      "Train: Epoch [6], Batch [312/938], Loss: 1.088842511177063\n",
      "Train: Epoch [6], Batch [313/938], Loss: 0.9392945766448975\n",
      "Train: Epoch [6], Batch [314/938], Loss: 1.2378754615783691\n",
      "Train: Epoch [6], Batch [315/938], Loss: 1.1192972660064697\n",
      "Train: Epoch [6], Batch [316/938], Loss: 0.8723432421684265\n",
      "Train: Epoch [6], Batch [317/938], Loss: 0.9615494608879089\n",
      "Train: Epoch [6], Batch [318/938], Loss: 1.1916710138320923\n",
      "Train: Epoch [6], Batch [319/938], Loss: 0.9472126960754395\n",
      "Train: Epoch [6], Batch [320/938], Loss: 0.9734456539154053\n",
      "Train: Epoch [6], Batch [321/938], Loss: 1.2978633642196655\n",
      "Train: Epoch [6], Batch [322/938], Loss: 1.174984335899353\n",
      "Train: Epoch [6], Batch [323/938], Loss: 1.1864604949951172\n",
      "Train: Epoch [6], Batch [324/938], Loss: 1.0048030614852905\n",
      "Train: Epoch [6], Batch [325/938], Loss: 1.0882437229156494\n",
      "Train: Epoch [6], Batch [326/938], Loss: 0.7717024683952332\n",
      "Train: Epoch [6], Batch [327/938], Loss: 1.1505160331726074\n",
      "Train: Epoch [6], Batch [328/938], Loss: 1.1547698974609375\n",
      "Train: Epoch [6], Batch [329/938], Loss: 1.080096960067749\n",
      "Train: Epoch [6], Batch [330/938], Loss: 1.2872648239135742\n",
      "Train: Epoch [6], Batch [331/938], Loss: 0.9390838146209717\n",
      "Train: Epoch [6], Batch [332/938], Loss: 0.9794068336486816\n",
      "Train: Epoch [6], Batch [333/938], Loss: 0.9439073204994202\n",
      "Train: Epoch [6], Batch [334/938], Loss: 1.0259051322937012\n",
      "Train: Epoch [6], Batch [335/938], Loss: 1.2602131366729736\n",
      "Train: Epoch [6], Batch [336/938], Loss: 0.8290207386016846\n",
      "Train: Epoch [6], Batch [337/938], Loss: 0.9755709171295166\n",
      "Train: Epoch [6], Batch [338/938], Loss: 0.922060489654541\n",
      "Train: Epoch [6], Batch [339/938], Loss: 0.9468683004379272\n",
      "Train: Epoch [6], Batch [340/938], Loss: 1.0684211254119873\n",
      "Train: Epoch [6], Batch [341/938], Loss: 0.8622641563415527\n",
      "Train: Epoch [6], Batch [342/938], Loss: 0.8940836787223816\n",
      "Train: Epoch [6], Batch [343/938], Loss: 0.9181877970695496\n",
      "Train: Epoch [6], Batch [344/938], Loss: 0.795246422290802\n",
      "Train: Epoch [6], Batch [345/938], Loss: 0.9831336736679077\n",
      "Train: Epoch [6], Batch [346/938], Loss: 1.1614727973937988\n",
      "Train: Epoch [6], Batch [347/938], Loss: 1.0848416090011597\n",
      "Train: Epoch [6], Batch [348/938], Loss: 1.075455904006958\n",
      "Train: Epoch [6], Batch [349/938], Loss: 0.9218304753303528\n",
      "Train: Epoch [6], Batch [350/938], Loss: 0.8754588961601257\n",
      "Train: Epoch [6], Batch [351/938], Loss: 1.045823097229004\n",
      "Train: Epoch [6], Batch [352/938], Loss: 1.103563666343689\n",
      "Train: Epoch [6], Batch [353/938], Loss: 0.7083429098129272\n",
      "Train: Epoch [6], Batch [354/938], Loss: 0.9824748039245605\n",
      "Train: Epoch [6], Batch [355/938], Loss: 1.3025556802749634\n",
      "Train: Epoch [6], Batch [356/938], Loss: 0.906548023223877\n",
      "Train: Epoch [6], Batch [357/938], Loss: 0.9524365663528442\n",
      "Train: Epoch [6], Batch [358/938], Loss: 1.094306230545044\n",
      "Train: Epoch [6], Batch [359/938], Loss: 1.0655903816223145\n",
      "Train: Epoch [6], Batch [360/938], Loss: 1.0045311450958252\n",
      "Train: Epoch [6], Batch [361/938], Loss: 0.8988367319107056\n",
      "Train: Epoch [6], Batch [362/938], Loss: 0.8954331874847412\n",
      "Train: Epoch [6], Batch [363/938], Loss: 0.9054677486419678\n",
      "Train: Epoch [6], Batch [364/938], Loss: 1.0436618328094482\n",
      "Train: Epoch [6], Batch [365/938], Loss: 0.8992089629173279\n",
      "Train: Epoch [6], Batch [366/938], Loss: 1.0831787586212158\n",
      "Train: Epoch [6], Batch [367/938], Loss: 1.2195155620574951\n",
      "Train: Epoch [6], Batch [368/938], Loss: 1.0563437938690186\n",
      "Train: Epoch [6], Batch [369/938], Loss: 1.2153596878051758\n",
      "Train: Epoch [6], Batch [370/938], Loss: 1.1495708227157593\n",
      "Train: Epoch [6], Batch [371/938], Loss: 1.0941985845565796\n",
      "Train: Epoch [6], Batch [372/938], Loss: 1.1164937019348145\n",
      "Train: Epoch [6], Batch [373/938], Loss: 1.1150221824645996\n",
      "Train: Epoch [6], Batch [374/938], Loss: 1.2817800045013428\n",
      "Train: Epoch [6], Batch [375/938], Loss: 0.9430031180381775\n",
      "Train: Epoch [6], Batch [376/938], Loss: 0.7313012480735779\n",
      "Train: Epoch [6], Batch [377/938], Loss: 1.1817418336868286\n",
      "Train: Epoch [6], Batch [378/938], Loss: 0.9981516599655151\n",
      "Train: Epoch [6], Batch [379/938], Loss: 1.0740439891815186\n",
      "Train: Epoch [6], Batch [380/938], Loss: 0.9740859270095825\n",
      "Train: Epoch [6], Batch [381/938], Loss: 0.9657542705535889\n",
      "Train: Epoch [6], Batch [382/938], Loss: 1.1813362836837769\n",
      "Train: Epoch [6], Batch [383/938], Loss: 0.9182732105255127\n",
      "Train: Epoch [6], Batch [384/938], Loss: 1.1048728227615356\n",
      "Train: Epoch [6], Batch [385/938], Loss: 0.881574273109436\n",
      "Train: Epoch [6], Batch [386/938], Loss: 1.0827234983444214\n",
      "Train: Epoch [6], Batch [387/938], Loss: 1.156402587890625\n",
      "Train: Epoch [6], Batch [388/938], Loss: 0.8454375863075256\n",
      "Train: Epoch [6], Batch [389/938], Loss: 0.8818340301513672\n",
      "Train: Epoch [6], Batch [390/938], Loss: 1.2414743900299072\n",
      "Train: Epoch [6], Batch [391/938], Loss: 1.0754958391189575\n",
      "Train: Epoch [6], Batch [392/938], Loss: 0.915378749370575\n",
      "Train: Epoch [6], Batch [393/938], Loss: 0.8539173603057861\n",
      "Train: Epoch [6], Batch [394/938], Loss: 0.9077960252761841\n",
      "Train: Epoch [6], Batch [395/938], Loss: 1.0285184383392334\n",
      "Train: Epoch [6], Batch [396/938], Loss: 1.1771492958068848\n",
      "Train: Epoch [6], Batch [397/938], Loss: 0.9268912076950073\n",
      "Train: Epoch [6], Batch [398/938], Loss: 0.9824973940849304\n",
      "Train: Epoch [6], Batch [399/938], Loss: 1.1232125759124756\n",
      "Train: Epoch [6], Batch [400/938], Loss: 1.067229986190796\n",
      "Train: Epoch [6], Batch [401/938], Loss: 0.8606472611427307\n",
      "Train: Epoch [6], Batch [402/938], Loss: 0.933125913143158\n",
      "Train: Epoch [6], Batch [403/938], Loss: 1.2853469848632812\n",
      "Train: Epoch [6], Batch [404/938], Loss: 1.031388759613037\n",
      "Train: Epoch [6], Batch [405/938], Loss: 1.0832291841506958\n",
      "Train: Epoch [6], Batch [406/938], Loss: 1.0504512786865234\n",
      "Train: Epoch [6], Batch [407/938], Loss: 1.1352936029434204\n",
      "Train: Epoch [6], Batch [408/938], Loss: 1.1770395040512085\n",
      "Train: Epoch [6], Batch [409/938], Loss: 1.1126853227615356\n",
      "Train: Epoch [6], Batch [410/938], Loss: 0.9366463422775269\n",
      "Train: Epoch [6], Batch [411/938], Loss: 1.1741582155227661\n",
      "Train: Epoch [6], Batch [412/938], Loss: 1.2138373851776123\n",
      "Train: Epoch [6], Batch [413/938], Loss: 1.1329141855239868\n",
      "Train: Epoch [6], Batch [414/938], Loss: 1.0058069229125977\n",
      "Train: Epoch [6], Batch [415/938], Loss: 0.6793710589408875\n",
      "Train: Epoch [6], Batch [416/938], Loss: 1.03815495967865\n",
      "Train: Epoch [6], Batch [417/938], Loss: 1.1113957166671753\n",
      "Train: Epoch [6], Batch [418/938], Loss: 0.9389190077781677\n",
      "Train: Epoch [6], Batch [419/938], Loss: 0.9060133695602417\n",
      "Train: Epoch [6], Batch [420/938], Loss: 0.890322208404541\n",
      "Train: Epoch [6], Batch [421/938], Loss: 1.091800332069397\n",
      "Train: Epoch [6], Batch [422/938], Loss: 1.0584464073181152\n",
      "Train: Epoch [6], Batch [423/938], Loss: 0.9201548099517822\n",
      "Train: Epoch [6], Batch [424/938], Loss: 0.9950554370880127\n",
      "Train: Epoch [6], Batch [425/938], Loss: 1.0945422649383545\n",
      "Train: Epoch [6], Batch [426/938], Loss: 1.1533160209655762\n",
      "Train: Epoch [6], Batch [427/938], Loss: 0.8289543390274048\n",
      "Train: Epoch [6], Batch [428/938], Loss: 1.2503703832626343\n",
      "Train: Epoch [6], Batch [429/938], Loss: 0.8912228941917419\n",
      "Train: Epoch [6], Batch [430/938], Loss: 0.8241647481918335\n",
      "Train: Epoch [6], Batch [431/938], Loss: 0.7660149335861206\n",
      "Train: Epoch [6], Batch [432/938], Loss: 1.0646257400512695\n",
      "Train: Epoch [6], Batch [433/938], Loss: 0.8430305123329163\n",
      "Train: Epoch [6], Batch [434/938], Loss: 0.8267064690589905\n",
      "Train: Epoch [6], Batch [435/938], Loss: 0.9058805108070374\n",
      "Train: Epoch [6], Batch [436/938], Loss: 1.0074177980422974\n",
      "Train: Epoch [6], Batch [437/938], Loss: 0.8903658390045166\n",
      "Train: Epoch [6], Batch [438/938], Loss: 1.0199015140533447\n",
      "Train: Epoch [6], Batch [439/938], Loss: 0.7973724603652954\n",
      "Train: Epoch [6], Batch [440/938], Loss: 0.7433767914772034\n",
      "Train: Epoch [6], Batch [441/938], Loss: 1.1675491333007812\n",
      "Train: Epoch [6], Batch [442/938], Loss: 0.5960854291915894\n",
      "Train: Epoch [6], Batch [443/938], Loss: 1.1233793497085571\n",
      "Train: Epoch [6], Batch [444/938], Loss: 1.0468522310256958\n",
      "Train: Epoch [6], Batch [445/938], Loss: 0.9405383467674255\n",
      "Train: Epoch [6], Batch [446/938], Loss: 1.0293712615966797\n",
      "Train: Epoch [6], Batch [447/938], Loss: 0.96669602394104\n",
      "Train: Epoch [6], Batch [448/938], Loss: 1.2163786888122559\n",
      "Train: Epoch [6], Batch [449/938], Loss: 1.0494911670684814\n",
      "Train: Epoch [6], Batch [450/938], Loss: 0.8982825875282288\n",
      "Train: Epoch [6], Batch [451/938], Loss: 1.0705562829971313\n",
      "Train: Epoch [6], Batch [452/938], Loss: 0.990969181060791\n",
      "Train: Epoch [6], Batch [453/938], Loss: 1.0658388137817383\n",
      "Train: Epoch [6], Batch [454/938], Loss: 1.003275752067566\n",
      "Train: Epoch [6], Batch [455/938], Loss: 0.8188539743423462\n",
      "Train: Epoch [6], Batch [456/938], Loss: 0.9817864894866943\n",
      "Train: Epoch [6], Batch [457/938], Loss: 0.8187336921691895\n",
      "Train: Epoch [6], Batch [458/938], Loss: 1.0787453651428223\n",
      "Train: Epoch [6], Batch [459/938], Loss: 1.0089150667190552\n",
      "Train: Epoch [6], Batch [460/938], Loss: 0.8790171146392822\n",
      "Train: Epoch [6], Batch [461/938], Loss: 0.9668233394622803\n",
      "Train: Epoch [6], Batch [462/938], Loss: 0.9868770241737366\n",
      "Train: Epoch [6], Batch [463/938], Loss: 0.8077075481414795\n",
      "Train: Epoch [6], Batch [464/938], Loss: 0.9414365887641907\n",
      "Train: Epoch [6], Batch [465/938], Loss: 1.0910173654556274\n",
      "Train: Epoch [6], Batch [466/938], Loss: 1.0346046686172485\n",
      "Train: Epoch [6], Batch [467/938], Loss: 1.1415555477142334\n",
      "Train: Epoch [6], Batch [468/938], Loss: 0.9580637216567993\n",
      "Train: Epoch [6], Batch [469/938], Loss: 0.8488271832466125\n",
      "Train: Epoch [6], Batch [470/938], Loss: 1.2216439247131348\n",
      "Train: Epoch [6], Batch [471/938], Loss: 1.0841786861419678\n",
      "Train: Epoch [6], Batch [472/938], Loss: 0.8736826777458191\n",
      "Train: Epoch [6], Batch [473/938], Loss: 0.8957942724227905\n",
      "Train: Epoch [6], Batch [474/938], Loss: 0.9467219114303589\n",
      "Train: Epoch [6], Batch [475/938], Loss: 0.8420583009719849\n",
      "Train: Epoch [6], Batch [476/938], Loss: 0.9401318430900574\n",
      "Train: Epoch [6], Batch [477/938], Loss: 1.0447466373443604\n",
      "Train: Epoch [6], Batch [478/938], Loss: 0.9714866876602173\n",
      "Train: Epoch [6], Batch [479/938], Loss: 0.8813152313232422\n",
      "Train: Epoch [6], Batch [480/938], Loss: 0.8986918926239014\n",
      "Train: Epoch [6], Batch [481/938], Loss: 0.7127818465232849\n",
      "Train: Epoch [6], Batch [482/938], Loss: 0.8978736400604248\n",
      "Train: Epoch [6], Batch [483/938], Loss: 0.9080179929733276\n",
      "Train: Epoch [6], Batch [484/938], Loss: 1.0992010831832886\n",
      "Train: Epoch [6], Batch [485/938], Loss: 1.0712356567382812\n",
      "Train: Epoch [6], Batch [486/938], Loss: 0.785273551940918\n",
      "Train: Epoch [6], Batch [487/938], Loss: 1.0007023811340332\n",
      "Train: Epoch [6], Batch [488/938], Loss: 0.8544894456863403\n",
      "Train: Epoch [6], Batch [489/938], Loss: 0.9779225587844849\n",
      "Train: Epoch [6], Batch [490/938], Loss: 1.0645028352737427\n",
      "Train: Epoch [6], Batch [491/938], Loss: 1.0803592205047607\n",
      "Train: Epoch [6], Batch [492/938], Loss: 1.176926612854004\n",
      "Train: Epoch [6], Batch [493/938], Loss: 1.094222903251648\n",
      "Train: Epoch [6], Batch [494/938], Loss: 1.0110952854156494\n",
      "Train: Epoch [6], Batch [495/938], Loss: 0.9796826243400574\n",
      "Train: Epoch [6], Batch [496/938], Loss: 0.9474275708198547\n",
      "Train: Epoch [6], Batch [497/938], Loss: 0.9085948467254639\n",
      "Train: Epoch [6], Batch [498/938], Loss: 0.8031193614006042\n",
      "Train: Epoch [6], Batch [499/938], Loss: 0.9767380952835083\n",
      "Train: Epoch [6], Batch [500/938], Loss: 0.8924766182899475\n",
      "Train: Epoch [6], Batch [501/938], Loss: 0.9964630603790283\n",
      "Train: Epoch [6], Batch [502/938], Loss: 0.9944515228271484\n",
      "Train: Epoch [6], Batch [503/938], Loss: 0.9126766324043274\n",
      "Train: Epoch [6], Batch [504/938], Loss: 1.0147922039031982\n",
      "Train: Epoch [6], Batch [505/938], Loss: 1.0255900621414185\n",
      "Train: Epoch [6], Batch [506/938], Loss: 1.0552845001220703\n",
      "Train: Epoch [6], Batch [507/938], Loss: 0.943896472454071\n",
      "Train: Epoch [6], Batch [508/938], Loss: 0.9593707919120789\n",
      "Train: Epoch [6], Batch [509/938], Loss: 0.7878645658493042\n",
      "Train: Epoch [6], Batch [510/938], Loss: 0.9826749563217163\n",
      "Train: Epoch [6], Batch [511/938], Loss: 0.8310362100601196\n",
      "Train: Epoch [6], Batch [512/938], Loss: 1.1139748096466064\n",
      "Train: Epoch [6], Batch [513/938], Loss: 0.7255067825317383\n",
      "Train: Epoch [6], Batch [514/938], Loss: 1.1477826833724976\n",
      "Train: Epoch [6], Batch [515/938], Loss: 1.037126064300537\n",
      "Train: Epoch [6], Batch [516/938], Loss: 0.8513567447662354\n",
      "Train: Epoch [6], Batch [517/938], Loss: 1.0315974950790405\n",
      "Train: Epoch [6], Batch [518/938], Loss: 1.070239782333374\n",
      "Train: Epoch [6], Batch [519/938], Loss: 1.0660110712051392\n",
      "Train: Epoch [6], Batch [520/938], Loss: 0.7701020240783691\n",
      "Train: Epoch [6], Batch [521/938], Loss: 0.678918182849884\n",
      "Train: Epoch [6], Batch [522/938], Loss: 1.1154509782791138\n",
      "Train: Epoch [6], Batch [523/938], Loss: 1.2019068002700806\n",
      "Train: Epoch [6], Batch [524/938], Loss: 1.0312654972076416\n",
      "Train: Epoch [6], Batch [525/938], Loss: 1.1042872667312622\n",
      "Train: Epoch [6], Batch [526/938], Loss: 1.183489203453064\n",
      "Train: Epoch [6], Batch [527/938], Loss: 1.132501482963562\n",
      "Train: Epoch [6], Batch [528/938], Loss: 1.1749820709228516\n",
      "Train: Epoch [6], Batch [529/938], Loss: 0.8324425220489502\n",
      "Train: Epoch [6], Batch [530/938], Loss: 1.0887105464935303\n",
      "Train: Epoch [6], Batch [531/938], Loss: 0.9631121754646301\n",
      "Train: Epoch [6], Batch [532/938], Loss: 1.0458064079284668\n",
      "Train: Epoch [6], Batch [533/938], Loss: 0.9816431999206543\n",
      "Train: Epoch [6], Batch [534/938], Loss: 0.8596256971359253\n",
      "Train: Epoch [6], Batch [535/938], Loss: 1.0418879985809326\n",
      "Train: Epoch [6], Batch [536/938], Loss: 1.0487407445907593\n",
      "Train: Epoch [6], Batch [537/938], Loss: 1.0969057083129883\n",
      "Train: Epoch [6], Batch [538/938], Loss: 0.9425419569015503\n",
      "Train: Epoch [6], Batch [539/938], Loss: 0.9967999458312988\n",
      "Train: Epoch [6], Batch [540/938], Loss: 0.9289661645889282\n",
      "Train: Epoch [6], Batch [541/938], Loss: 0.9321730136871338\n",
      "Train: Epoch [6], Batch [542/938], Loss: 1.0544239282608032\n",
      "Train: Epoch [6], Batch [543/938], Loss: 0.9530296325683594\n",
      "Train: Epoch [6], Batch [544/938], Loss: 0.9176352024078369\n",
      "Train: Epoch [6], Batch [545/938], Loss: 1.0553966760635376\n",
      "Train: Epoch [6], Batch [546/938], Loss: 0.9601941108703613\n",
      "Train: Epoch [6], Batch [547/938], Loss: 0.9203378558158875\n",
      "Train: Epoch [6], Batch [548/938], Loss: 0.928278386592865\n",
      "Train: Epoch [6], Batch [549/938], Loss: 1.219147801399231\n",
      "Train: Epoch [6], Batch [550/938], Loss: 0.999852180480957\n",
      "Train: Epoch [6], Batch [551/938], Loss: 0.8770522475242615\n",
      "Train: Epoch [6], Batch [552/938], Loss: 1.074972152709961\n",
      "Train: Epoch [6], Batch [553/938], Loss: 0.8387948870658875\n",
      "Train: Epoch [6], Batch [554/938], Loss: 1.1686726808547974\n",
      "Train: Epoch [6], Batch [555/938], Loss: 1.0268572568893433\n",
      "Train: Epoch [6], Batch [556/938], Loss: 0.8157662153244019\n",
      "Train: Epoch [6], Batch [557/938], Loss: 0.9672056436538696\n",
      "Train: Epoch [6], Batch [558/938], Loss: 0.938877284526825\n",
      "Train: Epoch [6], Batch [559/938], Loss: 1.0027936697006226\n",
      "Train: Epoch [6], Batch [560/938], Loss: 0.9046746492385864\n",
      "Train: Epoch [6], Batch [561/938], Loss: 0.7677602171897888\n",
      "Train: Epoch [6], Batch [562/938], Loss: 0.9665865898132324\n",
      "Train: Epoch [6], Batch [563/938], Loss: 1.0347874164581299\n",
      "Train: Epoch [6], Batch [564/938], Loss: 0.9752019047737122\n",
      "Train: Epoch [6], Batch [565/938], Loss: 1.0273385047912598\n",
      "Train: Epoch [6], Batch [566/938], Loss: 0.7790244817733765\n",
      "Train: Epoch [6], Batch [567/938], Loss: 1.027050256729126\n",
      "Train: Epoch [6], Batch [568/938], Loss: 0.9333842992782593\n",
      "Train: Epoch [6], Batch [569/938], Loss: 0.8513169884681702\n",
      "Train: Epoch [6], Batch [570/938], Loss: 0.8904458284378052\n",
      "Train: Epoch [6], Batch [571/938], Loss: 1.2250275611877441\n",
      "Train: Epoch [6], Batch [572/938], Loss: 0.9790687561035156\n",
      "Train: Epoch [6], Batch [573/938], Loss: 1.0043830871582031\n",
      "Train: Epoch [6], Batch [574/938], Loss: 1.1045958995819092\n",
      "Train: Epoch [6], Batch [575/938], Loss: 0.9864524006843567\n",
      "Train: Epoch [6], Batch [576/938], Loss: 0.8248719573020935\n",
      "Train: Epoch [6], Batch [577/938], Loss: 0.8185482025146484\n",
      "Train: Epoch [6], Batch [578/938], Loss: 0.8504469990730286\n",
      "Train: Epoch [6], Batch [579/938], Loss: 1.052059292793274\n",
      "Train: Epoch [6], Batch [580/938], Loss: 0.9569248557090759\n",
      "Train: Epoch [6], Batch [581/938], Loss: 0.9598515033721924\n",
      "Train: Epoch [6], Batch [582/938], Loss: 1.0513091087341309\n",
      "Train: Epoch [6], Batch [583/938], Loss: 1.0559481382369995\n",
      "Train: Epoch [6], Batch [584/938], Loss: 0.789456307888031\n",
      "Train: Epoch [6], Batch [585/938], Loss: 0.8042466640472412\n",
      "Train: Epoch [6], Batch [586/938], Loss: 0.957969069480896\n",
      "Train: Epoch [6], Batch [587/938], Loss: 1.133754849433899\n",
      "Train: Epoch [6], Batch [588/938], Loss: 0.7920517325401306\n",
      "Train: Epoch [6], Batch [589/938], Loss: 0.9204054474830627\n",
      "Train: Epoch [6], Batch [590/938], Loss: 1.0160326957702637\n",
      "Train: Epoch [6], Batch [591/938], Loss: 1.077373743057251\n",
      "Train: Epoch [6], Batch [592/938], Loss: 0.739835262298584\n",
      "Train: Epoch [6], Batch [593/938], Loss: 0.8828567862510681\n",
      "Train: Epoch [6], Batch [594/938], Loss: 0.9803008437156677\n",
      "Train: Epoch [6], Batch [595/938], Loss: 0.9902658462524414\n",
      "Train: Epoch [6], Batch [596/938], Loss: 1.270280361175537\n",
      "Train: Epoch [6], Batch [597/938], Loss: 1.2463984489440918\n",
      "Train: Epoch [6], Batch [598/938], Loss: 1.0249704122543335\n",
      "Train: Epoch [6], Batch [599/938], Loss: 0.8399903774261475\n",
      "Train: Epoch [6], Batch [600/938], Loss: 0.8378124833106995\n",
      "Train: Epoch [6], Batch [601/938], Loss: 0.9179843664169312\n",
      "Train: Epoch [6], Batch [602/938], Loss: 0.6460888385772705\n",
      "Train: Epoch [6], Batch [603/938], Loss: 1.0041437149047852\n",
      "Train: Epoch [6], Batch [604/938], Loss: 1.0158042907714844\n",
      "Train: Epoch [6], Batch [605/938], Loss: 1.0158354043960571\n",
      "Train: Epoch [6], Batch [606/938], Loss: 0.9778609275817871\n",
      "Train: Epoch [6], Batch [607/938], Loss: 0.9734463691711426\n",
      "Train: Epoch [6], Batch [608/938], Loss: 1.0940107107162476\n",
      "Train: Epoch [6], Batch [609/938], Loss: 1.335770845413208\n",
      "Train: Epoch [6], Batch [610/938], Loss: 0.9220279455184937\n",
      "Train: Epoch [6], Batch [611/938], Loss: 1.1934921741485596\n",
      "Train: Epoch [6], Batch [612/938], Loss: 0.9644814729690552\n",
      "Train: Epoch [6], Batch [613/938], Loss: 1.0194425582885742\n",
      "Train: Epoch [6], Batch [614/938], Loss: 0.8936930894851685\n",
      "Train: Epoch [6], Batch [615/938], Loss: 0.935219407081604\n",
      "Train: Epoch [6], Batch [616/938], Loss: 1.0756722688674927\n",
      "Train: Epoch [6], Batch [617/938], Loss: 0.8998494148254395\n",
      "Train: Epoch [6], Batch [618/938], Loss: 1.0005146265029907\n",
      "Train: Epoch [6], Batch [619/938], Loss: 0.9461879730224609\n",
      "Train: Epoch [6], Batch [620/938], Loss: 0.705610990524292\n",
      "Train: Epoch [6], Batch [621/938], Loss: 1.0091652870178223\n",
      "Train: Epoch [6], Batch [622/938], Loss: 0.7906843423843384\n",
      "Train: Epoch [6], Batch [623/938], Loss: 0.9953368306159973\n",
      "Train: Epoch [6], Batch [624/938], Loss: 1.0502501726150513\n",
      "Train: Epoch [6], Batch [625/938], Loss: 0.960526704788208\n",
      "Train: Epoch [6], Batch [626/938], Loss: 1.1600196361541748\n",
      "Train: Epoch [6], Batch [627/938], Loss: 0.9418969750404358\n",
      "Train: Epoch [6], Batch [628/938], Loss: 0.7978713512420654\n",
      "Train: Epoch [6], Batch [629/938], Loss: 0.9731179475784302\n",
      "Train: Epoch [6], Batch [630/938], Loss: 1.0619908571243286\n",
      "Train: Epoch [6], Batch [631/938], Loss: 1.0991227626800537\n",
      "Train: Epoch [6], Batch [632/938], Loss: 0.9137741327285767\n",
      "Train: Epoch [6], Batch [633/938], Loss: 0.927070140838623\n",
      "Train: Epoch [6], Batch [634/938], Loss: 1.1643894910812378\n",
      "Train: Epoch [6], Batch [635/938], Loss: 0.9222672581672668\n",
      "Train: Epoch [6], Batch [636/938], Loss: 0.8164482116699219\n",
      "Train: Epoch [6], Batch [637/938], Loss: 1.1267367601394653\n",
      "Train: Epoch [6], Batch [638/938], Loss: 0.9730325937271118\n",
      "Train: Epoch [6], Batch [639/938], Loss: 0.9753119349479675\n",
      "Train: Epoch [6], Batch [640/938], Loss: 1.0617048740386963\n",
      "Train: Epoch [6], Batch [641/938], Loss: 1.077090859413147\n",
      "Train: Epoch [6], Batch [642/938], Loss: 0.9426324367523193\n",
      "Train: Epoch [6], Batch [643/938], Loss: 0.8044601082801819\n",
      "Train: Epoch [6], Batch [644/938], Loss: 0.9626966714859009\n",
      "Train: Epoch [6], Batch [645/938], Loss: 0.911643922328949\n",
      "Train: Epoch [6], Batch [646/938], Loss: 0.9470058083534241\n",
      "Train: Epoch [6], Batch [647/938], Loss: 0.9092676639556885\n",
      "Train: Epoch [6], Batch [648/938], Loss: 0.9642606973648071\n",
      "Train: Epoch [6], Batch [649/938], Loss: 0.7815172672271729\n",
      "Train: Epoch [6], Batch [650/938], Loss: 1.0640729665756226\n",
      "Train: Epoch [6], Batch [651/938], Loss: 1.1346267461776733\n",
      "Train: Epoch [6], Batch [652/938], Loss: 1.237837553024292\n",
      "Train: Epoch [6], Batch [653/938], Loss: 0.8225438594818115\n",
      "Train: Epoch [6], Batch [654/938], Loss: 0.9080050587654114\n",
      "Train: Epoch [6], Batch [655/938], Loss: 0.8048048615455627\n",
      "Train: Epoch [6], Batch [656/938], Loss: 0.9051185250282288\n",
      "Train: Epoch [6], Batch [657/938], Loss: 1.1237530708312988\n",
      "Train: Epoch [6], Batch [658/938], Loss: 0.8427532911300659\n",
      "Train: Epoch [6], Batch [659/938], Loss: 0.9310063123703003\n",
      "Train: Epoch [6], Batch [660/938], Loss: 0.938056230545044\n",
      "Train: Epoch [6], Batch [661/938], Loss: 0.8309724926948547\n",
      "Train: Epoch [6], Batch [662/938], Loss: 1.1282299757003784\n",
      "Train: Epoch [6], Batch [663/938], Loss: 0.6273947954177856\n",
      "Train: Epoch [6], Batch [664/938], Loss: 1.0383656024932861\n",
      "Train: Epoch [6], Batch [665/938], Loss: 0.7965874671936035\n",
      "Train: Epoch [6], Batch [666/938], Loss: 1.0778577327728271\n",
      "Train: Epoch [6], Batch [667/938], Loss: 0.9526157379150391\n",
      "Train: Epoch [6], Batch [668/938], Loss: 0.9394983053207397\n",
      "Train: Epoch [6], Batch [669/938], Loss: 1.2204546928405762\n",
      "Train: Epoch [6], Batch [670/938], Loss: 0.9001328349113464\n",
      "Train: Epoch [6], Batch [671/938], Loss: 0.9433544874191284\n",
      "Train: Epoch [6], Batch [672/938], Loss: 1.0305418968200684\n",
      "Train: Epoch [6], Batch [673/938], Loss: 1.03351891040802\n",
      "Train: Epoch [6], Batch [674/938], Loss: 1.0699875354766846\n",
      "Train: Epoch [6], Batch [675/938], Loss: 0.8899946808815002\n",
      "Train: Epoch [6], Batch [676/938], Loss: 1.0248396396636963\n",
      "Train: Epoch [6], Batch [677/938], Loss: 0.9146618843078613\n",
      "Train: Epoch [6], Batch [678/938], Loss: 0.9968914985656738\n",
      "Train: Epoch [6], Batch [679/938], Loss: 1.0180467367172241\n",
      "Train: Epoch [6], Batch [680/938], Loss: 0.8764126896858215\n",
      "Train: Epoch [6], Batch [681/938], Loss: 0.7968783974647522\n",
      "Train: Epoch [6], Batch [682/938], Loss: 0.7059956192970276\n",
      "Train: Epoch [6], Batch [683/938], Loss: 1.064545750617981\n",
      "Train: Epoch [6], Batch [684/938], Loss: 0.9533345103263855\n",
      "Train: Epoch [6], Batch [685/938], Loss: 0.9620217084884644\n",
      "Train: Epoch [6], Batch [686/938], Loss: 0.9779955148696899\n",
      "Train: Epoch [6], Batch [687/938], Loss: 1.0801036357879639\n",
      "Train: Epoch [6], Batch [688/938], Loss: 0.764151930809021\n",
      "Train: Epoch [6], Batch [689/938], Loss: 0.8243431448936462\n",
      "Train: Epoch [6], Batch [690/938], Loss: 0.8936214447021484\n",
      "Train: Epoch [6], Batch [691/938], Loss: 0.9394576549530029\n",
      "Train: Epoch [6], Batch [692/938], Loss: 0.9438941478729248\n",
      "Train: Epoch [6], Batch [693/938], Loss: 0.8682354688644409\n",
      "Train: Epoch [6], Batch [694/938], Loss: 0.8271840810775757\n",
      "Train: Epoch [6], Batch [695/938], Loss: 0.7714489698410034\n",
      "Train: Epoch [6], Batch [696/938], Loss: 0.9964922070503235\n",
      "Train: Epoch [6], Batch [697/938], Loss: 0.9204880595207214\n",
      "Train: Epoch [6], Batch [698/938], Loss: 0.9287406206130981\n",
      "Train: Epoch [6], Batch [699/938], Loss: 0.8798772096633911\n",
      "Train: Epoch [6], Batch [700/938], Loss: 1.0310484170913696\n",
      "Train: Epoch [6], Batch [701/938], Loss: 0.8135409355163574\n",
      "Train: Epoch [6], Batch [702/938], Loss: 0.9042971134185791\n",
      "Train: Epoch [6], Batch [703/938], Loss: 0.7577358484268188\n",
      "Train: Epoch [6], Batch [704/938], Loss: 0.8387222290039062\n",
      "Train: Epoch [6], Batch [705/938], Loss: 0.9023937582969666\n",
      "Train: Epoch [6], Batch [706/938], Loss: 0.9857484102249146\n",
      "Train: Epoch [6], Batch [707/938], Loss: 0.8984361886978149\n",
      "Train: Epoch [6], Batch [708/938], Loss: 0.99735426902771\n",
      "Train: Epoch [6], Batch [709/938], Loss: 0.8455268740653992\n",
      "Train: Epoch [6], Batch [710/938], Loss: 0.9114474058151245\n",
      "Train: Epoch [6], Batch [711/938], Loss: 0.9235750436782837\n",
      "Train: Epoch [6], Batch [712/938], Loss: 0.8659642338752747\n",
      "Train: Epoch [6], Batch [713/938], Loss: 1.0841017961502075\n",
      "Train: Epoch [6], Batch [714/938], Loss: 0.7017717361450195\n",
      "Train: Epoch [6], Batch [715/938], Loss: 1.1562403440475464\n",
      "Train: Epoch [6], Batch [716/938], Loss: 1.1594730615615845\n",
      "Train: Epoch [6], Batch [717/938], Loss: 0.8758995532989502\n",
      "Train: Epoch [6], Batch [718/938], Loss: 0.9982852935791016\n",
      "Train: Epoch [6], Batch [719/938], Loss: 0.9056124687194824\n",
      "Train: Epoch [6], Batch [720/938], Loss: 0.9235541820526123\n",
      "Train: Epoch [6], Batch [721/938], Loss: 0.9634193778038025\n",
      "Train: Epoch [6], Batch [722/938], Loss: 0.9105652570724487\n",
      "Train: Epoch [6], Batch [723/938], Loss: 1.111396312713623\n",
      "Train: Epoch [6], Batch [724/938], Loss: 0.8849393129348755\n",
      "Train: Epoch [6], Batch [725/938], Loss: 0.9756408929824829\n",
      "Train: Epoch [6], Batch [726/938], Loss: 1.1096110343933105\n",
      "Train: Epoch [6], Batch [727/938], Loss: 0.9975746273994446\n",
      "Train: Epoch [6], Batch [728/938], Loss: 0.9295312166213989\n",
      "Train: Epoch [6], Batch [729/938], Loss: 0.9728055000305176\n",
      "Train: Epoch [6], Batch [730/938], Loss: 0.7552826404571533\n",
      "Train: Epoch [6], Batch [731/938], Loss: 0.8762726187705994\n",
      "Train: Epoch [6], Batch [732/938], Loss: 1.0785295963287354\n",
      "Train: Epoch [6], Batch [733/938], Loss: 0.920635998249054\n",
      "Train: Epoch [6], Batch [734/938], Loss: 0.917481541633606\n",
      "Train: Epoch [6], Batch [735/938], Loss: 1.035576343536377\n",
      "Train: Epoch [6], Batch [736/938], Loss: 1.0682872533798218\n",
      "Train: Epoch [6], Batch [737/938], Loss: 0.8650739192962646\n",
      "Train: Epoch [6], Batch [738/938], Loss: 0.7151615619659424\n",
      "Train: Epoch [6], Batch [739/938], Loss: 0.8738292455673218\n",
      "Train: Epoch [6], Batch [740/938], Loss: 1.2434405088424683\n",
      "Train: Epoch [6], Batch [741/938], Loss: 0.9228615164756775\n",
      "Train: Epoch [6], Batch [742/938], Loss: 0.8109526634216309\n",
      "Train: Epoch [6], Batch [743/938], Loss: 1.0972411632537842\n",
      "Train: Epoch [6], Batch [744/938], Loss: 0.8494178652763367\n",
      "Train: Epoch [6], Batch [745/938], Loss: 0.8937801718711853\n",
      "Train: Epoch [6], Batch [746/938], Loss: 0.877540111541748\n",
      "Train: Epoch [6], Batch [747/938], Loss: 1.0755031108856201\n",
      "Train: Epoch [6], Batch [748/938], Loss: 1.160168170928955\n",
      "Train: Epoch [6], Batch [749/938], Loss: 0.8379185795783997\n",
      "Train: Epoch [6], Batch [750/938], Loss: 1.0715043544769287\n",
      "Train: Epoch [6], Batch [751/938], Loss: 1.355851411819458\n",
      "Train: Epoch [6], Batch [752/938], Loss: 0.9059984683990479\n",
      "Train: Epoch [6], Batch [753/938], Loss: 0.8447557687759399\n",
      "Train: Epoch [6], Batch [754/938], Loss: 0.8640868067741394\n",
      "Train: Epoch [6], Batch [755/938], Loss: 0.8814323544502258\n",
      "Train: Epoch [6], Batch [756/938], Loss: 0.9141221046447754\n",
      "Train: Epoch [6], Batch [757/938], Loss: 0.8878630995750427\n",
      "Train: Epoch [6], Batch [758/938], Loss: 0.7354415655136108\n",
      "Train: Epoch [6], Batch [759/938], Loss: 1.1550712585449219\n",
      "Train: Epoch [6], Batch [760/938], Loss: 0.9269441366195679\n",
      "Train: Epoch [6], Batch [761/938], Loss: 0.865928590297699\n",
      "Train: Epoch [6], Batch [762/938], Loss: 1.058622121810913\n",
      "Train: Epoch [6], Batch [763/938], Loss: 0.8646212816238403\n",
      "Train: Epoch [6], Batch [764/938], Loss: 1.0154868364334106\n",
      "Train: Epoch [6], Batch [765/938], Loss: 1.126699686050415\n",
      "Train: Epoch [6], Batch [766/938], Loss: 1.0402319431304932\n",
      "Train: Epoch [6], Batch [767/938], Loss: 1.1141715049743652\n",
      "Train: Epoch [6], Batch [768/938], Loss: 0.9073886871337891\n",
      "Train: Epoch [6], Batch [769/938], Loss: 0.837320864200592\n",
      "Train: Epoch [6], Batch [770/938], Loss: 0.8744577169418335\n",
      "Train: Epoch [6], Batch [771/938], Loss: 1.3676656484603882\n",
      "Train: Epoch [6], Batch [772/938], Loss: 0.9494413733482361\n",
      "Train: Epoch [6], Batch [773/938], Loss: 1.276236891746521\n",
      "Train: Epoch [6], Batch [774/938], Loss: 0.9755108952522278\n",
      "Train: Epoch [6], Batch [775/938], Loss: 0.9578955769538879\n",
      "Train: Epoch [6], Batch [776/938], Loss: 0.7599602937698364\n",
      "Train: Epoch [6], Batch [777/938], Loss: 0.9397717714309692\n",
      "Train: Epoch [6], Batch [778/938], Loss: 0.9976354837417603\n",
      "Train: Epoch [6], Batch [779/938], Loss: 0.8388753533363342\n",
      "Train: Epoch [6], Batch [780/938], Loss: 1.188292145729065\n",
      "Train: Epoch [6], Batch [781/938], Loss: 0.8821887969970703\n",
      "Train: Epoch [6], Batch [782/938], Loss: 0.9636162519454956\n",
      "Train: Epoch [6], Batch [783/938], Loss: 0.8380106091499329\n",
      "Train: Epoch [6], Batch [784/938], Loss: 0.8182902336120605\n",
      "Train: Epoch [6], Batch [785/938], Loss: 1.0611934661865234\n",
      "Train: Epoch [6], Batch [786/938], Loss: 0.9165979623794556\n",
      "Train: Epoch [6], Batch [787/938], Loss: 0.7763381004333496\n",
      "Train: Epoch [6], Batch [788/938], Loss: 1.0364233255386353\n",
      "Train: Epoch [6], Batch [789/938], Loss: 0.9129291772842407\n",
      "Train: Epoch [6], Batch [790/938], Loss: 1.051382303237915\n",
      "Train: Epoch [6], Batch [791/938], Loss: 0.9951547384262085\n",
      "Train: Epoch [6], Batch [792/938], Loss: 0.9910842776298523\n",
      "Train: Epoch [6], Batch [793/938], Loss: 0.7771778702735901\n",
      "Train: Epoch [6], Batch [794/938], Loss: 1.0349640846252441\n",
      "Train: Epoch [6], Batch [795/938], Loss: 0.9757668375968933\n",
      "Train: Epoch [6], Batch [796/938], Loss: 1.113929271697998\n",
      "Train: Epoch [6], Batch [797/938], Loss: 1.0950990915298462\n",
      "Train: Epoch [6], Batch [798/938], Loss: 1.3790733814239502\n",
      "Train: Epoch [6], Batch [799/938], Loss: 0.829820990562439\n",
      "Train: Epoch [6], Batch [800/938], Loss: 1.0418649911880493\n",
      "Train: Epoch [6], Batch [801/938], Loss: 0.8951274156570435\n",
      "Train: Epoch [6], Batch [802/938], Loss: 1.1256376504898071\n",
      "Train: Epoch [6], Batch [803/938], Loss: 0.7642918825149536\n",
      "Train: Epoch [6], Batch [804/938], Loss: 1.0542309284210205\n",
      "Train: Epoch [6], Batch [805/938], Loss: 1.1151480674743652\n",
      "Train: Epoch [6], Batch [806/938], Loss: 1.0053763389587402\n",
      "Train: Epoch [6], Batch [807/938], Loss: 0.7620867490768433\n",
      "Train: Epoch [6], Batch [808/938], Loss: 1.0909292697906494\n",
      "Train: Epoch [6], Batch [809/938], Loss: 1.1307274103164673\n",
      "Train: Epoch [6], Batch [810/938], Loss: 0.921238899230957\n",
      "Train: Epoch [6], Batch [811/938], Loss: 0.8727517127990723\n",
      "Train: Epoch [6], Batch [812/938], Loss: 0.998104989528656\n",
      "Train: Epoch [6], Batch [813/938], Loss: 0.8969714641571045\n",
      "Train: Epoch [6], Batch [814/938], Loss: 0.9238516688346863\n",
      "Train: Epoch [6], Batch [815/938], Loss: 0.9207568168640137\n",
      "Train: Epoch [6], Batch [816/938], Loss: 0.8257445693016052\n",
      "Train: Epoch [6], Batch [817/938], Loss: 0.8800680637359619\n",
      "Train: Epoch [6], Batch [818/938], Loss: 0.7545351386070251\n",
      "Train: Epoch [6], Batch [819/938], Loss: 0.7738825082778931\n",
      "Train: Epoch [6], Batch [820/938], Loss: 0.961785078048706\n",
      "Train: Epoch [6], Batch [821/938], Loss: 1.0653918981552124\n",
      "Train: Epoch [6], Batch [822/938], Loss: 1.0992189645767212\n",
      "Train: Epoch [6], Batch [823/938], Loss: 0.8329651355743408\n",
      "Train: Epoch [6], Batch [824/938], Loss: 0.9504780173301697\n",
      "Train: Epoch [6], Batch [825/938], Loss: 0.8295198678970337\n",
      "Train: Epoch [6], Batch [826/938], Loss: 1.145057201385498\n",
      "Train: Epoch [6], Batch [827/938], Loss: 1.019081473350525\n",
      "Train: Epoch [6], Batch [828/938], Loss: 0.8851963877677917\n",
      "Train: Epoch [6], Batch [829/938], Loss: 0.8564016819000244\n",
      "Train: Epoch [6], Batch [830/938], Loss: 0.9631472826004028\n",
      "Train: Epoch [6], Batch [831/938], Loss: 0.9277543425559998\n",
      "Train: Epoch [6], Batch [832/938], Loss: 0.9397756457328796\n",
      "Train: Epoch [6], Batch [833/938], Loss: 1.224572777748108\n",
      "Train: Epoch [6], Batch [834/938], Loss: 0.8362753391265869\n",
      "Train: Epoch [6], Batch [835/938], Loss: 0.9909026026725769\n",
      "Train: Epoch [6], Batch [836/938], Loss: 1.065342664718628\n",
      "Train: Epoch [6], Batch [837/938], Loss: 1.196014404296875\n",
      "Train: Epoch [6], Batch [838/938], Loss: 0.9935634136199951\n",
      "Train: Epoch [6], Batch [839/938], Loss: 1.0292675495147705\n",
      "Train: Epoch [6], Batch [840/938], Loss: 0.8401033282279968\n",
      "Train: Epoch [6], Batch [841/938], Loss: 0.9269789457321167\n",
      "Train: Epoch [6], Batch [842/938], Loss: 0.8769460916519165\n",
      "Train: Epoch [6], Batch [843/938], Loss: 1.0093193054199219\n",
      "Train: Epoch [6], Batch [844/938], Loss: 1.01385498046875\n",
      "Train: Epoch [6], Batch [845/938], Loss: 1.0081197023391724\n",
      "Train: Epoch [6], Batch [846/938], Loss: 0.9052696228027344\n",
      "Train: Epoch [6], Batch [847/938], Loss: 1.2052937746047974\n",
      "Train: Epoch [6], Batch [848/938], Loss: 0.9905089139938354\n",
      "Train: Epoch [6], Batch [849/938], Loss: 0.9458385109901428\n",
      "Train: Epoch [6], Batch [850/938], Loss: 1.0918514728546143\n",
      "Train: Epoch [6], Batch [851/938], Loss: 0.8344898223876953\n",
      "Train: Epoch [6], Batch [852/938], Loss: 0.8594703674316406\n",
      "Train: Epoch [6], Batch [853/938], Loss: 0.8612935543060303\n",
      "Train: Epoch [6], Batch [854/938], Loss: 0.8144505023956299\n",
      "Train: Epoch [6], Batch [855/938], Loss: 1.0759180784225464\n",
      "Train: Epoch [6], Batch [856/938], Loss: 0.9291030764579773\n",
      "Train: Epoch [6], Batch [857/938], Loss: 0.8770337104797363\n",
      "Train: Epoch [6], Batch [858/938], Loss: 1.0237611532211304\n",
      "Train: Epoch [6], Batch [859/938], Loss: 0.9285537004470825\n",
      "Train: Epoch [6], Batch [860/938], Loss: 0.9018212556838989\n",
      "Train: Epoch [6], Batch [861/938], Loss: 0.9764729738235474\n",
      "Train: Epoch [6], Batch [862/938], Loss: 0.8449889421463013\n",
      "Train: Epoch [6], Batch [863/938], Loss: 0.7739776968955994\n",
      "Train: Epoch [6], Batch [864/938], Loss: 0.827202558517456\n",
      "Train: Epoch [6], Batch [865/938], Loss: 0.8151801824569702\n",
      "Train: Epoch [6], Batch [866/938], Loss: 0.8892015218734741\n",
      "Train: Epoch [6], Batch [867/938], Loss: 0.8381997346878052\n",
      "Train: Epoch [6], Batch [868/938], Loss: 0.5956514477729797\n",
      "Train: Epoch [6], Batch [869/938], Loss: 1.1410796642303467\n",
      "Train: Epoch [6], Batch [870/938], Loss: 1.138967514038086\n",
      "Train: Epoch [6], Batch [871/938], Loss: 0.7488992810249329\n",
      "Train: Epoch [6], Batch [872/938], Loss: 0.7787787914276123\n",
      "Train: Epoch [6], Batch [873/938], Loss: 0.64412522315979\n",
      "Train: Epoch [6], Batch [874/938], Loss: 0.8059860467910767\n",
      "Train: Epoch [6], Batch [875/938], Loss: 1.0917935371398926\n",
      "Train: Epoch [6], Batch [876/938], Loss: 0.6741482019424438\n",
      "Train: Epoch [6], Batch [877/938], Loss: 0.7763405442237854\n",
      "Train: Epoch [6], Batch [878/938], Loss: 0.9192053079605103\n",
      "Train: Epoch [6], Batch [879/938], Loss: 1.0026167631149292\n",
      "Train: Epoch [6], Batch [880/938], Loss: 1.0301233530044556\n",
      "Train: Epoch [6], Batch [881/938], Loss: 0.9122793674468994\n",
      "Train: Epoch [6], Batch [882/938], Loss: 0.8718798160552979\n",
      "Train: Epoch [6], Batch [883/938], Loss: 0.8656629323959351\n",
      "Train: Epoch [6], Batch [884/938], Loss: 0.8641549944877625\n",
      "Train: Epoch [6], Batch [885/938], Loss: 0.7284253835678101\n",
      "Train: Epoch [6], Batch [886/938], Loss: 1.074927806854248\n",
      "Train: Epoch [6], Batch [887/938], Loss: 1.000364065170288\n",
      "Train: Epoch [6], Batch [888/938], Loss: 0.7077290415763855\n",
      "Train: Epoch [6], Batch [889/938], Loss: 1.0834710597991943\n",
      "Train: Epoch [6], Batch [890/938], Loss: 1.0811254978179932\n",
      "Train: Epoch [6], Batch [891/938], Loss: 0.8570096492767334\n",
      "Train: Epoch [6], Batch [892/938], Loss: 1.2113903760910034\n",
      "Train: Epoch [6], Batch [893/938], Loss: 1.075003743171692\n",
      "Train: Epoch [6], Batch [894/938], Loss: 0.8456695079803467\n",
      "Train: Epoch [6], Batch [895/938], Loss: 0.7518355250358582\n",
      "Train: Epoch [6], Batch [896/938], Loss: 0.724906325340271\n",
      "Train: Epoch [6], Batch [897/938], Loss: 0.826592743396759\n",
      "Train: Epoch [6], Batch [898/938], Loss: 0.8820064067840576\n",
      "Train: Epoch [6], Batch [899/938], Loss: 1.012907862663269\n",
      "Train: Epoch [6], Batch [900/938], Loss: 0.8889989852905273\n",
      "Train: Epoch [6], Batch [901/938], Loss: 0.9116089344024658\n",
      "Train: Epoch [6], Batch [902/938], Loss: 0.9201797246932983\n",
      "Train: Epoch [6], Batch [903/938], Loss: 0.7293455004692078\n",
      "Train: Epoch [6], Batch [904/938], Loss: 0.9414503574371338\n",
      "Train: Epoch [6], Batch [905/938], Loss: 1.0916950702667236\n",
      "Train: Epoch [6], Batch [906/938], Loss: 0.7464583516120911\n",
      "Train: Epoch [6], Batch [907/938], Loss: 0.9314097762107849\n",
      "Train: Epoch [6], Batch [908/938], Loss: 0.7316545248031616\n",
      "Train: Epoch [6], Batch [909/938], Loss: 0.6877206563949585\n",
      "Train: Epoch [6], Batch [910/938], Loss: 0.8429132699966431\n",
      "Train: Epoch [6], Batch [911/938], Loss: 0.8211416602134705\n",
      "Train: Epoch [6], Batch [912/938], Loss: 0.8406301736831665\n",
      "Train: Epoch [6], Batch [913/938], Loss: 0.7815243005752563\n",
      "Train: Epoch [6], Batch [914/938], Loss: 0.8837992548942566\n",
      "Train: Epoch [6], Batch [915/938], Loss: 0.974567174911499\n",
      "Train: Epoch [6], Batch [916/938], Loss: 0.8922967314720154\n",
      "Train: Epoch [6], Batch [917/938], Loss: 1.0008118152618408\n",
      "Train: Epoch [6], Batch [918/938], Loss: 1.2812422513961792\n",
      "Train: Epoch [6], Batch [919/938], Loss: 1.0043752193450928\n",
      "Train: Epoch [6], Batch [920/938], Loss: 0.7973892092704773\n",
      "Train: Epoch [6], Batch [921/938], Loss: 0.9311090111732483\n",
      "Train: Epoch [6], Batch [922/938], Loss: 1.1565282344818115\n",
      "Train: Epoch [6], Batch [923/938], Loss: 0.923252284526825\n",
      "Train: Epoch [6], Batch [924/938], Loss: 0.8888899087905884\n",
      "Train: Epoch [6], Batch [925/938], Loss: 0.8820802569389343\n",
      "Train: Epoch [6], Batch [926/938], Loss: 0.97933429479599\n",
      "Train: Epoch [6], Batch [927/938], Loss: 0.8441839218139648\n",
      "Train: Epoch [6], Batch [928/938], Loss: 0.8148672580718994\n",
      "Train: Epoch [6], Batch [929/938], Loss: 0.9689058661460876\n",
      "Train: Epoch [6], Batch [930/938], Loss: 0.8702343702316284\n",
      "Train: Epoch [6], Batch [931/938], Loss: 0.7330804467201233\n",
      "Train: Epoch [6], Batch [932/938], Loss: 1.0504745244979858\n",
      "Train: Epoch [6], Batch [933/938], Loss: 0.9584395885467529\n",
      "Train: Epoch [6], Batch [934/938], Loss: 0.8274063467979431\n",
      "Train: Epoch [6], Batch [935/938], Loss: 1.071523904800415\n",
      "Train: Epoch [6], Batch [936/938], Loss: 0.8504630327224731\n",
      "Train: Epoch [6], Batch [937/938], Loss: 1.0753531455993652\n",
      "Train: Epoch [6], Batch [938/938], Loss: 0.7678274512290955\n",
      "Accuracy of train set: 0.6394333333333333\n",
      "Validation: Epoch [6], Batch [1/938], Loss: 1.1812376976013184\n",
      "Validation: Epoch [6], Batch [2/938], Loss: 1.0350489616394043\n",
      "Validation: Epoch [6], Batch [3/938], Loss: 0.7850092649459839\n",
      "Validation: Epoch [6], Batch [4/938], Loss: 1.0291175842285156\n",
      "Validation: Epoch [6], Batch [5/938], Loss: 1.1244621276855469\n",
      "Validation: Epoch [6], Batch [6/938], Loss: 0.8932228088378906\n",
      "Validation: Epoch [6], Batch [7/938], Loss: 1.042435646057129\n",
      "Validation: Epoch [6], Batch [8/938], Loss: 0.9781805872917175\n",
      "Validation: Epoch [6], Batch [9/938], Loss: 1.0599462985992432\n",
      "Validation: Epoch [6], Batch [10/938], Loss: 0.785925567150116\n",
      "Validation: Epoch [6], Batch [11/938], Loss: 0.9757044315338135\n",
      "Validation: Epoch [6], Batch [12/938], Loss: 0.8890950083732605\n",
      "Validation: Epoch [6], Batch [13/938], Loss: 0.8631271719932556\n",
      "Validation: Epoch [6], Batch [14/938], Loss: 1.086450457572937\n",
      "Validation: Epoch [6], Batch [15/938], Loss: 0.9015718698501587\n",
      "Validation: Epoch [6], Batch [16/938], Loss: 0.819246768951416\n",
      "Validation: Epoch [6], Batch [17/938], Loss: 0.9838119149208069\n",
      "Validation: Epoch [6], Batch [18/938], Loss: 1.0116831064224243\n",
      "Validation: Epoch [6], Batch [19/938], Loss: 0.771354079246521\n",
      "Validation: Epoch [6], Batch [20/938], Loss: 0.9782806038856506\n",
      "Validation: Epoch [6], Batch [21/938], Loss: 0.9906935691833496\n",
      "Validation: Epoch [6], Batch [22/938], Loss: 0.759595513343811\n",
      "Validation: Epoch [6], Batch [23/938], Loss: 1.0251579284667969\n",
      "Validation: Epoch [6], Batch [24/938], Loss: 1.0320265293121338\n",
      "Validation: Epoch [6], Batch [25/938], Loss: 0.8037375211715698\n",
      "Validation: Epoch [6], Batch [26/938], Loss: 0.8654877543449402\n",
      "Validation: Epoch [6], Batch [27/938], Loss: 0.9636410474777222\n",
      "Validation: Epoch [6], Batch [28/938], Loss: 0.7324037551879883\n",
      "Validation: Epoch [6], Batch [29/938], Loss: 0.8510699272155762\n",
      "Validation: Epoch [6], Batch [30/938], Loss: 1.359931230545044\n",
      "Validation: Epoch [6], Batch [31/938], Loss: 1.1514286994934082\n",
      "Validation: Epoch [6], Batch [32/938], Loss: 0.8229207992553711\n",
      "Validation: Epoch [6], Batch [33/938], Loss: 1.030198574066162\n",
      "Validation: Epoch [6], Batch [34/938], Loss: 0.8509185314178467\n",
      "Validation: Epoch [6], Batch [35/938], Loss: 0.9645991921424866\n",
      "Validation: Epoch [6], Batch [36/938], Loss: 1.041806936264038\n",
      "Validation: Epoch [6], Batch [37/938], Loss: 0.932020902633667\n",
      "Validation: Epoch [6], Batch [38/938], Loss: 0.9006468057632446\n",
      "Validation: Epoch [6], Batch [39/938], Loss: 0.9692801833152771\n",
      "Validation: Epoch [6], Batch [40/938], Loss: 0.8997424840927124\n",
      "Validation: Epoch [6], Batch [41/938], Loss: 1.002571702003479\n",
      "Validation: Epoch [6], Batch [42/938], Loss: 1.1994911432266235\n",
      "Validation: Epoch [6], Batch [43/938], Loss: 1.283271312713623\n",
      "Validation: Epoch [6], Batch [44/938], Loss: 1.1178574562072754\n",
      "Validation: Epoch [6], Batch [45/938], Loss: 0.927450954914093\n",
      "Validation: Epoch [6], Batch [46/938], Loss: 0.9539233446121216\n",
      "Validation: Epoch [6], Batch [47/938], Loss: 0.9437664747238159\n",
      "Validation: Epoch [6], Batch [48/938], Loss: 1.172263264656067\n",
      "Validation: Epoch [6], Batch [49/938], Loss: 0.7828933000564575\n",
      "Validation: Epoch [6], Batch [50/938], Loss: 0.9669698476791382\n",
      "Validation: Epoch [6], Batch [51/938], Loss: 0.8025356531143188\n",
      "Validation: Epoch [6], Batch [52/938], Loss: 1.055728554725647\n",
      "Validation: Epoch [6], Batch [53/938], Loss: 0.918272852897644\n",
      "Validation: Epoch [6], Batch [54/938], Loss: 0.7982029318809509\n",
      "Validation: Epoch [6], Batch [55/938], Loss: 1.0161428451538086\n",
      "Validation: Epoch [6], Batch [56/938], Loss: 0.9108830094337463\n",
      "Validation: Epoch [6], Batch [57/938], Loss: 0.9831678867340088\n",
      "Validation: Epoch [6], Batch [58/938], Loss: 0.8062078952789307\n",
      "Validation: Epoch [6], Batch [59/938], Loss: 0.7809811234474182\n",
      "Validation: Epoch [6], Batch [60/938], Loss: 0.9847007989883423\n",
      "Validation: Epoch [6], Batch [61/938], Loss: 0.8581474423408508\n",
      "Validation: Epoch [6], Batch [62/938], Loss: 1.0129562616348267\n",
      "Validation: Epoch [6], Batch [63/938], Loss: 1.0590652227401733\n",
      "Validation: Epoch [6], Batch [64/938], Loss: 1.0820151567459106\n",
      "Validation: Epoch [6], Batch [65/938], Loss: 0.8948155045509338\n",
      "Validation: Epoch [6], Batch [66/938], Loss: 0.853451669216156\n",
      "Validation: Epoch [6], Batch [67/938], Loss: 1.0197588205337524\n",
      "Validation: Epoch [6], Batch [68/938], Loss: 0.8255983591079712\n",
      "Validation: Epoch [6], Batch [69/938], Loss: 0.9736735820770264\n",
      "Validation: Epoch [6], Batch [70/938], Loss: 0.8323728442192078\n",
      "Validation: Epoch [6], Batch [71/938], Loss: 1.1972612142562866\n",
      "Validation: Epoch [6], Batch [72/938], Loss: 0.9676686525344849\n",
      "Validation: Epoch [6], Batch [73/938], Loss: 1.0093764066696167\n",
      "Validation: Epoch [6], Batch [74/938], Loss: 0.8419053554534912\n",
      "Validation: Epoch [6], Batch [75/938], Loss: 1.0236454010009766\n",
      "Validation: Epoch [6], Batch [76/938], Loss: 0.8327881097793579\n",
      "Validation: Epoch [6], Batch [77/938], Loss: 1.073946237564087\n",
      "Validation: Epoch [6], Batch [78/938], Loss: 1.047041893005371\n",
      "Validation: Epoch [6], Batch [79/938], Loss: 1.0188926458358765\n",
      "Validation: Epoch [6], Batch [80/938], Loss: 1.0039587020874023\n",
      "Validation: Epoch [6], Batch [81/938], Loss: 0.8936935067176819\n",
      "Validation: Epoch [6], Batch [82/938], Loss: 1.1127970218658447\n",
      "Validation: Epoch [6], Batch [83/938], Loss: 0.8065425157546997\n",
      "Validation: Epoch [6], Batch [84/938], Loss: 1.1604269742965698\n",
      "Validation: Epoch [6], Batch [85/938], Loss: 0.7308846116065979\n",
      "Validation: Epoch [6], Batch [86/938], Loss: 0.8976694941520691\n",
      "Validation: Epoch [6], Batch [87/938], Loss: 0.7995872497558594\n",
      "Validation: Epoch [6], Batch [88/938], Loss: 0.846909761428833\n",
      "Validation: Epoch [6], Batch [89/938], Loss: 0.82184898853302\n",
      "Validation: Epoch [6], Batch [90/938], Loss: 0.9097902774810791\n",
      "Validation: Epoch [6], Batch [91/938], Loss: 0.8928708434104919\n",
      "Validation: Epoch [6], Batch [92/938], Loss: 1.059951663017273\n",
      "Validation: Epoch [6], Batch [93/938], Loss: 0.9785706996917725\n",
      "Validation: Epoch [6], Batch [94/938], Loss: 0.8772322535514832\n",
      "Validation: Epoch [6], Batch [95/938], Loss: 0.9351085424423218\n",
      "Validation: Epoch [6], Batch [96/938], Loss: 0.9225061535835266\n",
      "Validation: Epoch [6], Batch [97/938], Loss: 0.8418088555335999\n",
      "Validation: Epoch [6], Batch [98/938], Loss: 1.0538588762283325\n",
      "Validation: Epoch [6], Batch [99/938], Loss: 0.7562274932861328\n",
      "Validation: Epoch [6], Batch [100/938], Loss: 1.0344904661178589\n",
      "Validation: Epoch [6], Batch [101/938], Loss: 0.9156041145324707\n",
      "Validation: Epoch [6], Batch [102/938], Loss: 0.9139400124549866\n",
      "Validation: Epoch [6], Batch [103/938], Loss: 0.6790231466293335\n",
      "Validation: Epoch [6], Batch [104/938], Loss: 0.8814640045166016\n",
      "Validation: Epoch [6], Batch [105/938], Loss: 0.7674874663352966\n",
      "Validation: Epoch [6], Batch [106/938], Loss: 0.9184715747833252\n",
      "Validation: Epoch [6], Batch [107/938], Loss: 0.9311099052429199\n",
      "Validation: Epoch [6], Batch [108/938], Loss: 0.8842233419418335\n",
      "Validation: Epoch [6], Batch [109/938], Loss: 0.7949022650718689\n",
      "Validation: Epoch [6], Batch [110/938], Loss: 0.8575292825698853\n",
      "Validation: Epoch [6], Batch [111/938], Loss: 0.8865463733673096\n",
      "Validation: Epoch [6], Batch [112/938], Loss: 0.7328341007232666\n",
      "Validation: Epoch [6], Batch [113/938], Loss: 0.9433802366256714\n",
      "Validation: Epoch [6], Batch [114/938], Loss: 0.9277616143226624\n",
      "Validation: Epoch [6], Batch [115/938], Loss: 1.0888670682907104\n",
      "Validation: Epoch [6], Batch [116/938], Loss: 0.7475324869155884\n",
      "Validation: Epoch [6], Batch [117/938], Loss: 1.1834912300109863\n",
      "Validation: Epoch [6], Batch [118/938], Loss: 0.9169490933418274\n",
      "Validation: Epoch [6], Batch [119/938], Loss: 0.8372257351875305\n",
      "Validation: Epoch [6], Batch [120/938], Loss: 0.9295328855514526\n",
      "Validation: Epoch [6], Batch [121/938], Loss: 0.8070520162582397\n",
      "Validation: Epoch [6], Batch [122/938], Loss: 1.059609055519104\n",
      "Validation: Epoch [6], Batch [123/938], Loss: 0.9886480569839478\n",
      "Validation: Epoch [6], Batch [124/938], Loss: 1.095037817955017\n",
      "Validation: Epoch [6], Batch [125/938], Loss: 0.8298032283782959\n",
      "Validation: Epoch [6], Batch [126/938], Loss: 0.7030598521232605\n",
      "Validation: Epoch [6], Batch [127/938], Loss: 0.8215737342834473\n",
      "Validation: Epoch [6], Batch [128/938], Loss: 1.0486736297607422\n",
      "Validation: Epoch [6], Batch [129/938], Loss: 1.0450626611709595\n",
      "Validation: Epoch [6], Batch [130/938], Loss: 1.0241953134536743\n",
      "Validation: Epoch [6], Batch [131/938], Loss: 0.8411723375320435\n",
      "Validation: Epoch [6], Batch [132/938], Loss: 0.8508143424987793\n",
      "Validation: Epoch [6], Batch [133/938], Loss: 0.7862918376922607\n",
      "Validation: Epoch [6], Batch [134/938], Loss: 1.1972949504852295\n",
      "Validation: Epoch [6], Batch [135/938], Loss: 0.8159134984016418\n",
      "Validation: Epoch [6], Batch [136/938], Loss: 0.8874834775924683\n",
      "Validation: Epoch [6], Batch [137/938], Loss: 0.9128077030181885\n",
      "Validation: Epoch [6], Batch [138/938], Loss: 0.9092661142349243\n",
      "Validation: Epoch [6], Batch [139/938], Loss: 1.1651654243469238\n",
      "Validation: Epoch [6], Batch [140/938], Loss: 0.9990863800048828\n",
      "Validation: Epoch [6], Batch [141/938], Loss: 0.910881757736206\n",
      "Validation: Epoch [6], Batch [142/938], Loss: 0.8877046704292297\n",
      "Validation: Epoch [6], Batch [143/938], Loss: 0.9960390329360962\n",
      "Validation: Epoch [6], Batch [144/938], Loss: 0.8445215225219727\n",
      "Validation: Epoch [6], Batch [145/938], Loss: 0.8908821940422058\n",
      "Validation: Epoch [6], Batch [146/938], Loss: 0.9725995063781738\n",
      "Validation: Epoch [6], Batch [147/938], Loss: 0.9225904941558838\n",
      "Validation: Epoch [6], Batch [148/938], Loss: 0.9908653497695923\n",
      "Validation: Epoch [6], Batch [149/938], Loss: 0.929070770740509\n",
      "Validation: Epoch [6], Batch [150/938], Loss: 1.134477972984314\n",
      "Validation: Epoch [6], Batch [151/938], Loss: 0.9939501285552979\n",
      "Validation: Epoch [6], Batch [152/938], Loss: 0.8593042492866516\n",
      "Validation: Epoch [6], Batch [153/938], Loss: 0.9340392351150513\n",
      "Validation: Epoch [6], Batch [154/938], Loss: 0.8186819553375244\n",
      "Validation: Epoch [6], Batch [155/938], Loss: 0.9770451188087463\n",
      "Validation: Epoch [6], Batch [156/938], Loss: 1.0218218564987183\n",
      "Validation: Epoch [6], Batch [157/938], Loss: 1.025455117225647\n",
      "Validation: Epoch [6], Batch [158/938], Loss: 1.1744134426116943\n",
      "Validation: Epoch [6], Batch [159/938], Loss: 1.0091232061386108\n",
      "Validation: Epoch [6], Batch [160/938], Loss: 1.0112274885177612\n",
      "Validation: Epoch [6], Batch [161/938], Loss: 1.1088297367095947\n",
      "Validation: Epoch [6], Batch [162/938], Loss: 0.7211093902587891\n",
      "Validation: Epoch [6], Batch [163/938], Loss: 0.7225958108901978\n",
      "Validation: Epoch [6], Batch [164/938], Loss: 1.100417971611023\n",
      "Validation: Epoch [6], Batch [165/938], Loss: 1.0304406881332397\n",
      "Validation: Epoch [6], Batch [166/938], Loss: 0.9390432834625244\n",
      "Validation: Epoch [6], Batch [167/938], Loss: 0.8428510427474976\n",
      "Validation: Epoch [6], Batch [168/938], Loss: 1.0036126375198364\n",
      "Validation: Epoch [6], Batch [169/938], Loss: 0.9216076731681824\n",
      "Validation: Epoch [6], Batch [170/938], Loss: 0.903985321521759\n",
      "Validation: Epoch [6], Batch [171/938], Loss: 0.7495149970054626\n",
      "Validation: Epoch [6], Batch [172/938], Loss: 0.9658794403076172\n",
      "Validation: Epoch [6], Batch [173/938], Loss: 0.917676568031311\n",
      "Validation: Epoch [6], Batch [174/938], Loss: 1.0931459665298462\n",
      "Validation: Epoch [6], Batch [175/938], Loss: 0.8923943638801575\n",
      "Validation: Epoch [6], Batch [176/938], Loss: 0.7699941396713257\n",
      "Validation: Epoch [6], Batch [177/938], Loss: 0.8566672801971436\n",
      "Validation: Epoch [6], Batch [178/938], Loss: 1.0875320434570312\n",
      "Validation: Epoch [6], Batch [179/938], Loss: 0.876722514629364\n",
      "Validation: Epoch [6], Batch [180/938], Loss: 0.8528681993484497\n",
      "Validation: Epoch [6], Batch [181/938], Loss: 0.8596698045730591\n",
      "Validation: Epoch [6], Batch [182/938], Loss: 0.852258563041687\n",
      "Validation: Epoch [6], Batch [183/938], Loss: 1.1401309967041016\n",
      "Validation: Epoch [6], Batch [184/938], Loss: 0.8603334426879883\n",
      "Validation: Epoch [6], Batch [185/938], Loss: 0.9358530640602112\n",
      "Validation: Epoch [6], Batch [186/938], Loss: 0.9111899137496948\n",
      "Validation: Epoch [6], Batch [187/938], Loss: 1.08230459690094\n",
      "Validation: Epoch [6], Batch [188/938], Loss: 0.8305269479751587\n",
      "Validation: Epoch [6], Batch [189/938], Loss: 1.092574119567871\n",
      "Validation: Epoch [6], Batch [190/938], Loss: 0.8243380784988403\n",
      "Validation: Epoch [6], Batch [191/938], Loss: 0.9741441011428833\n",
      "Validation: Epoch [6], Batch [192/938], Loss: 0.7970452904701233\n",
      "Validation: Epoch [6], Batch [193/938], Loss: 1.1137995719909668\n",
      "Validation: Epoch [6], Batch [194/938], Loss: 0.7591503858566284\n",
      "Validation: Epoch [6], Batch [195/938], Loss: 1.1048563718795776\n",
      "Validation: Epoch [6], Batch [196/938], Loss: 0.9285005927085876\n",
      "Validation: Epoch [6], Batch [197/938], Loss: 0.7843189835548401\n",
      "Validation: Epoch [6], Batch [198/938], Loss: 0.9620646238327026\n",
      "Validation: Epoch [6], Batch [199/938], Loss: 0.80237877368927\n",
      "Validation: Epoch [6], Batch [200/938], Loss: 0.8376241326332092\n",
      "Validation: Epoch [6], Batch [201/938], Loss: 0.900536835193634\n",
      "Validation: Epoch [6], Batch [202/938], Loss: 1.12102472782135\n",
      "Validation: Epoch [6], Batch [203/938], Loss: 1.1065458059310913\n",
      "Validation: Epoch [6], Batch [204/938], Loss: 1.1759696006774902\n",
      "Validation: Epoch [6], Batch [205/938], Loss: 0.9674175381660461\n",
      "Validation: Epoch [6], Batch [206/938], Loss: 0.8458870649337769\n",
      "Validation: Epoch [6], Batch [207/938], Loss: 0.8788087368011475\n",
      "Validation: Epoch [6], Batch [208/938], Loss: 1.1158515214920044\n",
      "Validation: Epoch [6], Batch [209/938], Loss: 0.8445655107498169\n",
      "Validation: Epoch [6], Batch [210/938], Loss: 1.0616449117660522\n",
      "Validation: Epoch [6], Batch [211/938], Loss: 1.2362327575683594\n",
      "Validation: Epoch [6], Batch [212/938], Loss: 1.0391662120819092\n",
      "Validation: Epoch [6], Batch [213/938], Loss: 1.0380215644836426\n",
      "Validation: Epoch [6], Batch [214/938], Loss: 0.9543033838272095\n",
      "Validation: Epoch [6], Batch [215/938], Loss: 0.9859234094619751\n",
      "Validation: Epoch [6], Batch [216/938], Loss: 0.9204970002174377\n",
      "Validation: Epoch [6], Batch [217/938], Loss: 0.8674933910369873\n",
      "Validation: Epoch [6], Batch [218/938], Loss: 1.0946540832519531\n",
      "Validation: Epoch [6], Batch [219/938], Loss: 0.83817458152771\n",
      "Validation: Epoch [6], Batch [220/938], Loss: 0.9412953853607178\n",
      "Validation: Epoch [6], Batch [221/938], Loss: 0.9267450571060181\n",
      "Validation: Epoch [6], Batch [222/938], Loss: 0.7993112206459045\n",
      "Validation: Epoch [6], Batch [223/938], Loss: 1.0492995977401733\n",
      "Validation: Epoch [6], Batch [224/938], Loss: 0.675605297088623\n",
      "Validation: Epoch [6], Batch [225/938], Loss: 0.6946120858192444\n",
      "Validation: Epoch [6], Batch [226/938], Loss: 0.8419715762138367\n",
      "Validation: Epoch [6], Batch [227/938], Loss: 0.9720715284347534\n",
      "Validation: Epoch [6], Batch [228/938], Loss: 0.9645671248435974\n",
      "Validation: Epoch [6], Batch [229/938], Loss: 0.9149868488311768\n",
      "Validation: Epoch [6], Batch [230/938], Loss: 0.9025781154632568\n",
      "Validation: Epoch [6], Batch [231/938], Loss: 1.0023078918457031\n",
      "Validation: Epoch [6], Batch [232/938], Loss: 1.0320173501968384\n",
      "Validation: Epoch [6], Batch [233/938], Loss: 0.7904400825500488\n",
      "Validation: Epoch [6], Batch [234/938], Loss: 0.9316419363021851\n",
      "Validation: Epoch [6], Batch [235/938], Loss: 1.0717535018920898\n",
      "Validation: Epoch [6], Batch [236/938], Loss: 0.9718462824821472\n",
      "Validation: Epoch [6], Batch [237/938], Loss: 0.9805526733398438\n",
      "Validation: Epoch [6], Batch [238/938], Loss: 1.0105005502700806\n",
      "Validation: Epoch [6], Batch [239/938], Loss: 1.1459994316101074\n",
      "Validation: Epoch [6], Batch [240/938], Loss: 0.9914369583129883\n",
      "Validation: Epoch [6], Batch [241/938], Loss: 0.9693341851234436\n",
      "Validation: Epoch [6], Batch [242/938], Loss: 0.8814080953598022\n",
      "Validation: Epoch [6], Batch [243/938], Loss: 0.7756317257881165\n",
      "Validation: Epoch [6], Batch [244/938], Loss: 0.9065718650817871\n",
      "Validation: Epoch [6], Batch [245/938], Loss: 1.1559916734695435\n",
      "Validation: Epoch [6], Batch [246/938], Loss: 0.9526499509811401\n",
      "Validation: Epoch [6], Batch [247/938], Loss: 0.8719019293785095\n",
      "Validation: Epoch [6], Batch [248/938], Loss: 0.7817641496658325\n",
      "Validation: Epoch [6], Batch [249/938], Loss: 1.0610171556472778\n",
      "Validation: Epoch [6], Batch [250/938], Loss: 0.7007021903991699\n",
      "Validation: Epoch [6], Batch [251/938], Loss: 0.8034090995788574\n",
      "Validation: Epoch [6], Batch [252/938], Loss: 1.2264277935028076\n",
      "Validation: Epoch [6], Batch [253/938], Loss: 0.7302443385124207\n",
      "Validation: Epoch [6], Batch [254/938], Loss: 0.949440062046051\n",
      "Validation: Epoch [6], Batch [255/938], Loss: 1.0146431922912598\n",
      "Validation: Epoch [6], Batch [256/938], Loss: 0.8634853363037109\n",
      "Validation: Epoch [6], Batch [257/938], Loss: 0.7787246108055115\n",
      "Validation: Epoch [6], Batch [258/938], Loss: 0.8552291393280029\n",
      "Validation: Epoch [6], Batch [259/938], Loss: 0.8444175124168396\n",
      "Validation: Epoch [6], Batch [260/938], Loss: 1.0295188426971436\n",
      "Validation: Epoch [6], Batch [261/938], Loss: 0.9172959327697754\n",
      "Validation: Epoch [6], Batch [262/938], Loss: 1.0200382471084595\n",
      "Validation: Epoch [6], Batch [263/938], Loss: 0.975172758102417\n",
      "Validation: Epoch [6], Batch [264/938], Loss: 0.8783782720565796\n",
      "Validation: Epoch [6], Batch [265/938], Loss: 0.941249430179596\n",
      "Validation: Epoch [6], Batch [266/938], Loss: 0.7822390198707581\n",
      "Validation: Epoch [6], Batch [267/938], Loss: 1.0473641157150269\n",
      "Validation: Epoch [6], Batch [268/938], Loss: 1.1132270097732544\n",
      "Validation: Epoch [6], Batch [269/938], Loss: 1.0658451318740845\n",
      "Validation: Epoch [6], Batch [270/938], Loss: 0.9572510719299316\n",
      "Validation: Epoch [6], Batch [271/938], Loss: 1.12630295753479\n",
      "Validation: Epoch [6], Batch [272/938], Loss: 0.7292036414146423\n",
      "Validation: Epoch [6], Batch [273/938], Loss: 0.921107828617096\n",
      "Validation: Epoch [6], Batch [274/938], Loss: 1.1264795064926147\n",
      "Validation: Epoch [6], Batch [275/938], Loss: 0.9787991642951965\n",
      "Validation: Epoch [6], Batch [276/938], Loss: 1.0925958156585693\n",
      "Validation: Epoch [6], Batch [277/938], Loss: 0.8621541857719421\n",
      "Validation: Epoch [6], Batch [278/938], Loss: 0.9520993232727051\n",
      "Validation: Epoch [6], Batch [279/938], Loss: 1.1184751987457275\n",
      "Validation: Epoch [6], Batch [280/938], Loss: 0.7187091708183289\n",
      "Validation: Epoch [6], Batch [281/938], Loss: 0.840336799621582\n",
      "Validation: Epoch [6], Batch [282/938], Loss: 1.0258204936981201\n",
      "Validation: Epoch [6], Batch [283/938], Loss: 1.101872205734253\n",
      "Validation: Epoch [6], Batch [284/938], Loss: 0.9274238348007202\n",
      "Validation: Epoch [6], Batch [285/938], Loss: 0.8981930017471313\n",
      "Validation: Epoch [6], Batch [286/938], Loss: 0.7195485234260559\n",
      "Validation: Epoch [6], Batch [287/938], Loss: 0.8903107047080994\n",
      "Validation: Epoch [6], Batch [288/938], Loss: 0.808306872844696\n",
      "Validation: Epoch [6], Batch [289/938], Loss: 1.230371117591858\n",
      "Validation: Epoch [6], Batch [290/938], Loss: 1.0613148212432861\n",
      "Validation: Epoch [6], Batch [291/938], Loss: 1.0212175846099854\n",
      "Validation: Epoch [6], Batch [292/938], Loss: 0.8308295011520386\n",
      "Validation: Epoch [6], Batch [293/938], Loss: 1.0342084169387817\n",
      "Validation: Epoch [6], Batch [294/938], Loss: 0.7945250272750854\n",
      "Validation: Epoch [6], Batch [295/938], Loss: 0.9711058139801025\n",
      "Validation: Epoch [6], Batch [296/938], Loss: 0.8941667079925537\n",
      "Validation: Epoch [6], Batch [297/938], Loss: 0.858150839805603\n",
      "Validation: Epoch [6], Batch [298/938], Loss: 0.7262395620346069\n",
      "Validation: Epoch [6], Batch [299/938], Loss: 0.8524972796440125\n",
      "Validation: Epoch [6], Batch [300/938], Loss: 0.8446861505508423\n",
      "Validation: Epoch [6], Batch [301/938], Loss: 1.0617696046829224\n",
      "Validation: Epoch [6], Batch [302/938], Loss: 0.9911274313926697\n",
      "Validation: Epoch [6], Batch [303/938], Loss: 0.8578213453292847\n",
      "Validation: Epoch [6], Batch [304/938], Loss: 0.9297870993614197\n",
      "Validation: Epoch [6], Batch [305/938], Loss: 0.9762868881225586\n",
      "Validation: Epoch [6], Batch [306/938], Loss: 1.0321168899536133\n",
      "Validation: Epoch [6], Batch [307/938], Loss: 0.8903480172157288\n",
      "Validation: Epoch [6], Batch [308/938], Loss: 1.1784497499465942\n",
      "Validation: Epoch [6], Batch [309/938], Loss: 0.8233108520507812\n",
      "Validation: Epoch [6], Batch [310/938], Loss: 0.8546812534332275\n",
      "Validation: Epoch [6], Batch [311/938], Loss: 0.8558831214904785\n",
      "Validation: Epoch [6], Batch [312/938], Loss: 1.015764832496643\n",
      "Validation: Epoch [6], Batch [313/938], Loss: 1.118074655532837\n",
      "Validation: Epoch [6], Batch [314/938], Loss: 1.2941310405731201\n",
      "Validation: Epoch [6], Batch [315/938], Loss: 0.8268498182296753\n",
      "Validation: Epoch [6], Batch [316/938], Loss: 1.104526162147522\n",
      "Validation: Epoch [6], Batch [317/938], Loss: 0.947730302810669\n",
      "Validation: Epoch [6], Batch [318/938], Loss: 0.7401332259178162\n",
      "Validation: Epoch [6], Batch [319/938], Loss: 0.9939565658569336\n",
      "Validation: Epoch [6], Batch [320/938], Loss: 0.9266893863677979\n",
      "Validation: Epoch [6], Batch [321/938], Loss: 1.00668203830719\n",
      "Validation: Epoch [6], Batch [322/938], Loss: 0.6623159050941467\n",
      "Validation: Epoch [6], Batch [323/938], Loss: 0.7868943214416504\n",
      "Validation: Epoch [6], Batch [324/938], Loss: 1.1016173362731934\n",
      "Validation: Epoch [6], Batch [325/938], Loss: 0.9054242968559265\n",
      "Validation: Epoch [6], Batch [326/938], Loss: 0.7880417704582214\n",
      "Validation: Epoch [6], Batch [327/938], Loss: 1.0781792402267456\n",
      "Validation: Epoch [6], Batch [328/938], Loss: 0.8880317807197571\n",
      "Validation: Epoch [6], Batch [329/938], Loss: 0.7505923509597778\n",
      "Validation: Epoch [6], Batch [330/938], Loss: 0.7656263113021851\n",
      "Validation: Epoch [6], Batch [331/938], Loss: 0.757372260093689\n",
      "Validation: Epoch [6], Batch [332/938], Loss: 1.0119010210037231\n",
      "Validation: Epoch [6], Batch [333/938], Loss: 0.904276967048645\n",
      "Validation: Epoch [6], Batch [334/938], Loss: 0.9479620456695557\n",
      "Validation: Epoch [6], Batch [335/938], Loss: 1.0683164596557617\n",
      "Validation: Epoch [6], Batch [336/938], Loss: 0.7738946676254272\n",
      "Validation: Epoch [6], Batch [337/938], Loss: 0.8942039012908936\n",
      "Validation: Epoch [6], Batch [338/938], Loss: 0.9825184345245361\n",
      "Validation: Epoch [6], Batch [339/938], Loss: 0.8769861459732056\n",
      "Validation: Epoch [6], Batch [340/938], Loss: 0.8571797013282776\n",
      "Validation: Epoch [6], Batch [341/938], Loss: 0.8367093801498413\n",
      "Validation: Epoch [6], Batch [342/938], Loss: 0.7964849472045898\n",
      "Validation: Epoch [6], Batch [343/938], Loss: 0.9032084345817566\n",
      "Validation: Epoch [6], Batch [344/938], Loss: 0.9082441329956055\n",
      "Validation: Epoch [6], Batch [345/938], Loss: 1.0494022369384766\n",
      "Validation: Epoch [6], Batch [346/938], Loss: 1.1225640773773193\n",
      "Validation: Epoch [6], Batch [347/938], Loss: 1.0697182416915894\n",
      "Validation: Epoch [6], Batch [348/938], Loss: 0.8388442993164062\n",
      "Validation: Epoch [6], Batch [349/938], Loss: 0.6548804640769958\n",
      "Validation: Epoch [6], Batch [350/938], Loss: 0.9853485822677612\n",
      "Validation: Epoch [6], Batch [351/938], Loss: 0.9218404293060303\n",
      "Validation: Epoch [6], Batch [352/938], Loss: 0.8608726263046265\n",
      "Validation: Epoch [6], Batch [353/938], Loss: 0.8664416670799255\n",
      "Validation: Epoch [6], Batch [354/938], Loss: 0.7052145004272461\n",
      "Validation: Epoch [6], Batch [355/938], Loss: 0.889462947845459\n",
      "Validation: Epoch [6], Batch [356/938], Loss: 0.957200288772583\n",
      "Validation: Epoch [6], Batch [357/938], Loss: 1.0020500421524048\n",
      "Validation: Epoch [6], Batch [358/938], Loss: 0.9009434580802917\n",
      "Validation: Epoch [6], Batch [359/938], Loss: 0.9875926971435547\n",
      "Validation: Epoch [6], Batch [360/938], Loss: 1.0918889045715332\n",
      "Validation: Epoch [6], Batch [361/938], Loss: 1.305883526802063\n",
      "Validation: Epoch [6], Batch [362/938], Loss: 0.967353105545044\n",
      "Validation: Epoch [6], Batch [363/938], Loss: 0.8248161673545837\n",
      "Validation: Epoch [6], Batch [364/938], Loss: 1.0347260236740112\n",
      "Validation: Epoch [6], Batch [365/938], Loss: 0.9404181241989136\n",
      "Validation: Epoch [6], Batch [366/938], Loss: 0.8531224131584167\n",
      "Validation: Epoch [6], Batch [367/938], Loss: 0.8637354373931885\n",
      "Validation: Epoch [6], Batch [368/938], Loss: 1.069678783416748\n",
      "Validation: Epoch [6], Batch [369/938], Loss: 0.7706267833709717\n",
      "Validation: Epoch [6], Batch [370/938], Loss: 0.9248402118682861\n",
      "Validation: Epoch [6], Batch [371/938], Loss: 0.7168920636177063\n",
      "Validation: Epoch [6], Batch [372/938], Loss: 1.081307291984558\n",
      "Validation: Epoch [6], Batch [373/938], Loss: 0.9982245564460754\n",
      "Validation: Epoch [6], Batch [374/938], Loss: 0.9052187204360962\n",
      "Validation: Epoch [6], Batch [375/938], Loss: 1.1193904876708984\n",
      "Validation: Epoch [6], Batch [376/938], Loss: 0.8991455435752869\n",
      "Validation: Epoch [6], Batch [377/938], Loss: 0.8307813405990601\n",
      "Validation: Epoch [6], Batch [378/938], Loss: 0.6890154480934143\n",
      "Validation: Epoch [6], Batch [379/938], Loss: 0.9851292371749878\n",
      "Validation: Epoch [6], Batch [380/938], Loss: 0.7356094121932983\n",
      "Validation: Epoch [6], Batch [381/938], Loss: 0.8725627660751343\n",
      "Validation: Epoch [6], Batch [382/938], Loss: 0.7703135013580322\n",
      "Validation: Epoch [6], Batch [383/938], Loss: 0.930463969707489\n",
      "Validation: Epoch [6], Batch [384/938], Loss: 1.0707125663757324\n",
      "Validation: Epoch [6], Batch [385/938], Loss: 0.9085307717323303\n",
      "Validation: Epoch [6], Batch [386/938], Loss: 0.8658323884010315\n",
      "Validation: Epoch [6], Batch [387/938], Loss: 0.802649974822998\n",
      "Validation: Epoch [6], Batch [388/938], Loss: 0.8942279815673828\n",
      "Validation: Epoch [6], Batch [389/938], Loss: 0.9904465079307556\n",
      "Validation: Epoch [6], Batch [390/938], Loss: 0.783252477645874\n",
      "Validation: Epoch [6], Batch [391/938], Loss: 1.0565195083618164\n",
      "Validation: Epoch [6], Batch [392/938], Loss: 0.9414762258529663\n",
      "Validation: Epoch [6], Batch [393/938], Loss: 0.6864150762557983\n",
      "Validation: Epoch [6], Batch [394/938], Loss: 0.8505418300628662\n",
      "Validation: Epoch [6], Batch [395/938], Loss: 0.7430168986320496\n",
      "Validation: Epoch [6], Batch [396/938], Loss: 1.0055077075958252\n",
      "Validation: Epoch [6], Batch [397/938], Loss: 0.8520636558532715\n",
      "Validation: Epoch [6], Batch [398/938], Loss: 0.828589677810669\n",
      "Validation: Epoch [6], Batch [399/938], Loss: 0.8602911233901978\n",
      "Validation: Epoch [6], Batch [400/938], Loss: 1.0793172121047974\n",
      "Validation: Epoch [6], Batch [401/938], Loss: 1.0444679260253906\n",
      "Validation: Epoch [6], Batch [402/938], Loss: 1.0485652685165405\n",
      "Validation: Epoch [6], Batch [403/938], Loss: 0.9741976261138916\n",
      "Validation: Epoch [6], Batch [404/938], Loss: 0.9098765254020691\n",
      "Validation: Epoch [6], Batch [405/938], Loss: 0.894260048866272\n",
      "Validation: Epoch [6], Batch [406/938], Loss: 0.879092812538147\n",
      "Validation: Epoch [6], Batch [407/938], Loss: 0.8744503855705261\n",
      "Validation: Epoch [6], Batch [408/938], Loss: 1.170069694519043\n",
      "Validation: Epoch [6], Batch [409/938], Loss: 0.8196531534194946\n",
      "Validation: Epoch [6], Batch [410/938], Loss: 1.2465190887451172\n",
      "Validation: Epoch [6], Batch [411/938], Loss: 1.0178550481796265\n",
      "Validation: Epoch [6], Batch [412/938], Loss: 0.7996590733528137\n",
      "Validation: Epoch [6], Batch [413/938], Loss: 0.8578845262527466\n",
      "Validation: Epoch [6], Batch [414/938], Loss: 0.7821311354637146\n",
      "Validation: Epoch [6], Batch [415/938], Loss: 0.9528161287307739\n",
      "Validation: Epoch [6], Batch [416/938], Loss: 1.1219663619995117\n",
      "Validation: Epoch [6], Batch [417/938], Loss: 0.9939404726028442\n",
      "Validation: Epoch [6], Batch [418/938], Loss: 0.9249957799911499\n",
      "Validation: Epoch [6], Batch [419/938], Loss: 1.0334526300430298\n",
      "Validation: Epoch [6], Batch [420/938], Loss: 1.0377906560897827\n",
      "Validation: Epoch [6], Batch [421/938], Loss: 0.9724925756454468\n",
      "Validation: Epoch [6], Batch [422/938], Loss: 0.9859506487846375\n",
      "Validation: Epoch [6], Batch [423/938], Loss: 0.8702371120452881\n",
      "Validation: Epoch [6], Batch [424/938], Loss: 0.9343931674957275\n",
      "Validation: Epoch [6], Batch [425/938], Loss: 1.0615209341049194\n",
      "Validation: Epoch [6], Batch [426/938], Loss: 1.0931718349456787\n",
      "Validation: Epoch [6], Batch [427/938], Loss: 0.9387896060943604\n",
      "Validation: Epoch [6], Batch [428/938], Loss: 0.6649651527404785\n",
      "Validation: Epoch [6], Batch [429/938], Loss: 1.1705788373947144\n",
      "Validation: Epoch [6], Batch [430/938], Loss: 0.947954535484314\n",
      "Validation: Epoch [6], Batch [431/938], Loss: 0.9203212857246399\n",
      "Validation: Epoch [6], Batch [432/938], Loss: 1.1664756536483765\n",
      "Validation: Epoch [6], Batch [433/938], Loss: 0.957404613494873\n",
      "Validation: Epoch [6], Batch [434/938], Loss: 0.8654685616493225\n",
      "Validation: Epoch [6], Batch [435/938], Loss: 0.9784274101257324\n",
      "Validation: Epoch [6], Batch [436/938], Loss: 0.8478691577911377\n",
      "Validation: Epoch [6], Batch [437/938], Loss: 0.8960152864456177\n",
      "Validation: Epoch [6], Batch [438/938], Loss: 0.8506103754043579\n",
      "Validation: Epoch [6], Batch [439/938], Loss: 0.716509222984314\n",
      "Validation: Epoch [6], Batch [440/938], Loss: 0.9088363647460938\n",
      "Validation: Epoch [6], Batch [441/938], Loss: 0.6530241370201111\n",
      "Validation: Epoch [6], Batch [442/938], Loss: 0.9848813414573669\n",
      "Validation: Epoch [6], Batch [443/938], Loss: 0.6872966289520264\n",
      "Validation: Epoch [6], Batch [444/938], Loss: 0.9615878462791443\n",
      "Validation: Epoch [6], Batch [445/938], Loss: 0.9831054210662842\n",
      "Validation: Epoch [6], Batch [446/938], Loss: 0.7849921584129333\n",
      "Validation: Epoch [6], Batch [447/938], Loss: 0.9821999669075012\n",
      "Validation: Epoch [6], Batch [448/938], Loss: 1.109440803527832\n",
      "Validation: Epoch [6], Batch [449/938], Loss: 1.0979738235473633\n",
      "Validation: Epoch [6], Batch [450/938], Loss: 0.8665693998336792\n",
      "Validation: Epoch [6], Batch [451/938], Loss: 0.9701938033103943\n",
      "Validation: Epoch [6], Batch [452/938], Loss: 0.874321460723877\n",
      "Validation: Epoch [6], Batch [453/938], Loss: 0.803763210773468\n",
      "Validation: Epoch [6], Batch [454/938], Loss: 0.8673880100250244\n",
      "Validation: Epoch [6], Batch [455/938], Loss: 0.9919753074645996\n",
      "Validation: Epoch [6], Batch [456/938], Loss: 0.7633167505264282\n",
      "Validation: Epoch [6], Batch [457/938], Loss: 0.9634714722633362\n",
      "Validation: Epoch [6], Batch [458/938], Loss: 1.191628098487854\n",
      "Validation: Epoch [6], Batch [459/938], Loss: 1.007941722869873\n",
      "Validation: Epoch [6], Batch [460/938], Loss: 1.1267883777618408\n",
      "Validation: Epoch [6], Batch [461/938], Loss: 0.9600504636764526\n",
      "Validation: Epoch [6], Batch [462/938], Loss: 0.7217968702316284\n",
      "Validation: Epoch [6], Batch [463/938], Loss: 0.9717100262641907\n",
      "Validation: Epoch [6], Batch [464/938], Loss: 0.8948869109153748\n",
      "Validation: Epoch [6], Batch [465/938], Loss: 0.8843414783477783\n",
      "Validation: Epoch [6], Batch [466/938], Loss: 1.031765341758728\n",
      "Validation: Epoch [6], Batch [467/938], Loss: 0.9994741678237915\n",
      "Validation: Epoch [6], Batch [468/938], Loss: 0.9833652973175049\n",
      "Validation: Epoch [6], Batch [469/938], Loss: 0.8923997282981873\n",
      "Validation: Epoch [6], Batch [470/938], Loss: 0.9353712201118469\n",
      "Validation: Epoch [6], Batch [471/938], Loss: 0.7758762240409851\n",
      "Validation: Epoch [6], Batch [472/938], Loss: 0.691589891910553\n",
      "Validation: Epoch [6], Batch [473/938], Loss: 1.1039068698883057\n",
      "Validation: Epoch [6], Batch [474/938], Loss: 1.0540516376495361\n",
      "Validation: Epoch [6], Batch [475/938], Loss: 0.9438652992248535\n",
      "Validation: Epoch [6], Batch [476/938], Loss: 0.8728125691413879\n",
      "Validation: Epoch [6], Batch [477/938], Loss: 1.0800626277923584\n",
      "Validation: Epoch [6], Batch [478/938], Loss: 0.9096530675888062\n",
      "Validation: Epoch [6], Batch [479/938], Loss: 0.8694497346878052\n",
      "Validation: Epoch [6], Batch [480/938], Loss: 0.7894029021263123\n",
      "Validation: Epoch [6], Batch [481/938], Loss: 1.1577417850494385\n",
      "Validation: Epoch [6], Batch [482/938], Loss: 0.9845663905143738\n",
      "Validation: Epoch [6], Batch [483/938], Loss: 1.0115379095077515\n",
      "Validation: Epoch [6], Batch [484/938], Loss: 0.8280055522918701\n",
      "Validation: Epoch [6], Batch [485/938], Loss: 0.8345655202865601\n",
      "Validation: Epoch [6], Batch [486/938], Loss: 0.854390025138855\n",
      "Validation: Epoch [6], Batch [487/938], Loss: 0.8269312381744385\n",
      "Validation: Epoch [6], Batch [488/938], Loss: 0.8471596240997314\n",
      "Validation: Epoch [6], Batch [489/938], Loss: 1.0004267692565918\n",
      "Validation: Epoch [6], Batch [490/938], Loss: 1.1586978435516357\n",
      "Validation: Epoch [6], Batch [491/938], Loss: 1.1388577222824097\n",
      "Validation: Epoch [6], Batch [492/938], Loss: 1.0227315425872803\n",
      "Validation: Epoch [6], Batch [493/938], Loss: 0.8746766448020935\n",
      "Validation: Epoch [6], Batch [494/938], Loss: 0.870269238948822\n",
      "Validation: Epoch [6], Batch [495/938], Loss: 0.8714773654937744\n",
      "Validation: Epoch [6], Batch [496/938], Loss: 0.9083840847015381\n",
      "Validation: Epoch [6], Batch [497/938], Loss: 1.0296012163162231\n",
      "Validation: Epoch [6], Batch [498/938], Loss: 0.8971055746078491\n",
      "Validation: Epoch [6], Batch [499/938], Loss: 0.9044129848480225\n",
      "Validation: Epoch [6], Batch [500/938], Loss: 1.1207153797149658\n",
      "Validation: Epoch [6], Batch [501/938], Loss: 0.8443306088447571\n",
      "Validation: Epoch [6], Batch [502/938], Loss: 0.8601441979408264\n",
      "Validation: Epoch [6], Batch [503/938], Loss: 1.0239429473876953\n",
      "Validation: Epoch [6], Batch [504/938], Loss: 1.2141035795211792\n",
      "Validation: Epoch [6], Batch [505/938], Loss: 1.028065800666809\n",
      "Validation: Epoch [6], Batch [506/938], Loss: 1.1316261291503906\n",
      "Validation: Epoch [6], Batch [507/938], Loss: 0.8571491241455078\n",
      "Validation: Epoch [6], Batch [508/938], Loss: 1.0693303346633911\n",
      "Validation: Epoch [6], Batch [509/938], Loss: 0.8980032801628113\n",
      "Validation: Epoch [6], Batch [510/938], Loss: 0.8219467997550964\n",
      "Validation: Epoch [6], Batch [511/938], Loss: 0.7432950735092163\n",
      "Validation: Epoch [6], Batch [512/938], Loss: 0.8800345063209534\n",
      "Validation: Epoch [6], Batch [513/938], Loss: 1.136419653892517\n",
      "Validation: Epoch [6], Batch [514/938], Loss: 0.8738440871238708\n",
      "Validation: Epoch [6], Batch [515/938], Loss: 1.0831068754196167\n",
      "Validation: Epoch [6], Batch [516/938], Loss: 0.9713438749313354\n",
      "Validation: Epoch [6], Batch [517/938], Loss: 0.94829261302948\n",
      "Validation: Epoch [6], Batch [518/938], Loss: 1.1664921045303345\n",
      "Validation: Epoch [6], Batch [519/938], Loss: 0.9226561784744263\n",
      "Validation: Epoch [6], Batch [520/938], Loss: 0.9338972568511963\n",
      "Validation: Epoch [6], Batch [521/938], Loss: 0.8878563642501831\n",
      "Validation: Epoch [6], Batch [522/938], Loss: 0.790044903755188\n",
      "Validation: Epoch [6], Batch [523/938], Loss: 0.9382315278053284\n",
      "Validation: Epoch [6], Batch [524/938], Loss: 1.0381622314453125\n",
      "Validation: Epoch [6], Batch [525/938], Loss: 0.869533896446228\n",
      "Validation: Epoch [6], Batch [526/938], Loss: 0.9058789014816284\n",
      "Validation: Epoch [6], Batch [527/938], Loss: 0.9041194915771484\n",
      "Validation: Epoch [6], Batch [528/938], Loss: 0.9094645380973816\n",
      "Validation: Epoch [6], Batch [529/938], Loss: 0.9040672779083252\n",
      "Validation: Epoch [6], Batch [530/938], Loss: 0.9176740646362305\n",
      "Validation: Epoch [6], Batch [531/938], Loss: 0.9131419658660889\n",
      "Validation: Epoch [6], Batch [532/938], Loss: 0.8885121941566467\n",
      "Validation: Epoch [6], Batch [533/938], Loss: 0.7785115242004395\n",
      "Validation: Epoch [6], Batch [534/938], Loss: 1.0208044052124023\n",
      "Validation: Epoch [6], Batch [535/938], Loss: 0.7309780716896057\n",
      "Validation: Epoch [6], Batch [536/938], Loss: 0.8145523071289062\n",
      "Validation: Epoch [6], Batch [537/938], Loss: 0.7889277935028076\n",
      "Validation: Epoch [6], Batch [538/938], Loss: 0.6477621793746948\n",
      "Validation: Epoch [6], Batch [539/938], Loss: 1.0444912910461426\n",
      "Validation: Epoch [6], Batch [540/938], Loss: 0.954582691192627\n",
      "Validation: Epoch [6], Batch [541/938], Loss: 0.8765354156494141\n",
      "Validation: Epoch [6], Batch [542/938], Loss: 0.8581393957138062\n",
      "Validation: Epoch [6], Batch [543/938], Loss: 1.1444131135940552\n",
      "Validation: Epoch [6], Batch [544/938], Loss: 0.9455949664115906\n",
      "Validation: Epoch [6], Batch [545/938], Loss: 0.8694908618927002\n",
      "Validation: Epoch [6], Batch [546/938], Loss: 1.0238250494003296\n",
      "Validation: Epoch [6], Batch [547/938], Loss: 0.8683847188949585\n",
      "Validation: Epoch [6], Batch [548/938], Loss: 0.7573190927505493\n",
      "Validation: Epoch [6], Batch [549/938], Loss: 0.8916902542114258\n",
      "Validation: Epoch [6], Batch [550/938], Loss: 0.9460691213607788\n",
      "Validation: Epoch [6], Batch [551/938], Loss: 0.8785598874092102\n",
      "Validation: Epoch [6], Batch [552/938], Loss: 0.8621391654014587\n",
      "Validation: Epoch [6], Batch [553/938], Loss: 0.9413604736328125\n",
      "Validation: Epoch [6], Batch [554/938], Loss: 0.8385961055755615\n",
      "Validation: Epoch [6], Batch [555/938], Loss: 0.8830124735832214\n",
      "Validation: Epoch [6], Batch [556/938], Loss: 0.8684199452400208\n",
      "Validation: Epoch [6], Batch [557/938], Loss: 1.0058671236038208\n",
      "Validation: Epoch [6], Batch [558/938], Loss: 0.9268696308135986\n",
      "Validation: Epoch [6], Batch [559/938], Loss: 0.8149646520614624\n",
      "Validation: Epoch [6], Batch [560/938], Loss: 0.8412104845046997\n",
      "Validation: Epoch [6], Batch [561/938], Loss: 0.9093247056007385\n",
      "Validation: Epoch [6], Batch [562/938], Loss: 1.040022611618042\n",
      "Validation: Epoch [6], Batch [563/938], Loss: 0.9162612557411194\n",
      "Validation: Epoch [6], Batch [564/938], Loss: 0.9864749908447266\n",
      "Validation: Epoch [6], Batch [565/938], Loss: 1.03290593624115\n",
      "Validation: Epoch [6], Batch [566/938], Loss: 0.9900479316711426\n",
      "Validation: Epoch [6], Batch [567/938], Loss: 0.8969547152519226\n",
      "Validation: Epoch [6], Batch [568/938], Loss: 1.1730895042419434\n",
      "Validation: Epoch [6], Batch [569/938], Loss: 0.7286865711212158\n",
      "Validation: Epoch [6], Batch [570/938], Loss: 0.8158583045005798\n",
      "Validation: Epoch [6], Batch [571/938], Loss: 0.876201331615448\n",
      "Validation: Epoch [6], Batch [572/938], Loss: 0.7952300906181335\n",
      "Validation: Epoch [6], Batch [573/938], Loss: 0.9027001857757568\n",
      "Validation: Epoch [6], Batch [574/938], Loss: 0.8806689977645874\n",
      "Validation: Epoch [6], Batch [575/938], Loss: 0.9242563843727112\n",
      "Validation: Epoch [6], Batch [576/938], Loss: 0.9015740752220154\n",
      "Validation: Epoch [6], Batch [577/938], Loss: 0.8560397028923035\n",
      "Validation: Epoch [6], Batch [578/938], Loss: 0.9349480867385864\n",
      "Validation: Epoch [6], Batch [579/938], Loss: 1.0578060150146484\n",
      "Validation: Epoch [6], Batch [580/938], Loss: 0.8194233179092407\n",
      "Validation: Epoch [6], Batch [581/938], Loss: 0.9420281648635864\n",
      "Validation: Epoch [6], Batch [582/938], Loss: 0.7587996125221252\n",
      "Validation: Epoch [6], Batch [583/938], Loss: 0.9588941335678101\n",
      "Validation: Epoch [6], Batch [584/938], Loss: 0.9230046272277832\n",
      "Validation: Epoch [6], Batch [585/938], Loss: 1.0852546691894531\n",
      "Validation: Epoch [6], Batch [586/938], Loss: 0.7745188474655151\n",
      "Validation: Epoch [6], Batch [587/938], Loss: 1.0757957696914673\n",
      "Validation: Epoch [6], Batch [588/938], Loss: 1.0464742183685303\n",
      "Validation: Epoch [6], Batch [589/938], Loss: 0.9715462327003479\n",
      "Validation: Epoch [6], Batch [590/938], Loss: 0.9328482151031494\n",
      "Validation: Epoch [6], Batch [591/938], Loss: 0.9763786196708679\n",
      "Validation: Epoch [6], Batch [592/938], Loss: 0.959173321723938\n",
      "Validation: Epoch [6], Batch [593/938], Loss: 0.9142752289772034\n",
      "Validation: Epoch [6], Batch [594/938], Loss: 1.3330092430114746\n",
      "Validation: Epoch [6], Batch [595/938], Loss: 0.837060809135437\n",
      "Validation: Epoch [6], Batch [596/938], Loss: 0.8923987746238708\n",
      "Validation: Epoch [6], Batch [597/938], Loss: 0.9325003027915955\n",
      "Validation: Epoch [6], Batch [598/938], Loss: 0.7939399480819702\n",
      "Validation: Epoch [6], Batch [599/938], Loss: 0.997204065322876\n",
      "Validation: Epoch [6], Batch [600/938], Loss: 1.1148041486740112\n",
      "Validation: Epoch [6], Batch [601/938], Loss: 0.9262422323226929\n",
      "Validation: Epoch [6], Batch [602/938], Loss: 0.852196216583252\n",
      "Validation: Epoch [6], Batch [603/938], Loss: 0.9309101700782776\n",
      "Validation: Epoch [6], Batch [604/938], Loss: 0.8216240406036377\n",
      "Validation: Epoch [6], Batch [605/938], Loss: 1.1825618743896484\n",
      "Validation: Epoch [6], Batch [606/938], Loss: 1.0574051141738892\n",
      "Validation: Epoch [6], Batch [607/938], Loss: 1.1359500885009766\n",
      "Validation: Epoch [6], Batch [608/938], Loss: 0.9309271574020386\n",
      "Validation: Epoch [6], Batch [609/938], Loss: 0.8921708464622498\n",
      "Validation: Epoch [6], Batch [610/938], Loss: 0.9887503385543823\n",
      "Validation: Epoch [6], Batch [611/938], Loss: 0.9218339920043945\n",
      "Validation: Epoch [6], Batch [612/938], Loss: 1.0796527862548828\n",
      "Validation: Epoch [6], Batch [613/938], Loss: 0.9646128416061401\n",
      "Validation: Epoch [6], Batch [614/938], Loss: 0.8011681437492371\n",
      "Validation: Epoch [6], Batch [615/938], Loss: 1.1499059200286865\n",
      "Validation: Epoch [6], Batch [616/938], Loss: 1.0203168392181396\n",
      "Validation: Epoch [6], Batch [617/938], Loss: 0.8725911378860474\n",
      "Validation: Epoch [6], Batch [618/938], Loss: 0.8558269143104553\n",
      "Validation: Epoch [6], Batch [619/938], Loss: 0.8540547490119934\n",
      "Validation: Epoch [6], Batch [620/938], Loss: 0.8746346831321716\n",
      "Validation: Epoch [6], Batch [621/938], Loss: 1.0306954383850098\n",
      "Validation: Epoch [6], Batch [622/938], Loss: 1.0916883945465088\n",
      "Validation: Epoch [6], Batch [623/938], Loss: 0.9133246541023254\n",
      "Validation: Epoch [6], Batch [624/938], Loss: 0.9194241762161255\n",
      "Validation: Epoch [6], Batch [625/938], Loss: 1.1037030220031738\n",
      "Validation: Epoch [6], Batch [626/938], Loss: 1.0932856798171997\n",
      "Validation: Epoch [6], Batch [627/938], Loss: 0.9056460857391357\n",
      "Validation: Epoch [6], Batch [628/938], Loss: 1.2123026847839355\n",
      "Validation: Epoch [6], Batch [629/938], Loss: 0.7999472618103027\n",
      "Validation: Epoch [6], Batch [630/938], Loss: 1.118359923362732\n",
      "Validation: Epoch [6], Batch [631/938], Loss: 0.7144110798835754\n",
      "Validation: Epoch [6], Batch [632/938], Loss: 1.0835294723510742\n",
      "Validation: Epoch [6], Batch [633/938], Loss: 0.887499988079071\n",
      "Validation: Epoch [6], Batch [634/938], Loss: 0.8564627170562744\n",
      "Validation: Epoch [6], Batch [635/938], Loss: 0.8906946182250977\n",
      "Validation: Epoch [6], Batch [636/938], Loss: 0.9996077418327332\n",
      "Validation: Epoch [6], Batch [637/938], Loss: 0.9236778616905212\n",
      "Validation: Epoch [6], Batch [638/938], Loss: 0.7397662401199341\n",
      "Validation: Epoch [6], Batch [639/938], Loss: 1.1420607566833496\n",
      "Validation: Epoch [6], Batch [640/938], Loss: 0.9855436086654663\n",
      "Validation: Epoch [6], Batch [641/938], Loss: 0.8353676795959473\n",
      "Validation: Epoch [6], Batch [642/938], Loss: 0.8534308075904846\n",
      "Validation: Epoch [6], Batch [643/938], Loss: 0.8945350050926208\n",
      "Validation: Epoch [6], Batch [644/938], Loss: 1.0212254524230957\n",
      "Validation: Epoch [6], Batch [645/938], Loss: 1.0217218399047852\n",
      "Validation: Epoch [6], Batch [646/938], Loss: 0.6965181827545166\n",
      "Validation: Epoch [6], Batch [647/938], Loss: 0.9925551414489746\n",
      "Validation: Epoch [6], Batch [648/938], Loss: 0.942744255065918\n",
      "Validation: Epoch [6], Batch [649/938], Loss: 0.9002885818481445\n",
      "Validation: Epoch [6], Batch [650/938], Loss: 0.716954231262207\n",
      "Validation: Epoch [6], Batch [651/938], Loss: 0.9001916646957397\n",
      "Validation: Epoch [6], Batch [652/938], Loss: 1.0246362686157227\n",
      "Validation: Epoch [6], Batch [653/938], Loss: 1.2207483053207397\n",
      "Validation: Epoch [6], Batch [654/938], Loss: 0.9841243028640747\n",
      "Validation: Epoch [6], Batch [655/938], Loss: 1.0625284910202026\n",
      "Validation: Epoch [6], Batch [656/938], Loss: 0.7654372453689575\n",
      "Validation: Epoch [6], Batch [657/938], Loss: 1.0213295221328735\n",
      "Validation: Epoch [6], Batch [658/938], Loss: 1.1509556770324707\n",
      "Validation: Epoch [6], Batch [659/938], Loss: 0.9628168940544128\n",
      "Validation: Epoch [6], Batch [660/938], Loss: 0.8814484477043152\n",
      "Validation: Epoch [6], Batch [661/938], Loss: 0.9674939513206482\n",
      "Validation: Epoch [6], Batch [662/938], Loss: 1.0083914995193481\n",
      "Validation: Epoch [6], Batch [663/938], Loss: 0.7101496458053589\n",
      "Validation: Epoch [6], Batch [664/938], Loss: 0.9970912933349609\n",
      "Validation: Epoch [6], Batch [665/938], Loss: 0.9993982911109924\n",
      "Validation: Epoch [6], Batch [666/938], Loss: 0.8247680068016052\n",
      "Validation: Epoch [6], Batch [667/938], Loss: 1.022101879119873\n",
      "Validation: Epoch [6], Batch [668/938], Loss: 1.0791616439819336\n",
      "Validation: Epoch [6], Batch [669/938], Loss: 1.1366066932678223\n",
      "Validation: Epoch [6], Batch [670/938], Loss: 1.0476512908935547\n",
      "Validation: Epoch [6], Batch [671/938], Loss: 0.8953620195388794\n",
      "Validation: Epoch [6], Batch [672/938], Loss: 0.9673020243644714\n",
      "Validation: Epoch [6], Batch [673/938], Loss: 0.8473764657974243\n",
      "Validation: Epoch [6], Batch [674/938], Loss: 0.9652628898620605\n",
      "Validation: Epoch [6], Batch [675/938], Loss: 1.1377098560333252\n",
      "Validation: Epoch [6], Batch [676/938], Loss: 0.7556642889976501\n",
      "Validation: Epoch [6], Batch [677/938], Loss: 0.9712473154067993\n",
      "Validation: Epoch [6], Batch [678/938], Loss: 0.7755282521247864\n",
      "Validation: Epoch [6], Batch [679/938], Loss: 1.097076654434204\n",
      "Validation: Epoch [6], Batch [680/938], Loss: 0.7684476971626282\n",
      "Validation: Epoch [6], Batch [681/938], Loss: 0.8724149465560913\n",
      "Validation: Epoch [6], Batch [682/938], Loss: 1.1082477569580078\n",
      "Validation: Epoch [6], Batch [683/938], Loss: 0.8297543525695801\n",
      "Validation: Epoch [6], Batch [684/938], Loss: 0.899677038192749\n",
      "Validation: Epoch [6], Batch [685/938], Loss: 1.278044581413269\n",
      "Validation: Epoch [6], Batch [686/938], Loss: 0.9664004445075989\n",
      "Validation: Epoch [6], Batch [687/938], Loss: 0.9034836292266846\n",
      "Validation: Epoch [6], Batch [688/938], Loss: 1.214536428451538\n",
      "Validation: Epoch [6], Batch [689/938], Loss: 0.9944453239440918\n",
      "Validation: Epoch [6], Batch [690/938], Loss: 0.9941955804824829\n",
      "Validation: Epoch [6], Batch [691/938], Loss: 0.8446926474571228\n",
      "Validation: Epoch [6], Batch [692/938], Loss: 0.9544564485549927\n",
      "Validation: Epoch [6], Batch [693/938], Loss: 0.8278554677963257\n",
      "Validation: Epoch [6], Batch [694/938], Loss: 0.7155929803848267\n",
      "Validation: Epoch [6], Batch [695/938], Loss: 0.9592570662498474\n",
      "Validation: Epoch [6], Batch [696/938], Loss: 1.0528818368911743\n",
      "Validation: Epoch [6], Batch [697/938], Loss: 0.8724603056907654\n",
      "Validation: Epoch [6], Batch [698/938], Loss: 1.0104235410690308\n",
      "Validation: Epoch [6], Batch [699/938], Loss: 0.8775982856750488\n",
      "Validation: Epoch [6], Batch [700/938], Loss: 1.110967755317688\n",
      "Validation: Epoch [6], Batch [701/938], Loss: 0.843056321144104\n",
      "Validation: Epoch [6], Batch [702/938], Loss: 0.9573447108268738\n",
      "Validation: Epoch [6], Batch [703/938], Loss: 0.8645034432411194\n",
      "Validation: Epoch [6], Batch [704/938], Loss: 0.9085220098495483\n",
      "Validation: Epoch [6], Batch [705/938], Loss: 0.867677628993988\n",
      "Validation: Epoch [6], Batch [706/938], Loss: 0.8195093870162964\n",
      "Validation: Epoch [6], Batch [707/938], Loss: 1.122849702835083\n",
      "Validation: Epoch [6], Batch [708/938], Loss: 1.2731599807739258\n",
      "Validation: Epoch [6], Batch [709/938], Loss: 0.7964632511138916\n",
      "Validation: Epoch [6], Batch [710/938], Loss: 0.9858940839767456\n",
      "Validation: Epoch [6], Batch [711/938], Loss: 1.1566954851150513\n",
      "Validation: Epoch [6], Batch [712/938], Loss: 0.7793224453926086\n",
      "Validation: Epoch [6], Batch [713/938], Loss: 0.8712712526321411\n",
      "Validation: Epoch [6], Batch [714/938], Loss: 0.772802472114563\n",
      "Validation: Epoch [6], Batch [715/938], Loss: 0.6665306687355042\n",
      "Validation: Epoch [6], Batch [716/938], Loss: 0.699433445930481\n",
      "Validation: Epoch [6], Batch [717/938], Loss: 0.7583900094032288\n",
      "Validation: Epoch [6], Batch [718/938], Loss: 0.8994035124778748\n",
      "Validation: Epoch [6], Batch [719/938], Loss: 0.9381141662597656\n",
      "Validation: Epoch [6], Batch [720/938], Loss: 0.8715457320213318\n",
      "Validation: Epoch [6], Batch [721/938], Loss: 0.9781206846237183\n",
      "Validation: Epoch [6], Batch [722/938], Loss: 0.8254324793815613\n",
      "Validation: Epoch [6], Batch [723/938], Loss: 0.7781552076339722\n",
      "Validation: Epoch [6], Batch [724/938], Loss: 1.005651831626892\n",
      "Validation: Epoch [6], Batch [725/938], Loss: 0.9359261989593506\n",
      "Validation: Epoch [6], Batch [726/938], Loss: 1.0192551612854004\n",
      "Validation: Epoch [6], Batch [727/938], Loss: 0.8252851963043213\n",
      "Validation: Epoch [6], Batch [728/938], Loss: 0.8414061069488525\n",
      "Validation: Epoch [6], Batch [729/938], Loss: 0.9514873027801514\n",
      "Validation: Epoch [6], Batch [730/938], Loss: 1.1490960121154785\n",
      "Validation: Epoch [6], Batch [731/938], Loss: 0.7383375763893127\n",
      "Validation: Epoch [6], Batch [732/938], Loss: 1.0744283199310303\n",
      "Validation: Epoch [6], Batch [733/938], Loss: 0.9746958017349243\n",
      "Validation: Epoch [6], Batch [734/938], Loss: 0.8591435551643372\n",
      "Validation: Epoch [6], Batch [735/938], Loss: 0.9879668951034546\n",
      "Validation: Epoch [6], Batch [736/938], Loss: 1.2196147441864014\n",
      "Validation: Epoch [6], Batch [737/938], Loss: 1.0276135206222534\n",
      "Validation: Epoch [6], Batch [738/938], Loss: 0.7604044675827026\n",
      "Validation: Epoch [6], Batch [739/938], Loss: 0.9661723375320435\n",
      "Validation: Epoch [6], Batch [740/938], Loss: 1.0893315076828003\n",
      "Validation: Epoch [6], Batch [741/938], Loss: 0.990418553352356\n",
      "Validation: Epoch [6], Batch [742/938], Loss: 0.9712985754013062\n",
      "Validation: Epoch [6], Batch [743/938], Loss: 0.9845311045646667\n",
      "Validation: Epoch [6], Batch [744/938], Loss: 0.8209596276283264\n",
      "Validation: Epoch [6], Batch [745/938], Loss: 1.0036330223083496\n",
      "Validation: Epoch [6], Batch [746/938], Loss: 0.950180172920227\n",
      "Validation: Epoch [6], Batch [747/938], Loss: 1.1204051971435547\n",
      "Validation: Epoch [6], Batch [748/938], Loss: 0.961715579032898\n",
      "Validation: Epoch [6], Batch [749/938], Loss: 1.0047893524169922\n",
      "Validation: Epoch [6], Batch [750/938], Loss: 0.827829122543335\n",
      "Validation: Epoch [6], Batch [751/938], Loss: 1.0731441974639893\n",
      "Validation: Epoch [6], Batch [752/938], Loss: 0.9305262565612793\n",
      "Validation: Epoch [6], Batch [753/938], Loss: 1.1420217752456665\n",
      "Validation: Epoch [6], Batch [754/938], Loss: 0.8444834351539612\n",
      "Validation: Epoch [6], Batch [755/938], Loss: 0.90113365650177\n",
      "Validation: Epoch [6], Batch [756/938], Loss: 0.9170793890953064\n",
      "Validation: Epoch [6], Batch [757/938], Loss: 1.1464134454727173\n",
      "Validation: Epoch [6], Batch [758/938], Loss: 0.9122148752212524\n",
      "Validation: Epoch [6], Batch [759/938], Loss: 0.8469171524047852\n",
      "Validation: Epoch [6], Batch [760/938], Loss: 0.7566085457801819\n",
      "Validation: Epoch [6], Batch [761/938], Loss: 0.9248700737953186\n",
      "Validation: Epoch [6], Batch [762/938], Loss: 0.8140264749526978\n",
      "Validation: Epoch [6], Batch [763/938], Loss: 0.9770180583000183\n",
      "Validation: Epoch [6], Batch [764/938], Loss: 0.9182345271110535\n",
      "Validation: Epoch [6], Batch [765/938], Loss: 1.0375862121582031\n",
      "Validation: Epoch [6], Batch [766/938], Loss: 1.0902873277664185\n",
      "Validation: Epoch [6], Batch [767/938], Loss: 0.9124774932861328\n",
      "Validation: Epoch [6], Batch [768/938], Loss: 1.1577471494674683\n",
      "Validation: Epoch [6], Batch [769/938], Loss: 1.1086950302124023\n",
      "Validation: Epoch [6], Batch [770/938], Loss: 1.1346957683563232\n",
      "Validation: Epoch [6], Batch [771/938], Loss: 0.6427069902420044\n",
      "Validation: Epoch [6], Batch [772/938], Loss: 0.794848620891571\n",
      "Validation: Epoch [6], Batch [773/938], Loss: 0.9113560914993286\n",
      "Validation: Epoch [6], Batch [774/938], Loss: 0.9334129691123962\n",
      "Validation: Epoch [6], Batch [775/938], Loss: 0.8909326791763306\n",
      "Validation: Epoch [6], Batch [776/938], Loss: 0.8224155902862549\n",
      "Validation: Epoch [6], Batch [777/938], Loss: 0.9617966413497925\n",
      "Validation: Epoch [6], Batch [778/938], Loss: 1.0129369497299194\n",
      "Validation: Epoch [6], Batch [779/938], Loss: 0.8154314756393433\n",
      "Validation: Epoch [6], Batch [780/938], Loss: 0.626427173614502\n",
      "Validation: Epoch [6], Batch [781/938], Loss: 0.7489663362503052\n",
      "Validation: Epoch [6], Batch [782/938], Loss: 0.7890469431877136\n",
      "Validation: Epoch [6], Batch [783/938], Loss: 0.9179019331932068\n",
      "Validation: Epoch [6], Batch [784/938], Loss: 1.0519262552261353\n",
      "Validation: Epoch [6], Batch [785/938], Loss: 0.8880974054336548\n",
      "Validation: Epoch [6], Batch [786/938], Loss: 0.7877856492996216\n",
      "Validation: Epoch [6], Batch [787/938], Loss: 0.8677234649658203\n",
      "Validation: Epoch [6], Batch [788/938], Loss: 1.0369924306869507\n",
      "Validation: Epoch [6], Batch [789/938], Loss: 1.12822687625885\n",
      "Validation: Epoch [6], Batch [790/938], Loss: 0.8500217199325562\n",
      "Validation: Epoch [6], Batch [791/938], Loss: 1.0600029230117798\n",
      "Validation: Epoch [6], Batch [792/938], Loss: 0.8232815861701965\n",
      "Validation: Epoch [6], Batch [793/938], Loss: 0.8224539160728455\n",
      "Validation: Epoch [6], Batch [794/938], Loss: 1.099866509437561\n",
      "Validation: Epoch [6], Batch [795/938], Loss: 0.8344503045082092\n",
      "Validation: Epoch [6], Batch [796/938], Loss: 1.0822285413742065\n",
      "Validation: Epoch [6], Batch [797/938], Loss: 1.0402824878692627\n",
      "Validation: Epoch [6], Batch [798/938], Loss: 0.6674894094467163\n",
      "Validation: Epoch [6], Batch [799/938], Loss: 1.139045238494873\n",
      "Validation: Epoch [6], Batch [800/938], Loss: 1.0603848695755005\n",
      "Validation: Epoch [6], Batch [801/938], Loss: 0.7953513860702515\n",
      "Validation: Epoch [6], Batch [802/938], Loss: 0.9064407348632812\n",
      "Validation: Epoch [6], Batch [803/938], Loss: 1.0196287631988525\n",
      "Validation: Epoch [6], Batch [804/938], Loss: 0.9699671268463135\n",
      "Validation: Epoch [6], Batch [805/938], Loss: 0.9289554953575134\n",
      "Validation: Epoch [6], Batch [806/938], Loss: 0.8307523727416992\n",
      "Validation: Epoch [6], Batch [807/938], Loss: 0.9652174115180969\n",
      "Validation: Epoch [6], Batch [808/938], Loss: 1.1033248901367188\n",
      "Validation: Epoch [6], Batch [809/938], Loss: 0.8605319261550903\n",
      "Validation: Epoch [6], Batch [810/938], Loss: 1.0815653800964355\n",
      "Validation: Epoch [6], Batch [811/938], Loss: 0.920156717300415\n",
      "Validation: Epoch [6], Batch [812/938], Loss: 0.6118988990783691\n",
      "Validation: Epoch [6], Batch [813/938], Loss: 0.937595784664154\n",
      "Validation: Epoch [6], Batch [814/938], Loss: 0.9770382642745972\n",
      "Validation: Epoch [6], Batch [815/938], Loss: 1.0525485277175903\n",
      "Validation: Epoch [6], Batch [816/938], Loss: 1.051943302154541\n",
      "Validation: Epoch [6], Batch [817/938], Loss: 0.8458142876625061\n",
      "Validation: Epoch [6], Batch [818/938], Loss: 0.7101255059242249\n",
      "Validation: Epoch [6], Batch [819/938], Loss: 0.9365702867507935\n",
      "Validation: Epoch [6], Batch [820/938], Loss: 0.9142301678657532\n",
      "Validation: Epoch [6], Batch [821/938], Loss: 1.0296878814697266\n",
      "Validation: Epoch [6], Batch [822/938], Loss: 0.8787387609481812\n",
      "Validation: Epoch [6], Batch [823/938], Loss: 0.8454216718673706\n",
      "Validation: Epoch [6], Batch [824/938], Loss: 1.0836255550384521\n",
      "Validation: Epoch [6], Batch [825/938], Loss: 0.9731838703155518\n",
      "Validation: Epoch [6], Batch [826/938], Loss: 1.0221636295318604\n",
      "Validation: Epoch [6], Batch [827/938], Loss: 0.8744853734970093\n",
      "Validation: Epoch [6], Batch [828/938], Loss: 1.1991798877716064\n",
      "Validation: Epoch [6], Batch [829/938], Loss: 0.7494051456451416\n",
      "Validation: Epoch [6], Batch [830/938], Loss: 1.0083152055740356\n",
      "Validation: Epoch [6], Batch [831/938], Loss: 0.7210700511932373\n",
      "Validation: Epoch [6], Batch [832/938], Loss: 1.0890737771987915\n",
      "Validation: Epoch [6], Batch [833/938], Loss: 0.8550904393196106\n",
      "Validation: Epoch [6], Batch [834/938], Loss: 0.7971984148025513\n",
      "Validation: Epoch [6], Batch [835/938], Loss: 0.960295557975769\n",
      "Validation: Epoch [6], Batch [836/938], Loss: 0.9617660045623779\n",
      "Validation: Epoch [6], Batch [837/938], Loss: 1.1251299381256104\n",
      "Validation: Epoch [6], Batch [838/938], Loss: 0.9768552184104919\n",
      "Validation: Epoch [6], Batch [839/938], Loss: 1.0336591005325317\n",
      "Validation: Epoch [6], Batch [840/938], Loss: 1.1041462421417236\n",
      "Validation: Epoch [6], Batch [841/938], Loss: 0.8954604268074036\n",
      "Validation: Epoch [6], Batch [842/938], Loss: 0.9967466592788696\n",
      "Validation: Epoch [6], Batch [843/938], Loss: 0.9932124614715576\n",
      "Validation: Epoch [6], Batch [844/938], Loss: 0.9581459164619446\n",
      "Validation: Epoch [6], Batch [845/938], Loss: 0.75798100233078\n",
      "Validation: Epoch [6], Batch [846/938], Loss: 0.7514922022819519\n",
      "Validation: Epoch [6], Batch [847/938], Loss: 0.8398818969726562\n",
      "Validation: Epoch [6], Batch [848/938], Loss: 0.7565672397613525\n",
      "Validation: Epoch [6], Batch [849/938], Loss: 0.7160052061080933\n",
      "Validation: Epoch [6], Batch [850/938], Loss: 1.1825748682022095\n",
      "Validation: Epoch [6], Batch [851/938], Loss: 0.994163453578949\n",
      "Validation: Epoch [6], Batch [852/938], Loss: 0.9340260028839111\n",
      "Validation: Epoch [6], Batch [853/938], Loss: 1.190686821937561\n",
      "Validation: Epoch [6], Batch [854/938], Loss: 0.9305667281150818\n",
      "Validation: Epoch [6], Batch [855/938], Loss: 0.9608155488967896\n",
      "Validation: Epoch [6], Batch [856/938], Loss: 0.8655183911323547\n",
      "Validation: Epoch [6], Batch [857/938], Loss: 0.9219551682472229\n",
      "Validation: Epoch [6], Batch [858/938], Loss: 0.9906603097915649\n",
      "Validation: Epoch [6], Batch [859/938], Loss: 0.8742822408676147\n",
      "Validation: Epoch [6], Batch [860/938], Loss: 1.1244566440582275\n",
      "Validation: Epoch [6], Batch [861/938], Loss: 0.8534797430038452\n",
      "Validation: Epoch [6], Batch [862/938], Loss: 0.9393144249916077\n",
      "Validation: Epoch [6], Batch [863/938], Loss: 0.8524200320243835\n",
      "Validation: Epoch [6], Batch [864/938], Loss: 0.8683835864067078\n",
      "Validation: Epoch [6], Batch [865/938], Loss: 0.7671241760253906\n",
      "Validation: Epoch [6], Batch [866/938], Loss: 0.9899736642837524\n",
      "Validation: Epoch [6], Batch [867/938], Loss: 0.9535846710205078\n",
      "Validation: Epoch [6], Batch [868/938], Loss: 1.1433947086334229\n",
      "Validation: Epoch [6], Batch [869/938], Loss: 0.9255321025848389\n",
      "Validation: Epoch [6], Batch [870/938], Loss: 0.6984443664550781\n",
      "Validation: Epoch [6], Batch [871/938], Loss: 0.9257328510284424\n",
      "Validation: Epoch [6], Batch [872/938], Loss: 0.8443468809127808\n",
      "Validation: Epoch [6], Batch [873/938], Loss: 1.0497276782989502\n",
      "Validation: Epoch [6], Batch [874/938], Loss: 0.8112854361534119\n",
      "Validation: Epoch [6], Batch [875/938], Loss: 1.1054773330688477\n",
      "Validation: Epoch [6], Batch [876/938], Loss: 0.9397597312927246\n",
      "Validation: Epoch [6], Batch [877/938], Loss: 1.1015655994415283\n",
      "Validation: Epoch [6], Batch [878/938], Loss: 1.180024266242981\n",
      "Validation: Epoch [6], Batch [879/938], Loss: 0.8546277284622192\n",
      "Validation: Epoch [6], Batch [880/938], Loss: 0.7015330791473389\n",
      "Validation: Epoch [6], Batch [881/938], Loss: 0.9947137236595154\n",
      "Validation: Epoch [6], Batch [882/938], Loss: 0.6849733591079712\n",
      "Validation: Epoch [6], Batch [883/938], Loss: 0.9380881786346436\n",
      "Validation: Epoch [6], Batch [884/938], Loss: 0.9037947654724121\n",
      "Validation: Epoch [6], Batch [885/938], Loss: 0.8246464133262634\n",
      "Validation: Epoch [6], Batch [886/938], Loss: 1.060848593711853\n",
      "Validation: Epoch [6], Batch [887/938], Loss: 0.8869978785514832\n",
      "Validation: Epoch [6], Batch [888/938], Loss: 0.9857365489006042\n",
      "Validation: Epoch [6], Batch [889/938], Loss: 0.8926075100898743\n",
      "Validation: Epoch [6], Batch [890/938], Loss: 0.9624875783920288\n",
      "Validation: Epoch [6], Batch [891/938], Loss: 0.8454018235206604\n",
      "Validation: Epoch [6], Batch [892/938], Loss: 0.8745237588882446\n",
      "Validation: Epoch [6], Batch [893/938], Loss: 0.8045877814292908\n",
      "Validation: Epoch [6], Batch [894/938], Loss: 0.898554801940918\n",
      "Validation: Epoch [6], Batch [895/938], Loss: 0.9458211064338684\n",
      "Validation: Epoch [6], Batch [896/938], Loss: 0.6916911602020264\n",
      "Validation: Epoch [6], Batch [897/938], Loss: 0.8378745317459106\n",
      "Validation: Epoch [6], Batch [898/938], Loss: 1.1157151460647583\n",
      "Validation: Epoch [6], Batch [899/938], Loss: 0.9933766722679138\n",
      "Validation: Epoch [6], Batch [900/938], Loss: 1.1431647539138794\n",
      "Validation: Epoch [6], Batch [901/938], Loss: 0.7925838232040405\n",
      "Validation: Epoch [6], Batch [902/938], Loss: 0.9252952337265015\n",
      "Validation: Epoch [6], Batch [903/938], Loss: 0.9159925580024719\n",
      "Validation: Epoch [6], Batch [904/938], Loss: 1.0537214279174805\n",
      "Validation: Epoch [6], Batch [905/938], Loss: 0.8283146023750305\n",
      "Validation: Epoch [6], Batch [906/938], Loss: 1.0551408529281616\n",
      "Validation: Epoch [6], Batch [907/938], Loss: 0.7789266109466553\n",
      "Validation: Epoch [6], Batch [908/938], Loss: 0.6380580067634583\n",
      "Validation: Epoch [6], Batch [909/938], Loss: 0.9595439434051514\n",
      "Validation: Epoch [6], Batch [910/938], Loss: 0.831062912940979\n",
      "Validation: Epoch [6], Batch [911/938], Loss: 0.892920970916748\n",
      "Validation: Epoch [6], Batch [912/938], Loss: 1.0252747535705566\n",
      "Validation: Epoch [6], Batch [913/938], Loss: 0.8686164021492004\n",
      "Validation: Epoch [6], Batch [914/938], Loss: 0.8920180797576904\n",
      "Validation: Epoch [6], Batch [915/938], Loss: 0.9169512987136841\n",
      "Validation: Epoch [6], Batch [916/938], Loss: 0.8001301288604736\n",
      "Validation: Epoch [6], Batch [917/938], Loss: 1.1618263721466064\n",
      "Validation: Epoch [6], Batch [918/938], Loss: 0.6290727257728577\n",
      "Validation: Epoch [6], Batch [919/938], Loss: 0.9157360792160034\n",
      "Validation: Epoch [6], Batch [920/938], Loss: 0.9609828591346741\n",
      "Validation: Epoch [6], Batch [921/938], Loss: 0.8871952891349792\n",
      "Validation: Epoch [6], Batch [922/938], Loss: 0.9245295524597168\n",
      "Validation: Epoch [6], Batch [923/938], Loss: 0.9203565120697021\n",
      "Validation: Epoch [6], Batch [924/938], Loss: 1.0745846033096313\n",
      "Validation: Epoch [6], Batch [925/938], Loss: 0.7489771842956543\n",
      "Validation: Epoch [6], Batch [926/938], Loss: 0.728784441947937\n",
      "Validation: Epoch [6], Batch [927/938], Loss: 0.7930106520652771\n",
      "Validation: Epoch [6], Batch [928/938], Loss: 1.0792955160140991\n",
      "Validation: Epoch [6], Batch [929/938], Loss: 1.1029969453811646\n",
      "Validation: Epoch [6], Batch [930/938], Loss: 0.8766140937805176\n",
      "Validation: Epoch [6], Batch [931/938], Loss: 0.7631216049194336\n",
      "Validation: Epoch [6], Batch [932/938], Loss: 0.8599830865859985\n",
      "Validation: Epoch [6], Batch [933/938], Loss: 0.7034100294113159\n",
      "Validation: Epoch [6], Batch [934/938], Loss: 1.0977190732955933\n",
      "Validation: Epoch [6], Batch [935/938], Loss: 0.7840547561645508\n",
      "Validation: Epoch [6], Batch [936/938], Loss: 0.8752832412719727\n",
      "Validation: Epoch [6], Batch [937/938], Loss: 1.0917484760284424\n",
      "Validation: Epoch [6], Batch [938/938], Loss: 0.7688344120979309\n",
      "Accuracy of test set: 0.6633333333333333\n",
      "Train: Epoch [7], Batch [1/938], Loss: 1.0881398916244507\n",
      "Train: Epoch [7], Batch [2/938], Loss: 0.9161251783370972\n",
      "Train: Epoch [7], Batch [3/938], Loss: 1.0719860792160034\n",
      "Train: Epoch [7], Batch [4/938], Loss: 0.9017449617385864\n",
      "Train: Epoch [7], Batch [5/938], Loss: 0.9391803741455078\n",
      "Train: Epoch [7], Batch [6/938], Loss: 0.971196174621582\n",
      "Train: Epoch [7], Batch [7/938], Loss: 0.9089542627334595\n",
      "Train: Epoch [7], Batch [8/938], Loss: 1.0043116807937622\n",
      "Train: Epoch [7], Batch [9/938], Loss: 0.9277001619338989\n",
      "Train: Epoch [7], Batch [10/938], Loss: 0.8693891167640686\n",
      "Train: Epoch [7], Batch [11/938], Loss: 0.8476315140724182\n",
      "Train: Epoch [7], Batch [12/938], Loss: 1.021235466003418\n",
      "Train: Epoch [7], Batch [13/938], Loss: 1.2077856063842773\n",
      "Train: Epoch [7], Batch [14/938], Loss: 0.8314812183380127\n",
      "Train: Epoch [7], Batch [15/938], Loss: 0.8257990479469299\n",
      "Train: Epoch [7], Batch [16/938], Loss: 0.9413120746612549\n",
      "Train: Epoch [7], Batch [17/938], Loss: 0.9319290518760681\n",
      "Train: Epoch [7], Batch [18/938], Loss: 0.8168536424636841\n",
      "Train: Epoch [7], Batch [19/938], Loss: 0.9021445512771606\n",
      "Train: Epoch [7], Batch [20/938], Loss: 0.8719294667243958\n",
      "Train: Epoch [7], Batch [21/938], Loss: 0.7829065322875977\n",
      "Train: Epoch [7], Batch [22/938], Loss: 0.9860973358154297\n",
      "Train: Epoch [7], Batch [23/938], Loss: 0.9178701639175415\n",
      "Train: Epoch [7], Batch [24/938], Loss: 1.0385963916778564\n",
      "Train: Epoch [7], Batch [25/938], Loss: 1.0592952966690063\n",
      "Train: Epoch [7], Batch [26/938], Loss: 1.2158446311950684\n",
      "Train: Epoch [7], Batch [27/938], Loss: 0.9661065340042114\n",
      "Train: Epoch [7], Batch [28/938], Loss: 0.7487738132476807\n",
      "Train: Epoch [7], Batch [29/938], Loss: 0.8337041139602661\n",
      "Train: Epoch [7], Batch [30/938], Loss: 0.7875273823738098\n",
      "Train: Epoch [7], Batch [31/938], Loss: 0.9666649103164673\n",
      "Train: Epoch [7], Batch [32/938], Loss: 0.8809473514556885\n",
      "Train: Epoch [7], Batch [33/938], Loss: 0.9041643142700195\n",
      "Train: Epoch [7], Batch [34/938], Loss: 1.1756067276000977\n",
      "Train: Epoch [7], Batch [35/938], Loss: 0.8248502016067505\n",
      "Train: Epoch [7], Batch [36/938], Loss: 0.9905543327331543\n",
      "Train: Epoch [7], Batch [37/938], Loss: 1.0740721225738525\n",
      "Train: Epoch [7], Batch [38/938], Loss: 1.1227995157241821\n",
      "Train: Epoch [7], Batch [39/938], Loss: 0.8050585985183716\n",
      "Train: Epoch [7], Batch [40/938], Loss: 0.9336795806884766\n",
      "Train: Epoch [7], Batch [41/938], Loss: 0.9561830759048462\n",
      "Train: Epoch [7], Batch [42/938], Loss: 1.0428054332733154\n",
      "Train: Epoch [7], Batch [43/938], Loss: 0.717603325843811\n",
      "Train: Epoch [7], Batch [44/938], Loss: 0.9172012805938721\n",
      "Train: Epoch [7], Batch [45/938], Loss: 0.9433934092521667\n",
      "Train: Epoch [7], Batch [46/938], Loss: 1.1037365198135376\n",
      "Train: Epoch [7], Batch [47/938], Loss: 1.3153696060180664\n",
      "Train: Epoch [7], Batch [48/938], Loss: 0.9634060859680176\n",
      "Train: Epoch [7], Batch [49/938], Loss: 0.8772547245025635\n",
      "Train: Epoch [7], Batch [50/938], Loss: 1.0112731456756592\n",
      "Train: Epoch [7], Batch [51/938], Loss: 1.0889414548873901\n",
      "Train: Epoch [7], Batch [52/938], Loss: 1.0252974033355713\n",
      "Train: Epoch [7], Batch [53/938], Loss: 0.7521297335624695\n",
      "Train: Epoch [7], Batch [54/938], Loss: 0.755098819732666\n",
      "Train: Epoch [7], Batch [55/938], Loss: 1.0657727718353271\n",
      "Train: Epoch [7], Batch [56/938], Loss: 0.8997060656547546\n",
      "Train: Epoch [7], Batch [57/938], Loss: 0.9959351420402527\n",
      "Train: Epoch [7], Batch [58/938], Loss: 0.8877205848693848\n",
      "Train: Epoch [7], Batch [59/938], Loss: 1.11933434009552\n",
      "Train: Epoch [7], Batch [60/938], Loss: 1.0374035835266113\n",
      "Train: Epoch [7], Batch [61/938], Loss: 0.8766958713531494\n",
      "Train: Epoch [7], Batch [62/938], Loss: 0.8626953959465027\n",
      "Train: Epoch [7], Batch [63/938], Loss: 0.941574215888977\n",
      "Train: Epoch [7], Batch [64/938], Loss: 1.0413833856582642\n",
      "Train: Epoch [7], Batch [65/938], Loss: 1.176390528678894\n",
      "Train: Epoch [7], Batch [66/938], Loss: 0.9017534255981445\n",
      "Train: Epoch [7], Batch [67/938], Loss: 1.0626981258392334\n",
      "Train: Epoch [7], Batch [68/938], Loss: 0.856937825679779\n",
      "Train: Epoch [7], Batch [69/938], Loss: 0.8796717524528503\n",
      "Train: Epoch [7], Batch [70/938], Loss: 0.8538450598716736\n",
      "Train: Epoch [7], Batch [71/938], Loss: 1.0741209983825684\n",
      "Train: Epoch [7], Batch [72/938], Loss: 0.7301863431930542\n",
      "Train: Epoch [7], Batch [73/938], Loss: 0.9401777386665344\n",
      "Train: Epoch [7], Batch [74/938], Loss: 0.7665395736694336\n",
      "Train: Epoch [7], Batch [75/938], Loss: 1.1933540105819702\n",
      "Train: Epoch [7], Batch [76/938], Loss: 0.8207265734672546\n",
      "Train: Epoch [7], Batch [77/938], Loss: 0.8999443054199219\n",
      "Train: Epoch [7], Batch [78/938], Loss: 0.9852708578109741\n",
      "Train: Epoch [7], Batch [79/938], Loss: 0.8156852126121521\n",
      "Train: Epoch [7], Batch [80/938], Loss: 0.8526737093925476\n",
      "Train: Epoch [7], Batch [81/938], Loss: 0.9783443212509155\n",
      "Train: Epoch [7], Batch [82/938], Loss: 0.8890565037727356\n",
      "Train: Epoch [7], Batch [83/938], Loss: 0.8875857591629028\n",
      "Train: Epoch [7], Batch [84/938], Loss: 0.7589888572692871\n",
      "Train: Epoch [7], Batch [85/938], Loss: 0.9766444563865662\n",
      "Train: Epoch [7], Batch [86/938], Loss: 1.2768381834030151\n",
      "Train: Epoch [7], Batch [87/938], Loss: 0.8189777135848999\n",
      "Train: Epoch [7], Batch [88/938], Loss: 1.2321151494979858\n",
      "Train: Epoch [7], Batch [89/938], Loss: 0.8646032810211182\n",
      "Train: Epoch [7], Batch [90/938], Loss: 1.0212624073028564\n",
      "Train: Epoch [7], Batch [91/938], Loss: 1.1566162109375\n",
      "Train: Epoch [7], Batch [92/938], Loss: 1.0220121145248413\n",
      "Train: Epoch [7], Batch [93/938], Loss: 0.8910462260246277\n",
      "Train: Epoch [7], Batch [94/938], Loss: 0.999150276184082\n",
      "Train: Epoch [7], Batch [95/938], Loss: 0.9230096340179443\n",
      "Train: Epoch [7], Batch [96/938], Loss: 0.9293894171714783\n",
      "Train: Epoch [7], Batch [97/938], Loss: 0.9705262184143066\n",
      "Train: Epoch [7], Batch [98/938], Loss: 0.9497110843658447\n",
      "Train: Epoch [7], Batch [99/938], Loss: 0.8454593420028687\n",
      "Train: Epoch [7], Batch [100/938], Loss: 1.068995475769043\n",
      "Train: Epoch [7], Batch [101/938], Loss: 0.9395684003829956\n",
      "Train: Epoch [7], Batch [102/938], Loss: 1.1429003477096558\n",
      "Train: Epoch [7], Batch [103/938], Loss: 0.7274290323257446\n",
      "Train: Epoch [7], Batch [104/938], Loss: 0.9145063757896423\n",
      "Train: Epoch [7], Batch [105/938], Loss: 0.9498568177223206\n",
      "Train: Epoch [7], Batch [106/938], Loss: 1.054335594177246\n",
      "Train: Epoch [7], Batch [107/938], Loss: 1.0998893976211548\n",
      "Train: Epoch [7], Batch [108/938], Loss: 0.9109089374542236\n",
      "Train: Epoch [7], Batch [109/938], Loss: 1.1866507530212402\n",
      "Train: Epoch [7], Batch [110/938], Loss: 0.8034394979476929\n",
      "Train: Epoch [7], Batch [111/938], Loss: 0.8292526602745056\n",
      "Train: Epoch [7], Batch [112/938], Loss: 0.8754550814628601\n",
      "Train: Epoch [7], Batch [113/938], Loss: 0.8030831813812256\n",
      "Train: Epoch [7], Batch [114/938], Loss: 0.9721193909645081\n",
      "Train: Epoch [7], Batch [115/938], Loss: 0.9042704105377197\n",
      "Train: Epoch [7], Batch [116/938], Loss: 1.0141440629959106\n",
      "Train: Epoch [7], Batch [117/938], Loss: 0.6615476012229919\n",
      "Train: Epoch [7], Batch [118/938], Loss: 0.9769374132156372\n",
      "Train: Epoch [7], Batch [119/938], Loss: 1.1421358585357666\n",
      "Train: Epoch [7], Batch [120/938], Loss: 0.8280104398727417\n",
      "Train: Epoch [7], Batch [121/938], Loss: 0.8832432627677917\n",
      "Train: Epoch [7], Batch [122/938], Loss: 0.9545895457267761\n",
      "Train: Epoch [7], Batch [123/938], Loss: 0.9014480710029602\n",
      "Train: Epoch [7], Batch [124/938], Loss: 0.8459449410438538\n",
      "Train: Epoch [7], Batch [125/938], Loss: 1.0587307214736938\n",
      "Train: Epoch [7], Batch [126/938], Loss: 0.9872170090675354\n",
      "Train: Epoch [7], Batch [127/938], Loss: 0.9321602582931519\n",
      "Train: Epoch [7], Batch [128/938], Loss: 1.002755880355835\n",
      "Train: Epoch [7], Batch [129/938], Loss: 0.7821134328842163\n",
      "Train: Epoch [7], Batch [130/938], Loss: 0.7498559355735779\n",
      "Train: Epoch [7], Batch [131/938], Loss: 0.7361606359481812\n",
      "Train: Epoch [7], Batch [132/938], Loss: 0.9503796696662903\n",
      "Train: Epoch [7], Batch [133/938], Loss: 0.8513113260269165\n",
      "Train: Epoch [7], Batch [134/938], Loss: 1.1231870651245117\n",
      "Train: Epoch [7], Batch [135/938], Loss: 1.1008888483047485\n",
      "Train: Epoch [7], Batch [136/938], Loss: 0.9645994305610657\n",
      "Train: Epoch [7], Batch [137/938], Loss: 0.9214229583740234\n",
      "Train: Epoch [7], Batch [138/938], Loss: 0.9028013348579407\n",
      "Train: Epoch [7], Batch [139/938], Loss: 0.922314465045929\n",
      "Train: Epoch [7], Batch [140/938], Loss: 0.9455181956291199\n",
      "Train: Epoch [7], Batch [141/938], Loss: 0.8482339978218079\n",
      "Train: Epoch [7], Batch [142/938], Loss: 1.1251004934310913\n",
      "Train: Epoch [7], Batch [143/938], Loss: 0.715154230594635\n",
      "Train: Epoch [7], Batch [144/938], Loss: 0.8231757283210754\n",
      "Train: Epoch [7], Batch [145/938], Loss: 0.995780348777771\n",
      "Train: Epoch [7], Batch [146/938], Loss: 0.7903400659561157\n",
      "Train: Epoch [7], Batch [147/938], Loss: 0.9929034113883972\n",
      "Train: Epoch [7], Batch [148/938], Loss: 0.9037449359893799\n",
      "Train: Epoch [7], Batch [149/938], Loss: 0.9667518138885498\n",
      "Train: Epoch [7], Batch [150/938], Loss: 0.9579694271087646\n",
      "Train: Epoch [7], Batch [151/938], Loss: 0.884682297706604\n",
      "Train: Epoch [7], Batch [152/938], Loss: 0.8199326395988464\n",
      "Train: Epoch [7], Batch [153/938], Loss: 1.0731533765792847\n",
      "Train: Epoch [7], Batch [154/938], Loss: 0.9210163354873657\n",
      "Train: Epoch [7], Batch [155/938], Loss: 0.9332277178764343\n",
      "Train: Epoch [7], Batch [156/938], Loss: 1.1833534240722656\n",
      "Train: Epoch [7], Batch [157/938], Loss: 0.9556401371955872\n",
      "Train: Epoch [7], Batch [158/938], Loss: 1.0260721445083618\n",
      "Train: Epoch [7], Batch [159/938], Loss: 0.902893602848053\n",
      "Train: Epoch [7], Batch [160/938], Loss: 1.0223149061203003\n",
      "Train: Epoch [7], Batch [161/938], Loss: 1.0338627099990845\n",
      "Train: Epoch [7], Batch [162/938], Loss: 0.8676568269729614\n",
      "Train: Epoch [7], Batch [163/938], Loss: 1.013329267501831\n",
      "Train: Epoch [7], Batch [164/938], Loss: 0.9957345724105835\n",
      "Train: Epoch [7], Batch [165/938], Loss: 0.960890531539917\n",
      "Train: Epoch [7], Batch [166/938], Loss: 0.9647725224494934\n",
      "Train: Epoch [7], Batch [167/938], Loss: 0.9543111324310303\n",
      "Train: Epoch [7], Batch [168/938], Loss: 0.9533517360687256\n",
      "Train: Epoch [7], Batch [169/938], Loss: 0.8792726993560791\n",
      "Train: Epoch [7], Batch [170/938], Loss: 1.1140222549438477\n",
      "Train: Epoch [7], Batch [171/938], Loss: 0.9921243190765381\n",
      "Train: Epoch [7], Batch [172/938], Loss: 0.9251595735549927\n",
      "Train: Epoch [7], Batch [173/938], Loss: 0.9320224523544312\n",
      "Train: Epoch [7], Batch [174/938], Loss: 1.0217176675796509\n",
      "Train: Epoch [7], Batch [175/938], Loss: 0.9906389117240906\n",
      "Train: Epoch [7], Batch [176/938], Loss: 0.6984525322914124\n",
      "Train: Epoch [7], Batch [177/938], Loss: 0.7965964078903198\n",
      "Train: Epoch [7], Batch [178/938], Loss: 0.9246158003807068\n",
      "Train: Epoch [7], Batch [179/938], Loss: 0.7845180630683899\n",
      "Train: Epoch [7], Batch [180/938], Loss: 0.771176278591156\n",
      "Train: Epoch [7], Batch [181/938], Loss: 1.0338807106018066\n",
      "Train: Epoch [7], Batch [182/938], Loss: 0.9472997784614563\n",
      "Train: Epoch [7], Batch [183/938], Loss: 0.8913353085517883\n",
      "Train: Epoch [7], Batch [184/938], Loss: 0.824715793132782\n",
      "Train: Epoch [7], Batch [185/938], Loss: 1.1534210443496704\n",
      "Train: Epoch [7], Batch [186/938], Loss: 0.9192163944244385\n",
      "Train: Epoch [7], Batch [187/938], Loss: 0.8235579133033752\n",
      "Train: Epoch [7], Batch [188/938], Loss: 0.8178951740264893\n",
      "Train: Epoch [7], Batch [189/938], Loss: 1.43877375125885\n",
      "Train: Epoch [7], Batch [190/938], Loss: 0.7302401065826416\n",
      "Train: Epoch [7], Batch [191/938], Loss: 0.9765820503234863\n",
      "Train: Epoch [7], Batch [192/938], Loss: 1.0456531047821045\n",
      "Train: Epoch [7], Batch [193/938], Loss: 0.9128702878952026\n",
      "Train: Epoch [7], Batch [194/938], Loss: 0.7523922920227051\n",
      "Train: Epoch [7], Batch [195/938], Loss: 1.1444814205169678\n",
      "Train: Epoch [7], Batch [196/938], Loss: 0.8431087136268616\n",
      "Train: Epoch [7], Batch [197/938], Loss: 0.8932477831840515\n",
      "Train: Epoch [7], Batch [198/938], Loss: 1.0077154636383057\n",
      "Train: Epoch [7], Batch [199/938], Loss: 0.7558050751686096\n",
      "Train: Epoch [7], Batch [200/938], Loss: 0.8484934568405151\n",
      "Train: Epoch [7], Batch [201/938], Loss: 1.028127908706665\n",
      "Train: Epoch [7], Batch [202/938], Loss: 0.9146917462348938\n",
      "Train: Epoch [7], Batch [203/938], Loss: 0.8384705185890198\n",
      "Train: Epoch [7], Batch [204/938], Loss: 0.6513043642044067\n",
      "Train: Epoch [7], Batch [205/938], Loss: 0.9754344820976257\n",
      "Train: Epoch [7], Batch [206/938], Loss: 0.9167758822441101\n",
      "Train: Epoch [7], Batch [207/938], Loss: 0.8752730488777161\n",
      "Train: Epoch [7], Batch [208/938], Loss: 0.8921409845352173\n",
      "Train: Epoch [7], Batch [209/938], Loss: 0.9781235456466675\n",
      "Train: Epoch [7], Batch [210/938], Loss: 1.0026029348373413\n",
      "Train: Epoch [7], Batch [211/938], Loss: 0.9309912323951721\n",
      "Train: Epoch [7], Batch [212/938], Loss: 1.0472337007522583\n",
      "Train: Epoch [7], Batch [213/938], Loss: 1.1745728254318237\n",
      "Train: Epoch [7], Batch [214/938], Loss: 0.7486197352409363\n",
      "Train: Epoch [7], Batch [215/938], Loss: 0.9898245930671692\n",
      "Train: Epoch [7], Batch [216/938], Loss: 0.8349111080169678\n",
      "Train: Epoch [7], Batch [217/938], Loss: 0.6635153889656067\n",
      "Train: Epoch [7], Batch [218/938], Loss: 0.6722955703735352\n",
      "Train: Epoch [7], Batch [219/938], Loss: 0.7762323617935181\n",
      "Train: Epoch [7], Batch [220/938], Loss: 0.9346053600311279\n",
      "Train: Epoch [7], Batch [221/938], Loss: 0.9170422554016113\n",
      "Train: Epoch [7], Batch [222/938], Loss: 0.7179009914398193\n",
      "Train: Epoch [7], Batch [223/938], Loss: 0.9051448106765747\n",
      "Train: Epoch [7], Batch [224/938], Loss: 1.057386875152588\n",
      "Train: Epoch [7], Batch [225/938], Loss: 0.9780914783477783\n",
      "Train: Epoch [7], Batch [226/938], Loss: 0.8331840634346008\n",
      "Train: Epoch [7], Batch [227/938], Loss: 0.9314715266227722\n",
      "Train: Epoch [7], Batch [228/938], Loss: 0.8942683935165405\n",
      "Train: Epoch [7], Batch [229/938], Loss: 0.810208261013031\n",
      "Train: Epoch [7], Batch [230/938], Loss: 0.9083842635154724\n",
      "Train: Epoch [7], Batch [231/938], Loss: 1.0879749059677124\n",
      "Train: Epoch [7], Batch [232/938], Loss: 1.102095365524292\n",
      "Train: Epoch [7], Batch [233/938], Loss: 0.8031886219978333\n",
      "Train: Epoch [7], Batch [234/938], Loss: 0.824840784072876\n",
      "Train: Epoch [7], Batch [235/938], Loss: 0.7858566045761108\n",
      "Train: Epoch [7], Batch [236/938], Loss: 0.7516692280769348\n",
      "Train: Epoch [7], Batch [237/938], Loss: 0.7727804780006409\n",
      "Train: Epoch [7], Batch [238/938], Loss: 1.1546013355255127\n",
      "Train: Epoch [7], Batch [239/938], Loss: 0.8846423625946045\n",
      "Train: Epoch [7], Batch [240/938], Loss: 0.8857187032699585\n",
      "Train: Epoch [7], Batch [241/938], Loss: 0.9630028605461121\n",
      "Train: Epoch [7], Batch [242/938], Loss: 1.022942066192627\n",
      "Train: Epoch [7], Batch [243/938], Loss: 0.8983132839202881\n",
      "Train: Epoch [7], Batch [244/938], Loss: 1.0514534711837769\n",
      "Train: Epoch [7], Batch [245/938], Loss: 0.7451277375221252\n",
      "Train: Epoch [7], Batch [246/938], Loss: 0.8125405311584473\n",
      "Train: Epoch [7], Batch [247/938], Loss: 0.9043325185775757\n",
      "Train: Epoch [7], Batch [248/938], Loss: 0.753976047039032\n",
      "Train: Epoch [7], Batch [249/938], Loss: 0.8521827459335327\n",
      "Train: Epoch [7], Batch [250/938], Loss: 0.9477272033691406\n",
      "Train: Epoch [7], Batch [251/938], Loss: 0.8224759101867676\n",
      "Train: Epoch [7], Batch [252/938], Loss: 0.8436138033866882\n",
      "Train: Epoch [7], Batch [253/938], Loss: 0.8199513554573059\n",
      "Train: Epoch [7], Batch [254/938], Loss: 1.109997034072876\n",
      "Train: Epoch [7], Batch [255/938], Loss: 0.75084388256073\n",
      "Train: Epoch [7], Batch [256/938], Loss: 1.0304391384124756\n",
      "Train: Epoch [7], Batch [257/938], Loss: 0.8194340467453003\n",
      "Train: Epoch [7], Batch [258/938], Loss: 1.0326316356658936\n",
      "Train: Epoch [7], Batch [259/938], Loss: 0.9972999691963196\n",
      "Train: Epoch [7], Batch [260/938], Loss: 0.9495264887809753\n",
      "Train: Epoch [7], Batch [261/938], Loss: 0.9381191730499268\n",
      "Train: Epoch [7], Batch [262/938], Loss: 0.9224859476089478\n",
      "Train: Epoch [7], Batch [263/938], Loss: 0.875920832157135\n",
      "Train: Epoch [7], Batch [264/938], Loss: 0.8998669385910034\n",
      "Train: Epoch [7], Batch [265/938], Loss: 0.8027105927467346\n",
      "Train: Epoch [7], Batch [266/938], Loss: 0.9525085687637329\n",
      "Train: Epoch [7], Batch [267/938], Loss: 0.8781579732894897\n",
      "Train: Epoch [7], Batch [268/938], Loss: 0.8862402439117432\n",
      "Train: Epoch [7], Batch [269/938], Loss: 0.7819423079490662\n",
      "Train: Epoch [7], Batch [270/938], Loss: 0.9050226211547852\n",
      "Train: Epoch [7], Batch [271/938], Loss: 1.0652475357055664\n",
      "Train: Epoch [7], Batch [272/938], Loss: 0.848114013671875\n",
      "Train: Epoch [7], Batch [273/938], Loss: 1.1361931562423706\n",
      "Train: Epoch [7], Batch [274/938], Loss: 0.7431855797767639\n",
      "Train: Epoch [7], Batch [275/938], Loss: 0.9472396373748779\n",
      "Train: Epoch [7], Batch [276/938], Loss: 0.9510400295257568\n",
      "Train: Epoch [7], Batch [277/938], Loss: 0.9727208614349365\n",
      "Train: Epoch [7], Batch [278/938], Loss: 0.8664048910140991\n",
      "Train: Epoch [7], Batch [279/938], Loss: 0.9955838918685913\n",
      "Train: Epoch [7], Batch [280/938], Loss: 0.7864248156547546\n",
      "Train: Epoch [7], Batch [281/938], Loss: 1.1680482625961304\n",
      "Train: Epoch [7], Batch [282/938], Loss: 0.9522597193717957\n",
      "Train: Epoch [7], Batch [283/938], Loss: 0.8623589277267456\n",
      "Train: Epoch [7], Batch [284/938], Loss: 0.7471010088920593\n",
      "Train: Epoch [7], Batch [285/938], Loss: 0.9816547632217407\n",
      "Train: Epoch [7], Batch [286/938], Loss: 0.9037594795227051\n",
      "Train: Epoch [7], Batch [287/938], Loss: 1.0068602561950684\n",
      "Train: Epoch [7], Batch [288/938], Loss: 0.8941997289657593\n",
      "Train: Epoch [7], Batch [289/938], Loss: 0.8828344941139221\n",
      "Train: Epoch [7], Batch [290/938], Loss: 0.8833379745483398\n",
      "Train: Epoch [7], Batch [291/938], Loss: 0.9842363595962524\n",
      "Train: Epoch [7], Batch [292/938], Loss: 1.0020819902420044\n",
      "Train: Epoch [7], Batch [293/938], Loss: 1.0015918016433716\n",
      "Train: Epoch [7], Batch [294/938], Loss: 0.7703866362571716\n",
      "Train: Epoch [7], Batch [295/938], Loss: 0.8656716346740723\n",
      "Train: Epoch [7], Batch [296/938], Loss: 0.9556271433830261\n",
      "Train: Epoch [7], Batch [297/938], Loss: 0.7902492880821228\n",
      "Train: Epoch [7], Batch [298/938], Loss: 0.7362589240074158\n",
      "Train: Epoch [7], Batch [299/938], Loss: 0.8719127774238586\n",
      "Train: Epoch [7], Batch [300/938], Loss: 0.8752008080482483\n",
      "Train: Epoch [7], Batch [301/938], Loss: 1.0098376274108887\n",
      "Train: Epoch [7], Batch [302/938], Loss: 0.9953522682189941\n",
      "Train: Epoch [7], Batch [303/938], Loss: 1.1256276369094849\n",
      "Train: Epoch [7], Batch [304/938], Loss: 0.764284610748291\n",
      "Train: Epoch [7], Batch [305/938], Loss: 0.8563448786735535\n",
      "Train: Epoch [7], Batch [306/938], Loss: 1.126833438873291\n",
      "Train: Epoch [7], Batch [307/938], Loss: 1.1578269004821777\n",
      "Train: Epoch [7], Batch [308/938], Loss: 0.9027286767959595\n",
      "Train: Epoch [7], Batch [309/938], Loss: 0.9364234209060669\n",
      "Train: Epoch [7], Batch [310/938], Loss: 1.18949556350708\n",
      "Train: Epoch [7], Batch [311/938], Loss: 0.8932202458381653\n",
      "Train: Epoch [7], Batch [312/938], Loss: 0.7561061382293701\n",
      "Train: Epoch [7], Batch [313/938], Loss: 0.7200018167495728\n",
      "Train: Epoch [7], Batch [314/938], Loss: 0.9285174608230591\n",
      "Train: Epoch [7], Batch [315/938], Loss: 0.8799023628234863\n",
      "Train: Epoch [7], Batch [316/938], Loss: 0.8291650414466858\n",
      "Train: Epoch [7], Batch [317/938], Loss: 0.9954027533531189\n",
      "Train: Epoch [7], Batch [318/938], Loss: 0.8091952204704285\n",
      "Train: Epoch [7], Batch [319/938], Loss: 0.8215785026550293\n",
      "Train: Epoch [7], Batch [320/938], Loss: 0.9979381561279297\n",
      "Train: Epoch [7], Batch [321/938], Loss: 0.7752833366394043\n",
      "Train: Epoch [7], Batch [322/938], Loss: 0.9793652296066284\n",
      "Train: Epoch [7], Batch [323/938], Loss: 0.8816073536872864\n",
      "Train: Epoch [7], Batch [324/938], Loss: 0.9216426610946655\n",
      "Train: Epoch [7], Batch [325/938], Loss: 0.8988575339317322\n",
      "Train: Epoch [7], Batch [326/938], Loss: 0.7926741242408752\n",
      "Train: Epoch [7], Batch [327/938], Loss: 0.9692655205726624\n",
      "Train: Epoch [7], Batch [328/938], Loss: 0.929263174533844\n",
      "Train: Epoch [7], Batch [329/938], Loss: 0.8855810165405273\n",
      "Train: Epoch [7], Batch [330/938], Loss: 1.0221471786499023\n",
      "Train: Epoch [7], Batch [331/938], Loss: 0.9162291288375854\n",
      "Train: Epoch [7], Batch [332/938], Loss: 0.9323281645774841\n",
      "Train: Epoch [7], Batch [333/938], Loss: 0.8344119191169739\n",
      "Train: Epoch [7], Batch [334/938], Loss: 0.8944575190544128\n",
      "Train: Epoch [7], Batch [335/938], Loss: 0.9161309003829956\n",
      "Train: Epoch [7], Batch [336/938], Loss: 0.8147469758987427\n",
      "Train: Epoch [7], Batch [337/938], Loss: 1.0342440605163574\n",
      "Train: Epoch [7], Batch [338/938], Loss: 0.8355906009674072\n",
      "Train: Epoch [7], Batch [339/938], Loss: 0.7642806768417358\n",
      "Train: Epoch [7], Batch [340/938], Loss: 0.6916332244873047\n",
      "Train: Epoch [7], Batch [341/938], Loss: 0.8333192467689514\n",
      "Train: Epoch [7], Batch [342/938], Loss: 0.8299022912979126\n",
      "Train: Epoch [7], Batch [343/938], Loss: 1.132168173789978\n",
      "Train: Epoch [7], Batch [344/938], Loss: 1.1160513162612915\n",
      "Train: Epoch [7], Batch [345/938], Loss: 0.7531148791313171\n",
      "Train: Epoch [7], Batch [346/938], Loss: 0.7895570993423462\n",
      "Train: Epoch [7], Batch [347/938], Loss: 0.8462793827056885\n",
      "Train: Epoch [7], Batch [348/938], Loss: 0.978258490562439\n",
      "Train: Epoch [7], Batch [349/938], Loss: 0.7302087545394897\n",
      "Train: Epoch [7], Batch [350/938], Loss: 1.005212426185608\n",
      "Train: Epoch [7], Batch [351/938], Loss: 0.9667958617210388\n",
      "Train: Epoch [7], Batch [352/938], Loss: 0.8129132986068726\n",
      "Train: Epoch [7], Batch [353/938], Loss: 0.8063216209411621\n",
      "Train: Epoch [7], Batch [354/938], Loss: 0.7459903955459595\n",
      "Train: Epoch [7], Batch [355/938], Loss: 0.9351932406425476\n",
      "Train: Epoch [7], Batch [356/938], Loss: 0.7486344575881958\n",
      "Train: Epoch [7], Batch [357/938], Loss: 0.9930498600006104\n",
      "Train: Epoch [7], Batch [358/938], Loss: 0.8981574177742004\n",
      "Train: Epoch [7], Batch [359/938], Loss: 0.7677605152130127\n",
      "Train: Epoch [7], Batch [360/938], Loss: 0.7507696747779846\n",
      "Train: Epoch [7], Batch [361/938], Loss: 0.7717454433441162\n",
      "Train: Epoch [7], Batch [362/938], Loss: 0.8522930145263672\n",
      "Train: Epoch [7], Batch [363/938], Loss: 1.0206423997879028\n",
      "Train: Epoch [7], Batch [364/938], Loss: 1.0523350238800049\n",
      "Train: Epoch [7], Batch [365/938], Loss: 0.78715980052948\n",
      "Train: Epoch [7], Batch [366/938], Loss: 0.9720184803009033\n",
      "Train: Epoch [7], Batch [367/938], Loss: 0.9432963132858276\n",
      "Train: Epoch [7], Batch [368/938], Loss: 0.935676634311676\n",
      "Train: Epoch [7], Batch [369/938], Loss: 1.0338935852050781\n",
      "Train: Epoch [7], Batch [370/938], Loss: 0.8360408544540405\n",
      "Train: Epoch [7], Batch [371/938], Loss: 0.8127869963645935\n",
      "Train: Epoch [7], Batch [372/938], Loss: 0.9660736322402954\n",
      "Train: Epoch [7], Batch [373/938], Loss: 0.8772939443588257\n",
      "Train: Epoch [7], Batch [374/938], Loss: 1.0062538385391235\n",
      "Train: Epoch [7], Batch [375/938], Loss: 0.9213497638702393\n",
      "Train: Epoch [7], Batch [376/938], Loss: 0.8511175513267517\n",
      "Train: Epoch [7], Batch [377/938], Loss: 0.8411552309989929\n",
      "Train: Epoch [7], Batch [378/938], Loss: 0.8593720197677612\n",
      "Train: Epoch [7], Batch [379/938], Loss: 0.7367053627967834\n",
      "Train: Epoch [7], Batch [380/938], Loss: 1.0143685340881348\n",
      "Train: Epoch [7], Batch [381/938], Loss: 0.9647113084793091\n",
      "Train: Epoch [7], Batch [382/938], Loss: 0.48166197538375854\n",
      "Train: Epoch [7], Batch [383/938], Loss: 0.9672166109085083\n",
      "Train: Epoch [7], Batch [384/938], Loss: 0.9476317167282104\n",
      "Train: Epoch [7], Batch [385/938], Loss: 1.0028328895568848\n",
      "Train: Epoch [7], Batch [386/938], Loss: 1.0829455852508545\n",
      "Train: Epoch [7], Batch [387/938], Loss: 1.0520241260528564\n",
      "Train: Epoch [7], Batch [388/938], Loss: 0.9432932138442993\n",
      "Train: Epoch [7], Batch [389/938], Loss: 0.9660521745681763\n",
      "Train: Epoch [7], Batch [390/938], Loss: 0.6970462799072266\n",
      "Train: Epoch [7], Batch [391/938], Loss: 0.9236166477203369\n",
      "Train: Epoch [7], Batch [392/938], Loss: 0.9214141964912415\n",
      "Train: Epoch [7], Batch [393/938], Loss: 0.9197290539741516\n",
      "Train: Epoch [7], Batch [394/938], Loss: 0.8251590132713318\n",
      "Train: Epoch [7], Batch [395/938], Loss: 0.9379673004150391\n",
      "Train: Epoch [7], Batch [396/938], Loss: 0.8188807368278503\n",
      "Train: Epoch [7], Batch [397/938], Loss: 0.8152931332588196\n",
      "Train: Epoch [7], Batch [398/938], Loss: 0.8849364519119263\n",
      "Train: Epoch [7], Batch [399/938], Loss: 0.951453447341919\n",
      "Train: Epoch [7], Batch [400/938], Loss: 0.974005937576294\n",
      "Train: Epoch [7], Batch [401/938], Loss: 0.6732117533683777\n",
      "Train: Epoch [7], Batch [402/938], Loss: 1.0070074796676636\n",
      "Train: Epoch [7], Batch [403/938], Loss: 0.9543744325637817\n",
      "Train: Epoch [7], Batch [404/938], Loss: 0.8130329847335815\n",
      "Train: Epoch [7], Batch [405/938], Loss: 1.041496753692627\n",
      "Train: Epoch [7], Batch [406/938], Loss: 1.0630940198898315\n",
      "Train: Epoch [7], Batch [407/938], Loss: 0.9807286262512207\n",
      "Train: Epoch [7], Batch [408/938], Loss: 0.9895533919334412\n",
      "Train: Epoch [7], Batch [409/938], Loss: 0.962852418422699\n",
      "Train: Epoch [7], Batch [410/938], Loss: 1.00101900100708\n",
      "Train: Epoch [7], Batch [411/938], Loss: 0.9980921149253845\n",
      "Train: Epoch [7], Batch [412/938], Loss: 0.7522804737091064\n",
      "Train: Epoch [7], Batch [413/938], Loss: 0.8509582281112671\n",
      "Train: Epoch [7], Batch [414/938], Loss: 1.0429754257202148\n",
      "Train: Epoch [7], Batch [415/938], Loss: 0.9762313961982727\n",
      "Train: Epoch [7], Batch [416/938], Loss: 0.7011452913284302\n",
      "Train: Epoch [7], Batch [417/938], Loss: 0.9267237186431885\n",
      "Train: Epoch [7], Batch [418/938], Loss: 0.9710310101509094\n",
      "Train: Epoch [7], Batch [419/938], Loss: 0.8032048344612122\n",
      "Train: Epoch [7], Batch [420/938], Loss: 1.061723232269287\n",
      "Train: Epoch [7], Batch [421/938], Loss: 1.0924110412597656\n",
      "Train: Epoch [7], Batch [422/938], Loss: 0.883905291557312\n",
      "Train: Epoch [7], Batch [423/938], Loss: 1.0490211248397827\n",
      "Train: Epoch [7], Batch [424/938], Loss: 0.8500338196754456\n",
      "Train: Epoch [7], Batch [425/938], Loss: 0.7877935171127319\n",
      "Train: Epoch [7], Batch [426/938], Loss: 1.0851255655288696\n",
      "Train: Epoch [7], Batch [427/938], Loss: 0.7876269221305847\n",
      "Train: Epoch [7], Batch [428/938], Loss: 0.7608197927474976\n",
      "Train: Epoch [7], Batch [429/938], Loss: 0.859613299369812\n",
      "Train: Epoch [7], Batch [430/938], Loss: 0.8473204970359802\n",
      "Train: Epoch [7], Batch [431/938], Loss: 0.7036843299865723\n",
      "Train: Epoch [7], Batch [432/938], Loss: 0.8825116753578186\n",
      "Train: Epoch [7], Batch [433/938], Loss: 0.9255692362785339\n",
      "Train: Epoch [7], Batch [434/938], Loss: 1.0399363040924072\n",
      "Train: Epoch [7], Batch [435/938], Loss: 0.9029555320739746\n",
      "Train: Epoch [7], Batch [436/938], Loss: 0.8394222855567932\n",
      "Train: Epoch [7], Batch [437/938], Loss: 0.6545975208282471\n",
      "Train: Epoch [7], Batch [438/938], Loss: 0.8710489869117737\n",
      "Train: Epoch [7], Batch [439/938], Loss: 0.8489049673080444\n",
      "Train: Epoch [7], Batch [440/938], Loss: 1.0397344827651978\n",
      "Train: Epoch [7], Batch [441/938], Loss: 0.9182811379432678\n",
      "Train: Epoch [7], Batch [442/938], Loss: 0.6559597849845886\n",
      "Train: Epoch [7], Batch [443/938], Loss: 0.8799332976341248\n",
      "Train: Epoch [7], Batch [444/938], Loss: 0.8826888203620911\n",
      "Train: Epoch [7], Batch [445/938], Loss: 1.0335180759429932\n",
      "Train: Epoch [7], Batch [446/938], Loss: 0.8607075810432434\n",
      "Train: Epoch [7], Batch [447/938], Loss: 1.0113215446472168\n",
      "Train: Epoch [7], Batch [448/938], Loss: 1.10333251953125\n",
      "Train: Epoch [7], Batch [449/938], Loss: 0.7371693849563599\n",
      "Train: Epoch [7], Batch [450/938], Loss: 0.9715176224708557\n",
      "Train: Epoch [7], Batch [451/938], Loss: 1.142704963684082\n",
      "Train: Epoch [7], Batch [452/938], Loss: 1.0900884866714478\n",
      "Train: Epoch [7], Batch [453/938], Loss: 0.9375940561294556\n",
      "Train: Epoch [7], Batch [454/938], Loss: 0.839794933795929\n",
      "Train: Epoch [7], Batch [455/938], Loss: 1.2319833040237427\n",
      "Train: Epoch [7], Batch [456/938], Loss: 1.1307796239852905\n",
      "Train: Epoch [7], Batch [457/938], Loss: 0.7213020324707031\n",
      "Train: Epoch [7], Batch [458/938], Loss: 1.1197947263717651\n",
      "Train: Epoch [7], Batch [459/938], Loss: 0.7445458173751831\n",
      "Train: Epoch [7], Batch [460/938], Loss: 0.9783680438995361\n",
      "Train: Epoch [7], Batch [461/938], Loss: 1.0269066095352173\n",
      "Train: Epoch [7], Batch [462/938], Loss: 0.9869239926338196\n",
      "Train: Epoch [7], Batch [463/938], Loss: 1.1229817867279053\n",
      "Train: Epoch [7], Batch [464/938], Loss: 1.08036470413208\n",
      "Train: Epoch [7], Batch [465/938], Loss: 1.0812666416168213\n",
      "Train: Epoch [7], Batch [466/938], Loss: 1.1310049295425415\n",
      "Train: Epoch [7], Batch [467/938], Loss: 1.0272845029830933\n",
      "Train: Epoch [7], Batch [468/938], Loss: 0.9465267658233643\n",
      "Train: Epoch [7], Batch [469/938], Loss: 0.7281083464622498\n",
      "Train: Epoch [7], Batch [470/938], Loss: 0.9552880525588989\n",
      "Train: Epoch [7], Batch [471/938], Loss: 0.8082443475723267\n",
      "Train: Epoch [7], Batch [472/938], Loss: 0.854181706905365\n",
      "Train: Epoch [7], Batch [473/938], Loss: 0.9703042507171631\n",
      "Train: Epoch [7], Batch [474/938], Loss: 0.7947224974632263\n",
      "Train: Epoch [7], Batch [475/938], Loss: 0.8282638192176819\n",
      "Train: Epoch [7], Batch [476/938], Loss: 0.8816708326339722\n",
      "Train: Epoch [7], Batch [477/938], Loss: 0.9117950201034546\n",
      "Train: Epoch [7], Batch [478/938], Loss: 1.0856876373291016\n",
      "Train: Epoch [7], Batch [479/938], Loss: 0.7792856097221375\n",
      "Train: Epoch [7], Batch [480/938], Loss: 0.8922819495201111\n",
      "Train: Epoch [7], Batch [481/938], Loss: 0.6584405303001404\n",
      "Train: Epoch [7], Batch [482/938], Loss: 0.7640147805213928\n",
      "Train: Epoch [7], Batch [483/938], Loss: 1.0113288164138794\n",
      "Train: Epoch [7], Batch [484/938], Loss: 0.9722492694854736\n",
      "Train: Epoch [7], Batch [485/938], Loss: 1.0580346584320068\n",
      "Train: Epoch [7], Batch [486/938], Loss: 0.8696890473365784\n",
      "Train: Epoch [7], Batch [487/938], Loss: 0.9462552666664124\n",
      "Train: Epoch [7], Batch [488/938], Loss: 1.0017104148864746\n",
      "Train: Epoch [7], Batch [489/938], Loss: 0.7971484661102295\n",
      "Train: Epoch [7], Batch [490/938], Loss: 1.0068014860153198\n",
      "Train: Epoch [7], Batch [491/938], Loss: 0.9327516555786133\n",
      "Train: Epoch [7], Batch [492/938], Loss: 0.6717328429222107\n",
      "Train: Epoch [7], Batch [493/938], Loss: 0.8500913977622986\n",
      "Train: Epoch [7], Batch [494/938], Loss: 0.9303838014602661\n",
      "Train: Epoch [7], Batch [495/938], Loss: 1.114547610282898\n",
      "Train: Epoch [7], Batch [496/938], Loss: 1.0242421627044678\n",
      "Train: Epoch [7], Batch [497/938], Loss: 1.1490464210510254\n",
      "Train: Epoch [7], Batch [498/938], Loss: 0.8093511462211609\n",
      "Train: Epoch [7], Batch [499/938], Loss: 0.8995645046234131\n",
      "Train: Epoch [7], Batch [500/938], Loss: 0.7712138891220093\n",
      "Train: Epoch [7], Batch [501/938], Loss: 0.769925594329834\n",
      "Train: Epoch [7], Batch [502/938], Loss: 1.0062953233718872\n",
      "Train: Epoch [7], Batch [503/938], Loss: 0.8242039680480957\n",
      "Train: Epoch [7], Batch [504/938], Loss: 1.0421793460845947\n",
      "Train: Epoch [7], Batch [505/938], Loss: 1.0002108812332153\n",
      "Train: Epoch [7], Batch [506/938], Loss: 0.6914302706718445\n",
      "Train: Epoch [7], Batch [507/938], Loss: 0.9863578677177429\n",
      "Train: Epoch [7], Batch [508/938], Loss: 1.156736135482788\n",
      "Train: Epoch [7], Batch [509/938], Loss: 1.0047471523284912\n",
      "Train: Epoch [7], Batch [510/938], Loss: 1.1228690147399902\n",
      "Train: Epoch [7], Batch [511/938], Loss: 1.0409245491027832\n",
      "Train: Epoch [7], Batch [512/938], Loss: 1.0088287591934204\n",
      "Train: Epoch [7], Batch [513/938], Loss: 0.7834324240684509\n",
      "Train: Epoch [7], Batch [514/938], Loss: 0.781608521938324\n",
      "Train: Epoch [7], Batch [515/938], Loss: 1.024145245552063\n",
      "Train: Epoch [7], Batch [516/938], Loss: 0.9076849818229675\n",
      "Train: Epoch [7], Batch [517/938], Loss: 1.0267361402511597\n",
      "Train: Epoch [7], Batch [518/938], Loss: 0.921123206615448\n",
      "Train: Epoch [7], Batch [519/938], Loss: 1.0249532461166382\n",
      "Train: Epoch [7], Batch [520/938], Loss: 0.7750884890556335\n",
      "Train: Epoch [7], Batch [521/938], Loss: 0.9224048852920532\n",
      "Train: Epoch [7], Batch [522/938], Loss: 0.9098493456840515\n",
      "Train: Epoch [7], Batch [523/938], Loss: 1.2040750980377197\n",
      "Train: Epoch [7], Batch [524/938], Loss: 0.9833493828773499\n",
      "Train: Epoch [7], Batch [525/938], Loss: 0.9674329161643982\n",
      "Train: Epoch [7], Batch [526/938], Loss: 0.9393962621688843\n",
      "Train: Epoch [7], Batch [527/938], Loss: 0.8510091304779053\n",
      "Train: Epoch [7], Batch [528/938], Loss: 1.090699553489685\n",
      "Train: Epoch [7], Batch [529/938], Loss: 0.9289058446884155\n",
      "Train: Epoch [7], Batch [530/938], Loss: 0.9563971757888794\n",
      "Train: Epoch [7], Batch [531/938], Loss: 0.7224793434143066\n",
      "Train: Epoch [7], Batch [532/938], Loss: 0.9114635586738586\n",
      "Train: Epoch [7], Batch [533/938], Loss: 0.8255569934844971\n",
      "Train: Epoch [7], Batch [534/938], Loss: 0.9354088306427002\n",
      "Train: Epoch [7], Batch [535/938], Loss: 0.8294453620910645\n",
      "Train: Epoch [7], Batch [536/938], Loss: 0.941442608833313\n",
      "Train: Epoch [7], Batch [537/938], Loss: 0.9023552536964417\n",
      "Train: Epoch [7], Batch [538/938], Loss: 0.8147503733634949\n",
      "Train: Epoch [7], Batch [539/938], Loss: 0.8400397896766663\n",
      "Train: Epoch [7], Batch [540/938], Loss: 0.8004812598228455\n",
      "Train: Epoch [7], Batch [541/938], Loss: 0.968400776386261\n",
      "Train: Epoch [7], Batch [542/938], Loss: 0.7583559155464172\n",
      "Train: Epoch [7], Batch [543/938], Loss: 1.0093772411346436\n",
      "Train: Epoch [7], Batch [544/938], Loss: 0.6278336048126221\n",
      "Train: Epoch [7], Batch [545/938], Loss: 0.8437963724136353\n",
      "Train: Epoch [7], Batch [546/938], Loss: 0.6962507367134094\n",
      "Train: Epoch [7], Batch [547/938], Loss: 0.8074475526809692\n",
      "Train: Epoch [7], Batch [548/938], Loss: 0.8148168325424194\n",
      "Train: Epoch [7], Batch [549/938], Loss: 0.7855682373046875\n",
      "Train: Epoch [7], Batch [550/938], Loss: 0.8531525135040283\n",
      "Train: Epoch [7], Batch [551/938], Loss: 1.0098228454589844\n",
      "Train: Epoch [7], Batch [552/938], Loss: 0.856609046459198\n",
      "Train: Epoch [7], Batch [553/938], Loss: 1.0038793087005615\n",
      "Train: Epoch [7], Batch [554/938], Loss: 0.9529857039451599\n",
      "Train: Epoch [7], Batch [555/938], Loss: 0.8839257955551147\n",
      "Train: Epoch [7], Batch [556/938], Loss: 1.0408070087432861\n",
      "Train: Epoch [7], Batch [557/938], Loss: 1.2129645347595215\n",
      "Train: Epoch [7], Batch [558/938], Loss: 0.7511981725692749\n",
      "Train: Epoch [7], Batch [559/938], Loss: 0.8409580588340759\n",
      "Train: Epoch [7], Batch [560/938], Loss: 0.9513083696365356\n",
      "Train: Epoch [7], Batch [561/938], Loss: 1.0787595510482788\n",
      "Train: Epoch [7], Batch [562/938], Loss: 1.1777188777923584\n",
      "Train: Epoch [7], Batch [563/938], Loss: 1.1895415782928467\n",
      "Train: Epoch [7], Batch [564/938], Loss: 0.7513353228569031\n",
      "Train: Epoch [7], Batch [565/938], Loss: 0.9815868139266968\n",
      "Train: Epoch [7], Batch [566/938], Loss: 0.8849927186965942\n",
      "Train: Epoch [7], Batch [567/938], Loss: 0.8609569072723389\n",
      "Train: Epoch [7], Batch [568/938], Loss: 0.877678394317627\n",
      "Train: Epoch [7], Batch [569/938], Loss: 0.6846492290496826\n",
      "Train: Epoch [7], Batch [570/938], Loss: 0.974280059337616\n",
      "Train: Epoch [7], Batch [571/938], Loss: 0.6626603007316589\n",
      "Train: Epoch [7], Batch [572/938], Loss: 0.9203239679336548\n",
      "Train: Epoch [7], Batch [573/938], Loss: 0.9484757781028748\n",
      "Train: Epoch [7], Batch [574/938], Loss: 0.8647115230560303\n",
      "Train: Epoch [7], Batch [575/938], Loss: 0.8807036280632019\n",
      "Train: Epoch [7], Batch [576/938], Loss: 0.7484524250030518\n",
      "Train: Epoch [7], Batch [577/938], Loss: 0.7625856399536133\n",
      "Train: Epoch [7], Batch [578/938], Loss: 0.7812994122505188\n",
      "Train: Epoch [7], Batch [579/938], Loss: 0.9436091184616089\n",
      "Train: Epoch [7], Batch [580/938], Loss: 0.8194690942764282\n",
      "Train: Epoch [7], Batch [581/938], Loss: 0.8934330940246582\n",
      "Train: Epoch [7], Batch [582/938], Loss: 0.7539269924163818\n",
      "Train: Epoch [7], Batch [583/938], Loss: 0.9088422060012817\n",
      "Train: Epoch [7], Batch [584/938], Loss: 0.9491900205612183\n",
      "Train: Epoch [7], Batch [585/938], Loss: 0.9518551826477051\n",
      "Train: Epoch [7], Batch [586/938], Loss: 0.8478450775146484\n",
      "Train: Epoch [7], Batch [587/938], Loss: 0.7813483476638794\n",
      "Train: Epoch [7], Batch [588/938], Loss: 0.6644290685653687\n",
      "Train: Epoch [7], Batch [589/938], Loss: 0.8192940354347229\n",
      "Train: Epoch [7], Batch [590/938], Loss: 0.7489126920700073\n",
      "Train: Epoch [7], Batch [591/938], Loss: 0.6338372230529785\n",
      "Train: Epoch [7], Batch [592/938], Loss: 0.914060652256012\n",
      "Train: Epoch [7], Batch [593/938], Loss: 0.9402747750282288\n",
      "Train: Epoch [7], Batch [594/938], Loss: 0.8928550481796265\n",
      "Train: Epoch [7], Batch [595/938], Loss: 0.8549050688743591\n",
      "Train: Epoch [7], Batch [596/938], Loss: 0.9166438579559326\n",
      "Train: Epoch [7], Batch [597/938], Loss: 0.9359644651412964\n",
      "Train: Epoch [7], Batch [598/938], Loss: 1.0610482692718506\n",
      "Train: Epoch [7], Batch [599/938], Loss: 0.8930498361587524\n",
      "Train: Epoch [7], Batch [600/938], Loss: 0.9573536515235901\n",
      "Train: Epoch [7], Batch [601/938], Loss: 0.6584745645523071\n",
      "Train: Epoch [7], Batch [602/938], Loss: 0.7248767614364624\n",
      "Train: Epoch [7], Batch [603/938], Loss: 0.6478766798973083\n",
      "Train: Epoch [7], Batch [604/938], Loss: 0.901390016078949\n",
      "Train: Epoch [7], Batch [605/938], Loss: 0.7696037888526917\n",
      "Train: Epoch [7], Batch [606/938], Loss: 0.8200669288635254\n",
      "Train: Epoch [7], Batch [607/938], Loss: 0.9229443073272705\n",
      "Train: Epoch [7], Batch [608/938], Loss: 0.6973784565925598\n",
      "Train: Epoch [7], Batch [609/938], Loss: 1.0155750513076782\n",
      "Train: Epoch [7], Batch [610/938], Loss: 0.8771246671676636\n",
      "Train: Epoch [7], Batch [611/938], Loss: 0.8192805051803589\n",
      "Train: Epoch [7], Batch [612/938], Loss: 0.9664936065673828\n",
      "Train: Epoch [7], Batch [613/938], Loss: 0.9390405416488647\n",
      "Train: Epoch [7], Batch [614/938], Loss: 0.7592117190361023\n",
      "Train: Epoch [7], Batch [615/938], Loss: 0.7704061269760132\n",
      "Train: Epoch [7], Batch [616/938], Loss: 0.859126627445221\n",
      "Train: Epoch [7], Batch [617/938], Loss: 0.8616622090339661\n",
      "Train: Epoch [7], Batch [618/938], Loss: 1.0112292766571045\n",
      "Train: Epoch [7], Batch [619/938], Loss: 1.0915861129760742\n",
      "Train: Epoch [7], Batch [620/938], Loss: 1.0496004819869995\n",
      "Train: Epoch [7], Batch [621/938], Loss: 0.8396710157394409\n",
      "Train: Epoch [7], Batch [622/938], Loss: 1.1880528926849365\n",
      "Train: Epoch [7], Batch [623/938], Loss: 0.982912540435791\n",
      "Train: Epoch [7], Batch [624/938], Loss: 0.8445469737052917\n",
      "Train: Epoch [7], Batch [625/938], Loss: 0.854123055934906\n",
      "Train: Epoch [7], Batch [626/938], Loss: 0.9315935373306274\n",
      "Train: Epoch [7], Batch [627/938], Loss: 1.0896426439285278\n",
      "Train: Epoch [7], Batch [628/938], Loss: 1.0158790349960327\n",
      "Train: Epoch [7], Batch [629/938], Loss: 0.9599329829216003\n",
      "Train: Epoch [7], Batch [630/938], Loss: 1.102280855178833\n",
      "Train: Epoch [7], Batch [631/938], Loss: 0.6606112718582153\n",
      "Train: Epoch [7], Batch [632/938], Loss: 0.8143380880355835\n",
      "Train: Epoch [7], Batch [633/938], Loss: 0.8473379611968994\n",
      "Train: Epoch [7], Batch [634/938], Loss: 0.8309906721115112\n",
      "Train: Epoch [7], Batch [635/938], Loss: 0.9509044885635376\n",
      "Train: Epoch [7], Batch [636/938], Loss: 0.5836759209632874\n",
      "Train: Epoch [7], Batch [637/938], Loss: 0.7575084567070007\n",
      "Train: Epoch [7], Batch [638/938], Loss: 1.0598952770233154\n",
      "Train: Epoch [7], Batch [639/938], Loss: 0.9465470910072327\n",
      "Train: Epoch [7], Batch [640/938], Loss: 0.7711379528045654\n",
      "Train: Epoch [7], Batch [641/938], Loss: 0.9074438810348511\n",
      "Train: Epoch [7], Batch [642/938], Loss: 0.957981288433075\n",
      "Train: Epoch [7], Batch [643/938], Loss: 1.0474135875701904\n",
      "Train: Epoch [7], Batch [644/938], Loss: 0.9513774514198303\n",
      "Train: Epoch [7], Batch [645/938], Loss: 0.977234423160553\n",
      "Train: Epoch [7], Batch [646/938], Loss: 1.017156720161438\n",
      "Train: Epoch [7], Batch [647/938], Loss: 0.8732162714004517\n",
      "Train: Epoch [7], Batch [648/938], Loss: 0.8109184503555298\n",
      "Train: Epoch [7], Batch [649/938], Loss: 0.9599910974502563\n",
      "Train: Epoch [7], Batch [650/938], Loss: 0.9103654623031616\n",
      "Train: Epoch [7], Batch [651/938], Loss: 0.8235651850700378\n",
      "Train: Epoch [7], Batch [652/938], Loss: 0.8800802826881409\n",
      "Train: Epoch [7], Batch [653/938], Loss: 0.8579217791557312\n",
      "Train: Epoch [7], Batch [654/938], Loss: 0.9172893166542053\n",
      "Train: Epoch [7], Batch [655/938], Loss: 0.7827315926551819\n",
      "Train: Epoch [7], Batch [656/938], Loss: 0.874380350112915\n",
      "Train: Epoch [7], Batch [657/938], Loss: 0.8877201080322266\n",
      "Train: Epoch [7], Batch [658/938], Loss: 0.9886692762374878\n",
      "Train: Epoch [7], Batch [659/938], Loss: 0.8390952348709106\n",
      "Train: Epoch [7], Batch [660/938], Loss: 0.7829415202140808\n",
      "Train: Epoch [7], Batch [661/938], Loss: 0.9203689098358154\n",
      "Train: Epoch [7], Batch [662/938], Loss: 1.0722582340240479\n",
      "Train: Epoch [7], Batch [663/938], Loss: 0.9179126024246216\n",
      "Train: Epoch [7], Batch [664/938], Loss: 0.8694061040878296\n",
      "Train: Epoch [7], Batch [665/938], Loss: 0.7330091595649719\n",
      "Train: Epoch [7], Batch [666/938], Loss: 0.8244733810424805\n",
      "Train: Epoch [7], Batch [667/938], Loss: 0.8838158249855042\n",
      "Train: Epoch [7], Batch [668/938], Loss: 0.721403181552887\n",
      "Train: Epoch [7], Batch [669/938], Loss: 0.9225403070449829\n",
      "Train: Epoch [7], Batch [670/938], Loss: 1.0341616868972778\n",
      "Train: Epoch [7], Batch [671/938], Loss: 0.8066970109939575\n",
      "Train: Epoch [7], Batch [672/938], Loss: 1.1146451234817505\n",
      "Train: Epoch [7], Batch [673/938], Loss: 1.155740737915039\n",
      "Train: Epoch [7], Batch [674/938], Loss: 1.0279514789581299\n",
      "Train: Epoch [7], Batch [675/938], Loss: 1.0982416868209839\n",
      "Train: Epoch [7], Batch [676/938], Loss: 1.053438663482666\n",
      "Train: Epoch [7], Batch [677/938], Loss: 1.1397147178649902\n",
      "Train: Epoch [7], Batch [678/938], Loss: 0.8351303935050964\n",
      "Train: Epoch [7], Batch [679/938], Loss: 0.9808205366134644\n",
      "Train: Epoch [7], Batch [680/938], Loss: 0.8821671009063721\n",
      "Train: Epoch [7], Batch [681/938], Loss: 0.8909595012664795\n",
      "Train: Epoch [7], Batch [682/938], Loss: 1.0088914632797241\n",
      "Train: Epoch [7], Batch [683/938], Loss: 0.9356271028518677\n",
      "Train: Epoch [7], Batch [684/938], Loss: 0.7814944982528687\n",
      "Train: Epoch [7], Batch [685/938], Loss: 0.6894276142120361\n",
      "Train: Epoch [7], Batch [686/938], Loss: 0.8988243341445923\n",
      "Train: Epoch [7], Batch [687/938], Loss: 0.6578643321990967\n",
      "Train: Epoch [7], Batch [688/938], Loss: 0.9415482878684998\n",
      "Train: Epoch [7], Batch [689/938], Loss: 0.9134826064109802\n",
      "Train: Epoch [7], Batch [690/938], Loss: 1.073522925376892\n",
      "Train: Epoch [7], Batch [691/938], Loss: 1.0553804636001587\n",
      "Train: Epoch [7], Batch [692/938], Loss: 0.8920257687568665\n",
      "Train: Epoch [7], Batch [693/938], Loss: 0.8271884918212891\n",
      "Train: Epoch [7], Batch [694/938], Loss: 0.9534922242164612\n",
      "Train: Epoch [7], Batch [695/938], Loss: 0.6581553220748901\n",
      "Train: Epoch [7], Batch [696/938], Loss: 0.8398430943489075\n",
      "Train: Epoch [7], Batch [697/938], Loss: 0.8257088661193848\n",
      "Train: Epoch [7], Batch [698/938], Loss: 0.762994110584259\n",
      "Train: Epoch [7], Batch [699/938], Loss: 0.7876594662666321\n",
      "Train: Epoch [7], Batch [700/938], Loss: 0.9092208743095398\n",
      "Train: Epoch [7], Batch [701/938], Loss: 0.9855866432189941\n",
      "Train: Epoch [7], Batch [702/938], Loss: 0.9935786724090576\n",
      "Train: Epoch [7], Batch [703/938], Loss: 0.7761995196342468\n",
      "Train: Epoch [7], Batch [704/938], Loss: 1.029727816581726\n",
      "Train: Epoch [7], Batch [705/938], Loss: 0.9073014855384827\n",
      "Train: Epoch [7], Batch [706/938], Loss: 0.7533853650093079\n",
      "Train: Epoch [7], Batch [707/938], Loss: 1.0967769622802734\n",
      "Train: Epoch [7], Batch [708/938], Loss: 1.027367353439331\n",
      "Train: Epoch [7], Batch [709/938], Loss: 0.6676470637321472\n",
      "Train: Epoch [7], Batch [710/938], Loss: 0.9751160740852356\n",
      "Train: Epoch [7], Batch [711/938], Loss: 0.9426693916320801\n",
      "Train: Epoch [7], Batch [712/938], Loss: 1.180082082748413\n",
      "Train: Epoch [7], Batch [713/938], Loss: 0.850590705871582\n",
      "Train: Epoch [7], Batch [714/938], Loss: 0.8904761075973511\n",
      "Train: Epoch [7], Batch [715/938], Loss: 0.816106379032135\n",
      "Train: Epoch [7], Batch [716/938], Loss: 0.656365156173706\n",
      "Train: Epoch [7], Batch [717/938], Loss: 1.1343621015548706\n",
      "Train: Epoch [7], Batch [718/938], Loss: 0.9933416843414307\n",
      "Train: Epoch [7], Batch [719/938], Loss: 0.8141822218894958\n",
      "Train: Epoch [7], Batch [720/938], Loss: 0.8736577033996582\n",
      "Train: Epoch [7], Batch [721/938], Loss: 0.9918287396430969\n",
      "Train: Epoch [7], Batch [722/938], Loss: 1.0490018129348755\n",
      "Train: Epoch [7], Batch [723/938], Loss: 0.9622522592544556\n",
      "Train: Epoch [7], Batch [724/938], Loss: 0.8183168768882751\n",
      "Train: Epoch [7], Batch [725/938], Loss: 0.8052172660827637\n",
      "Train: Epoch [7], Batch [726/938], Loss: 0.9808525443077087\n",
      "Train: Epoch [7], Batch [727/938], Loss: 0.8500893712043762\n",
      "Train: Epoch [7], Batch [728/938], Loss: 0.7987847924232483\n",
      "Train: Epoch [7], Batch [729/938], Loss: 1.00577974319458\n",
      "Train: Epoch [7], Batch [730/938], Loss: 0.7751048803329468\n",
      "Train: Epoch [7], Batch [731/938], Loss: 0.9554424285888672\n",
      "Train: Epoch [7], Batch [732/938], Loss: 0.8975924253463745\n",
      "Train: Epoch [7], Batch [733/938], Loss: 0.9143640398979187\n",
      "Train: Epoch [7], Batch [734/938], Loss: 0.9707094430923462\n",
      "Train: Epoch [7], Batch [735/938], Loss: 0.8426191210746765\n",
      "Train: Epoch [7], Batch [736/938], Loss: 0.8425341844558716\n",
      "Train: Epoch [7], Batch [737/938], Loss: 0.8137221336364746\n",
      "Train: Epoch [7], Batch [738/938], Loss: 0.873185396194458\n",
      "Train: Epoch [7], Batch [739/938], Loss: 0.6852489709854126\n",
      "Train: Epoch [7], Batch [740/938], Loss: 0.9465085864067078\n",
      "Train: Epoch [7], Batch [741/938], Loss: 0.9185529947280884\n",
      "Train: Epoch [7], Batch [742/938], Loss: 0.8642799258232117\n",
      "Train: Epoch [7], Batch [743/938], Loss: 0.7273381352424622\n",
      "Train: Epoch [7], Batch [744/938], Loss: 0.783218264579773\n",
      "Train: Epoch [7], Batch [745/938], Loss: 0.7549823522567749\n",
      "Train: Epoch [7], Batch [746/938], Loss: 0.9515386819839478\n",
      "Train: Epoch [7], Batch [747/938], Loss: 0.86485755443573\n",
      "Train: Epoch [7], Batch [748/938], Loss: 0.8574502468109131\n",
      "Train: Epoch [7], Batch [749/938], Loss: 0.8994755148887634\n",
      "Train: Epoch [7], Batch [750/938], Loss: 0.833106279373169\n",
      "Train: Epoch [7], Batch [751/938], Loss: 0.9820566177368164\n",
      "Train: Epoch [7], Batch [752/938], Loss: 1.076658844947815\n",
      "Train: Epoch [7], Batch [753/938], Loss: 0.897449791431427\n",
      "Train: Epoch [7], Batch [754/938], Loss: 0.9043283462524414\n",
      "Train: Epoch [7], Batch [755/938], Loss: 0.8952453136444092\n",
      "Train: Epoch [7], Batch [756/938], Loss: 0.9950972199440002\n",
      "Train: Epoch [7], Batch [757/938], Loss: 1.0854512453079224\n",
      "Train: Epoch [7], Batch [758/938], Loss: 0.8336507081985474\n",
      "Train: Epoch [7], Batch [759/938], Loss: 1.1032465696334839\n",
      "Train: Epoch [7], Batch [760/938], Loss: 0.5914572477340698\n",
      "Train: Epoch [7], Batch [761/938], Loss: 0.7784113883972168\n",
      "Train: Epoch [7], Batch [762/938], Loss: 0.8934496641159058\n",
      "Train: Epoch [7], Batch [763/938], Loss: 0.844365119934082\n",
      "Train: Epoch [7], Batch [764/938], Loss: 1.0614964962005615\n",
      "Train: Epoch [7], Batch [765/938], Loss: 1.0106251239776611\n",
      "Train: Epoch [7], Batch [766/938], Loss: 0.8338205814361572\n",
      "Train: Epoch [7], Batch [767/938], Loss: 1.0392675399780273\n",
      "Train: Epoch [7], Batch [768/938], Loss: 0.8427051305770874\n",
      "Train: Epoch [7], Batch [769/938], Loss: 0.8378775119781494\n",
      "Train: Epoch [7], Batch [770/938], Loss: 1.1670039892196655\n",
      "Train: Epoch [7], Batch [771/938], Loss: 1.0912957191467285\n",
      "Train: Epoch [7], Batch [772/938], Loss: 0.7382661700248718\n",
      "Train: Epoch [7], Batch [773/938], Loss: 0.9120147824287415\n",
      "Train: Epoch [7], Batch [774/938], Loss: 0.8454901576042175\n",
      "Train: Epoch [7], Batch [775/938], Loss: 0.8573060035705566\n",
      "Train: Epoch [7], Batch [776/938], Loss: 0.708427906036377\n",
      "Train: Epoch [7], Batch [777/938], Loss: 0.8617780804634094\n",
      "Train: Epoch [7], Batch [778/938], Loss: 0.8859682679176331\n",
      "Train: Epoch [7], Batch [779/938], Loss: 0.834255576133728\n",
      "Train: Epoch [7], Batch [780/938], Loss: 0.69187331199646\n",
      "Train: Epoch [7], Batch [781/938], Loss: 0.7793025970458984\n",
      "Train: Epoch [7], Batch [782/938], Loss: 0.774992048740387\n",
      "Train: Epoch [7], Batch [783/938], Loss: 0.9869818091392517\n",
      "Train: Epoch [7], Batch [784/938], Loss: 1.0564478635787964\n",
      "Train: Epoch [7], Batch [785/938], Loss: 0.8862667083740234\n",
      "Train: Epoch [7], Batch [786/938], Loss: 0.9720056056976318\n",
      "Train: Epoch [7], Batch [787/938], Loss: 1.1492886543273926\n",
      "Train: Epoch [7], Batch [788/938], Loss: 0.9137136340141296\n",
      "Train: Epoch [7], Batch [789/938], Loss: 0.8493006229400635\n",
      "Train: Epoch [7], Batch [790/938], Loss: 0.9361804723739624\n",
      "Train: Epoch [7], Batch [791/938], Loss: 0.9004931449890137\n",
      "Train: Epoch [7], Batch [792/938], Loss: 0.7921193242073059\n",
      "Train: Epoch [7], Batch [793/938], Loss: 1.0789062976837158\n",
      "Train: Epoch [7], Batch [794/938], Loss: 0.9395468235015869\n",
      "Train: Epoch [7], Batch [795/938], Loss: 0.9010740518569946\n",
      "Train: Epoch [7], Batch [796/938], Loss: 0.8990851044654846\n",
      "Train: Epoch [7], Batch [797/938], Loss: 0.9864557385444641\n",
      "Train: Epoch [7], Batch [798/938], Loss: 0.7951841354370117\n",
      "Train: Epoch [7], Batch [799/938], Loss: 0.8803920745849609\n",
      "Train: Epoch [7], Batch [800/938], Loss: 0.9240303039550781\n",
      "Train: Epoch [7], Batch [801/938], Loss: 0.835211992263794\n",
      "Train: Epoch [7], Batch [802/938], Loss: 0.7547908425331116\n",
      "Train: Epoch [7], Batch [803/938], Loss: 0.8909263014793396\n",
      "Train: Epoch [7], Batch [804/938], Loss: 0.9654103517532349\n",
      "Train: Epoch [7], Batch [805/938], Loss: 0.9454601407051086\n",
      "Train: Epoch [7], Batch [806/938], Loss: 0.6704133749008179\n",
      "Train: Epoch [7], Batch [807/938], Loss: 1.0021679401397705\n",
      "Train: Epoch [7], Batch [808/938], Loss: 0.8098618388175964\n",
      "Train: Epoch [7], Batch [809/938], Loss: 0.6849076747894287\n",
      "Train: Epoch [7], Batch [810/938], Loss: 0.8744648694992065\n",
      "Train: Epoch [7], Batch [811/938], Loss: 0.8558529615402222\n",
      "Train: Epoch [7], Batch [812/938], Loss: 1.0178853273391724\n",
      "Train: Epoch [7], Batch [813/938], Loss: 0.7649290561676025\n",
      "Train: Epoch [7], Batch [814/938], Loss: 0.6921161413192749\n",
      "Train: Epoch [7], Batch [815/938], Loss: 0.8999613523483276\n",
      "Train: Epoch [7], Batch [816/938], Loss: 0.8892874717712402\n",
      "Train: Epoch [7], Batch [817/938], Loss: 1.0494391918182373\n",
      "Train: Epoch [7], Batch [818/938], Loss: 0.7914053201675415\n",
      "Train: Epoch [7], Batch [819/938], Loss: 0.8978974223136902\n",
      "Train: Epoch [7], Batch [820/938], Loss: 0.9205197095870972\n",
      "Train: Epoch [7], Batch [821/938], Loss: 0.8279759883880615\n",
      "Train: Epoch [7], Batch [822/938], Loss: 0.9708359241485596\n",
      "Train: Epoch [7], Batch [823/938], Loss: 0.9759036302566528\n",
      "Train: Epoch [7], Batch [824/938], Loss: 0.635685384273529\n",
      "Train: Epoch [7], Batch [825/938], Loss: 0.8959987163543701\n",
      "Train: Epoch [7], Batch [826/938], Loss: 1.1488535404205322\n",
      "Train: Epoch [7], Batch [827/938], Loss: 0.8433271050453186\n",
      "Train: Epoch [7], Batch [828/938], Loss: 0.841426432132721\n",
      "Train: Epoch [7], Batch [829/938], Loss: 0.7748807072639465\n",
      "Train: Epoch [7], Batch [830/938], Loss: 0.8296153545379639\n",
      "Train: Epoch [7], Batch [831/938], Loss: 0.878242552280426\n",
      "Train: Epoch [7], Batch [832/938], Loss: 0.8848050832748413\n",
      "Train: Epoch [7], Batch [833/938], Loss: 0.9923543930053711\n",
      "Train: Epoch [7], Batch [834/938], Loss: 0.7488179206848145\n",
      "Train: Epoch [7], Batch [835/938], Loss: 0.8850351572036743\n",
      "Train: Epoch [7], Batch [836/938], Loss: 0.9742304682731628\n",
      "Train: Epoch [7], Batch [837/938], Loss: 1.2662322521209717\n",
      "Train: Epoch [7], Batch [838/938], Loss: 1.0027110576629639\n",
      "Train: Epoch [7], Batch [839/938], Loss: 0.8545656204223633\n",
      "Train: Epoch [7], Batch [840/938], Loss: 0.780461311340332\n",
      "Train: Epoch [7], Batch [841/938], Loss: 0.6174967288970947\n",
      "Train: Epoch [7], Batch [842/938], Loss: 0.7491031289100647\n",
      "Train: Epoch [7], Batch [843/938], Loss: 0.6618359684944153\n",
      "Train: Epoch [7], Batch [844/938], Loss: 0.8803501725196838\n",
      "Train: Epoch [7], Batch [845/938], Loss: 1.0236430168151855\n",
      "Train: Epoch [7], Batch [846/938], Loss: 0.7861059308052063\n",
      "Train: Epoch [7], Batch [847/938], Loss: 0.7360399961471558\n",
      "Train: Epoch [7], Batch [848/938], Loss: 0.9830067157745361\n",
      "Train: Epoch [7], Batch [849/938], Loss: 0.8472452163696289\n",
      "Train: Epoch [7], Batch [850/938], Loss: 0.9894818067550659\n",
      "Train: Epoch [7], Batch [851/938], Loss: 0.8168038129806519\n",
      "Train: Epoch [7], Batch [852/938], Loss: 0.7669064998626709\n",
      "Train: Epoch [7], Batch [853/938], Loss: 1.047256350517273\n",
      "Train: Epoch [7], Batch [854/938], Loss: 0.9287390112876892\n",
      "Train: Epoch [7], Batch [855/938], Loss: 1.03597092628479\n",
      "Train: Epoch [7], Batch [856/938], Loss: 1.090076208114624\n",
      "Train: Epoch [7], Batch [857/938], Loss: 0.6510792374610901\n",
      "Train: Epoch [7], Batch [858/938], Loss: 0.668011486530304\n",
      "Train: Epoch [7], Batch [859/938], Loss: 0.622245728969574\n",
      "Train: Epoch [7], Batch [860/938], Loss: 0.9235392212867737\n",
      "Train: Epoch [7], Batch [861/938], Loss: 1.0912933349609375\n",
      "Train: Epoch [7], Batch [862/938], Loss: 0.7723566293716431\n",
      "Train: Epoch [7], Batch [863/938], Loss: 1.0124406814575195\n",
      "Train: Epoch [7], Batch [864/938], Loss: 0.9138057231903076\n",
      "Train: Epoch [7], Batch [865/938], Loss: 0.781159520149231\n",
      "Train: Epoch [7], Batch [866/938], Loss: 0.9198230504989624\n",
      "Train: Epoch [7], Batch [867/938], Loss: 0.9368561506271362\n",
      "Train: Epoch [7], Batch [868/938], Loss: 0.9787394404411316\n",
      "Train: Epoch [7], Batch [869/938], Loss: 0.9735348224639893\n",
      "Train: Epoch [7], Batch [870/938], Loss: 0.7727971076965332\n",
      "Train: Epoch [7], Batch [871/938], Loss: 0.8145145177841187\n",
      "Train: Epoch [7], Batch [872/938], Loss: 0.7320936918258667\n",
      "Train: Epoch [7], Batch [873/938], Loss: 0.7935130596160889\n",
      "Train: Epoch [7], Batch [874/938], Loss: 0.8535570502281189\n",
      "Train: Epoch [7], Batch [875/938], Loss: 0.7181739807128906\n",
      "Train: Epoch [7], Batch [876/938], Loss: 0.7352735996246338\n",
      "Train: Epoch [7], Batch [877/938], Loss: 0.7004008293151855\n",
      "Train: Epoch [7], Batch [878/938], Loss: 0.9257693290710449\n",
      "Train: Epoch [7], Batch [879/938], Loss: 0.8180449604988098\n",
      "Train: Epoch [7], Batch [880/938], Loss: 0.9149627685546875\n",
      "Train: Epoch [7], Batch [881/938], Loss: 1.0169811248779297\n",
      "Train: Epoch [7], Batch [882/938], Loss: 0.8626930117607117\n",
      "Train: Epoch [7], Batch [883/938], Loss: 0.956599235534668\n",
      "Train: Epoch [7], Batch [884/938], Loss: 0.908972442150116\n",
      "Train: Epoch [7], Batch [885/938], Loss: 0.8473575115203857\n",
      "Train: Epoch [7], Batch [886/938], Loss: 0.7548882961273193\n",
      "Train: Epoch [7], Batch [887/938], Loss: 0.7948715686798096\n",
      "Train: Epoch [7], Batch [888/938], Loss: 0.7748351097106934\n",
      "Train: Epoch [7], Batch [889/938], Loss: 0.7141170501708984\n",
      "Train: Epoch [7], Batch [890/938], Loss: 1.0911381244659424\n",
      "Train: Epoch [7], Batch [891/938], Loss: 0.8824299573898315\n",
      "Train: Epoch [7], Batch [892/938], Loss: 0.8631783723831177\n",
      "Train: Epoch [7], Batch [893/938], Loss: 0.8748422861099243\n",
      "Train: Epoch [7], Batch [894/938], Loss: 0.8331472873687744\n",
      "Train: Epoch [7], Batch [895/938], Loss: 0.8335902094841003\n",
      "Train: Epoch [7], Batch [896/938], Loss: 1.0535261631011963\n",
      "Train: Epoch [7], Batch [897/938], Loss: 0.7090939283370972\n",
      "Train: Epoch [7], Batch [898/938], Loss: 0.7259242534637451\n",
      "Train: Epoch [7], Batch [899/938], Loss: 1.0984591245651245\n",
      "Train: Epoch [7], Batch [900/938], Loss: 0.7015567421913147\n",
      "Train: Epoch [7], Batch [901/938], Loss: 0.8577895760536194\n",
      "Train: Epoch [7], Batch [902/938], Loss: 0.7478614449501038\n",
      "Train: Epoch [7], Batch [903/938], Loss: 0.9128212928771973\n",
      "Train: Epoch [7], Batch [904/938], Loss: 0.7942911386489868\n",
      "Train: Epoch [7], Batch [905/938], Loss: 0.7884138822555542\n",
      "Train: Epoch [7], Batch [906/938], Loss: 0.7590599060058594\n",
      "Train: Epoch [7], Batch [907/938], Loss: 0.8131698369979858\n",
      "Train: Epoch [7], Batch [908/938], Loss: 1.0385639667510986\n",
      "Train: Epoch [7], Batch [909/938], Loss: 0.8229250311851501\n",
      "Train: Epoch [7], Batch [910/938], Loss: 0.866847038269043\n",
      "Train: Epoch [7], Batch [911/938], Loss: 0.7625491619110107\n",
      "Train: Epoch [7], Batch [912/938], Loss: 1.22602379322052\n",
      "Train: Epoch [7], Batch [913/938], Loss: 0.7426379919052124\n",
      "Train: Epoch [7], Batch [914/938], Loss: 1.1274749040603638\n",
      "Train: Epoch [7], Batch [915/938], Loss: 0.8897396326065063\n",
      "Train: Epoch [7], Batch [916/938], Loss: 0.9804707169532776\n",
      "Train: Epoch [7], Batch [917/938], Loss: 0.8429791927337646\n",
      "Train: Epoch [7], Batch [918/938], Loss: 0.8783712387084961\n",
      "Train: Epoch [7], Batch [919/938], Loss: 0.8809894323348999\n",
      "Train: Epoch [7], Batch [920/938], Loss: 0.8808039426803589\n",
      "Train: Epoch [7], Batch [921/938], Loss: 0.9125425219535828\n",
      "Train: Epoch [7], Batch [922/938], Loss: 0.7111360430717468\n",
      "Train: Epoch [7], Batch [923/938], Loss: 1.078151822090149\n",
      "Train: Epoch [7], Batch [924/938], Loss: 0.8622316122055054\n",
      "Train: Epoch [7], Batch [925/938], Loss: 0.8258838653564453\n",
      "Train: Epoch [7], Batch [926/938], Loss: 1.0130952596664429\n",
      "Train: Epoch [7], Batch [927/938], Loss: 0.7638400793075562\n",
      "Train: Epoch [7], Batch [928/938], Loss: 0.8206688761711121\n",
      "Train: Epoch [7], Batch [929/938], Loss: 0.9364001154899597\n",
      "Train: Epoch [7], Batch [930/938], Loss: 0.7690709233283997\n",
      "Train: Epoch [7], Batch [931/938], Loss: 0.65230792760849\n",
      "Train: Epoch [7], Batch [932/938], Loss: 0.7850371599197388\n",
      "Train: Epoch [7], Batch [933/938], Loss: 1.0177806615829468\n",
      "Train: Epoch [7], Batch [934/938], Loss: 0.871307373046875\n",
      "Train: Epoch [7], Batch [935/938], Loss: 0.7196314334869385\n",
      "Train: Epoch [7], Batch [936/938], Loss: 1.0714516639709473\n",
      "Train: Epoch [7], Batch [937/938], Loss: 0.8846177458763123\n",
      "Train: Epoch [7], Batch [938/938], Loss: 0.7962995171546936\n",
      "Accuracy of train set: 0.68075\n",
      "Validation: Epoch [7], Batch [1/938], Loss: 1.1748907566070557\n",
      "Validation: Epoch [7], Batch [2/938], Loss: 0.8459682464599609\n",
      "Validation: Epoch [7], Batch [3/938], Loss: 0.8701488971710205\n",
      "Validation: Epoch [7], Batch [4/938], Loss: 0.8203595876693726\n",
      "Validation: Epoch [7], Batch [5/938], Loss: 0.7507707476615906\n",
      "Validation: Epoch [7], Batch [6/938], Loss: 0.9121226072311401\n",
      "Validation: Epoch [7], Batch [7/938], Loss: 1.1069200038909912\n",
      "Validation: Epoch [7], Batch [8/938], Loss: 0.7531424760818481\n",
      "Validation: Epoch [7], Batch [9/938], Loss: 0.989185631275177\n",
      "Validation: Epoch [7], Batch [10/938], Loss: 0.9018375277519226\n",
      "Validation: Epoch [7], Batch [11/938], Loss: 0.7370458245277405\n",
      "Validation: Epoch [7], Batch [12/938], Loss: 0.8615244030952454\n",
      "Validation: Epoch [7], Batch [13/938], Loss: 0.9910606145858765\n",
      "Validation: Epoch [7], Batch [14/938], Loss: 1.0293642282485962\n",
      "Validation: Epoch [7], Batch [15/938], Loss: 0.8791187405586243\n",
      "Validation: Epoch [7], Batch [16/938], Loss: 0.8597772121429443\n",
      "Validation: Epoch [7], Batch [17/938], Loss: 0.8233977556228638\n",
      "Validation: Epoch [7], Batch [18/938], Loss: 1.0705931186676025\n",
      "Validation: Epoch [7], Batch [19/938], Loss: 1.0660901069641113\n",
      "Validation: Epoch [7], Batch [20/938], Loss: 0.9113197922706604\n",
      "Validation: Epoch [7], Batch [21/938], Loss: 0.8351380825042725\n",
      "Validation: Epoch [7], Batch [22/938], Loss: 0.6896631121635437\n",
      "Validation: Epoch [7], Batch [23/938], Loss: 0.797196626663208\n",
      "Validation: Epoch [7], Batch [24/938], Loss: 0.9290153384208679\n",
      "Validation: Epoch [7], Batch [25/938], Loss: 1.0846508741378784\n",
      "Validation: Epoch [7], Batch [26/938], Loss: 1.0800628662109375\n",
      "Validation: Epoch [7], Batch [27/938], Loss: 0.6730451583862305\n",
      "Validation: Epoch [7], Batch [28/938], Loss: 0.9167148470878601\n",
      "Validation: Epoch [7], Batch [29/938], Loss: 0.9953898191452026\n",
      "Validation: Epoch [7], Batch [30/938], Loss: 0.7714511752128601\n",
      "Validation: Epoch [7], Batch [31/938], Loss: 0.7684346437454224\n",
      "Validation: Epoch [7], Batch [32/938], Loss: 0.6903626918792725\n",
      "Validation: Epoch [7], Batch [33/938], Loss: 0.664970338344574\n",
      "Validation: Epoch [7], Batch [34/938], Loss: 0.7570722103118896\n",
      "Validation: Epoch [7], Batch [35/938], Loss: 0.7246017456054688\n",
      "Validation: Epoch [7], Batch [36/938], Loss: 0.6807916760444641\n",
      "Validation: Epoch [7], Batch [37/938], Loss: 0.8994269967079163\n",
      "Validation: Epoch [7], Batch [38/938], Loss: 0.8098517060279846\n",
      "Validation: Epoch [7], Batch [39/938], Loss: 0.8895729780197144\n",
      "Validation: Epoch [7], Batch [40/938], Loss: 0.9243892431259155\n",
      "Validation: Epoch [7], Batch [41/938], Loss: 0.8927226066589355\n",
      "Validation: Epoch [7], Batch [42/938], Loss: 0.9103168249130249\n",
      "Validation: Epoch [7], Batch [43/938], Loss: 0.7749879360198975\n",
      "Validation: Epoch [7], Batch [44/938], Loss: 0.7535427808761597\n",
      "Validation: Epoch [7], Batch [45/938], Loss: 1.0196099281311035\n",
      "Validation: Epoch [7], Batch [46/938], Loss: 0.9885963797569275\n",
      "Validation: Epoch [7], Batch [47/938], Loss: 0.8339632153511047\n",
      "Validation: Epoch [7], Batch [48/938], Loss: 0.76756352186203\n",
      "Validation: Epoch [7], Batch [49/938], Loss: 1.0834853649139404\n",
      "Validation: Epoch [7], Batch [50/938], Loss: 0.9512624740600586\n",
      "Validation: Epoch [7], Batch [51/938], Loss: 0.9045875668525696\n",
      "Validation: Epoch [7], Batch [52/938], Loss: 0.7942640781402588\n",
      "Validation: Epoch [7], Batch [53/938], Loss: 0.7582129836082458\n",
      "Validation: Epoch [7], Batch [54/938], Loss: 0.8989131450653076\n",
      "Validation: Epoch [7], Batch [55/938], Loss: 0.7772098779678345\n",
      "Validation: Epoch [7], Batch [56/938], Loss: 0.9380121231079102\n",
      "Validation: Epoch [7], Batch [57/938], Loss: 0.7514927387237549\n",
      "Validation: Epoch [7], Batch [58/938], Loss: 0.6501907110214233\n",
      "Validation: Epoch [7], Batch [59/938], Loss: 0.7669638395309448\n",
      "Validation: Epoch [7], Batch [60/938], Loss: 0.9144659042358398\n",
      "Validation: Epoch [7], Batch [61/938], Loss: 0.8913223147392273\n",
      "Validation: Epoch [7], Batch [62/938], Loss: 0.7901480197906494\n",
      "Validation: Epoch [7], Batch [63/938], Loss: 0.7626237869262695\n",
      "Validation: Epoch [7], Batch [64/938], Loss: 0.9323521256446838\n",
      "Validation: Epoch [7], Batch [65/938], Loss: 1.0457412004470825\n",
      "Validation: Epoch [7], Batch [66/938], Loss: 0.8952873945236206\n",
      "Validation: Epoch [7], Batch [67/938], Loss: 0.9276614785194397\n",
      "Validation: Epoch [7], Batch [68/938], Loss: 1.0165101289749146\n",
      "Validation: Epoch [7], Batch [69/938], Loss: 0.9658867716789246\n",
      "Validation: Epoch [7], Batch [70/938], Loss: 0.7593065500259399\n",
      "Validation: Epoch [7], Batch [71/938], Loss: 0.7194379568099976\n",
      "Validation: Epoch [7], Batch [72/938], Loss: 0.8552353978157043\n",
      "Validation: Epoch [7], Batch [73/938], Loss: 0.8859888315200806\n",
      "Validation: Epoch [7], Batch [74/938], Loss: 0.8829152584075928\n",
      "Validation: Epoch [7], Batch [75/938], Loss: 0.777276337146759\n",
      "Validation: Epoch [7], Batch [76/938], Loss: 0.6748136878013611\n",
      "Validation: Epoch [7], Batch [77/938], Loss: 1.015047311782837\n",
      "Validation: Epoch [7], Batch [78/938], Loss: 0.8058171272277832\n",
      "Validation: Epoch [7], Batch [79/938], Loss: 0.847809374332428\n",
      "Validation: Epoch [7], Batch [80/938], Loss: 0.6742027997970581\n",
      "Validation: Epoch [7], Batch [81/938], Loss: 0.8934674263000488\n",
      "Validation: Epoch [7], Batch [82/938], Loss: 1.0158435106277466\n",
      "Validation: Epoch [7], Batch [83/938], Loss: 0.8780778646469116\n",
      "Validation: Epoch [7], Batch [84/938], Loss: 0.806509792804718\n",
      "Validation: Epoch [7], Batch [85/938], Loss: 0.8486730456352234\n",
      "Validation: Epoch [7], Batch [86/938], Loss: 0.6806453466415405\n",
      "Validation: Epoch [7], Batch [87/938], Loss: 1.0152076482772827\n",
      "Validation: Epoch [7], Batch [88/938], Loss: 0.6432740688323975\n",
      "Validation: Epoch [7], Batch [89/938], Loss: 1.021815538406372\n",
      "Validation: Epoch [7], Batch [90/938], Loss: 0.7451366186141968\n",
      "Validation: Epoch [7], Batch [91/938], Loss: 0.9740383625030518\n",
      "Validation: Epoch [7], Batch [92/938], Loss: 0.8568613529205322\n",
      "Validation: Epoch [7], Batch [93/938], Loss: 0.6716999411582947\n",
      "Validation: Epoch [7], Batch [94/938], Loss: 0.7487146258354187\n",
      "Validation: Epoch [7], Batch [95/938], Loss: 0.8608814477920532\n",
      "Validation: Epoch [7], Batch [96/938], Loss: 0.9758228063583374\n",
      "Validation: Epoch [7], Batch [97/938], Loss: 0.8171209692955017\n",
      "Validation: Epoch [7], Batch [98/938], Loss: 0.8417231440544128\n",
      "Validation: Epoch [7], Batch [99/938], Loss: 0.8937245607376099\n",
      "Validation: Epoch [7], Batch [100/938], Loss: 0.808142900466919\n",
      "Validation: Epoch [7], Batch [101/938], Loss: 0.762118935585022\n",
      "Validation: Epoch [7], Batch [102/938], Loss: 0.9111949801445007\n",
      "Validation: Epoch [7], Batch [103/938], Loss: 1.0731017589569092\n",
      "Validation: Epoch [7], Batch [104/938], Loss: 0.9420100450515747\n",
      "Validation: Epoch [7], Batch [105/938], Loss: 0.7639167308807373\n",
      "Validation: Epoch [7], Batch [106/938], Loss: 0.8260186910629272\n",
      "Validation: Epoch [7], Batch [107/938], Loss: 0.8942201733589172\n",
      "Validation: Epoch [7], Batch [108/938], Loss: 1.0041488409042358\n",
      "Validation: Epoch [7], Batch [109/938], Loss: 1.018980860710144\n",
      "Validation: Epoch [7], Batch [110/938], Loss: 1.0642421245574951\n",
      "Validation: Epoch [7], Batch [111/938], Loss: 0.6803078651428223\n",
      "Validation: Epoch [7], Batch [112/938], Loss: 0.8525627851486206\n",
      "Validation: Epoch [7], Batch [113/938], Loss: 0.7757363319396973\n",
      "Validation: Epoch [7], Batch [114/938], Loss: 0.7979004383087158\n",
      "Validation: Epoch [7], Batch [115/938], Loss: 0.7815526723861694\n",
      "Validation: Epoch [7], Batch [116/938], Loss: 0.8782734274864197\n",
      "Validation: Epoch [7], Batch [117/938], Loss: 0.9038861989974976\n",
      "Validation: Epoch [7], Batch [118/938], Loss: 0.9073375463485718\n",
      "Validation: Epoch [7], Batch [119/938], Loss: 0.9949884414672852\n",
      "Validation: Epoch [7], Batch [120/938], Loss: 0.8770009279251099\n",
      "Validation: Epoch [7], Batch [121/938], Loss: 0.8409447073936462\n",
      "Validation: Epoch [7], Batch [122/938], Loss: 0.8564590215682983\n",
      "Validation: Epoch [7], Batch [123/938], Loss: 0.8905357718467712\n",
      "Validation: Epoch [7], Batch [124/938], Loss: 0.6435298919677734\n",
      "Validation: Epoch [7], Batch [125/938], Loss: 1.029663324356079\n",
      "Validation: Epoch [7], Batch [126/938], Loss: 0.9129987955093384\n",
      "Validation: Epoch [7], Batch [127/938], Loss: 1.0071392059326172\n",
      "Validation: Epoch [7], Batch [128/938], Loss: 0.9300491809844971\n",
      "Validation: Epoch [7], Batch [129/938], Loss: 1.019902229309082\n",
      "Validation: Epoch [7], Batch [130/938], Loss: 0.9010396003723145\n",
      "Validation: Epoch [7], Batch [131/938], Loss: 0.7011255025863647\n",
      "Validation: Epoch [7], Batch [132/938], Loss: 1.0674192905426025\n",
      "Validation: Epoch [7], Batch [133/938], Loss: 0.820732831954956\n",
      "Validation: Epoch [7], Batch [134/938], Loss: 1.1448140144348145\n",
      "Validation: Epoch [7], Batch [135/938], Loss: 0.8559104204177856\n",
      "Validation: Epoch [7], Batch [136/938], Loss: 1.0188381671905518\n",
      "Validation: Epoch [7], Batch [137/938], Loss: 0.8341292142868042\n",
      "Validation: Epoch [7], Batch [138/938], Loss: 0.9107582569122314\n",
      "Validation: Epoch [7], Batch [139/938], Loss: 0.8613641858100891\n",
      "Validation: Epoch [7], Batch [140/938], Loss: 0.8576542139053345\n",
      "Validation: Epoch [7], Batch [141/938], Loss: 1.0195138454437256\n",
      "Validation: Epoch [7], Batch [142/938], Loss: 0.943026065826416\n",
      "Validation: Epoch [7], Batch [143/938], Loss: 0.9440077543258667\n",
      "Validation: Epoch [7], Batch [144/938], Loss: 0.7986609935760498\n",
      "Validation: Epoch [7], Batch [145/938], Loss: 1.1066213846206665\n",
      "Validation: Epoch [7], Batch [146/938], Loss: 0.8374773263931274\n",
      "Validation: Epoch [7], Batch [147/938], Loss: 0.7794593572616577\n",
      "Validation: Epoch [7], Batch [148/938], Loss: 0.9559411406517029\n",
      "Validation: Epoch [7], Batch [149/938], Loss: 0.8463143706321716\n",
      "Validation: Epoch [7], Batch [150/938], Loss: 0.9122874736785889\n",
      "Validation: Epoch [7], Batch [151/938], Loss: 0.9016933441162109\n",
      "Validation: Epoch [7], Batch [152/938], Loss: 0.6944735646247864\n",
      "Validation: Epoch [7], Batch [153/938], Loss: 0.8109943866729736\n",
      "Validation: Epoch [7], Batch [154/938], Loss: 0.8091386556625366\n",
      "Validation: Epoch [7], Batch [155/938], Loss: 1.0354300737380981\n",
      "Validation: Epoch [7], Batch [156/938], Loss: 0.7355093955993652\n",
      "Validation: Epoch [7], Batch [157/938], Loss: 0.8266027569770813\n",
      "Validation: Epoch [7], Batch [158/938], Loss: 1.1101784706115723\n",
      "Validation: Epoch [7], Batch [159/938], Loss: 0.7794421315193176\n",
      "Validation: Epoch [7], Batch [160/938], Loss: 0.9547311663627625\n",
      "Validation: Epoch [7], Batch [161/938], Loss: 0.8140282034873962\n",
      "Validation: Epoch [7], Batch [162/938], Loss: 1.0345641374588013\n",
      "Validation: Epoch [7], Batch [163/938], Loss: 0.978657603263855\n",
      "Validation: Epoch [7], Batch [164/938], Loss: 0.8990181684494019\n",
      "Validation: Epoch [7], Batch [165/938], Loss: 0.9364597201347351\n",
      "Validation: Epoch [7], Batch [166/938], Loss: 0.7240347862243652\n",
      "Validation: Epoch [7], Batch [167/938], Loss: 1.0311912298202515\n",
      "Validation: Epoch [7], Batch [168/938], Loss: 1.0196197032928467\n",
      "Validation: Epoch [7], Batch [169/938], Loss: 0.6910267472267151\n",
      "Validation: Epoch [7], Batch [170/938], Loss: 0.9855398535728455\n",
      "Validation: Epoch [7], Batch [171/938], Loss: 0.9250627756118774\n",
      "Validation: Epoch [7], Batch [172/938], Loss: 0.8238465785980225\n",
      "Validation: Epoch [7], Batch [173/938], Loss: 0.8359764218330383\n",
      "Validation: Epoch [7], Batch [174/938], Loss: 0.9628607630729675\n",
      "Validation: Epoch [7], Batch [175/938], Loss: 0.8541532754898071\n",
      "Validation: Epoch [7], Batch [176/938], Loss: 1.1401941776275635\n",
      "Validation: Epoch [7], Batch [177/938], Loss: 0.8335837125778198\n",
      "Validation: Epoch [7], Batch [178/938], Loss: 0.9676165580749512\n",
      "Validation: Epoch [7], Batch [179/938], Loss: 0.9384629726409912\n",
      "Validation: Epoch [7], Batch [180/938], Loss: 0.948923647403717\n",
      "Validation: Epoch [7], Batch [181/938], Loss: 0.9549288749694824\n",
      "Validation: Epoch [7], Batch [182/938], Loss: 0.7863510847091675\n",
      "Validation: Epoch [7], Batch [183/938], Loss: 0.8417165279388428\n",
      "Validation: Epoch [7], Batch [184/938], Loss: 0.9775669574737549\n",
      "Validation: Epoch [7], Batch [185/938], Loss: 0.7993523478507996\n",
      "Validation: Epoch [7], Batch [186/938], Loss: 0.86211097240448\n",
      "Validation: Epoch [7], Batch [187/938], Loss: 0.6478389501571655\n",
      "Validation: Epoch [7], Batch [188/938], Loss: 0.9759644269943237\n",
      "Validation: Epoch [7], Batch [189/938], Loss: 0.9744784832000732\n",
      "Validation: Epoch [7], Batch [190/938], Loss: 0.8501282334327698\n",
      "Validation: Epoch [7], Batch [191/938], Loss: 1.0413932800292969\n",
      "Validation: Epoch [7], Batch [192/938], Loss: 1.0127226114273071\n",
      "Validation: Epoch [7], Batch [193/938], Loss: 0.9321097135543823\n",
      "Validation: Epoch [7], Batch [194/938], Loss: 0.6759728193283081\n",
      "Validation: Epoch [7], Batch [195/938], Loss: 0.8707911968231201\n",
      "Validation: Epoch [7], Batch [196/938], Loss: 0.7920150756835938\n",
      "Validation: Epoch [7], Batch [197/938], Loss: 0.8870575428009033\n",
      "Validation: Epoch [7], Batch [198/938], Loss: 1.0129410028457642\n",
      "Validation: Epoch [7], Batch [199/938], Loss: 0.8835673332214355\n",
      "Validation: Epoch [7], Batch [200/938], Loss: 0.9200723171234131\n",
      "Validation: Epoch [7], Batch [201/938], Loss: 0.9823445081710815\n",
      "Validation: Epoch [7], Batch [202/938], Loss: 0.9108916521072388\n",
      "Validation: Epoch [7], Batch [203/938], Loss: 0.9023855924606323\n",
      "Validation: Epoch [7], Batch [204/938], Loss: 0.9548124074935913\n",
      "Validation: Epoch [7], Batch [205/938], Loss: 0.7423880100250244\n",
      "Validation: Epoch [7], Batch [206/938], Loss: 0.7072266936302185\n",
      "Validation: Epoch [7], Batch [207/938], Loss: 1.148484468460083\n",
      "Validation: Epoch [7], Batch [208/938], Loss: 0.9493323564529419\n",
      "Validation: Epoch [7], Batch [209/938], Loss: 0.7525147795677185\n",
      "Validation: Epoch [7], Batch [210/938], Loss: 0.8820906281471252\n",
      "Validation: Epoch [7], Batch [211/938], Loss: 0.7711371183395386\n",
      "Validation: Epoch [7], Batch [212/938], Loss: 0.934064507484436\n",
      "Validation: Epoch [7], Batch [213/938], Loss: 0.745185375213623\n",
      "Validation: Epoch [7], Batch [214/938], Loss: 0.9075728058815002\n",
      "Validation: Epoch [7], Batch [215/938], Loss: 0.8141914010047913\n",
      "Validation: Epoch [7], Batch [216/938], Loss: 1.0282692909240723\n",
      "Validation: Epoch [7], Batch [217/938], Loss: 0.8714935779571533\n",
      "Validation: Epoch [7], Batch [218/938], Loss: 1.02194082736969\n",
      "Validation: Epoch [7], Batch [219/938], Loss: 0.9520459175109863\n",
      "Validation: Epoch [7], Batch [220/938], Loss: 0.8117872476577759\n",
      "Validation: Epoch [7], Batch [221/938], Loss: 0.7843621969223022\n",
      "Validation: Epoch [7], Batch [222/938], Loss: 0.8120400905609131\n",
      "Validation: Epoch [7], Batch [223/938], Loss: 0.9765569567680359\n",
      "Validation: Epoch [7], Batch [224/938], Loss: 1.1571067571640015\n",
      "Validation: Epoch [7], Batch [225/938], Loss: 1.0863745212554932\n",
      "Validation: Epoch [7], Batch [226/938], Loss: 0.8314931988716125\n",
      "Validation: Epoch [7], Batch [227/938], Loss: 1.1316279172897339\n",
      "Validation: Epoch [7], Batch [228/938], Loss: 0.8446968793869019\n",
      "Validation: Epoch [7], Batch [229/938], Loss: 0.8976223468780518\n",
      "Validation: Epoch [7], Batch [230/938], Loss: 0.7623907327651978\n",
      "Validation: Epoch [7], Batch [231/938], Loss: 0.9752072095870972\n",
      "Validation: Epoch [7], Batch [232/938], Loss: 0.8933988213539124\n",
      "Validation: Epoch [7], Batch [233/938], Loss: 0.8828001022338867\n",
      "Validation: Epoch [7], Batch [234/938], Loss: 0.9144415259361267\n",
      "Validation: Epoch [7], Batch [235/938], Loss: 0.7045238614082336\n",
      "Validation: Epoch [7], Batch [236/938], Loss: 0.8602128028869629\n",
      "Validation: Epoch [7], Batch [237/938], Loss: 0.9575387835502625\n",
      "Validation: Epoch [7], Batch [238/938], Loss: 0.9862507581710815\n",
      "Validation: Epoch [7], Batch [239/938], Loss: 0.8827542066574097\n",
      "Validation: Epoch [7], Batch [240/938], Loss: 0.788406252861023\n",
      "Validation: Epoch [7], Batch [241/938], Loss: 0.9417126774787903\n",
      "Validation: Epoch [7], Batch [242/938], Loss: 0.9310005307197571\n",
      "Validation: Epoch [7], Batch [243/938], Loss: 1.0465137958526611\n",
      "Validation: Epoch [7], Batch [244/938], Loss: 0.9339814782142639\n",
      "Validation: Epoch [7], Batch [245/938], Loss: 0.8409020304679871\n",
      "Validation: Epoch [7], Batch [246/938], Loss: 0.8726049661636353\n",
      "Validation: Epoch [7], Batch [247/938], Loss: 0.773123025894165\n",
      "Validation: Epoch [7], Batch [248/938], Loss: 0.8603744506835938\n",
      "Validation: Epoch [7], Batch [249/938], Loss: 0.8712710738182068\n",
      "Validation: Epoch [7], Batch [250/938], Loss: 0.7423567771911621\n",
      "Validation: Epoch [7], Batch [251/938], Loss: 0.7244014739990234\n",
      "Validation: Epoch [7], Batch [252/938], Loss: 0.860317587852478\n",
      "Validation: Epoch [7], Batch [253/938], Loss: 0.7308981418609619\n",
      "Validation: Epoch [7], Batch [254/938], Loss: 0.9133643507957458\n",
      "Validation: Epoch [7], Batch [255/938], Loss: 0.9862146377563477\n",
      "Validation: Epoch [7], Batch [256/938], Loss: 0.5643538236618042\n",
      "Validation: Epoch [7], Batch [257/938], Loss: 1.127678632736206\n",
      "Validation: Epoch [7], Batch [258/938], Loss: 1.0471539497375488\n",
      "Validation: Epoch [7], Batch [259/938], Loss: 0.6934130787849426\n",
      "Validation: Epoch [7], Batch [260/938], Loss: 0.8079687356948853\n",
      "Validation: Epoch [7], Batch [261/938], Loss: 0.9301754832267761\n",
      "Validation: Epoch [7], Batch [262/938], Loss: 0.8497082591056824\n",
      "Validation: Epoch [7], Batch [263/938], Loss: 0.8449103832244873\n",
      "Validation: Epoch [7], Batch [264/938], Loss: 0.9718673229217529\n",
      "Validation: Epoch [7], Batch [265/938], Loss: 0.8964013457298279\n",
      "Validation: Epoch [7], Batch [266/938], Loss: 0.7674162983894348\n",
      "Validation: Epoch [7], Batch [267/938], Loss: 1.0799800157546997\n",
      "Validation: Epoch [7], Batch [268/938], Loss: 1.0012609958648682\n",
      "Validation: Epoch [7], Batch [269/938], Loss: 0.8202695846557617\n",
      "Validation: Epoch [7], Batch [270/938], Loss: 0.7570329308509827\n",
      "Validation: Epoch [7], Batch [271/938], Loss: 0.9177706241607666\n",
      "Validation: Epoch [7], Batch [272/938], Loss: 0.7408977746963501\n",
      "Validation: Epoch [7], Batch [273/938], Loss: 0.8962805271148682\n",
      "Validation: Epoch [7], Batch [274/938], Loss: 0.7879800200462341\n",
      "Validation: Epoch [7], Batch [275/938], Loss: 0.8400080800056458\n",
      "Validation: Epoch [7], Batch [276/938], Loss: 1.015662670135498\n",
      "Validation: Epoch [7], Batch [277/938], Loss: 0.9807674884796143\n",
      "Validation: Epoch [7], Batch [278/938], Loss: 0.8332945108413696\n",
      "Validation: Epoch [7], Batch [279/938], Loss: 0.873464822769165\n",
      "Validation: Epoch [7], Batch [280/938], Loss: 1.2368566989898682\n",
      "Validation: Epoch [7], Batch [281/938], Loss: 1.0124982595443726\n",
      "Validation: Epoch [7], Batch [282/938], Loss: 0.7119455337524414\n",
      "Validation: Epoch [7], Batch [283/938], Loss: 0.8917956352233887\n",
      "Validation: Epoch [7], Batch [284/938], Loss: 0.9322965741157532\n",
      "Validation: Epoch [7], Batch [285/938], Loss: 0.7858091592788696\n",
      "Validation: Epoch [7], Batch [286/938], Loss: 0.8780590891838074\n",
      "Validation: Epoch [7], Batch [287/938], Loss: 0.8719993233680725\n",
      "Validation: Epoch [7], Batch [288/938], Loss: 0.7181114554405212\n",
      "Validation: Epoch [7], Batch [289/938], Loss: 0.7135650515556335\n",
      "Validation: Epoch [7], Batch [290/938], Loss: 0.8706769347190857\n",
      "Validation: Epoch [7], Batch [291/938], Loss: 0.8090256452560425\n",
      "Validation: Epoch [7], Batch [292/938], Loss: 0.8015421628952026\n",
      "Validation: Epoch [7], Batch [293/938], Loss: 0.7954850792884827\n",
      "Validation: Epoch [7], Batch [294/938], Loss: 0.820335865020752\n",
      "Validation: Epoch [7], Batch [295/938], Loss: 0.9264140725135803\n",
      "Validation: Epoch [7], Batch [296/938], Loss: 1.0280977487564087\n",
      "Validation: Epoch [7], Batch [297/938], Loss: 0.8062387108802795\n",
      "Validation: Epoch [7], Batch [298/938], Loss: 0.888357400894165\n",
      "Validation: Epoch [7], Batch [299/938], Loss: 0.8109399676322937\n",
      "Validation: Epoch [7], Batch [300/938], Loss: 0.7806941866874695\n",
      "Validation: Epoch [7], Batch [301/938], Loss: 0.5674630999565125\n",
      "Validation: Epoch [7], Batch [302/938], Loss: 1.0153509378433228\n",
      "Validation: Epoch [7], Batch [303/938], Loss: 1.08128023147583\n",
      "Validation: Epoch [7], Batch [304/938], Loss: 0.875940203666687\n",
      "Validation: Epoch [7], Batch [305/938], Loss: 0.9037950038909912\n",
      "Validation: Epoch [7], Batch [306/938], Loss: 0.8175241947174072\n",
      "Validation: Epoch [7], Batch [307/938], Loss: 0.9001849293708801\n",
      "Validation: Epoch [7], Batch [308/938], Loss: 0.9678359031677246\n",
      "Validation: Epoch [7], Batch [309/938], Loss: 1.0058765411376953\n",
      "Validation: Epoch [7], Batch [310/938], Loss: 1.0613077878952026\n",
      "Validation: Epoch [7], Batch [311/938], Loss: 1.1559338569641113\n",
      "Validation: Epoch [7], Batch [312/938], Loss: 0.9923402070999146\n",
      "Validation: Epoch [7], Batch [313/938], Loss: 0.8770855665206909\n",
      "Validation: Epoch [7], Batch [314/938], Loss: 0.7858010530471802\n",
      "Validation: Epoch [7], Batch [315/938], Loss: 0.8863782286643982\n",
      "Validation: Epoch [7], Batch [316/938], Loss: 0.7343789935112\n",
      "Validation: Epoch [7], Batch [317/938], Loss: 0.9595947265625\n",
      "Validation: Epoch [7], Batch [318/938], Loss: 0.9273632764816284\n",
      "Validation: Epoch [7], Batch [319/938], Loss: 0.8885775208473206\n",
      "Validation: Epoch [7], Batch [320/938], Loss: 0.9188978672027588\n",
      "Validation: Epoch [7], Batch [321/938], Loss: 1.1005975008010864\n",
      "Validation: Epoch [7], Batch [322/938], Loss: 0.9663079380989075\n",
      "Validation: Epoch [7], Batch [323/938], Loss: 1.0441348552703857\n",
      "Validation: Epoch [7], Batch [324/938], Loss: 1.0859659910202026\n",
      "Validation: Epoch [7], Batch [325/938], Loss: 1.219966173171997\n",
      "Validation: Epoch [7], Batch [326/938], Loss: 0.8404774069786072\n",
      "Validation: Epoch [7], Batch [327/938], Loss: 0.9071661233901978\n",
      "Validation: Epoch [7], Batch [328/938], Loss: 0.9680383205413818\n",
      "Validation: Epoch [7], Batch [329/938], Loss: 1.019910216331482\n",
      "Validation: Epoch [7], Batch [330/938], Loss: 0.7923876047134399\n",
      "Validation: Epoch [7], Batch [331/938], Loss: 0.9222096800804138\n",
      "Validation: Epoch [7], Batch [332/938], Loss: 0.8167237043380737\n",
      "Validation: Epoch [7], Batch [333/938], Loss: 0.8219685554504395\n",
      "Validation: Epoch [7], Batch [334/938], Loss: 0.8731528520584106\n",
      "Validation: Epoch [7], Batch [335/938], Loss: 0.9702174663543701\n",
      "Validation: Epoch [7], Batch [336/938], Loss: 0.8426333665847778\n",
      "Validation: Epoch [7], Batch [337/938], Loss: 0.86178058385849\n",
      "Validation: Epoch [7], Batch [338/938], Loss: 0.9742223620414734\n",
      "Validation: Epoch [7], Batch [339/938], Loss: 1.2257083654403687\n",
      "Validation: Epoch [7], Batch [340/938], Loss: 0.9947547316551208\n",
      "Validation: Epoch [7], Batch [341/938], Loss: 0.6359824538230896\n",
      "Validation: Epoch [7], Batch [342/938], Loss: 0.9403610825538635\n",
      "Validation: Epoch [7], Batch [343/938], Loss: 1.0873112678527832\n",
      "Validation: Epoch [7], Batch [344/938], Loss: 0.7761103510856628\n",
      "Validation: Epoch [7], Batch [345/938], Loss: 0.9118568897247314\n",
      "Validation: Epoch [7], Batch [346/938], Loss: 0.902530312538147\n",
      "Validation: Epoch [7], Batch [347/938], Loss: 0.8697057366371155\n",
      "Validation: Epoch [7], Batch [348/938], Loss: 1.0995601415634155\n",
      "Validation: Epoch [7], Batch [349/938], Loss: 0.8925470113754272\n",
      "Validation: Epoch [7], Batch [350/938], Loss: 0.7982293367385864\n",
      "Validation: Epoch [7], Batch [351/938], Loss: 0.8518131971359253\n",
      "Validation: Epoch [7], Batch [352/938], Loss: 0.9328072667121887\n",
      "Validation: Epoch [7], Batch [353/938], Loss: 0.8000726103782654\n",
      "Validation: Epoch [7], Batch [354/938], Loss: 1.2192610502243042\n",
      "Validation: Epoch [7], Batch [355/938], Loss: 0.8494731187820435\n",
      "Validation: Epoch [7], Batch [356/938], Loss: 0.737680196762085\n",
      "Validation: Epoch [7], Batch [357/938], Loss: 0.9119610786437988\n",
      "Validation: Epoch [7], Batch [358/938], Loss: 0.661196231842041\n",
      "Validation: Epoch [7], Batch [359/938], Loss: 0.899185299873352\n",
      "Validation: Epoch [7], Batch [360/938], Loss: 1.0384330749511719\n",
      "Validation: Epoch [7], Batch [361/938], Loss: 1.1255247592926025\n",
      "Validation: Epoch [7], Batch [362/938], Loss: 0.7986788749694824\n",
      "Validation: Epoch [7], Batch [363/938], Loss: 0.9333087801933289\n",
      "Validation: Epoch [7], Batch [364/938], Loss: 0.9514613747596741\n",
      "Validation: Epoch [7], Batch [365/938], Loss: 0.9404851198196411\n",
      "Validation: Epoch [7], Batch [366/938], Loss: 0.7692583203315735\n",
      "Validation: Epoch [7], Batch [367/938], Loss: 0.913221538066864\n",
      "Validation: Epoch [7], Batch [368/938], Loss: 0.8935710191726685\n",
      "Validation: Epoch [7], Batch [369/938], Loss: 0.8614864945411682\n",
      "Validation: Epoch [7], Batch [370/938], Loss: 1.002731204032898\n",
      "Validation: Epoch [7], Batch [371/938], Loss: 0.6732276082038879\n",
      "Validation: Epoch [7], Batch [372/938], Loss: 1.0871649980545044\n",
      "Validation: Epoch [7], Batch [373/938], Loss: 0.8466352224349976\n",
      "Validation: Epoch [7], Batch [374/938], Loss: 0.7836817502975464\n",
      "Validation: Epoch [7], Batch [375/938], Loss: 0.8801965117454529\n",
      "Validation: Epoch [7], Batch [376/938], Loss: 0.9909919500350952\n",
      "Validation: Epoch [7], Batch [377/938], Loss: 1.0795927047729492\n",
      "Validation: Epoch [7], Batch [378/938], Loss: 0.7634795308113098\n",
      "Validation: Epoch [7], Batch [379/938], Loss: 0.8741883039474487\n",
      "Validation: Epoch [7], Batch [380/938], Loss: 0.8678293228149414\n",
      "Validation: Epoch [7], Batch [381/938], Loss: 0.8153111934661865\n",
      "Validation: Epoch [7], Batch [382/938], Loss: 0.9118090867996216\n",
      "Validation: Epoch [7], Batch [383/938], Loss: 0.8688986301422119\n",
      "Validation: Epoch [7], Batch [384/938], Loss: 0.9572371244430542\n",
      "Validation: Epoch [7], Batch [385/938], Loss: 0.8404371738433838\n",
      "Validation: Epoch [7], Batch [386/938], Loss: 0.760028600692749\n",
      "Validation: Epoch [7], Batch [387/938], Loss: 0.7512137293815613\n",
      "Validation: Epoch [7], Batch [388/938], Loss: 1.0504708290100098\n",
      "Validation: Epoch [7], Batch [389/938], Loss: 0.7396458387374878\n",
      "Validation: Epoch [7], Batch [390/938], Loss: 1.0399565696716309\n",
      "Validation: Epoch [7], Batch [391/938], Loss: 0.9337195158004761\n",
      "Validation: Epoch [7], Batch [392/938], Loss: 0.7532190084457397\n",
      "Validation: Epoch [7], Batch [393/938], Loss: 0.8663198351860046\n",
      "Validation: Epoch [7], Batch [394/938], Loss: 0.8129286766052246\n",
      "Validation: Epoch [7], Batch [395/938], Loss: 0.8852944374084473\n",
      "Validation: Epoch [7], Batch [396/938], Loss: 1.0124073028564453\n",
      "Validation: Epoch [7], Batch [397/938], Loss: 0.8163617849349976\n",
      "Validation: Epoch [7], Batch [398/938], Loss: 0.6925060153007507\n",
      "Validation: Epoch [7], Batch [399/938], Loss: 1.0522056818008423\n",
      "Validation: Epoch [7], Batch [400/938], Loss: 0.9815897941589355\n",
      "Validation: Epoch [7], Batch [401/938], Loss: 1.1950856447219849\n",
      "Validation: Epoch [7], Batch [402/938], Loss: 0.8171529769897461\n",
      "Validation: Epoch [7], Batch [403/938], Loss: 0.7635975480079651\n",
      "Validation: Epoch [7], Batch [404/938], Loss: 0.9363272190093994\n",
      "Validation: Epoch [7], Batch [405/938], Loss: 1.1329582929611206\n",
      "Validation: Epoch [7], Batch [406/938], Loss: 0.8580225110054016\n",
      "Validation: Epoch [7], Batch [407/938], Loss: 0.963171124458313\n",
      "Validation: Epoch [7], Batch [408/938], Loss: 0.8969919681549072\n",
      "Validation: Epoch [7], Batch [409/938], Loss: 1.2827949523925781\n",
      "Validation: Epoch [7], Batch [410/938], Loss: 0.9551592469215393\n",
      "Validation: Epoch [7], Batch [411/938], Loss: 0.996500551700592\n",
      "Validation: Epoch [7], Batch [412/938], Loss: 1.076305866241455\n",
      "Validation: Epoch [7], Batch [413/938], Loss: 0.8647229075431824\n",
      "Validation: Epoch [7], Batch [414/938], Loss: 0.9027379751205444\n",
      "Validation: Epoch [7], Batch [415/938], Loss: 0.7675044536590576\n",
      "Validation: Epoch [7], Batch [416/938], Loss: 0.920717716217041\n",
      "Validation: Epoch [7], Batch [417/938], Loss: 1.1941301822662354\n",
      "Validation: Epoch [7], Batch [418/938], Loss: 0.9400582313537598\n",
      "Validation: Epoch [7], Batch [419/938], Loss: 0.9600284695625305\n",
      "Validation: Epoch [7], Batch [420/938], Loss: 1.007479190826416\n",
      "Validation: Epoch [7], Batch [421/938], Loss: 1.0490596294403076\n",
      "Validation: Epoch [7], Batch [422/938], Loss: 0.7166256904602051\n",
      "Validation: Epoch [7], Batch [423/938], Loss: 0.8139732480049133\n",
      "Validation: Epoch [7], Batch [424/938], Loss: 0.9519121646881104\n",
      "Validation: Epoch [7], Batch [425/938], Loss: 1.0371341705322266\n",
      "Validation: Epoch [7], Batch [426/938], Loss: 0.8615539073944092\n",
      "Validation: Epoch [7], Batch [427/938], Loss: 0.822822093963623\n",
      "Validation: Epoch [7], Batch [428/938], Loss: 0.9068722724914551\n",
      "Validation: Epoch [7], Batch [429/938], Loss: 1.0195554494857788\n",
      "Validation: Epoch [7], Batch [430/938], Loss: 0.9510820508003235\n",
      "Validation: Epoch [7], Batch [431/938], Loss: 0.7895593643188477\n",
      "Validation: Epoch [7], Batch [432/938], Loss: 0.877442479133606\n",
      "Validation: Epoch [7], Batch [433/938], Loss: 0.7490718960762024\n",
      "Validation: Epoch [7], Batch [434/938], Loss: 0.9110684990882874\n",
      "Validation: Epoch [7], Batch [435/938], Loss: 1.1358113288879395\n",
      "Validation: Epoch [7], Batch [436/938], Loss: 0.748889684677124\n",
      "Validation: Epoch [7], Batch [437/938], Loss: 0.8337382674217224\n",
      "Validation: Epoch [7], Batch [438/938], Loss: 0.9285425543785095\n",
      "Validation: Epoch [7], Batch [439/938], Loss: 1.0375051498413086\n",
      "Validation: Epoch [7], Batch [440/938], Loss: 0.7376073598861694\n",
      "Validation: Epoch [7], Batch [441/938], Loss: 0.7230610847473145\n",
      "Validation: Epoch [7], Batch [442/938], Loss: 0.9217386245727539\n",
      "Validation: Epoch [7], Batch [443/938], Loss: 1.0158743858337402\n",
      "Validation: Epoch [7], Batch [444/938], Loss: 1.0710091590881348\n",
      "Validation: Epoch [7], Batch [445/938], Loss: 0.6701599359512329\n",
      "Validation: Epoch [7], Batch [446/938], Loss: 1.0948162078857422\n",
      "Validation: Epoch [7], Batch [447/938], Loss: 0.9965490102767944\n",
      "Validation: Epoch [7], Batch [448/938], Loss: 1.030745267868042\n",
      "Validation: Epoch [7], Batch [449/938], Loss: 0.7557483911514282\n",
      "Validation: Epoch [7], Batch [450/938], Loss: 0.9313163161277771\n",
      "Validation: Epoch [7], Batch [451/938], Loss: 0.8259322643280029\n",
      "Validation: Epoch [7], Batch [452/938], Loss: 0.7007659077644348\n",
      "Validation: Epoch [7], Batch [453/938], Loss: 0.7290006875991821\n",
      "Validation: Epoch [7], Batch [454/938], Loss: 0.9389591217041016\n",
      "Validation: Epoch [7], Batch [455/938], Loss: 0.7627081871032715\n",
      "Validation: Epoch [7], Batch [456/938], Loss: 0.941238522529602\n",
      "Validation: Epoch [7], Batch [457/938], Loss: 0.67960125207901\n",
      "Validation: Epoch [7], Batch [458/938], Loss: 0.8107967972755432\n",
      "Validation: Epoch [7], Batch [459/938], Loss: 0.9323590993881226\n",
      "Validation: Epoch [7], Batch [460/938], Loss: 0.6886119842529297\n",
      "Validation: Epoch [7], Batch [461/938], Loss: 0.8773289918899536\n",
      "Validation: Epoch [7], Batch [462/938], Loss: 0.9316903352737427\n",
      "Validation: Epoch [7], Batch [463/938], Loss: 0.9049183130264282\n",
      "Validation: Epoch [7], Batch [464/938], Loss: 0.96882164478302\n",
      "Validation: Epoch [7], Batch [465/938], Loss: 0.9678744673728943\n",
      "Validation: Epoch [7], Batch [466/938], Loss: 1.0877634286880493\n",
      "Validation: Epoch [7], Batch [467/938], Loss: 0.9622501730918884\n",
      "Validation: Epoch [7], Batch [468/938], Loss: 0.9420236349105835\n",
      "Validation: Epoch [7], Batch [469/938], Loss: 0.7736624479293823\n",
      "Validation: Epoch [7], Batch [470/938], Loss: 1.160351276397705\n",
      "Validation: Epoch [7], Batch [471/938], Loss: 1.032861590385437\n",
      "Validation: Epoch [7], Batch [472/938], Loss: 0.8539943695068359\n",
      "Validation: Epoch [7], Batch [473/938], Loss: 1.0938671827316284\n",
      "Validation: Epoch [7], Batch [474/938], Loss: 0.7964757084846497\n",
      "Validation: Epoch [7], Batch [475/938], Loss: 0.8600959777832031\n",
      "Validation: Epoch [7], Batch [476/938], Loss: 1.2111269235610962\n",
      "Validation: Epoch [7], Batch [477/938], Loss: 1.0214203596115112\n",
      "Validation: Epoch [7], Batch [478/938], Loss: 0.6937686204910278\n",
      "Validation: Epoch [7], Batch [479/938], Loss: 0.9357565641403198\n",
      "Validation: Epoch [7], Batch [480/938], Loss: 1.0449179410934448\n",
      "Validation: Epoch [7], Batch [481/938], Loss: 0.6652612090110779\n",
      "Validation: Epoch [7], Batch [482/938], Loss: 1.0164400339126587\n",
      "Validation: Epoch [7], Batch [483/938], Loss: 1.027113676071167\n",
      "Validation: Epoch [7], Batch [484/938], Loss: 0.8967925906181335\n",
      "Validation: Epoch [7], Batch [485/938], Loss: 0.9836612343788147\n",
      "Validation: Epoch [7], Batch [486/938], Loss: 0.9225043654441833\n",
      "Validation: Epoch [7], Batch [487/938], Loss: 0.8039531707763672\n",
      "Validation: Epoch [7], Batch [488/938], Loss: 0.8468321561813354\n",
      "Validation: Epoch [7], Batch [489/938], Loss: 0.8301633596420288\n",
      "Validation: Epoch [7], Batch [490/938], Loss: 0.802908718585968\n",
      "Validation: Epoch [7], Batch [491/938], Loss: 0.8788700699806213\n",
      "Validation: Epoch [7], Batch [492/938], Loss: 0.9135950207710266\n",
      "Validation: Epoch [7], Batch [493/938], Loss: 0.7703792452812195\n",
      "Validation: Epoch [7], Batch [494/938], Loss: 0.5813451409339905\n",
      "Validation: Epoch [7], Batch [495/938], Loss: 1.0015393495559692\n",
      "Validation: Epoch [7], Batch [496/938], Loss: 0.787103533744812\n",
      "Validation: Epoch [7], Batch [497/938], Loss: 0.7536009550094604\n",
      "Validation: Epoch [7], Batch [498/938], Loss: 1.0281383991241455\n",
      "Validation: Epoch [7], Batch [499/938], Loss: 0.7206357717514038\n",
      "Validation: Epoch [7], Batch [500/938], Loss: 1.0347596406936646\n",
      "Validation: Epoch [7], Batch [501/938], Loss: 0.9446318745613098\n",
      "Validation: Epoch [7], Batch [502/938], Loss: 1.125725507736206\n",
      "Validation: Epoch [7], Batch [503/938], Loss: 0.9438799619674683\n",
      "Validation: Epoch [7], Batch [504/938], Loss: 0.8828909993171692\n",
      "Validation: Epoch [7], Batch [505/938], Loss: 0.856027364730835\n",
      "Validation: Epoch [7], Batch [506/938], Loss: 0.7212235927581787\n",
      "Validation: Epoch [7], Batch [507/938], Loss: 0.877458393573761\n",
      "Validation: Epoch [7], Batch [508/938], Loss: 0.9471444487571716\n",
      "Validation: Epoch [7], Batch [509/938], Loss: 0.7494201064109802\n",
      "Validation: Epoch [7], Batch [510/938], Loss: 0.9320128560066223\n",
      "Validation: Epoch [7], Batch [511/938], Loss: 0.8834329843521118\n",
      "Validation: Epoch [7], Batch [512/938], Loss: 0.8549555540084839\n",
      "Validation: Epoch [7], Batch [513/938], Loss: 0.6609271168708801\n",
      "Validation: Epoch [7], Batch [514/938], Loss: 1.0141346454620361\n",
      "Validation: Epoch [7], Batch [515/938], Loss: 0.9877903461456299\n",
      "Validation: Epoch [7], Batch [516/938], Loss: 0.9424511790275574\n",
      "Validation: Epoch [7], Batch [517/938], Loss: 0.9159714579582214\n",
      "Validation: Epoch [7], Batch [518/938], Loss: 0.800308883190155\n",
      "Validation: Epoch [7], Batch [519/938], Loss: 1.0661147832870483\n",
      "Validation: Epoch [7], Batch [520/938], Loss: 0.9933216571807861\n",
      "Validation: Epoch [7], Batch [521/938], Loss: 1.071388840675354\n",
      "Validation: Epoch [7], Batch [522/938], Loss: 0.6455395221710205\n",
      "Validation: Epoch [7], Batch [523/938], Loss: 0.8540703654289246\n",
      "Validation: Epoch [7], Batch [524/938], Loss: 0.7785117626190186\n",
      "Validation: Epoch [7], Batch [525/938], Loss: 1.021549105644226\n",
      "Validation: Epoch [7], Batch [526/938], Loss: 0.8947155475616455\n",
      "Validation: Epoch [7], Batch [527/938], Loss: 0.8118491172790527\n",
      "Validation: Epoch [7], Batch [528/938], Loss: 0.9211532473564148\n",
      "Validation: Epoch [7], Batch [529/938], Loss: 0.8340264558792114\n",
      "Validation: Epoch [7], Batch [530/938], Loss: 0.8141141533851624\n",
      "Validation: Epoch [7], Batch [531/938], Loss: 0.7020024061203003\n",
      "Validation: Epoch [7], Batch [532/938], Loss: 0.9563903212547302\n",
      "Validation: Epoch [7], Batch [533/938], Loss: 0.9374332427978516\n",
      "Validation: Epoch [7], Batch [534/938], Loss: 0.8305896520614624\n",
      "Validation: Epoch [7], Batch [535/938], Loss: 0.6614378690719604\n",
      "Validation: Epoch [7], Batch [536/938], Loss: 0.6856374144554138\n",
      "Validation: Epoch [7], Batch [537/938], Loss: 0.9954317808151245\n",
      "Validation: Epoch [7], Batch [538/938], Loss: 0.7255688309669495\n",
      "Validation: Epoch [7], Batch [539/938], Loss: 0.7345908284187317\n",
      "Validation: Epoch [7], Batch [540/938], Loss: 0.7114678025245667\n",
      "Validation: Epoch [7], Batch [541/938], Loss: 0.8118993043899536\n",
      "Validation: Epoch [7], Batch [542/938], Loss: 0.8250320553779602\n",
      "Validation: Epoch [7], Batch [543/938], Loss: 0.9978745579719543\n",
      "Validation: Epoch [7], Batch [544/938], Loss: 0.7798219323158264\n",
      "Validation: Epoch [7], Batch [545/938], Loss: 0.8708010315895081\n",
      "Validation: Epoch [7], Batch [546/938], Loss: 1.0077227354049683\n",
      "Validation: Epoch [7], Batch [547/938], Loss: 1.103218674659729\n",
      "Validation: Epoch [7], Batch [548/938], Loss: 0.874162495136261\n",
      "Validation: Epoch [7], Batch [549/938], Loss: 0.7813110947608948\n",
      "Validation: Epoch [7], Batch [550/938], Loss: 1.001510500907898\n",
      "Validation: Epoch [7], Batch [551/938], Loss: 0.8871691226959229\n",
      "Validation: Epoch [7], Batch [552/938], Loss: 0.9186074733734131\n",
      "Validation: Epoch [7], Batch [553/938], Loss: 0.6742193698883057\n",
      "Validation: Epoch [7], Batch [554/938], Loss: 0.7277857661247253\n",
      "Validation: Epoch [7], Batch [555/938], Loss: 1.0797666311264038\n",
      "Validation: Epoch [7], Batch [556/938], Loss: 0.8059401512145996\n",
      "Validation: Epoch [7], Batch [557/938], Loss: 1.0098620653152466\n",
      "Validation: Epoch [7], Batch [558/938], Loss: 0.8103382587432861\n",
      "Validation: Epoch [7], Batch [559/938], Loss: 0.8986425399780273\n",
      "Validation: Epoch [7], Batch [560/938], Loss: 0.9276491403579712\n",
      "Validation: Epoch [7], Batch [561/938], Loss: 0.8708013892173767\n",
      "Validation: Epoch [7], Batch [562/938], Loss: 1.1232599020004272\n",
      "Validation: Epoch [7], Batch [563/938], Loss: 0.7521257400512695\n",
      "Validation: Epoch [7], Batch [564/938], Loss: 1.033789873123169\n",
      "Validation: Epoch [7], Batch [565/938], Loss: 0.8600910902023315\n",
      "Validation: Epoch [7], Batch [566/938], Loss: 0.7360771298408508\n",
      "Validation: Epoch [7], Batch [567/938], Loss: 0.9386144876480103\n",
      "Validation: Epoch [7], Batch [568/938], Loss: 0.9345365762710571\n",
      "Validation: Epoch [7], Batch [569/938], Loss: 0.7593697309494019\n",
      "Validation: Epoch [7], Batch [570/938], Loss: 0.9929373860359192\n",
      "Validation: Epoch [7], Batch [571/938], Loss: 0.8421809077262878\n",
      "Validation: Epoch [7], Batch [572/938], Loss: 0.977340817451477\n",
      "Validation: Epoch [7], Batch [573/938], Loss: 1.152105450630188\n",
      "Validation: Epoch [7], Batch [574/938], Loss: 0.7588343024253845\n",
      "Validation: Epoch [7], Batch [575/938], Loss: 0.9457236528396606\n",
      "Validation: Epoch [7], Batch [576/938], Loss: 0.8153048753738403\n",
      "Validation: Epoch [7], Batch [577/938], Loss: 0.7447432279586792\n",
      "Validation: Epoch [7], Batch [578/938], Loss: 0.707962691783905\n",
      "Validation: Epoch [7], Batch [579/938], Loss: 0.7793558239936829\n",
      "Validation: Epoch [7], Batch [580/938], Loss: 1.044050931930542\n",
      "Validation: Epoch [7], Batch [581/938], Loss: 0.8989197015762329\n",
      "Validation: Epoch [7], Batch [582/938], Loss: 0.7771080732345581\n",
      "Validation: Epoch [7], Batch [583/938], Loss: 0.8623329401016235\n",
      "Validation: Epoch [7], Batch [584/938], Loss: 0.8066403269767761\n",
      "Validation: Epoch [7], Batch [585/938], Loss: 0.7992304563522339\n",
      "Validation: Epoch [7], Batch [586/938], Loss: 0.6308450698852539\n",
      "Validation: Epoch [7], Batch [587/938], Loss: 1.1194593906402588\n",
      "Validation: Epoch [7], Batch [588/938], Loss: 0.8382661938667297\n",
      "Validation: Epoch [7], Batch [589/938], Loss: 1.0699646472930908\n",
      "Validation: Epoch [7], Batch [590/938], Loss: 0.9446758031845093\n",
      "Validation: Epoch [7], Batch [591/938], Loss: 0.7546300888061523\n",
      "Validation: Epoch [7], Batch [592/938], Loss: 0.7739449739456177\n",
      "Validation: Epoch [7], Batch [593/938], Loss: 0.8889237642288208\n",
      "Validation: Epoch [7], Batch [594/938], Loss: 0.7746586203575134\n",
      "Validation: Epoch [7], Batch [595/938], Loss: 0.9723068475723267\n",
      "Validation: Epoch [7], Batch [596/938], Loss: 1.0460925102233887\n",
      "Validation: Epoch [7], Batch [597/938], Loss: 0.9773417711257935\n",
      "Validation: Epoch [7], Batch [598/938], Loss: 0.8622851967811584\n",
      "Validation: Epoch [7], Batch [599/938], Loss: 0.9367274641990662\n",
      "Validation: Epoch [7], Batch [600/938], Loss: 0.9302453994750977\n",
      "Validation: Epoch [7], Batch [601/938], Loss: 1.0111523866653442\n",
      "Validation: Epoch [7], Batch [602/938], Loss: 0.76094651222229\n",
      "Validation: Epoch [7], Batch [603/938], Loss: 0.9090170860290527\n",
      "Validation: Epoch [7], Batch [604/938], Loss: 1.0949784517288208\n",
      "Validation: Epoch [7], Batch [605/938], Loss: 0.8882509469985962\n",
      "Validation: Epoch [7], Batch [606/938], Loss: 0.7293175458908081\n",
      "Validation: Epoch [7], Batch [607/938], Loss: 1.074556827545166\n",
      "Validation: Epoch [7], Batch [608/938], Loss: 0.9173727035522461\n",
      "Validation: Epoch [7], Batch [609/938], Loss: 0.7492074966430664\n",
      "Validation: Epoch [7], Batch [610/938], Loss: 0.9584939479827881\n",
      "Validation: Epoch [7], Batch [611/938], Loss: 0.7339525818824768\n",
      "Validation: Epoch [7], Batch [612/938], Loss: 0.8614696264266968\n",
      "Validation: Epoch [7], Batch [613/938], Loss: 0.7939294576644897\n",
      "Validation: Epoch [7], Batch [614/938], Loss: 0.8934257626533508\n",
      "Validation: Epoch [7], Batch [615/938], Loss: 0.8285548090934753\n",
      "Validation: Epoch [7], Batch [616/938], Loss: 0.8034182786941528\n",
      "Validation: Epoch [7], Batch [617/938], Loss: 0.9427135586738586\n",
      "Validation: Epoch [7], Batch [618/938], Loss: 0.8094435334205627\n",
      "Validation: Epoch [7], Batch [619/938], Loss: 0.991149365901947\n",
      "Validation: Epoch [7], Batch [620/938], Loss: 0.7496107220649719\n",
      "Validation: Epoch [7], Batch [621/938], Loss: 0.8281626105308533\n",
      "Validation: Epoch [7], Batch [622/938], Loss: 0.8116829991340637\n",
      "Validation: Epoch [7], Batch [623/938], Loss: 0.8990519642829895\n",
      "Validation: Epoch [7], Batch [624/938], Loss: 0.9116721153259277\n",
      "Validation: Epoch [7], Batch [625/938], Loss: 0.8926774859428406\n",
      "Validation: Epoch [7], Batch [626/938], Loss: 1.0289188623428345\n",
      "Validation: Epoch [7], Batch [627/938], Loss: 0.9858790636062622\n",
      "Validation: Epoch [7], Batch [628/938], Loss: 1.038210153579712\n",
      "Validation: Epoch [7], Batch [629/938], Loss: 0.7452353835105896\n",
      "Validation: Epoch [7], Batch [630/938], Loss: 0.5556864142417908\n",
      "Validation: Epoch [7], Batch [631/938], Loss: 0.8018233776092529\n",
      "Validation: Epoch [7], Batch [632/938], Loss: 0.97251296043396\n",
      "Validation: Epoch [7], Batch [633/938], Loss: 0.870770275592804\n",
      "Validation: Epoch [7], Batch [634/938], Loss: 0.6693927645683289\n",
      "Validation: Epoch [7], Batch [635/938], Loss: 0.7042572498321533\n",
      "Validation: Epoch [7], Batch [636/938], Loss: 0.7499681711196899\n",
      "Validation: Epoch [7], Batch [637/938], Loss: 0.8357530236244202\n",
      "Validation: Epoch [7], Batch [638/938], Loss: 0.8351228833198547\n",
      "Validation: Epoch [7], Batch [639/938], Loss: 0.9842325448989868\n",
      "Validation: Epoch [7], Batch [640/938], Loss: 1.0885347127914429\n",
      "Validation: Epoch [7], Batch [641/938], Loss: 0.9113896489143372\n",
      "Validation: Epoch [7], Batch [642/938], Loss: 0.8178814649581909\n",
      "Validation: Epoch [7], Batch [643/938], Loss: 0.9184350967407227\n",
      "Validation: Epoch [7], Batch [644/938], Loss: 0.8104665875434875\n",
      "Validation: Epoch [7], Batch [645/938], Loss: 0.7828975915908813\n",
      "Validation: Epoch [7], Batch [646/938], Loss: 0.7866149544715881\n",
      "Validation: Epoch [7], Batch [647/938], Loss: 0.7584531903266907\n",
      "Validation: Epoch [7], Batch [648/938], Loss: 0.8796282410621643\n",
      "Validation: Epoch [7], Batch [649/938], Loss: 0.7245439291000366\n",
      "Validation: Epoch [7], Batch [650/938], Loss: 0.6680140495300293\n",
      "Validation: Epoch [7], Batch [651/938], Loss: 0.7806962132453918\n",
      "Validation: Epoch [7], Batch [652/938], Loss: 0.9148480892181396\n",
      "Validation: Epoch [7], Batch [653/938], Loss: 0.7908387184143066\n",
      "Validation: Epoch [7], Batch [654/938], Loss: 0.970810055732727\n",
      "Validation: Epoch [7], Batch [655/938], Loss: 0.7659204006195068\n",
      "Validation: Epoch [7], Batch [656/938], Loss: 1.031607747077942\n",
      "Validation: Epoch [7], Batch [657/938], Loss: 1.1120517253875732\n",
      "Validation: Epoch [7], Batch [658/938], Loss: 1.0383301973342896\n",
      "Validation: Epoch [7], Batch [659/938], Loss: 0.7495853900909424\n",
      "Validation: Epoch [7], Batch [660/938], Loss: 0.940735936164856\n",
      "Validation: Epoch [7], Batch [661/938], Loss: 1.0265580415725708\n",
      "Validation: Epoch [7], Batch [662/938], Loss: 0.8787559270858765\n",
      "Validation: Epoch [7], Batch [663/938], Loss: 0.8737754821777344\n",
      "Validation: Epoch [7], Batch [664/938], Loss: 0.8627448678016663\n",
      "Validation: Epoch [7], Batch [665/938], Loss: 0.9257256388664246\n",
      "Validation: Epoch [7], Batch [666/938], Loss: 0.8819845914840698\n",
      "Validation: Epoch [7], Batch [667/938], Loss: 1.1240495443344116\n",
      "Validation: Epoch [7], Batch [668/938], Loss: 1.0733267068862915\n",
      "Validation: Epoch [7], Batch [669/938], Loss: 0.7855728268623352\n",
      "Validation: Epoch [7], Batch [670/938], Loss: 1.1344106197357178\n",
      "Validation: Epoch [7], Batch [671/938], Loss: 0.774644136428833\n",
      "Validation: Epoch [7], Batch [672/938], Loss: 0.5997635126113892\n",
      "Validation: Epoch [7], Batch [673/938], Loss: 1.1127643585205078\n",
      "Validation: Epoch [7], Batch [674/938], Loss: 1.030764102935791\n",
      "Validation: Epoch [7], Batch [675/938], Loss: 0.9091208577156067\n",
      "Validation: Epoch [7], Batch [676/938], Loss: 0.989020586013794\n",
      "Validation: Epoch [7], Batch [677/938], Loss: 0.9436085820198059\n",
      "Validation: Epoch [7], Batch [678/938], Loss: 0.9769101738929749\n",
      "Validation: Epoch [7], Batch [679/938], Loss: 0.7094554305076599\n",
      "Validation: Epoch [7], Batch [680/938], Loss: 0.8360959887504578\n",
      "Validation: Epoch [7], Batch [681/938], Loss: 0.9774031639099121\n",
      "Validation: Epoch [7], Batch [682/938], Loss: 0.8997699618339539\n",
      "Validation: Epoch [7], Batch [683/938], Loss: 1.0515587329864502\n",
      "Validation: Epoch [7], Batch [684/938], Loss: 1.001960277557373\n",
      "Validation: Epoch [7], Batch [685/938], Loss: 0.7845157980918884\n",
      "Validation: Epoch [7], Batch [686/938], Loss: 1.0670746564865112\n",
      "Validation: Epoch [7], Batch [687/938], Loss: 0.9743645191192627\n",
      "Validation: Epoch [7], Batch [688/938], Loss: 0.8831424713134766\n",
      "Validation: Epoch [7], Batch [689/938], Loss: 0.7433894872665405\n",
      "Validation: Epoch [7], Batch [690/938], Loss: 0.8831484913825989\n",
      "Validation: Epoch [7], Batch [691/938], Loss: 0.9888714551925659\n",
      "Validation: Epoch [7], Batch [692/938], Loss: 0.7208057045936584\n",
      "Validation: Epoch [7], Batch [693/938], Loss: 0.9308496117591858\n",
      "Validation: Epoch [7], Batch [694/938], Loss: 0.9263015389442444\n",
      "Validation: Epoch [7], Batch [695/938], Loss: 0.8470697999000549\n",
      "Validation: Epoch [7], Batch [696/938], Loss: 0.9491987824440002\n",
      "Validation: Epoch [7], Batch [697/938], Loss: 1.2136633396148682\n",
      "Validation: Epoch [7], Batch [698/938], Loss: 0.876914381980896\n",
      "Validation: Epoch [7], Batch [699/938], Loss: 0.862167477607727\n",
      "Validation: Epoch [7], Batch [700/938], Loss: 0.8139654397964478\n",
      "Validation: Epoch [7], Batch [701/938], Loss: 1.0561832189559937\n",
      "Validation: Epoch [7], Batch [702/938], Loss: 1.0674307346343994\n",
      "Validation: Epoch [7], Batch [703/938], Loss: 0.9784287810325623\n",
      "Validation: Epoch [7], Batch [704/938], Loss: 1.0405043363571167\n",
      "Validation: Epoch [7], Batch [705/938], Loss: 0.8928557634353638\n",
      "Validation: Epoch [7], Batch [706/938], Loss: 0.8868244290351868\n",
      "Validation: Epoch [7], Batch [707/938], Loss: 1.0166337490081787\n",
      "Validation: Epoch [7], Batch [708/938], Loss: 0.7453793883323669\n",
      "Validation: Epoch [7], Batch [709/938], Loss: 0.8541804552078247\n",
      "Validation: Epoch [7], Batch [710/938], Loss: 0.698176383972168\n",
      "Validation: Epoch [7], Batch [711/938], Loss: 0.8654468655586243\n",
      "Validation: Epoch [7], Batch [712/938], Loss: 0.8044440746307373\n",
      "Validation: Epoch [7], Batch [713/938], Loss: 0.6925593614578247\n",
      "Validation: Epoch [7], Batch [714/938], Loss: 0.8819023370742798\n",
      "Validation: Epoch [7], Batch [715/938], Loss: 0.8779768943786621\n",
      "Validation: Epoch [7], Batch [716/938], Loss: 1.008662223815918\n",
      "Validation: Epoch [7], Batch [717/938], Loss: 0.728135883808136\n",
      "Validation: Epoch [7], Batch [718/938], Loss: 0.7217007279396057\n",
      "Validation: Epoch [7], Batch [719/938], Loss: 0.8046607971191406\n",
      "Validation: Epoch [7], Batch [720/938], Loss: 1.0172921419143677\n",
      "Validation: Epoch [7], Batch [721/938], Loss: 0.929241955280304\n",
      "Validation: Epoch [7], Batch [722/938], Loss: 0.9968807101249695\n",
      "Validation: Epoch [7], Batch [723/938], Loss: 0.8816787600517273\n",
      "Validation: Epoch [7], Batch [724/938], Loss: 0.9928118586540222\n",
      "Validation: Epoch [7], Batch [725/938], Loss: 0.9480869770050049\n",
      "Validation: Epoch [7], Batch [726/938], Loss: 0.6764066815376282\n",
      "Validation: Epoch [7], Batch [727/938], Loss: 0.7180044651031494\n",
      "Validation: Epoch [7], Batch [728/938], Loss: 0.7935463786125183\n",
      "Validation: Epoch [7], Batch [729/938], Loss: 0.8169131278991699\n",
      "Validation: Epoch [7], Batch [730/938], Loss: 0.684607744216919\n",
      "Validation: Epoch [7], Batch [731/938], Loss: 0.8806542754173279\n",
      "Validation: Epoch [7], Batch [732/938], Loss: 0.9805526733398438\n",
      "Validation: Epoch [7], Batch [733/938], Loss: 0.7922568917274475\n",
      "Validation: Epoch [7], Batch [734/938], Loss: 1.029735803604126\n",
      "Validation: Epoch [7], Batch [735/938], Loss: 0.8507769703865051\n",
      "Validation: Epoch [7], Batch [736/938], Loss: 0.8421268463134766\n",
      "Validation: Epoch [7], Batch [737/938], Loss: 0.9100871682167053\n",
      "Validation: Epoch [7], Batch [738/938], Loss: 0.8751171231269836\n",
      "Validation: Epoch [7], Batch [739/938], Loss: 0.9783507585525513\n",
      "Validation: Epoch [7], Batch [740/938], Loss: 1.0262430906295776\n",
      "Validation: Epoch [7], Batch [741/938], Loss: 0.961672306060791\n",
      "Validation: Epoch [7], Batch [742/938], Loss: 0.8528953194618225\n",
      "Validation: Epoch [7], Batch [743/938], Loss: 0.9690436124801636\n",
      "Validation: Epoch [7], Batch [744/938], Loss: 0.7148501873016357\n",
      "Validation: Epoch [7], Batch [745/938], Loss: 0.6305304169654846\n",
      "Validation: Epoch [7], Batch [746/938], Loss: 0.6770495176315308\n",
      "Validation: Epoch [7], Batch [747/938], Loss: 0.7971115112304688\n",
      "Validation: Epoch [7], Batch [748/938], Loss: 0.9902929663658142\n",
      "Validation: Epoch [7], Batch [749/938], Loss: 0.8155909180641174\n",
      "Validation: Epoch [7], Batch [750/938], Loss: 0.6995773911476135\n",
      "Validation: Epoch [7], Batch [751/938], Loss: 0.9029249548912048\n",
      "Validation: Epoch [7], Batch [752/938], Loss: 0.9296677112579346\n",
      "Validation: Epoch [7], Batch [753/938], Loss: 0.8125763535499573\n",
      "Validation: Epoch [7], Batch [754/938], Loss: 1.0410031080245972\n",
      "Validation: Epoch [7], Batch [755/938], Loss: 1.0251425504684448\n",
      "Validation: Epoch [7], Batch [756/938], Loss: 0.8005285263061523\n",
      "Validation: Epoch [7], Batch [757/938], Loss: 0.9340957999229431\n",
      "Validation: Epoch [7], Batch [758/938], Loss: 0.8844834566116333\n",
      "Validation: Epoch [7], Batch [759/938], Loss: 1.0387128591537476\n",
      "Validation: Epoch [7], Batch [760/938], Loss: 0.8727909326553345\n",
      "Validation: Epoch [7], Batch [761/938], Loss: 0.8513087630271912\n",
      "Validation: Epoch [7], Batch [762/938], Loss: 0.860039234161377\n",
      "Validation: Epoch [7], Batch [763/938], Loss: 0.9981133937835693\n",
      "Validation: Epoch [7], Batch [764/938], Loss: 0.9857563972473145\n",
      "Validation: Epoch [7], Batch [765/938], Loss: 0.9316899180412292\n",
      "Validation: Epoch [7], Batch [766/938], Loss: 0.904130756855011\n",
      "Validation: Epoch [7], Batch [767/938], Loss: 0.8109254240989685\n",
      "Validation: Epoch [7], Batch [768/938], Loss: 0.8055756092071533\n",
      "Validation: Epoch [7], Batch [769/938], Loss: 0.9659074544906616\n",
      "Validation: Epoch [7], Batch [770/938], Loss: 0.8872005939483643\n",
      "Validation: Epoch [7], Batch [771/938], Loss: 0.9868292212486267\n",
      "Validation: Epoch [7], Batch [772/938], Loss: 0.8288580775260925\n",
      "Validation: Epoch [7], Batch [773/938], Loss: 0.9996018409729004\n",
      "Validation: Epoch [7], Batch [774/938], Loss: 0.8219989538192749\n",
      "Validation: Epoch [7], Batch [775/938], Loss: 0.6474054455757141\n",
      "Validation: Epoch [7], Batch [776/938], Loss: 1.0940141677856445\n",
      "Validation: Epoch [7], Batch [777/938], Loss: 0.9169291853904724\n",
      "Validation: Epoch [7], Batch [778/938], Loss: 0.7145841717720032\n",
      "Validation: Epoch [7], Batch [779/938], Loss: 1.0292643308639526\n",
      "Validation: Epoch [7], Batch [780/938], Loss: 0.8073461055755615\n",
      "Validation: Epoch [7], Batch [781/938], Loss: 0.760684609413147\n",
      "Validation: Epoch [7], Batch [782/938], Loss: 0.8295647501945496\n",
      "Validation: Epoch [7], Batch [783/938], Loss: 0.776056706905365\n",
      "Validation: Epoch [7], Batch [784/938], Loss: 0.8559865951538086\n",
      "Validation: Epoch [7], Batch [785/938], Loss: 0.7456814050674438\n",
      "Validation: Epoch [7], Batch [786/938], Loss: 1.214732050895691\n",
      "Validation: Epoch [7], Batch [787/938], Loss: 0.9009846448898315\n",
      "Validation: Epoch [7], Batch [788/938], Loss: 0.8801106214523315\n",
      "Validation: Epoch [7], Batch [789/938], Loss: 0.7244572043418884\n",
      "Validation: Epoch [7], Batch [790/938], Loss: 0.9099843502044678\n",
      "Validation: Epoch [7], Batch [791/938], Loss: 0.7179244756698608\n",
      "Validation: Epoch [7], Batch [792/938], Loss: 0.7992643117904663\n",
      "Validation: Epoch [7], Batch [793/938], Loss: 0.7768033146858215\n",
      "Validation: Epoch [7], Batch [794/938], Loss: 0.8184448480606079\n",
      "Validation: Epoch [7], Batch [795/938], Loss: 0.8951649069786072\n",
      "Validation: Epoch [7], Batch [796/938], Loss: 0.8070257306098938\n",
      "Validation: Epoch [7], Batch [797/938], Loss: 0.964402973651886\n",
      "Validation: Epoch [7], Batch [798/938], Loss: 0.8206515312194824\n",
      "Validation: Epoch [7], Batch [799/938], Loss: 0.6130810976028442\n",
      "Validation: Epoch [7], Batch [800/938], Loss: 0.8679799437522888\n",
      "Validation: Epoch [7], Batch [801/938], Loss: 1.0838067531585693\n",
      "Validation: Epoch [7], Batch [802/938], Loss: 1.0231086015701294\n",
      "Validation: Epoch [7], Batch [803/938], Loss: 0.8393764495849609\n",
      "Validation: Epoch [7], Batch [804/938], Loss: 0.6937296390533447\n",
      "Validation: Epoch [7], Batch [805/938], Loss: 0.9215482473373413\n",
      "Validation: Epoch [7], Batch [806/938], Loss: 1.0719623565673828\n",
      "Validation: Epoch [7], Batch [807/938], Loss: 1.209585428237915\n",
      "Validation: Epoch [7], Batch [808/938], Loss: 0.8338008522987366\n",
      "Validation: Epoch [7], Batch [809/938], Loss: 1.2862906455993652\n",
      "Validation: Epoch [7], Batch [810/938], Loss: 0.8843883275985718\n",
      "Validation: Epoch [7], Batch [811/938], Loss: 0.9005603194236755\n",
      "Validation: Epoch [7], Batch [812/938], Loss: 0.8124618530273438\n",
      "Validation: Epoch [7], Batch [813/938], Loss: 1.1184260845184326\n",
      "Validation: Epoch [7], Batch [814/938], Loss: 0.8802427053451538\n",
      "Validation: Epoch [7], Batch [815/938], Loss: 0.8218368887901306\n",
      "Validation: Epoch [7], Batch [816/938], Loss: 0.9164212942123413\n",
      "Validation: Epoch [7], Batch [817/938], Loss: 0.9855679869651794\n",
      "Validation: Epoch [7], Batch [818/938], Loss: 0.867333173751831\n",
      "Validation: Epoch [7], Batch [819/938], Loss: 0.8593381643295288\n",
      "Validation: Epoch [7], Batch [820/938], Loss: 0.8060839176177979\n",
      "Validation: Epoch [7], Batch [821/938], Loss: 0.930199921131134\n",
      "Validation: Epoch [7], Batch [822/938], Loss: 0.9788604378700256\n",
      "Validation: Epoch [7], Batch [823/938], Loss: 0.9040048122406006\n",
      "Validation: Epoch [7], Batch [824/938], Loss: 0.878430962562561\n",
      "Validation: Epoch [7], Batch [825/938], Loss: 0.8073650598526001\n",
      "Validation: Epoch [7], Batch [826/938], Loss: 0.8246451616287231\n",
      "Validation: Epoch [7], Batch [827/938], Loss: 0.9007152915000916\n",
      "Validation: Epoch [7], Batch [828/938], Loss: 0.8509364724159241\n",
      "Validation: Epoch [7], Batch [829/938], Loss: 0.898734450340271\n",
      "Validation: Epoch [7], Batch [830/938], Loss: 0.8689411878585815\n",
      "Validation: Epoch [7], Batch [831/938], Loss: 0.858887791633606\n",
      "Validation: Epoch [7], Batch [832/938], Loss: 0.79936283826828\n",
      "Validation: Epoch [7], Batch [833/938], Loss: 0.8929561376571655\n",
      "Validation: Epoch [7], Batch [834/938], Loss: 1.073722243309021\n",
      "Validation: Epoch [7], Batch [835/938], Loss: 0.8798789381980896\n",
      "Validation: Epoch [7], Batch [836/938], Loss: 0.7857183218002319\n",
      "Validation: Epoch [7], Batch [837/938], Loss: 1.0416250228881836\n",
      "Validation: Epoch [7], Batch [838/938], Loss: 0.8750404715538025\n",
      "Validation: Epoch [7], Batch [839/938], Loss: 0.8754057288169861\n",
      "Validation: Epoch [7], Batch [840/938], Loss: 0.9048095941543579\n",
      "Validation: Epoch [7], Batch [841/938], Loss: 0.8860915899276733\n",
      "Validation: Epoch [7], Batch [842/938], Loss: 1.1016544103622437\n",
      "Validation: Epoch [7], Batch [843/938], Loss: 0.8795319199562073\n",
      "Validation: Epoch [7], Batch [844/938], Loss: 0.7727193832397461\n",
      "Validation: Epoch [7], Batch [845/938], Loss: 0.7763473391532898\n",
      "Validation: Epoch [7], Batch [846/938], Loss: 0.8384681344032288\n",
      "Validation: Epoch [7], Batch [847/938], Loss: 0.8660938739776611\n",
      "Validation: Epoch [7], Batch [848/938], Loss: 0.7338269948959351\n",
      "Validation: Epoch [7], Batch [849/938], Loss: 0.606142520904541\n",
      "Validation: Epoch [7], Batch [850/938], Loss: 0.8061587810516357\n",
      "Validation: Epoch [7], Batch [851/938], Loss: 0.9921241402626038\n",
      "Validation: Epoch [7], Batch [852/938], Loss: 0.9759930372238159\n",
      "Validation: Epoch [7], Batch [853/938], Loss: 0.7260594367980957\n",
      "Validation: Epoch [7], Batch [854/938], Loss: 0.9335692524909973\n",
      "Validation: Epoch [7], Batch [855/938], Loss: 0.9318593740463257\n",
      "Validation: Epoch [7], Batch [856/938], Loss: 0.8590220212936401\n",
      "Validation: Epoch [7], Batch [857/938], Loss: 1.1723326444625854\n",
      "Validation: Epoch [7], Batch [858/938], Loss: 0.869901716709137\n",
      "Validation: Epoch [7], Batch [859/938], Loss: 0.8798673152923584\n",
      "Validation: Epoch [7], Batch [860/938], Loss: 0.8454499244689941\n",
      "Validation: Epoch [7], Batch [861/938], Loss: 0.7865984439849854\n",
      "Validation: Epoch [7], Batch [862/938], Loss: 0.904680073261261\n",
      "Validation: Epoch [7], Batch [863/938], Loss: 0.8855292797088623\n",
      "Validation: Epoch [7], Batch [864/938], Loss: 0.9603428840637207\n",
      "Validation: Epoch [7], Batch [865/938], Loss: 0.9210494160652161\n",
      "Validation: Epoch [7], Batch [866/938], Loss: 0.7844439744949341\n",
      "Validation: Epoch [7], Batch [867/938], Loss: 0.8637319207191467\n",
      "Validation: Epoch [7], Batch [868/938], Loss: 0.9817800521850586\n",
      "Validation: Epoch [7], Batch [869/938], Loss: 0.9776200652122498\n",
      "Validation: Epoch [7], Batch [870/938], Loss: 0.8543175458908081\n",
      "Validation: Epoch [7], Batch [871/938], Loss: 0.7914760112762451\n",
      "Validation: Epoch [7], Batch [872/938], Loss: 0.8657904267311096\n",
      "Validation: Epoch [7], Batch [873/938], Loss: 0.8514599204063416\n",
      "Validation: Epoch [7], Batch [874/938], Loss: 0.8704907894134521\n",
      "Validation: Epoch [7], Batch [875/938], Loss: 0.7819960713386536\n",
      "Validation: Epoch [7], Batch [876/938], Loss: 0.7862378358840942\n",
      "Validation: Epoch [7], Batch [877/938], Loss: 0.8381985425949097\n",
      "Validation: Epoch [7], Batch [878/938], Loss: 1.0998375415802002\n",
      "Validation: Epoch [7], Batch [879/938], Loss: 0.9441463351249695\n",
      "Validation: Epoch [7], Batch [880/938], Loss: 0.748470664024353\n",
      "Validation: Epoch [7], Batch [881/938], Loss: 1.0552027225494385\n",
      "Validation: Epoch [7], Batch [882/938], Loss: 1.2084460258483887\n",
      "Validation: Epoch [7], Batch [883/938], Loss: 1.0045102834701538\n",
      "Validation: Epoch [7], Batch [884/938], Loss: 0.7187691926956177\n",
      "Validation: Epoch [7], Batch [885/938], Loss: 0.7689056992530823\n",
      "Validation: Epoch [7], Batch [886/938], Loss: 0.891582727432251\n",
      "Validation: Epoch [7], Batch [887/938], Loss: 1.0169562101364136\n",
      "Validation: Epoch [7], Batch [888/938], Loss: 0.7814745306968689\n",
      "Validation: Epoch [7], Batch [889/938], Loss: 0.8304463624954224\n",
      "Validation: Epoch [7], Batch [890/938], Loss: 0.9799476861953735\n",
      "Validation: Epoch [7], Batch [891/938], Loss: 0.7857800126075745\n",
      "Validation: Epoch [7], Batch [892/938], Loss: 0.7922661900520325\n",
      "Validation: Epoch [7], Batch [893/938], Loss: 0.8540000319480896\n",
      "Validation: Epoch [7], Batch [894/938], Loss: 0.9809274673461914\n",
      "Validation: Epoch [7], Batch [895/938], Loss: 0.7674338817596436\n",
      "Validation: Epoch [7], Batch [896/938], Loss: 0.9774501323699951\n",
      "Validation: Epoch [7], Batch [897/938], Loss: 0.971442461013794\n",
      "Validation: Epoch [7], Batch [898/938], Loss: 0.7874995470046997\n",
      "Validation: Epoch [7], Batch [899/938], Loss: 0.6904358863830566\n",
      "Validation: Epoch [7], Batch [900/938], Loss: 0.8301003575325012\n",
      "Validation: Epoch [7], Batch [901/938], Loss: 0.8017251491546631\n",
      "Validation: Epoch [7], Batch [902/938], Loss: 0.945803701877594\n",
      "Validation: Epoch [7], Batch [903/938], Loss: 0.7117898464202881\n",
      "Validation: Epoch [7], Batch [904/938], Loss: 0.9322558045387268\n",
      "Validation: Epoch [7], Batch [905/938], Loss: 0.7898317575454712\n",
      "Validation: Epoch [7], Batch [906/938], Loss: 0.9059895277023315\n",
      "Validation: Epoch [7], Batch [907/938], Loss: 0.7906484603881836\n",
      "Validation: Epoch [7], Batch [908/938], Loss: 0.7472342252731323\n",
      "Validation: Epoch [7], Batch [909/938], Loss: 0.8518750667572021\n",
      "Validation: Epoch [7], Batch [910/938], Loss: 0.9846357703208923\n",
      "Validation: Epoch [7], Batch [911/938], Loss: 0.9667792916297913\n",
      "Validation: Epoch [7], Batch [912/938], Loss: 0.7099172472953796\n",
      "Validation: Epoch [7], Batch [913/938], Loss: 0.9359089136123657\n",
      "Validation: Epoch [7], Batch [914/938], Loss: 0.8845778703689575\n",
      "Validation: Epoch [7], Batch [915/938], Loss: 0.7207955121994019\n",
      "Validation: Epoch [7], Batch [916/938], Loss: 0.9863117933273315\n",
      "Validation: Epoch [7], Batch [917/938], Loss: 0.849890947341919\n",
      "Validation: Epoch [7], Batch [918/938], Loss: 1.0501391887664795\n",
      "Validation: Epoch [7], Batch [919/938], Loss: 0.7129459381103516\n",
      "Validation: Epoch [7], Batch [920/938], Loss: 0.9443125128746033\n",
      "Validation: Epoch [7], Batch [921/938], Loss: 0.8771409392356873\n",
      "Validation: Epoch [7], Batch [922/938], Loss: 0.8225007653236389\n",
      "Validation: Epoch [7], Batch [923/938], Loss: 0.861373245716095\n",
      "Validation: Epoch [7], Batch [924/938], Loss: 0.7650238275527954\n",
      "Validation: Epoch [7], Batch [925/938], Loss: 0.7074917554855347\n",
      "Validation: Epoch [7], Batch [926/938], Loss: 0.8403139114379883\n",
      "Validation: Epoch [7], Batch [927/938], Loss: 0.7305143475532532\n",
      "Validation: Epoch [7], Batch [928/938], Loss: 0.9480388164520264\n",
      "Validation: Epoch [7], Batch [929/938], Loss: 0.8371942043304443\n",
      "Validation: Epoch [7], Batch [930/938], Loss: 1.0785083770751953\n",
      "Validation: Epoch [7], Batch [931/938], Loss: 1.0460445880889893\n",
      "Validation: Epoch [7], Batch [932/938], Loss: 0.8910214900970459\n",
      "Validation: Epoch [7], Batch [933/938], Loss: 0.8195273876190186\n",
      "Validation: Epoch [7], Batch [934/938], Loss: 0.7548081874847412\n",
      "Validation: Epoch [7], Batch [935/938], Loss: 0.8297218084335327\n",
      "Validation: Epoch [7], Batch [936/938], Loss: 0.7174544334411621\n",
      "Validation: Epoch [7], Batch [937/938], Loss: 0.9108877182006836\n",
      "Validation: Epoch [7], Batch [938/938], Loss: 1.196304440498352\n",
      "Accuracy of test set: 0.6943666666666667\n",
      "Train: Epoch [8], Batch [1/938], Loss: 0.7431710958480835\n",
      "Train: Epoch [8], Batch [2/938], Loss: 0.7671633958816528\n",
      "Train: Epoch [8], Batch [3/938], Loss: 0.8067885041236877\n",
      "Train: Epoch [8], Batch [4/938], Loss: 0.90487140417099\n",
      "Train: Epoch [8], Batch [5/938], Loss: 0.863600492477417\n",
      "Train: Epoch [8], Batch [6/938], Loss: 0.9414041042327881\n",
      "Train: Epoch [8], Batch [7/938], Loss: 0.5987661480903625\n",
      "Train: Epoch [8], Batch [8/938], Loss: 0.9639462828636169\n",
      "Train: Epoch [8], Batch [9/938], Loss: 0.8128385543823242\n",
      "Train: Epoch [8], Batch [10/938], Loss: 0.937997579574585\n",
      "Train: Epoch [8], Batch [11/938], Loss: 0.7751950025558472\n",
      "Train: Epoch [8], Batch [12/938], Loss: 0.9218294024467468\n",
      "Train: Epoch [8], Batch [13/938], Loss: 0.970363438129425\n",
      "Train: Epoch [8], Batch [14/938], Loss: 1.0593680143356323\n",
      "Train: Epoch [8], Batch [15/938], Loss: 1.0016950368881226\n",
      "Train: Epoch [8], Batch [16/938], Loss: 0.8971284031867981\n",
      "Train: Epoch [8], Batch [17/938], Loss: 0.8216314911842346\n",
      "Train: Epoch [8], Batch [18/938], Loss: 0.6985471248626709\n",
      "Train: Epoch [8], Batch [19/938], Loss: 0.8661314249038696\n",
      "Train: Epoch [8], Batch [20/938], Loss: 0.6399905681610107\n",
      "Train: Epoch [8], Batch [21/938], Loss: 0.8517856597900391\n",
      "Train: Epoch [8], Batch [22/938], Loss: 0.7187436819076538\n",
      "Train: Epoch [8], Batch [23/938], Loss: 1.0074710845947266\n",
      "Train: Epoch [8], Batch [24/938], Loss: 0.9251798987388611\n",
      "Train: Epoch [8], Batch [25/938], Loss: 0.6041384339332581\n",
      "Train: Epoch [8], Batch [26/938], Loss: 0.8860087394714355\n",
      "Train: Epoch [8], Batch [27/938], Loss: 0.9956130981445312\n",
      "Train: Epoch [8], Batch [28/938], Loss: 1.0426241159439087\n",
      "Train: Epoch [8], Batch [29/938], Loss: 0.8780210614204407\n",
      "Train: Epoch [8], Batch [30/938], Loss: 1.265609622001648\n",
      "Train: Epoch [8], Batch [31/938], Loss: 0.9170137047767639\n",
      "Train: Epoch [8], Batch [32/938], Loss: 0.8918505907058716\n",
      "Train: Epoch [8], Batch [33/938], Loss: 0.9927302598953247\n",
      "Train: Epoch [8], Batch [34/938], Loss: 0.7321481108665466\n",
      "Train: Epoch [8], Batch [35/938], Loss: 0.8200169801712036\n",
      "Train: Epoch [8], Batch [36/938], Loss: 0.880410373210907\n",
      "Train: Epoch [8], Batch [37/938], Loss: 0.9697266817092896\n",
      "Train: Epoch [8], Batch [38/938], Loss: 0.8160253167152405\n",
      "Train: Epoch [8], Batch [39/938], Loss: 0.826587438583374\n",
      "Train: Epoch [8], Batch [40/938], Loss: 1.0180209875106812\n",
      "Train: Epoch [8], Batch [41/938], Loss: 0.8680714964866638\n",
      "Train: Epoch [8], Batch [42/938], Loss: 0.7890404462814331\n",
      "Train: Epoch [8], Batch [43/938], Loss: 0.8516772985458374\n",
      "Train: Epoch [8], Batch [44/938], Loss: 0.835419237613678\n",
      "Train: Epoch [8], Batch [45/938], Loss: 0.9016605615615845\n",
      "Train: Epoch [8], Batch [46/938], Loss: 0.6392014622688293\n",
      "Train: Epoch [8], Batch [47/938], Loss: 0.7002118825912476\n",
      "Train: Epoch [8], Batch [48/938], Loss: 1.1036487817764282\n",
      "Train: Epoch [8], Batch [49/938], Loss: 1.1326032876968384\n",
      "Train: Epoch [8], Batch [50/938], Loss: 0.8785571455955505\n",
      "Train: Epoch [8], Batch [51/938], Loss: 0.9261665344238281\n",
      "Train: Epoch [8], Batch [52/938], Loss: 0.7548117637634277\n",
      "Train: Epoch [8], Batch [53/938], Loss: 0.7250962257385254\n",
      "Train: Epoch [8], Batch [54/938], Loss: 0.6401573419570923\n",
      "Train: Epoch [8], Batch [55/938], Loss: 0.6248975396156311\n",
      "Train: Epoch [8], Batch [56/938], Loss: 0.743889570236206\n",
      "Train: Epoch [8], Batch [57/938], Loss: 0.8152502179145813\n",
      "Train: Epoch [8], Batch [58/938], Loss: 0.9381502270698547\n",
      "Train: Epoch [8], Batch [59/938], Loss: 0.7942957282066345\n",
      "Train: Epoch [8], Batch [60/938], Loss: 0.8677253723144531\n",
      "Train: Epoch [8], Batch [61/938], Loss: 0.5980611443519592\n",
      "Train: Epoch [8], Batch [62/938], Loss: 0.87535560131073\n",
      "Train: Epoch [8], Batch [63/938], Loss: 0.7931347489356995\n",
      "Train: Epoch [8], Batch [64/938], Loss: 0.9049422740936279\n",
      "Train: Epoch [8], Batch [65/938], Loss: 0.9213362336158752\n",
      "Train: Epoch [8], Batch [66/938], Loss: 0.7844354510307312\n",
      "Train: Epoch [8], Batch [67/938], Loss: 0.7907243371009827\n",
      "Train: Epoch [8], Batch [68/938], Loss: 0.7924305200576782\n",
      "Train: Epoch [8], Batch [69/938], Loss: 0.818428635597229\n",
      "Train: Epoch [8], Batch [70/938], Loss: 0.7848181128501892\n",
      "Train: Epoch [8], Batch [71/938], Loss: 0.8438088297843933\n",
      "Train: Epoch [8], Batch [72/938], Loss: 0.7791875004768372\n",
      "Train: Epoch [8], Batch [73/938], Loss: 0.8958646059036255\n",
      "Train: Epoch [8], Batch [74/938], Loss: 0.9263883233070374\n",
      "Train: Epoch [8], Batch [75/938], Loss: 0.9708080291748047\n",
      "Train: Epoch [8], Batch [76/938], Loss: 0.8532571196556091\n",
      "Train: Epoch [8], Batch [77/938], Loss: 0.9713909029960632\n",
      "Train: Epoch [8], Batch [78/938], Loss: 0.8329924941062927\n",
      "Train: Epoch [8], Batch [79/938], Loss: 0.9092891812324524\n",
      "Train: Epoch [8], Batch [80/938], Loss: 0.900619387626648\n",
      "Train: Epoch [8], Batch [81/938], Loss: 0.7263754606246948\n",
      "Train: Epoch [8], Batch [82/938], Loss: 1.056913137435913\n",
      "Train: Epoch [8], Batch [83/938], Loss: 0.8289147019386292\n",
      "Train: Epoch [8], Batch [84/938], Loss: 0.9311638474464417\n",
      "Train: Epoch [8], Batch [85/938], Loss: 1.1610908508300781\n",
      "Train: Epoch [8], Batch [86/938], Loss: 1.0531811714172363\n",
      "Train: Epoch [8], Batch [87/938], Loss: 1.079365611076355\n",
      "Train: Epoch [8], Batch [88/938], Loss: 0.8427094221115112\n",
      "Train: Epoch [8], Batch [89/938], Loss: 0.6455064415931702\n",
      "Train: Epoch [8], Batch [90/938], Loss: 0.7746081352233887\n",
      "Train: Epoch [8], Batch [91/938], Loss: 0.8059920072555542\n",
      "Train: Epoch [8], Batch [92/938], Loss: 0.9663236141204834\n",
      "Train: Epoch [8], Batch [93/938], Loss: 0.9951450228691101\n",
      "Train: Epoch [8], Batch [94/938], Loss: 0.7525094747543335\n",
      "Train: Epoch [8], Batch [95/938], Loss: 0.9609768390655518\n",
      "Train: Epoch [8], Batch [96/938], Loss: 0.6764898300170898\n",
      "Train: Epoch [8], Batch [97/938], Loss: 1.083019733428955\n",
      "Train: Epoch [8], Batch [98/938], Loss: 1.0037665367126465\n",
      "Train: Epoch [8], Batch [99/938], Loss: 0.8420553207397461\n",
      "Train: Epoch [8], Batch [100/938], Loss: 0.8087145686149597\n",
      "Train: Epoch [8], Batch [101/938], Loss: 0.7832202911376953\n",
      "Train: Epoch [8], Batch [102/938], Loss: 0.8806329965591431\n",
      "Train: Epoch [8], Batch [103/938], Loss: 0.8366613984107971\n",
      "Train: Epoch [8], Batch [104/938], Loss: 0.9626573324203491\n",
      "Train: Epoch [8], Batch [105/938], Loss: 0.9882850050926208\n",
      "Train: Epoch [8], Batch [106/938], Loss: 0.8748545050621033\n",
      "Train: Epoch [8], Batch [107/938], Loss: 0.930409848690033\n",
      "Train: Epoch [8], Batch [108/938], Loss: 1.0304522514343262\n",
      "Train: Epoch [8], Batch [109/938], Loss: 0.79764324426651\n",
      "Train: Epoch [8], Batch [110/938], Loss: 0.7445248365402222\n",
      "Train: Epoch [8], Batch [111/938], Loss: 0.5512017607688904\n",
      "Train: Epoch [8], Batch [112/938], Loss: 0.9146407246589661\n",
      "Train: Epoch [8], Batch [113/938], Loss: 0.8559162616729736\n",
      "Train: Epoch [8], Batch [114/938], Loss: 0.8594541549682617\n",
      "Train: Epoch [8], Batch [115/938], Loss: 0.7593817114830017\n",
      "Train: Epoch [8], Batch [116/938], Loss: 0.5900617241859436\n",
      "Train: Epoch [8], Batch [117/938], Loss: 0.7742695808410645\n",
      "Train: Epoch [8], Batch [118/938], Loss: 0.7404699325561523\n",
      "Train: Epoch [8], Batch [119/938], Loss: 0.8562148809432983\n",
      "Train: Epoch [8], Batch [120/938], Loss: 0.7996218204498291\n",
      "Train: Epoch [8], Batch [121/938], Loss: 0.900422990322113\n",
      "Train: Epoch [8], Batch [122/938], Loss: 0.9822722673416138\n",
      "Train: Epoch [8], Batch [123/938], Loss: 0.7439477443695068\n",
      "Train: Epoch [8], Batch [124/938], Loss: 1.2220720052719116\n",
      "Train: Epoch [8], Batch [125/938], Loss: 0.625919759273529\n",
      "Train: Epoch [8], Batch [126/938], Loss: 0.9348936676979065\n",
      "Train: Epoch [8], Batch [127/938], Loss: 1.019727349281311\n",
      "Train: Epoch [8], Batch [128/938], Loss: 0.8740059733390808\n",
      "Train: Epoch [8], Batch [129/938], Loss: 0.9626991748809814\n",
      "Train: Epoch [8], Batch [130/938], Loss: 1.0425053834915161\n",
      "Train: Epoch [8], Batch [131/938], Loss: 0.8768264055252075\n",
      "Train: Epoch [8], Batch [132/938], Loss: 0.7294330596923828\n",
      "Train: Epoch [8], Batch [133/938], Loss: 0.8501204252243042\n",
      "Train: Epoch [8], Batch [134/938], Loss: 0.7170296907424927\n",
      "Train: Epoch [8], Batch [135/938], Loss: 0.8874123096466064\n",
      "Train: Epoch [8], Batch [136/938], Loss: 0.7002643346786499\n",
      "Train: Epoch [8], Batch [137/938], Loss: 0.8614779710769653\n",
      "Train: Epoch [8], Batch [138/938], Loss: 0.8488349318504333\n",
      "Train: Epoch [8], Batch [139/938], Loss: 1.060496211051941\n",
      "Train: Epoch [8], Batch [140/938], Loss: 0.9349950551986694\n",
      "Train: Epoch [8], Batch [141/938], Loss: 0.960594892501831\n",
      "Train: Epoch [8], Batch [142/938], Loss: 0.8915621042251587\n",
      "Train: Epoch [8], Batch [143/938], Loss: 0.9446095824241638\n",
      "Train: Epoch [8], Batch [144/938], Loss: 0.778560996055603\n",
      "Train: Epoch [8], Batch [145/938], Loss: 0.8312142491340637\n",
      "Train: Epoch [8], Batch [146/938], Loss: 0.9811149835586548\n",
      "Train: Epoch [8], Batch [147/938], Loss: 0.9458127021789551\n",
      "Train: Epoch [8], Batch [148/938], Loss: 0.9675894975662231\n",
      "Train: Epoch [8], Batch [149/938], Loss: 0.8981242179870605\n",
      "Train: Epoch [8], Batch [150/938], Loss: 0.8048961162567139\n",
      "Train: Epoch [8], Batch [151/938], Loss: 0.9686557650566101\n",
      "Train: Epoch [8], Batch [152/938], Loss: 1.0854583978652954\n",
      "Train: Epoch [8], Batch [153/938], Loss: 0.8361971974372864\n",
      "Train: Epoch [8], Batch [154/938], Loss: 0.999668300151825\n",
      "Train: Epoch [8], Batch [155/938], Loss: 0.7146080136299133\n",
      "Train: Epoch [8], Batch [156/938], Loss: 0.9783681631088257\n",
      "Train: Epoch [8], Batch [157/938], Loss: 0.9027059078216553\n",
      "Train: Epoch [8], Batch [158/938], Loss: 0.7086902856826782\n",
      "Train: Epoch [8], Batch [159/938], Loss: 0.953555703163147\n",
      "Train: Epoch [8], Batch [160/938], Loss: 0.769363522529602\n",
      "Train: Epoch [8], Batch [161/938], Loss: 0.7708409428596497\n",
      "Train: Epoch [8], Batch [162/938], Loss: 0.8533740043640137\n",
      "Train: Epoch [8], Batch [163/938], Loss: 0.9132686853408813\n",
      "Train: Epoch [8], Batch [164/938], Loss: 0.8774929642677307\n",
      "Train: Epoch [8], Batch [165/938], Loss: 0.7196801900863647\n",
      "Train: Epoch [8], Batch [166/938], Loss: 0.7651277184486389\n",
      "Train: Epoch [8], Batch [167/938], Loss: 0.840595006942749\n",
      "Train: Epoch [8], Batch [168/938], Loss: 0.6755337119102478\n",
      "Train: Epoch [8], Batch [169/938], Loss: 0.7941137552261353\n",
      "Train: Epoch [8], Batch [170/938], Loss: 0.8900519609451294\n",
      "Train: Epoch [8], Batch [171/938], Loss: 0.7581838965415955\n",
      "Train: Epoch [8], Batch [172/938], Loss: 0.9728017449378967\n",
      "Train: Epoch [8], Batch [173/938], Loss: 0.7518385052680969\n",
      "Train: Epoch [8], Batch [174/938], Loss: 1.0145336389541626\n",
      "Train: Epoch [8], Batch [175/938], Loss: 0.8044988512992859\n",
      "Train: Epoch [8], Batch [176/938], Loss: 0.9413471817970276\n",
      "Train: Epoch [8], Batch [177/938], Loss: 0.8100468516349792\n",
      "Train: Epoch [8], Batch [178/938], Loss: 1.044292688369751\n",
      "Train: Epoch [8], Batch [179/938], Loss: 0.8281986713409424\n",
      "Train: Epoch [8], Batch [180/938], Loss: 0.8583649396896362\n",
      "Train: Epoch [8], Batch [181/938], Loss: 0.693966805934906\n",
      "Train: Epoch [8], Batch [182/938], Loss: 0.8402776122093201\n",
      "Train: Epoch [8], Batch [183/938], Loss: 0.8000695109367371\n",
      "Train: Epoch [8], Batch [184/938], Loss: 0.9088819622993469\n",
      "Train: Epoch [8], Batch [185/938], Loss: 1.051433801651001\n",
      "Train: Epoch [8], Batch [186/938], Loss: 0.8413489460945129\n",
      "Train: Epoch [8], Batch [187/938], Loss: 0.743417501449585\n",
      "Train: Epoch [8], Batch [188/938], Loss: 0.6294384598731995\n",
      "Train: Epoch [8], Batch [189/938], Loss: 0.8128367066383362\n",
      "Train: Epoch [8], Batch [190/938], Loss: 0.8199284672737122\n",
      "Train: Epoch [8], Batch [191/938], Loss: 0.7911550998687744\n",
      "Train: Epoch [8], Batch [192/938], Loss: 0.7744946479797363\n",
      "Train: Epoch [8], Batch [193/938], Loss: 0.9216181039810181\n",
      "Train: Epoch [8], Batch [194/938], Loss: 1.12369704246521\n",
      "Train: Epoch [8], Batch [195/938], Loss: 0.9143896698951721\n",
      "Train: Epoch [8], Batch [196/938], Loss: 0.6787916421890259\n",
      "Train: Epoch [8], Batch [197/938], Loss: 0.9648527503013611\n",
      "Train: Epoch [8], Batch [198/938], Loss: 0.8185954093933105\n",
      "Train: Epoch [8], Batch [199/938], Loss: 0.8466789126396179\n",
      "Train: Epoch [8], Batch [200/938], Loss: 0.6776727437973022\n",
      "Train: Epoch [8], Batch [201/938], Loss: 0.6829871535301208\n",
      "Train: Epoch [8], Batch [202/938], Loss: 0.8951262831687927\n",
      "Train: Epoch [8], Batch [203/938], Loss: 1.1983516216278076\n",
      "Train: Epoch [8], Batch [204/938], Loss: 0.8849099278450012\n",
      "Train: Epoch [8], Batch [205/938], Loss: 0.8796328902244568\n",
      "Train: Epoch [8], Batch [206/938], Loss: 0.8017966747283936\n",
      "Train: Epoch [8], Batch [207/938], Loss: 0.7137575745582581\n",
      "Train: Epoch [8], Batch [208/938], Loss: 0.9887951016426086\n",
      "Train: Epoch [8], Batch [209/938], Loss: 0.7652042508125305\n",
      "Train: Epoch [8], Batch [210/938], Loss: 0.7156808376312256\n",
      "Train: Epoch [8], Batch [211/938], Loss: 0.7826203107833862\n",
      "Train: Epoch [8], Batch [212/938], Loss: 0.7691164612770081\n",
      "Train: Epoch [8], Batch [213/938], Loss: 0.9461640119552612\n",
      "Train: Epoch [8], Batch [214/938], Loss: 0.8817118406295776\n",
      "Train: Epoch [8], Batch [215/938], Loss: 0.8234290480613708\n",
      "Train: Epoch [8], Batch [216/938], Loss: 0.796553909778595\n",
      "Train: Epoch [8], Batch [217/938], Loss: 0.8942080736160278\n",
      "Train: Epoch [8], Batch [218/938], Loss: 0.8751387000083923\n",
      "Train: Epoch [8], Batch [219/938], Loss: 0.80399489402771\n",
      "Train: Epoch [8], Batch [220/938], Loss: 0.741360604763031\n",
      "Train: Epoch [8], Batch [221/938], Loss: 0.9683767557144165\n",
      "Train: Epoch [8], Batch [222/938], Loss: 0.8038073182106018\n",
      "Train: Epoch [8], Batch [223/938], Loss: 0.8124654293060303\n",
      "Train: Epoch [8], Batch [224/938], Loss: 1.050718069076538\n",
      "Train: Epoch [8], Batch [225/938], Loss: 1.0947768688201904\n",
      "Train: Epoch [8], Batch [226/938], Loss: 0.9657260179519653\n",
      "Train: Epoch [8], Batch [227/938], Loss: 0.8749865889549255\n",
      "Train: Epoch [8], Batch [228/938], Loss: 0.881842851638794\n",
      "Train: Epoch [8], Batch [229/938], Loss: 0.8329498767852783\n",
      "Train: Epoch [8], Batch [230/938], Loss: 0.9402318596839905\n",
      "Train: Epoch [8], Batch [231/938], Loss: 0.7808343172073364\n",
      "Train: Epoch [8], Batch [232/938], Loss: 0.8687053918838501\n",
      "Train: Epoch [8], Batch [233/938], Loss: 0.7280914187431335\n",
      "Train: Epoch [8], Batch [234/938], Loss: 0.9609450101852417\n",
      "Train: Epoch [8], Batch [235/938], Loss: 0.9750784039497375\n",
      "Train: Epoch [8], Batch [236/938], Loss: 1.0319180488586426\n",
      "Train: Epoch [8], Batch [237/938], Loss: 0.8632535934448242\n",
      "Train: Epoch [8], Batch [238/938], Loss: 0.8187973499298096\n",
      "Train: Epoch [8], Batch [239/938], Loss: 0.8668850064277649\n",
      "Train: Epoch [8], Batch [240/938], Loss: 0.9850425124168396\n",
      "Train: Epoch [8], Batch [241/938], Loss: 0.7435581684112549\n",
      "Train: Epoch [8], Batch [242/938], Loss: 0.8231354355812073\n",
      "Train: Epoch [8], Batch [243/938], Loss: 1.072244644165039\n",
      "Train: Epoch [8], Batch [244/938], Loss: 1.0090868473052979\n",
      "Train: Epoch [8], Batch [245/938], Loss: 0.9461362361907959\n",
      "Train: Epoch [8], Batch [246/938], Loss: 1.1309083700180054\n",
      "Train: Epoch [8], Batch [247/938], Loss: 0.9675567150115967\n",
      "Train: Epoch [8], Batch [248/938], Loss: 0.9558956623077393\n",
      "Train: Epoch [8], Batch [249/938], Loss: 0.9451250433921814\n",
      "Train: Epoch [8], Batch [250/938], Loss: 0.6478209495544434\n",
      "Train: Epoch [8], Batch [251/938], Loss: 0.7817951440811157\n",
      "Train: Epoch [8], Batch [252/938], Loss: 0.987830400466919\n",
      "Train: Epoch [8], Batch [253/938], Loss: 0.9125537276268005\n",
      "Train: Epoch [8], Batch [254/938], Loss: 0.701119065284729\n",
      "Train: Epoch [8], Batch [255/938], Loss: 0.7854542136192322\n",
      "Train: Epoch [8], Batch [256/938], Loss: 0.9769065976142883\n",
      "Train: Epoch [8], Batch [257/938], Loss: 0.795554518699646\n",
      "Train: Epoch [8], Batch [258/938], Loss: 0.8287646174430847\n",
      "Train: Epoch [8], Batch [259/938], Loss: 0.8279140591621399\n",
      "Train: Epoch [8], Batch [260/938], Loss: 0.8558323383331299\n",
      "Train: Epoch [8], Batch [261/938], Loss: 0.9181659817695618\n",
      "Train: Epoch [8], Batch [262/938], Loss: 0.8278168439865112\n",
      "Train: Epoch [8], Batch [263/938], Loss: 0.7571807503700256\n",
      "Train: Epoch [8], Batch [264/938], Loss: 0.8138647675514221\n",
      "Train: Epoch [8], Batch [265/938], Loss: 0.907985508441925\n",
      "Train: Epoch [8], Batch [266/938], Loss: 0.8570934534072876\n",
      "Train: Epoch [8], Batch [267/938], Loss: 0.7822648882865906\n",
      "Train: Epoch [8], Batch [268/938], Loss: 0.9269741177558899\n",
      "Train: Epoch [8], Batch [269/938], Loss: 0.9851968288421631\n",
      "Train: Epoch [8], Batch [270/938], Loss: 0.7164146900177002\n",
      "Train: Epoch [8], Batch [271/938], Loss: 0.8614327907562256\n",
      "Train: Epoch [8], Batch [272/938], Loss: 0.9806714057922363\n",
      "Train: Epoch [8], Batch [273/938], Loss: 1.0810348987579346\n",
      "Train: Epoch [8], Batch [274/938], Loss: 0.6437059640884399\n",
      "Train: Epoch [8], Batch [275/938], Loss: 0.6792898774147034\n",
      "Train: Epoch [8], Batch [276/938], Loss: 0.8457436561584473\n",
      "Train: Epoch [8], Batch [277/938], Loss: 0.7995755076408386\n",
      "Train: Epoch [8], Batch [278/938], Loss: 1.1928871870040894\n",
      "Train: Epoch [8], Batch [279/938], Loss: 0.601569652557373\n",
      "Train: Epoch [8], Batch [280/938], Loss: 0.871911346912384\n",
      "Train: Epoch [8], Batch [281/938], Loss: 0.8991117477416992\n",
      "Train: Epoch [8], Batch [282/938], Loss: 0.7385740876197815\n",
      "Train: Epoch [8], Batch [283/938], Loss: 0.8852831125259399\n",
      "Train: Epoch [8], Batch [284/938], Loss: 0.9504386782646179\n",
      "Train: Epoch [8], Batch [285/938], Loss: 0.9058622717857361\n",
      "Train: Epoch [8], Batch [286/938], Loss: 0.4848315715789795\n",
      "Train: Epoch [8], Batch [287/938], Loss: 0.8818171620368958\n",
      "Train: Epoch [8], Batch [288/938], Loss: 0.7919909358024597\n",
      "Train: Epoch [8], Batch [289/938], Loss: 0.8737980723381042\n",
      "Train: Epoch [8], Batch [290/938], Loss: 0.8406745791435242\n",
      "Train: Epoch [8], Batch [291/938], Loss: 1.005441665649414\n",
      "Train: Epoch [8], Batch [292/938], Loss: 0.7197727560997009\n",
      "Train: Epoch [8], Batch [293/938], Loss: 0.7913047075271606\n",
      "Train: Epoch [8], Batch [294/938], Loss: 0.8865218162536621\n",
      "Train: Epoch [8], Batch [295/938], Loss: 0.8211565017700195\n",
      "Train: Epoch [8], Batch [296/938], Loss: 0.8296633958816528\n",
      "Train: Epoch [8], Batch [297/938], Loss: 1.0166887044906616\n",
      "Train: Epoch [8], Batch [298/938], Loss: 0.8884046673774719\n",
      "Train: Epoch [8], Batch [299/938], Loss: 0.8770037889480591\n",
      "Train: Epoch [8], Batch [300/938], Loss: 0.7906466126441956\n",
      "Train: Epoch [8], Batch [301/938], Loss: 0.7629552483558655\n",
      "Train: Epoch [8], Batch [302/938], Loss: 0.7985259890556335\n",
      "Train: Epoch [8], Batch [303/938], Loss: 0.892318844795227\n",
      "Train: Epoch [8], Batch [304/938], Loss: 0.7329246401786804\n",
      "Train: Epoch [8], Batch [305/938], Loss: 1.0228455066680908\n",
      "Train: Epoch [8], Batch [306/938], Loss: 0.9372862577438354\n",
      "Train: Epoch [8], Batch [307/938], Loss: 0.901206374168396\n",
      "Train: Epoch [8], Batch [308/938], Loss: 0.8682441115379333\n",
      "Train: Epoch [8], Batch [309/938], Loss: 0.6765708327293396\n",
      "Train: Epoch [8], Batch [310/938], Loss: 0.8406909704208374\n",
      "Train: Epoch [8], Batch [311/938], Loss: 0.869013249874115\n",
      "Train: Epoch [8], Batch [312/938], Loss: 1.0132701396942139\n",
      "Train: Epoch [8], Batch [313/938], Loss: 0.846538782119751\n",
      "Train: Epoch [8], Batch [314/938], Loss: 1.0256595611572266\n",
      "Train: Epoch [8], Batch [315/938], Loss: 0.9251076579093933\n",
      "Train: Epoch [8], Batch [316/938], Loss: 0.631915807723999\n",
      "Train: Epoch [8], Batch [317/938], Loss: 0.9037625193595886\n",
      "Train: Epoch [8], Batch [318/938], Loss: 0.7699971199035645\n",
      "Train: Epoch [8], Batch [319/938], Loss: 0.8050205707550049\n",
      "Train: Epoch [8], Batch [320/938], Loss: 0.7392269372940063\n",
      "Train: Epoch [8], Batch [321/938], Loss: 0.8801737427711487\n",
      "Train: Epoch [8], Batch [322/938], Loss: 0.7653048634529114\n",
      "Train: Epoch [8], Batch [323/938], Loss: 0.8805475831031799\n",
      "Train: Epoch [8], Batch [324/938], Loss: 0.7938544154167175\n",
      "Train: Epoch [8], Batch [325/938], Loss: 1.0691686868667603\n",
      "Train: Epoch [8], Batch [326/938], Loss: 0.843759298324585\n",
      "Train: Epoch [8], Batch [327/938], Loss: 0.6963415145874023\n",
      "Train: Epoch [8], Batch [328/938], Loss: 0.9281678795814514\n",
      "Train: Epoch [8], Batch [329/938], Loss: 0.7287715077400208\n",
      "Train: Epoch [8], Batch [330/938], Loss: 0.7665749788284302\n",
      "Train: Epoch [8], Batch [331/938], Loss: 0.7882922887802124\n",
      "Train: Epoch [8], Batch [332/938], Loss: 0.7571792006492615\n",
      "Train: Epoch [8], Batch [333/938], Loss: 0.9422841668128967\n",
      "Train: Epoch [8], Batch [334/938], Loss: 0.8493872880935669\n",
      "Train: Epoch [8], Batch [335/938], Loss: 1.000035047531128\n",
      "Train: Epoch [8], Batch [336/938], Loss: 0.7563269734382629\n",
      "Train: Epoch [8], Batch [337/938], Loss: 1.0465972423553467\n",
      "Train: Epoch [8], Batch [338/938], Loss: 0.6876978874206543\n",
      "Train: Epoch [8], Batch [339/938], Loss: 0.8517240285873413\n",
      "Train: Epoch [8], Batch [340/938], Loss: 0.8935419917106628\n",
      "Train: Epoch [8], Batch [341/938], Loss: 0.7234628200531006\n",
      "Train: Epoch [8], Batch [342/938], Loss: 1.2142090797424316\n",
      "Train: Epoch [8], Batch [343/938], Loss: 0.9961729049682617\n",
      "Train: Epoch [8], Batch [344/938], Loss: 0.932209849357605\n",
      "Train: Epoch [8], Batch [345/938], Loss: 0.7913134694099426\n",
      "Train: Epoch [8], Batch [346/938], Loss: 0.7622041702270508\n",
      "Train: Epoch [8], Batch [347/938], Loss: 0.7569752931594849\n",
      "Train: Epoch [8], Batch [348/938], Loss: 1.0022883415222168\n",
      "Train: Epoch [8], Batch [349/938], Loss: 0.8576436638832092\n",
      "Train: Epoch [8], Batch [350/938], Loss: 1.0032432079315186\n",
      "Train: Epoch [8], Batch [351/938], Loss: 0.9598084092140198\n",
      "Train: Epoch [8], Batch [352/938], Loss: 1.0594778060913086\n",
      "Train: Epoch [8], Batch [353/938], Loss: 0.8299194574356079\n",
      "Train: Epoch [8], Batch [354/938], Loss: 1.06233549118042\n",
      "Train: Epoch [8], Batch [355/938], Loss: 0.8224895596504211\n",
      "Train: Epoch [8], Batch [356/938], Loss: 0.9823012351989746\n",
      "Train: Epoch [8], Batch [357/938], Loss: 0.8253921270370483\n",
      "Train: Epoch [8], Batch [358/938], Loss: 0.8776067495346069\n",
      "Train: Epoch [8], Batch [359/938], Loss: 0.8152042031288147\n",
      "Train: Epoch [8], Batch [360/938], Loss: 0.8560388088226318\n",
      "Train: Epoch [8], Batch [361/938], Loss: 0.8721317052841187\n",
      "Train: Epoch [8], Batch [362/938], Loss: 0.801849365234375\n",
      "Train: Epoch [8], Batch [363/938], Loss: 1.0407296419143677\n",
      "Train: Epoch [8], Batch [364/938], Loss: 0.7968746423721313\n",
      "Train: Epoch [8], Batch [365/938], Loss: 0.9514843821525574\n",
      "Train: Epoch [8], Batch [366/938], Loss: 0.7900645136833191\n",
      "Train: Epoch [8], Batch [367/938], Loss: 0.9342737793922424\n",
      "Train: Epoch [8], Batch [368/938], Loss: 0.7952816486358643\n",
      "Train: Epoch [8], Batch [369/938], Loss: 0.9748081564903259\n",
      "Train: Epoch [8], Batch [370/938], Loss: 0.7138609290122986\n",
      "Train: Epoch [8], Batch [371/938], Loss: 0.6914271116256714\n",
      "Train: Epoch [8], Batch [372/938], Loss: 0.6826183795928955\n",
      "Train: Epoch [8], Batch [373/938], Loss: 0.860010027885437\n",
      "Train: Epoch [8], Batch [374/938], Loss: 0.8262664079666138\n",
      "Train: Epoch [8], Batch [375/938], Loss: 0.8357667922973633\n",
      "Train: Epoch [8], Batch [376/938], Loss: 0.7229444980621338\n",
      "Train: Epoch [8], Batch [377/938], Loss: 0.9998971223831177\n",
      "Train: Epoch [8], Batch [378/938], Loss: 0.8181635737419128\n",
      "Train: Epoch [8], Batch [379/938], Loss: 0.869339644908905\n",
      "Train: Epoch [8], Batch [380/938], Loss: 0.6995118856430054\n",
      "Train: Epoch [8], Batch [381/938], Loss: 0.8011192083358765\n",
      "Train: Epoch [8], Batch [382/938], Loss: 0.8505393862724304\n",
      "Train: Epoch [8], Batch [383/938], Loss: 0.9532415866851807\n",
      "Train: Epoch [8], Batch [384/938], Loss: 0.9300801157951355\n",
      "Train: Epoch [8], Batch [385/938], Loss: 1.0379384756088257\n",
      "Train: Epoch [8], Batch [386/938], Loss: 0.697697639465332\n",
      "Train: Epoch [8], Batch [387/938], Loss: 0.8500751256942749\n",
      "Train: Epoch [8], Batch [388/938], Loss: 0.988021969795227\n",
      "Train: Epoch [8], Batch [389/938], Loss: 1.0301637649536133\n",
      "Train: Epoch [8], Batch [390/938], Loss: 0.7342789173126221\n",
      "Train: Epoch [8], Batch [391/938], Loss: 0.9735234975814819\n",
      "Train: Epoch [8], Batch [392/938], Loss: 0.867416262626648\n",
      "Train: Epoch [8], Batch [393/938], Loss: 0.901600182056427\n",
      "Train: Epoch [8], Batch [394/938], Loss: 0.8912426233291626\n",
      "Train: Epoch [8], Batch [395/938], Loss: 0.8101522922515869\n",
      "Train: Epoch [8], Batch [396/938], Loss: 0.9153358936309814\n",
      "Train: Epoch [8], Batch [397/938], Loss: 0.8268377780914307\n",
      "Train: Epoch [8], Batch [398/938], Loss: 0.9851175546646118\n",
      "Train: Epoch [8], Batch [399/938], Loss: 1.1585075855255127\n",
      "Train: Epoch [8], Batch [400/938], Loss: 0.6795623302459717\n",
      "Train: Epoch [8], Batch [401/938], Loss: 0.8614599108695984\n",
      "Train: Epoch [8], Batch [402/938], Loss: 0.9547163248062134\n",
      "Train: Epoch [8], Batch [403/938], Loss: 0.7713077068328857\n",
      "Train: Epoch [8], Batch [404/938], Loss: 0.9476949572563171\n",
      "Train: Epoch [8], Batch [405/938], Loss: 0.7448320388793945\n",
      "Train: Epoch [8], Batch [406/938], Loss: 0.8765686750411987\n",
      "Train: Epoch [8], Batch [407/938], Loss: 0.732887327671051\n",
      "Train: Epoch [8], Batch [408/938], Loss: 0.9066284894943237\n",
      "Train: Epoch [8], Batch [409/938], Loss: 0.7707463502883911\n",
      "Train: Epoch [8], Batch [410/938], Loss: 0.8707733750343323\n",
      "Train: Epoch [8], Batch [411/938], Loss: 0.8092541694641113\n",
      "Train: Epoch [8], Batch [412/938], Loss: 0.753740131855011\n",
      "Train: Epoch [8], Batch [413/938], Loss: 1.2476307153701782\n",
      "Train: Epoch [8], Batch [414/938], Loss: 0.6915326714515686\n",
      "Train: Epoch [8], Batch [415/938], Loss: 0.8528820872306824\n",
      "Train: Epoch [8], Batch [416/938], Loss: 0.9298850893974304\n",
      "Train: Epoch [8], Batch [417/938], Loss: 0.7871091961860657\n",
      "Train: Epoch [8], Batch [418/938], Loss: 0.7452113032341003\n",
      "Train: Epoch [8], Batch [419/938], Loss: 0.8086839914321899\n",
      "Train: Epoch [8], Batch [420/938], Loss: 0.9990488290786743\n",
      "Train: Epoch [8], Batch [421/938], Loss: 0.7453145980834961\n",
      "Train: Epoch [8], Batch [422/938], Loss: 0.9987571835517883\n",
      "Train: Epoch [8], Batch [423/938], Loss: 0.7970839142799377\n",
      "Train: Epoch [8], Batch [424/938], Loss: 0.5676903128623962\n",
      "Train: Epoch [8], Batch [425/938], Loss: 0.7637559175491333\n",
      "Train: Epoch [8], Batch [426/938], Loss: 0.7679183483123779\n",
      "Train: Epoch [8], Batch [427/938], Loss: 0.9664266705513\n",
      "Train: Epoch [8], Batch [428/938], Loss: 0.6363372802734375\n",
      "Train: Epoch [8], Batch [429/938], Loss: 0.9631719589233398\n",
      "Train: Epoch [8], Batch [430/938], Loss: 0.846786618232727\n",
      "Train: Epoch [8], Batch [431/938], Loss: 0.9130859971046448\n",
      "Train: Epoch [8], Batch [432/938], Loss: 0.9798827767372131\n",
      "Train: Epoch [8], Batch [433/938], Loss: 0.8094804286956787\n",
      "Train: Epoch [8], Batch [434/938], Loss: 0.7010350227355957\n",
      "Train: Epoch [8], Batch [435/938], Loss: 0.7448756694793701\n",
      "Train: Epoch [8], Batch [436/938], Loss: 0.9082573652267456\n",
      "Train: Epoch [8], Batch [437/938], Loss: 0.9805723428726196\n",
      "Train: Epoch [8], Batch [438/938], Loss: 0.7452152371406555\n",
      "Train: Epoch [8], Batch [439/938], Loss: 1.044830560684204\n",
      "Train: Epoch [8], Batch [440/938], Loss: 0.8191842436790466\n",
      "Train: Epoch [8], Batch [441/938], Loss: 0.8786361813545227\n",
      "Train: Epoch [8], Batch [442/938], Loss: 0.9008927345275879\n",
      "Train: Epoch [8], Batch [443/938], Loss: 0.9023489952087402\n",
      "Train: Epoch [8], Batch [444/938], Loss: 0.868887722492218\n",
      "Train: Epoch [8], Batch [445/938], Loss: 0.8022965788841248\n",
      "Train: Epoch [8], Batch [446/938], Loss: 0.9506194591522217\n",
      "Train: Epoch [8], Batch [447/938], Loss: 0.9480973482131958\n",
      "Train: Epoch [8], Batch [448/938], Loss: 0.9848456978797913\n",
      "Train: Epoch [8], Batch [449/938], Loss: 0.8428076505661011\n",
      "Train: Epoch [8], Batch [450/938], Loss: 1.0917538404464722\n",
      "Train: Epoch [8], Batch [451/938], Loss: 0.7070751190185547\n",
      "Train: Epoch [8], Batch [452/938], Loss: 0.7275112867355347\n",
      "Train: Epoch [8], Batch [453/938], Loss: 1.0463356971740723\n",
      "Train: Epoch [8], Batch [454/938], Loss: 0.9012753963470459\n",
      "Train: Epoch [8], Batch [455/938], Loss: 0.891616940498352\n",
      "Train: Epoch [8], Batch [456/938], Loss: 0.8897157907485962\n",
      "Train: Epoch [8], Batch [457/938], Loss: 0.899355947971344\n",
      "Train: Epoch [8], Batch [458/938], Loss: 0.7977385520935059\n",
      "Train: Epoch [8], Batch [459/938], Loss: 0.7843888401985168\n",
      "Train: Epoch [8], Batch [460/938], Loss: 0.7908243536949158\n",
      "Train: Epoch [8], Batch [461/938], Loss: 0.5505911111831665\n",
      "Train: Epoch [8], Batch [462/938], Loss: 0.7239968776702881\n",
      "Train: Epoch [8], Batch [463/938], Loss: 0.9607710838317871\n",
      "Train: Epoch [8], Batch [464/938], Loss: 0.7703439593315125\n",
      "Train: Epoch [8], Batch [465/938], Loss: 0.9769587516784668\n",
      "Train: Epoch [8], Batch [466/938], Loss: 0.9511566758155823\n",
      "Train: Epoch [8], Batch [467/938], Loss: 0.9737181663513184\n",
      "Train: Epoch [8], Batch [468/938], Loss: 0.7621501684188843\n",
      "Train: Epoch [8], Batch [469/938], Loss: 1.0497336387634277\n",
      "Train: Epoch [8], Batch [470/938], Loss: 0.8379663825035095\n",
      "Train: Epoch [8], Batch [471/938], Loss: 0.8077104687690735\n",
      "Train: Epoch [8], Batch [472/938], Loss: 1.2330901622772217\n",
      "Train: Epoch [8], Batch [473/938], Loss: 0.9611759185791016\n",
      "Train: Epoch [8], Batch [474/938], Loss: 0.7929260730743408\n",
      "Train: Epoch [8], Batch [475/938], Loss: 0.7975058555603027\n",
      "Train: Epoch [8], Batch [476/938], Loss: 0.9005171656608582\n",
      "Train: Epoch [8], Batch [477/938], Loss: 0.6617593765258789\n",
      "Train: Epoch [8], Batch [478/938], Loss: 0.8569319844245911\n",
      "Train: Epoch [8], Batch [479/938], Loss: 0.61372971534729\n",
      "Train: Epoch [8], Batch [480/938], Loss: 0.8526586294174194\n",
      "Train: Epoch [8], Batch [481/938], Loss: 0.9723302125930786\n",
      "Train: Epoch [8], Batch [482/938], Loss: 0.8977197408676147\n",
      "Train: Epoch [8], Batch [483/938], Loss: 0.8755354881286621\n",
      "Train: Epoch [8], Batch [484/938], Loss: 1.1754838228225708\n",
      "Train: Epoch [8], Batch [485/938], Loss: 0.8803305625915527\n",
      "Train: Epoch [8], Batch [486/938], Loss: 0.6942887902259827\n",
      "Train: Epoch [8], Batch [487/938], Loss: 0.8625808358192444\n",
      "Train: Epoch [8], Batch [488/938], Loss: 0.719902515411377\n",
      "Train: Epoch [8], Batch [489/938], Loss: 0.7551230788230896\n",
      "Train: Epoch [8], Batch [490/938], Loss: 0.7198450565338135\n",
      "Train: Epoch [8], Batch [491/938], Loss: 0.7336886525154114\n",
      "Train: Epoch [8], Batch [492/938], Loss: 0.7793968915939331\n",
      "Train: Epoch [8], Batch [493/938], Loss: 0.9145359396934509\n",
      "Train: Epoch [8], Batch [494/938], Loss: 0.7855145335197449\n",
      "Train: Epoch [8], Batch [495/938], Loss: 0.7995293736457825\n",
      "Train: Epoch [8], Batch [496/938], Loss: 0.8576520681381226\n",
      "Train: Epoch [8], Batch [497/938], Loss: 0.7708783745765686\n",
      "Train: Epoch [8], Batch [498/938], Loss: 0.972745418548584\n",
      "Train: Epoch [8], Batch [499/938], Loss: 0.6444655656814575\n",
      "Train: Epoch [8], Batch [500/938], Loss: 0.6624813079833984\n",
      "Train: Epoch [8], Batch [501/938], Loss: 0.8991467356681824\n",
      "Train: Epoch [8], Batch [502/938], Loss: 0.8688733577728271\n",
      "Train: Epoch [8], Batch [503/938], Loss: 0.7229726314544678\n",
      "Train: Epoch [8], Batch [504/938], Loss: 0.7846459746360779\n",
      "Train: Epoch [8], Batch [505/938], Loss: 0.7763351202011108\n",
      "Train: Epoch [8], Batch [506/938], Loss: 0.9308755993843079\n",
      "Train: Epoch [8], Batch [507/938], Loss: 0.9288548231124878\n",
      "Train: Epoch [8], Batch [508/938], Loss: 0.8088881969451904\n",
      "Train: Epoch [8], Batch [509/938], Loss: 0.9418373703956604\n",
      "Train: Epoch [8], Batch [510/938], Loss: 0.6020312309265137\n",
      "Train: Epoch [8], Batch [511/938], Loss: 0.8831952214241028\n",
      "Train: Epoch [8], Batch [512/938], Loss: 0.6362473964691162\n",
      "Train: Epoch [8], Batch [513/938], Loss: 0.935779333114624\n",
      "Train: Epoch [8], Batch [514/938], Loss: 0.8500087261199951\n",
      "Train: Epoch [8], Batch [515/938], Loss: 0.7288568019866943\n",
      "Train: Epoch [8], Batch [516/938], Loss: 0.9197157621383667\n",
      "Train: Epoch [8], Batch [517/938], Loss: 0.8152196407318115\n",
      "Train: Epoch [8], Batch [518/938], Loss: 0.962969958782196\n",
      "Train: Epoch [8], Batch [519/938], Loss: 0.7060045003890991\n",
      "Train: Epoch [8], Batch [520/938], Loss: 1.075871467590332\n",
      "Train: Epoch [8], Batch [521/938], Loss: 0.8985792994499207\n",
      "Train: Epoch [8], Batch [522/938], Loss: 0.7516881823539734\n",
      "Train: Epoch [8], Batch [523/938], Loss: 0.9510016441345215\n",
      "Train: Epoch [8], Batch [524/938], Loss: 1.2529724836349487\n",
      "Train: Epoch [8], Batch [525/938], Loss: 0.9449638724327087\n",
      "Train: Epoch [8], Batch [526/938], Loss: 1.0612168312072754\n",
      "Train: Epoch [8], Batch [527/938], Loss: 0.852628231048584\n",
      "Train: Epoch [8], Batch [528/938], Loss: 0.8915421366691589\n",
      "Train: Epoch [8], Batch [529/938], Loss: 0.833242654800415\n",
      "Train: Epoch [8], Batch [530/938], Loss: 0.8046115636825562\n",
      "Train: Epoch [8], Batch [531/938], Loss: 0.867891788482666\n",
      "Train: Epoch [8], Batch [532/938], Loss: 0.8257473707199097\n",
      "Train: Epoch [8], Batch [533/938], Loss: 0.7357502579689026\n",
      "Train: Epoch [8], Batch [534/938], Loss: 0.5601385831832886\n",
      "Train: Epoch [8], Batch [535/938], Loss: 0.6135204434394836\n",
      "Train: Epoch [8], Batch [536/938], Loss: 0.7930183410644531\n",
      "Train: Epoch [8], Batch [537/938], Loss: 0.7651718854904175\n",
      "Train: Epoch [8], Batch [538/938], Loss: 0.8122164011001587\n",
      "Train: Epoch [8], Batch [539/938], Loss: 0.8782449960708618\n",
      "Train: Epoch [8], Batch [540/938], Loss: 0.9351693391799927\n",
      "Train: Epoch [8], Batch [541/938], Loss: 0.814538836479187\n",
      "Train: Epoch [8], Batch [542/938], Loss: 0.6002756953239441\n",
      "Train: Epoch [8], Batch [543/938], Loss: 1.0692236423492432\n",
      "Train: Epoch [8], Batch [544/938], Loss: 0.8155507445335388\n",
      "Train: Epoch [8], Batch [545/938], Loss: 1.0179352760314941\n",
      "Train: Epoch [8], Batch [546/938], Loss: 0.850584864616394\n",
      "Train: Epoch [8], Batch [547/938], Loss: 0.8323560953140259\n",
      "Train: Epoch [8], Batch [548/938], Loss: 0.9604290723800659\n",
      "Train: Epoch [8], Batch [549/938], Loss: 0.7219637632369995\n",
      "Train: Epoch [8], Batch [550/938], Loss: 0.7952991127967834\n",
      "Train: Epoch [8], Batch [551/938], Loss: 1.0282680988311768\n",
      "Train: Epoch [8], Batch [552/938], Loss: 0.7430775761604309\n",
      "Train: Epoch [8], Batch [553/938], Loss: 0.9828932285308838\n",
      "Train: Epoch [8], Batch [554/938], Loss: 0.8873963952064514\n",
      "Train: Epoch [8], Batch [555/938], Loss: 0.9337755441665649\n",
      "Train: Epoch [8], Batch [556/938], Loss: 1.1114280223846436\n",
      "Train: Epoch [8], Batch [557/938], Loss: 0.7855483889579773\n",
      "Train: Epoch [8], Batch [558/938], Loss: 0.9511645436286926\n",
      "Train: Epoch [8], Batch [559/938], Loss: 0.8037598133087158\n",
      "Train: Epoch [8], Batch [560/938], Loss: 0.7025421857833862\n",
      "Train: Epoch [8], Batch [561/938], Loss: 0.8064247369766235\n",
      "Train: Epoch [8], Batch [562/938], Loss: 0.5664973855018616\n",
      "Train: Epoch [8], Batch [563/938], Loss: 0.7736353278160095\n",
      "Train: Epoch [8], Batch [564/938], Loss: 0.9571419954299927\n",
      "Train: Epoch [8], Batch [565/938], Loss: 0.9338604211807251\n",
      "Train: Epoch [8], Batch [566/938], Loss: 0.7542401552200317\n",
      "Train: Epoch [8], Batch [567/938], Loss: 0.8501451015472412\n",
      "Train: Epoch [8], Batch [568/938], Loss: 0.7655200958251953\n",
      "Train: Epoch [8], Batch [569/938], Loss: 0.8593765497207642\n",
      "Train: Epoch [8], Batch [570/938], Loss: 0.7934790253639221\n",
      "Train: Epoch [8], Batch [571/938], Loss: 1.0085889101028442\n",
      "Train: Epoch [8], Batch [572/938], Loss: 0.7658023834228516\n",
      "Train: Epoch [8], Batch [573/938], Loss: 0.8611347079277039\n",
      "Train: Epoch [8], Batch [574/938], Loss: 0.9183348417282104\n",
      "Train: Epoch [8], Batch [575/938], Loss: 0.8175460696220398\n",
      "Train: Epoch [8], Batch [576/938], Loss: 0.8896723389625549\n",
      "Train: Epoch [8], Batch [577/938], Loss: 0.8558432459831238\n",
      "Train: Epoch [8], Batch [578/938], Loss: 0.9025977253913879\n",
      "Train: Epoch [8], Batch [579/938], Loss: 0.6851136684417725\n",
      "Train: Epoch [8], Batch [580/938], Loss: 0.9511030912399292\n",
      "Train: Epoch [8], Batch [581/938], Loss: 0.7925902605056763\n",
      "Train: Epoch [8], Batch [582/938], Loss: 0.6500877737998962\n",
      "Train: Epoch [8], Batch [583/938], Loss: 0.8016234636306763\n",
      "Train: Epoch [8], Batch [584/938], Loss: 0.867607593536377\n",
      "Train: Epoch [8], Batch [585/938], Loss: 0.811315655708313\n",
      "Train: Epoch [8], Batch [586/938], Loss: 0.7845956683158875\n",
      "Train: Epoch [8], Batch [587/938], Loss: 0.9362690448760986\n",
      "Train: Epoch [8], Batch [588/938], Loss: 0.7968499660491943\n",
      "Train: Epoch [8], Batch [589/938], Loss: 0.8841185569763184\n",
      "Train: Epoch [8], Batch [590/938], Loss: 0.7901797294616699\n",
      "Train: Epoch [8], Batch [591/938], Loss: 0.6336675882339478\n",
      "Train: Epoch [8], Batch [592/938], Loss: 0.8643811941146851\n",
      "Train: Epoch [8], Batch [593/938], Loss: 0.699417769908905\n",
      "Train: Epoch [8], Batch [594/938], Loss: 0.8859606981277466\n",
      "Train: Epoch [8], Batch [595/938], Loss: 1.0895670652389526\n",
      "Train: Epoch [8], Batch [596/938], Loss: 0.5786774754524231\n",
      "Train: Epoch [8], Batch [597/938], Loss: 0.8962375521659851\n",
      "Train: Epoch [8], Batch [598/938], Loss: 0.7951747179031372\n",
      "Train: Epoch [8], Batch [599/938], Loss: 0.9714374542236328\n",
      "Train: Epoch [8], Batch [600/938], Loss: 0.9972232580184937\n",
      "Train: Epoch [8], Batch [601/938], Loss: 0.7152246236801147\n",
      "Train: Epoch [8], Batch [602/938], Loss: 0.7462653517723083\n",
      "Train: Epoch [8], Batch [603/938], Loss: 0.6988528966903687\n",
      "Train: Epoch [8], Batch [604/938], Loss: 1.0875612497329712\n",
      "Train: Epoch [8], Batch [605/938], Loss: 0.6590675711631775\n",
      "Train: Epoch [8], Batch [606/938], Loss: 0.9981975555419922\n",
      "Train: Epoch [8], Batch [607/938], Loss: 0.7713105082511902\n",
      "Train: Epoch [8], Batch [608/938], Loss: 0.8104995489120483\n",
      "Train: Epoch [8], Batch [609/938], Loss: 0.7368635535240173\n",
      "Train: Epoch [8], Batch [610/938], Loss: 0.8487866520881653\n",
      "Train: Epoch [8], Batch [611/938], Loss: 0.8285003900527954\n",
      "Train: Epoch [8], Batch [612/938], Loss: 0.6454022526741028\n",
      "Train: Epoch [8], Batch [613/938], Loss: 0.9821979999542236\n",
      "Train: Epoch [8], Batch [614/938], Loss: 0.6446367502212524\n",
      "Train: Epoch [8], Batch [615/938], Loss: 0.9360424280166626\n",
      "Train: Epoch [8], Batch [616/938], Loss: 0.7828515768051147\n",
      "Train: Epoch [8], Batch [617/938], Loss: 0.8951186537742615\n",
      "Train: Epoch [8], Batch [618/938], Loss: 0.8059995770454407\n",
      "Train: Epoch [8], Batch [619/938], Loss: 0.7089783549308777\n",
      "Train: Epoch [8], Batch [620/938], Loss: 0.8868184685707092\n",
      "Train: Epoch [8], Batch [621/938], Loss: 0.9158136248588562\n",
      "Train: Epoch [8], Batch [622/938], Loss: 0.8516502976417542\n",
      "Train: Epoch [8], Batch [623/938], Loss: 0.7301372289657593\n",
      "Train: Epoch [8], Batch [624/938], Loss: 0.721948504447937\n",
      "Train: Epoch [8], Batch [625/938], Loss: 0.7501740455627441\n",
      "Train: Epoch [8], Batch [626/938], Loss: 0.8823291063308716\n",
      "Train: Epoch [8], Batch [627/938], Loss: 1.0705360174179077\n",
      "Train: Epoch [8], Batch [628/938], Loss: 1.126706600189209\n",
      "Train: Epoch [8], Batch [629/938], Loss: 0.9297139644622803\n",
      "Train: Epoch [8], Batch [630/938], Loss: 0.8353590369224548\n",
      "Train: Epoch [8], Batch [631/938], Loss: 0.9028745889663696\n",
      "Train: Epoch [8], Batch [632/938], Loss: 0.9136450290679932\n",
      "Train: Epoch [8], Batch [633/938], Loss: 0.8515424132347107\n",
      "Train: Epoch [8], Batch [634/938], Loss: 0.9006852507591248\n",
      "Train: Epoch [8], Batch [635/938], Loss: 0.8375824689865112\n",
      "Train: Epoch [8], Batch [636/938], Loss: 0.9640572667121887\n",
      "Train: Epoch [8], Batch [637/938], Loss: 0.9362279772758484\n",
      "Train: Epoch [8], Batch [638/938], Loss: 0.8320417404174805\n",
      "Train: Epoch [8], Batch [639/938], Loss: 0.5614375472068787\n",
      "Train: Epoch [8], Batch [640/938], Loss: 0.9173425436019897\n",
      "Train: Epoch [8], Batch [641/938], Loss: 0.8480610251426697\n",
      "Train: Epoch [8], Batch [642/938], Loss: 1.001114010810852\n",
      "Train: Epoch [8], Batch [643/938], Loss: 0.7380658388137817\n",
      "Train: Epoch [8], Batch [644/938], Loss: 0.8712480664253235\n",
      "Train: Epoch [8], Batch [645/938], Loss: 0.7350976467132568\n",
      "Train: Epoch [8], Batch [646/938], Loss: 1.1045682430267334\n",
      "Train: Epoch [8], Batch [647/938], Loss: 0.90223628282547\n",
      "Train: Epoch [8], Batch [648/938], Loss: 0.7515221238136292\n",
      "Train: Epoch [8], Batch [649/938], Loss: 0.8377049565315247\n",
      "Train: Epoch [8], Batch [650/938], Loss: 0.9065688252449036\n",
      "Train: Epoch [8], Batch [651/938], Loss: 0.7474632263183594\n",
      "Train: Epoch [8], Batch [652/938], Loss: 0.724048912525177\n",
      "Train: Epoch [8], Batch [653/938], Loss: 0.8148083686828613\n",
      "Train: Epoch [8], Batch [654/938], Loss: 0.8828362226486206\n",
      "Train: Epoch [8], Batch [655/938], Loss: 1.0725021362304688\n",
      "Train: Epoch [8], Batch [656/938], Loss: 0.7816949486732483\n",
      "Train: Epoch [8], Batch [657/938], Loss: 1.1116684675216675\n",
      "Train: Epoch [8], Batch [658/938], Loss: 0.78971928358078\n",
      "Train: Epoch [8], Batch [659/938], Loss: 0.9709928631782532\n",
      "Train: Epoch [8], Batch [660/938], Loss: 0.9294372797012329\n",
      "Train: Epoch [8], Batch [661/938], Loss: 0.9341170191764832\n",
      "Train: Epoch [8], Batch [662/938], Loss: 0.8316903114318848\n",
      "Train: Epoch [8], Batch [663/938], Loss: 1.0091665983200073\n",
      "Train: Epoch [8], Batch [664/938], Loss: 0.8289802670478821\n",
      "Train: Epoch [8], Batch [665/938], Loss: 0.9488122463226318\n",
      "Train: Epoch [8], Batch [666/938], Loss: 0.7274854779243469\n",
      "Train: Epoch [8], Batch [667/938], Loss: 0.8669355511665344\n",
      "Train: Epoch [8], Batch [668/938], Loss: 0.7324225306510925\n",
      "Train: Epoch [8], Batch [669/938], Loss: 0.9952220916748047\n",
      "Train: Epoch [8], Batch [670/938], Loss: 0.6586329340934753\n",
      "Train: Epoch [8], Batch [671/938], Loss: 0.970294713973999\n",
      "Train: Epoch [8], Batch [672/938], Loss: 0.952564001083374\n",
      "Train: Epoch [8], Batch [673/938], Loss: 0.8841674327850342\n",
      "Train: Epoch [8], Batch [674/938], Loss: 0.9725444316864014\n",
      "Train: Epoch [8], Batch [675/938], Loss: 1.0917918682098389\n",
      "Train: Epoch [8], Batch [676/938], Loss: 0.8466549515724182\n",
      "Train: Epoch [8], Batch [677/938], Loss: 0.9334708452224731\n",
      "Train: Epoch [8], Batch [678/938], Loss: 0.6941792964935303\n",
      "Train: Epoch [8], Batch [679/938], Loss: 0.969990611076355\n",
      "Train: Epoch [8], Batch [680/938], Loss: 0.6923052072525024\n",
      "Train: Epoch [8], Batch [681/938], Loss: 0.7163844704627991\n",
      "Train: Epoch [8], Batch [682/938], Loss: 0.7622119188308716\n",
      "Train: Epoch [8], Batch [683/938], Loss: 0.7937569618225098\n",
      "Train: Epoch [8], Batch [684/938], Loss: 0.7833092212677002\n",
      "Train: Epoch [8], Batch [685/938], Loss: 0.9181280136108398\n",
      "Train: Epoch [8], Batch [686/938], Loss: 0.9918030500411987\n",
      "Train: Epoch [8], Batch [687/938], Loss: 0.7908370494842529\n",
      "Train: Epoch [8], Batch [688/938], Loss: 1.0059285163879395\n",
      "Train: Epoch [8], Batch [689/938], Loss: 0.8996098637580872\n",
      "Train: Epoch [8], Batch [690/938], Loss: 1.240005373954773\n",
      "Train: Epoch [8], Batch [691/938], Loss: 1.0640599727630615\n",
      "Train: Epoch [8], Batch [692/938], Loss: 0.8650791645050049\n",
      "Train: Epoch [8], Batch [693/938], Loss: 0.7600762844085693\n",
      "Train: Epoch [8], Batch [694/938], Loss: 0.78556889295578\n",
      "Train: Epoch [8], Batch [695/938], Loss: 0.9695599675178528\n",
      "Train: Epoch [8], Batch [696/938], Loss: 0.7582810521125793\n",
      "Train: Epoch [8], Batch [697/938], Loss: 0.7204475998878479\n",
      "Train: Epoch [8], Batch [698/938], Loss: 0.9279017448425293\n",
      "Train: Epoch [8], Batch [699/938], Loss: 0.9343001842498779\n",
      "Train: Epoch [8], Batch [700/938], Loss: 0.9156562089920044\n",
      "Train: Epoch [8], Batch [701/938], Loss: 1.0746729373931885\n",
      "Train: Epoch [8], Batch [702/938], Loss: 0.6854485869407654\n",
      "Train: Epoch [8], Batch [703/938], Loss: 0.8866043090820312\n",
      "Train: Epoch [8], Batch [704/938], Loss: 0.8347020745277405\n",
      "Train: Epoch [8], Batch [705/938], Loss: 1.090184211730957\n",
      "Train: Epoch [8], Batch [706/938], Loss: 0.8090701699256897\n",
      "Train: Epoch [8], Batch [707/938], Loss: 0.9029214382171631\n",
      "Train: Epoch [8], Batch [708/938], Loss: 0.728394627571106\n",
      "Train: Epoch [8], Batch [709/938], Loss: 0.978600800037384\n",
      "Train: Epoch [8], Batch [710/938], Loss: 0.745742678642273\n",
      "Train: Epoch [8], Batch [711/938], Loss: 0.7941460609436035\n",
      "Train: Epoch [8], Batch [712/938], Loss: 0.901131808757782\n",
      "Train: Epoch [8], Batch [713/938], Loss: 0.9342919588088989\n",
      "Train: Epoch [8], Batch [714/938], Loss: 0.9930702447891235\n",
      "Train: Epoch [8], Batch [715/938], Loss: 0.7140752077102661\n",
      "Train: Epoch [8], Batch [716/938], Loss: 0.7354810237884521\n",
      "Train: Epoch [8], Batch [717/938], Loss: 0.8209439516067505\n",
      "Train: Epoch [8], Batch [718/938], Loss: 0.8999172449111938\n",
      "Train: Epoch [8], Batch [719/938], Loss: 0.8030543327331543\n",
      "Train: Epoch [8], Batch [720/938], Loss: 1.222489356994629\n",
      "Train: Epoch [8], Batch [721/938], Loss: 0.9645755290985107\n",
      "Train: Epoch [8], Batch [722/938], Loss: 0.998117208480835\n",
      "Train: Epoch [8], Batch [723/938], Loss: 0.7832096219062805\n",
      "Train: Epoch [8], Batch [724/938], Loss: 0.8178924322128296\n",
      "Train: Epoch [8], Batch [725/938], Loss: 0.6480177640914917\n",
      "Train: Epoch [8], Batch [726/938], Loss: 0.8377780318260193\n",
      "Train: Epoch [8], Batch [727/938], Loss: 0.8146777749061584\n",
      "Train: Epoch [8], Batch [728/938], Loss: 0.8287409543991089\n",
      "Train: Epoch [8], Batch [729/938], Loss: 0.6745045781135559\n",
      "Train: Epoch [8], Batch [730/938], Loss: 0.8358590006828308\n",
      "Train: Epoch [8], Batch [731/938], Loss: 0.9088853597640991\n",
      "Train: Epoch [8], Batch [732/938], Loss: 0.7528167963027954\n",
      "Train: Epoch [8], Batch [733/938], Loss: 0.7673407196998596\n",
      "Train: Epoch [8], Batch [734/938], Loss: 0.751071572303772\n",
      "Train: Epoch [8], Batch [735/938], Loss: 0.7712364196777344\n",
      "Train: Epoch [8], Batch [736/938], Loss: 1.0174192190170288\n",
      "Train: Epoch [8], Batch [737/938], Loss: 0.735246479511261\n",
      "Train: Epoch [8], Batch [738/938], Loss: 0.8597816228866577\n",
      "Train: Epoch [8], Batch [739/938], Loss: 0.8488512635231018\n",
      "Train: Epoch [8], Batch [740/938], Loss: 0.5802923440933228\n",
      "Train: Epoch [8], Batch [741/938], Loss: 0.7996490001678467\n",
      "Train: Epoch [8], Batch [742/938], Loss: 1.0519354343414307\n",
      "Train: Epoch [8], Batch [743/938], Loss: 0.7374211549758911\n",
      "Train: Epoch [8], Batch [744/938], Loss: 1.030369520187378\n",
      "Train: Epoch [8], Batch [745/938], Loss: 0.581172525882721\n",
      "Train: Epoch [8], Batch [746/938], Loss: 0.8418154716491699\n",
      "Train: Epoch [8], Batch [747/938], Loss: 0.8171344995498657\n",
      "Train: Epoch [8], Batch [748/938], Loss: 0.9407553672790527\n",
      "Train: Epoch [8], Batch [749/938], Loss: 0.7407697439193726\n",
      "Train: Epoch [8], Batch [750/938], Loss: 0.8100341558456421\n",
      "Train: Epoch [8], Batch [751/938], Loss: 0.7719045877456665\n",
      "Train: Epoch [8], Batch [752/938], Loss: 1.0181324481964111\n",
      "Train: Epoch [8], Batch [753/938], Loss: 0.8042597770690918\n",
      "Train: Epoch [8], Batch [754/938], Loss: 0.677253246307373\n",
      "Train: Epoch [8], Batch [755/938], Loss: 0.7539135217666626\n",
      "Train: Epoch [8], Batch [756/938], Loss: 0.8268797993659973\n",
      "Train: Epoch [8], Batch [757/938], Loss: 0.6652204394340515\n",
      "Train: Epoch [8], Batch [758/938], Loss: 0.858171820640564\n",
      "Train: Epoch [8], Batch [759/938], Loss: 0.9493019580841064\n",
      "Train: Epoch [8], Batch [760/938], Loss: 0.868601381778717\n",
      "Train: Epoch [8], Batch [761/938], Loss: 1.0210341215133667\n",
      "Train: Epoch [8], Batch [762/938], Loss: 0.8303021788597107\n",
      "Train: Epoch [8], Batch [763/938], Loss: 0.5917866826057434\n",
      "Train: Epoch [8], Batch [764/938], Loss: 1.0579161643981934\n",
      "Train: Epoch [8], Batch [765/938], Loss: 0.8781135082244873\n",
      "Train: Epoch [8], Batch [766/938], Loss: 0.7746121883392334\n",
      "Train: Epoch [8], Batch [767/938], Loss: 0.9842444658279419\n",
      "Train: Epoch [8], Batch [768/938], Loss: 0.7632260918617249\n",
      "Train: Epoch [8], Batch [769/938], Loss: 0.6258121728897095\n",
      "Train: Epoch [8], Batch [770/938], Loss: 1.2636394500732422\n",
      "Train: Epoch [8], Batch [771/938], Loss: 0.8046629428863525\n",
      "Train: Epoch [8], Batch [772/938], Loss: 0.7539122700691223\n",
      "Train: Epoch [8], Batch [773/938], Loss: 1.0158886909484863\n",
      "Train: Epoch [8], Batch [774/938], Loss: 0.8745477199554443\n",
      "Train: Epoch [8], Batch [775/938], Loss: 0.6719645261764526\n",
      "Train: Epoch [8], Batch [776/938], Loss: 0.9945276975631714\n",
      "Train: Epoch [8], Batch [777/938], Loss: 0.9423773288726807\n",
      "Train: Epoch [8], Batch [778/938], Loss: 0.7580379843711853\n",
      "Train: Epoch [8], Batch [779/938], Loss: 0.971236526966095\n",
      "Train: Epoch [8], Batch [780/938], Loss: 0.7069202661514282\n",
      "Train: Epoch [8], Batch [781/938], Loss: 0.8587808609008789\n",
      "Train: Epoch [8], Batch [782/938], Loss: 0.9017373323440552\n",
      "Train: Epoch [8], Batch [783/938], Loss: 0.666526734828949\n",
      "Train: Epoch [8], Batch [784/938], Loss: 0.8293219804763794\n",
      "Train: Epoch [8], Batch [785/938], Loss: 0.9285871982574463\n",
      "Train: Epoch [8], Batch [786/938], Loss: 0.8259643316268921\n",
      "Train: Epoch [8], Batch [787/938], Loss: 0.8124234080314636\n",
      "Train: Epoch [8], Batch [788/938], Loss: 0.9068876504898071\n",
      "Train: Epoch [8], Batch [789/938], Loss: 0.8601640462875366\n",
      "Train: Epoch [8], Batch [790/938], Loss: 0.7827479243278503\n",
      "Train: Epoch [8], Batch [791/938], Loss: 1.0298418998718262\n",
      "Train: Epoch [8], Batch [792/938], Loss: 1.0466970205307007\n",
      "Train: Epoch [8], Batch [793/938], Loss: 0.5791522860527039\n",
      "Train: Epoch [8], Batch [794/938], Loss: 0.708259642124176\n",
      "Train: Epoch [8], Batch [795/938], Loss: 0.8132842183113098\n",
      "Train: Epoch [8], Batch [796/938], Loss: 0.9816995859146118\n",
      "Train: Epoch [8], Batch [797/938], Loss: 0.7461704015731812\n",
      "Train: Epoch [8], Batch [798/938], Loss: 0.9104160070419312\n",
      "Train: Epoch [8], Batch [799/938], Loss: 0.8333065509796143\n",
      "Train: Epoch [8], Batch [800/938], Loss: 0.7997467517852783\n",
      "Train: Epoch [8], Batch [801/938], Loss: 0.6538224220275879\n",
      "Train: Epoch [8], Batch [802/938], Loss: 0.7793933153152466\n",
      "Train: Epoch [8], Batch [803/938], Loss: 0.7444765567779541\n",
      "Train: Epoch [8], Batch [804/938], Loss: 0.6667203307151794\n",
      "Train: Epoch [8], Batch [805/938], Loss: 0.7094606757164001\n",
      "Train: Epoch [8], Batch [806/938], Loss: 0.8813157081604004\n",
      "Train: Epoch [8], Batch [807/938], Loss: 0.8435266613960266\n",
      "Train: Epoch [8], Batch [808/938], Loss: 1.1609874963760376\n",
      "Train: Epoch [8], Batch [809/938], Loss: 1.0450557470321655\n",
      "Train: Epoch [8], Batch [810/938], Loss: 0.8820450305938721\n",
      "Train: Epoch [8], Batch [811/938], Loss: 0.883100688457489\n",
      "Train: Epoch [8], Batch [812/938], Loss: 0.6654866933822632\n",
      "Train: Epoch [8], Batch [813/938], Loss: 0.7520290613174438\n",
      "Train: Epoch [8], Batch [814/938], Loss: 0.5790637135505676\n",
      "Train: Epoch [8], Batch [815/938], Loss: 1.0407204627990723\n",
      "Train: Epoch [8], Batch [816/938], Loss: 0.9640306234359741\n",
      "Train: Epoch [8], Batch [817/938], Loss: 1.193577527999878\n",
      "Train: Epoch [8], Batch [818/938], Loss: 0.7064032554626465\n",
      "Train: Epoch [8], Batch [819/938], Loss: 1.032970905303955\n",
      "Train: Epoch [8], Batch [820/938], Loss: 0.7604042887687683\n",
      "Train: Epoch [8], Batch [821/938], Loss: 0.8005837202072144\n",
      "Train: Epoch [8], Batch [822/938], Loss: 0.9835057854652405\n",
      "Train: Epoch [8], Batch [823/938], Loss: 0.8780949115753174\n",
      "Train: Epoch [8], Batch [824/938], Loss: 0.6645082235336304\n",
      "Train: Epoch [8], Batch [825/938], Loss: 0.7887463569641113\n",
      "Train: Epoch [8], Batch [826/938], Loss: 0.9370414614677429\n",
      "Train: Epoch [8], Batch [827/938], Loss: 0.7972027063369751\n",
      "Train: Epoch [8], Batch [828/938], Loss: 0.7919198274612427\n",
      "Train: Epoch [8], Batch [829/938], Loss: 0.7900075912475586\n",
      "Train: Epoch [8], Batch [830/938], Loss: 0.6091485619544983\n",
      "Train: Epoch [8], Batch [831/938], Loss: 0.6649813652038574\n",
      "Train: Epoch [8], Batch [832/938], Loss: 0.9833263158798218\n",
      "Train: Epoch [8], Batch [833/938], Loss: 0.851357102394104\n",
      "Train: Epoch [8], Batch [834/938], Loss: 0.9058685898780823\n",
      "Train: Epoch [8], Batch [835/938], Loss: 0.7773233652114868\n",
      "Train: Epoch [8], Batch [836/938], Loss: 0.6902083158493042\n",
      "Train: Epoch [8], Batch [837/938], Loss: 1.1008158922195435\n",
      "Train: Epoch [8], Batch [838/938], Loss: 0.7742483615875244\n",
      "Train: Epoch [8], Batch [839/938], Loss: 0.6481629610061646\n",
      "Train: Epoch [8], Batch [840/938], Loss: 0.8347268104553223\n",
      "Train: Epoch [8], Batch [841/938], Loss: 0.7122926712036133\n",
      "Train: Epoch [8], Batch [842/938], Loss: 0.7998840808868408\n",
      "Train: Epoch [8], Batch [843/938], Loss: 0.8197473287582397\n",
      "Train: Epoch [8], Batch [844/938], Loss: 1.0120875835418701\n",
      "Train: Epoch [8], Batch [845/938], Loss: 0.7847621440887451\n",
      "Train: Epoch [8], Batch [846/938], Loss: 0.9587658643722534\n",
      "Train: Epoch [8], Batch [847/938], Loss: 0.7892146706581116\n",
      "Train: Epoch [8], Batch [848/938], Loss: 0.906499445438385\n",
      "Train: Epoch [8], Batch [849/938], Loss: 0.7094837427139282\n",
      "Train: Epoch [8], Batch [850/938], Loss: 0.8277124762535095\n",
      "Train: Epoch [8], Batch [851/938], Loss: 0.8931154012680054\n",
      "Train: Epoch [8], Batch [852/938], Loss: 0.7401039600372314\n",
      "Train: Epoch [8], Batch [853/938], Loss: 0.7161276340484619\n",
      "Train: Epoch [8], Batch [854/938], Loss: 0.88003009557724\n",
      "Train: Epoch [8], Batch [855/938], Loss: 0.7356979846954346\n",
      "Train: Epoch [8], Batch [856/938], Loss: 0.927619993686676\n",
      "Train: Epoch [8], Batch [857/938], Loss: 0.7514654994010925\n",
      "Train: Epoch [8], Batch [858/938], Loss: 0.800995409488678\n",
      "Train: Epoch [8], Batch [859/938], Loss: 1.0004194974899292\n",
      "Train: Epoch [8], Batch [860/938], Loss: 0.7073178291320801\n",
      "Train: Epoch [8], Batch [861/938], Loss: 0.7265421152114868\n",
      "Train: Epoch [8], Batch [862/938], Loss: 0.9303185939788818\n",
      "Train: Epoch [8], Batch [863/938], Loss: 0.9181152582168579\n",
      "Train: Epoch [8], Batch [864/938], Loss: 0.7900368571281433\n",
      "Train: Epoch [8], Batch [865/938], Loss: 0.9701949954032898\n",
      "Train: Epoch [8], Batch [866/938], Loss: 0.8547230958938599\n",
      "Train: Epoch [8], Batch [867/938], Loss: 0.7333859205245972\n",
      "Train: Epoch [8], Batch [868/938], Loss: 0.9607253670692444\n",
      "Train: Epoch [8], Batch [869/938], Loss: 0.9068048000335693\n",
      "Train: Epoch [8], Batch [870/938], Loss: 0.7327191829681396\n",
      "Train: Epoch [8], Batch [871/938], Loss: 0.9641534090042114\n",
      "Train: Epoch [8], Batch [872/938], Loss: 1.0888712406158447\n",
      "Train: Epoch [8], Batch [873/938], Loss: 0.9094705581665039\n",
      "Train: Epoch [8], Batch [874/938], Loss: 1.016196846961975\n",
      "Train: Epoch [8], Batch [875/938], Loss: 0.9486851692199707\n",
      "Train: Epoch [8], Batch [876/938], Loss: 0.7400769591331482\n",
      "Train: Epoch [8], Batch [877/938], Loss: 0.7813391089439392\n",
      "Train: Epoch [8], Batch [878/938], Loss: 0.8454524278640747\n",
      "Train: Epoch [8], Batch [879/938], Loss: 0.7131528258323669\n",
      "Train: Epoch [8], Batch [880/938], Loss: 0.8535324335098267\n",
      "Train: Epoch [8], Batch [881/938], Loss: 0.8410033583641052\n",
      "Train: Epoch [8], Batch [882/938], Loss: 0.932287335395813\n",
      "Train: Epoch [8], Batch [883/938], Loss: 0.9008426666259766\n",
      "Train: Epoch [8], Batch [884/938], Loss: 0.8116943836212158\n",
      "Train: Epoch [8], Batch [885/938], Loss: 0.8277454376220703\n",
      "Train: Epoch [8], Batch [886/938], Loss: 0.8862535953521729\n",
      "Train: Epoch [8], Batch [887/938], Loss: 0.6567561626434326\n",
      "Train: Epoch [8], Batch [888/938], Loss: 1.0883811712265015\n",
      "Train: Epoch [8], Batch [889/938], Loss: 0.6180898547172546\n",
      "Train: Epoch [8], Batch [890/938], Loss: 0.9121574759483337\n",
      "Train: Epoch [8], Batch [891/938], Loss: 0.5415029525756836\n",
      "Train: Epoch [8], Batch [892/938], Loss: 0.7180298566818237\n",
      "Train: Epoch [8], Batch [893/938], Loss: 0.8785144090652466\n",
      "Train: Epoch [8], Batch [894/938], Loss: 0.7194281816482544\n",
      "Train: Epoch [8], Batch [895/938], Loss: 0.6588136553764343\n",
      "Train: Epoch [8], Batch [896/938], Loss: 0.9489247798919678\n",
      "Train: Epoch [8], Batch [897/938], Loss: 0.7596253156661987\n",
      "Train: Epoch [8], Batch [898/938], Loss: 0.6422461271286011\n",
      "Train: Epoch [8], Batch [899/938], Loss: 1.0991560220718384\n",
      "Train: Epoch [8], Batch [900/938], Loss: 0.8993473052978516\n",
      "Train: Epoch [8], Batch [901/938], Loss: 0.6851980686187744\n",
      "Train: Epoch [8], Batch [902/938], Loss: 0.9055014848709106\n",
      "Train: Epoch [8], Batch [903/938], Loss: 0.8022304177284241\n",
      "Train: Epoch [8], Batch [904/938], Loss: 0.7092691659927368\n",
      "Train: Epoch [8], Batch [905/938], Loss: 0.9716766476631165\n",
      "Train: Epoch [8], Batch [906/938], Loss: 0.9874793887138367\n",
      "Train: Epoch [8], Batch [907/938], Loss: 0.9303517937660217\n",
      "Train: Epoch [8], Batch [908/938], Loss: 0.7589772939682007\n",
      "Train: Epoch [8], Batch [909/938], Loss: 0.7667298913002014\n",
      "Train: Epoch [8], Batch [910/938], Loss: 0.8793395757675171\n",
      "Train: Epoch [8], Batch [911/938], Loss: 0.7258114814758301\n",
      "Train: Epoch [8], Batch [912/938], Loss: 0.8623409867286682\n",
      "Train: Epoch [8], Batch [913/938], Loss: 0.926096498966217\n",
      "Train: Epoch [8], Batch [914/938], Loss: 0.7913566827774048\n",
      "Train: Epoch [8], Batch [915/938], Loss: 0.9706131219863892\n",
      "Train: Epoch [8], Batch [916/938], Loss: 0.8675979375839233\n",
      "Train: Epoch [8], Batch [917/938], Loss: 0.9047799110412598\n",
      "Train: Epoch [8], Batch [918/938], Loss: 0.6560430526733398\n",
      "Train: Epoch [8], Batch [919/938], Loss: 0.6933303475379944\n",
      "Train: Epoch [8], Batch [920/938], Loss: 0.6561295986175537\n",
      "Train: Epoch [8], Batch [921/938], Loss: 0.8661120533943176\n",
      "Train: Epoch [8], Batch [922/938], Loss: 0.8895663022994995\n",
      "Train: Epoch [8], Batch [923/938], Loss: 0.7753798961639404\n",
      "Train: Epoch [8], Batch [924/938], Loss: 0.8882772922515869\n",
      "Train: Epoch [8], Batch [925/938], Loss: 1.0660991668701172\n",
      "Train: Epoch [8], Batch [926/938], Loss: 0.681064248085022\n",
      "Train: Epoch [8], Batch [927/938], Loss: 0.8368724584579468\n",
      "Train: Epoch [8], Batch [928/938], Loss: 0.6860646605491638\n",
      "Train: Epoch [8], Batch [929/938], Loss: 0.6989669799804688\n",
      "Train: Epoch [8], Batch [930/938], Loss: 0.6928673982620239\n",
      "Train: Epoch [8], Batch [931/938], Loss: 0.7889217138290405\n",
      "Train: Epoch [8], Batch [932/938], Loss: 0.688385546207428\n",
      "Train: Epoch [8], Batch [933/938], Loss: 0.9454368352890015\n",
      "Train: Epoch [8], Batch [934/938], Loss: 1.082784652709961\n",
      "Train: Epoch [8], Batch [935/938], Loss: 0.7731075882911682\n",
      "Train: Epoch [8], Batch [936/938], Loss: 1.007035255432129\n",
      "Train: Epoch [8], Batch [937/938], Loss: 0.8851069808006287\n",
      "Train: Epoch [8], Batch [938/938], Loss: 0.9546180963516235\n",
      "Accuracy of train set: 0.7123333333333334\n",
      "Validation: Epoch [8], Batch [1/938], Loss: 0.910155713558197\n",
      "Validation: Epoch [8], Batch [2/938], Loss: 0.8152149319648743\n",
      "Validation: Epoch [8], Batch [3/938], Loss: 0.6349560022354126\n",
      "Validation: Epoch [8], Batch [4/938], Loss: 0.817692220211029\n",
      "Validation: Epoch [8], Batch [5/938], Loss: 0.8212308883666992\n",
      "Validation: Epoch [8], Batch [6/938], Loss: 1.1072056293487549\n",
      "Validation: Epoch [8], Batch [7/938], Loss: 0.9822006225585938\n",
      "Validation: Epoch [8], Batch [8/938], Loss: 0.7374181151390076\n",
      "Validation: Epoch [8], Batch [9/938], Loss: 0.886881411075592\n",
      "Validation: Epoch [8], Batch [10/938], Loss: 1.1129882335662842\n",
      "Validation: Epoch [8], Batch [11/938], Loss: 0.7059354782104492\n",
      "Validation: Epoch [8], Batch [12/938], Loss: 0.9621012806892395\n",
      "Validation: Epoch [8], Batch [13/938], Loss: 0.7633891701698303\n",
      "Validation: Epoch [8], Batch [14/938], Loss: 0.9598774909973145\n",
      "Validation: Epoch [8], Batch [15/938], Loss: 0.9165077805519104\n",
      "Validation: Epoch [8], Batch [16/938], Loss: 0.8942728042602539\n",
      "Validation: Epoch [8], Batch [17/938], Loss: 0.9513711929321289\n",
      "Validation: Epoch [8], Batch [18/938], Loss: 1.0176104307174683\n",
      "Validation: Epoch [8], Batch [19/938], Loss: 0.9254204034805298\n",
      "Validation: Epoch [8], Batch [20/938], Loss: 0.9502128958702087\n",
      "Validation: Epoch [8], Batch [21/938], Loss: 0.7733460664749146\n",
      "Validation: Epoch [8], Batch [22/938], Loss: 0.8459696769714355\n",
      "Validation: Epoch [8], Batch [23/938], Loss: 0.9642462730407715\n",
      "Validation: Epoch [8], Batch [24/938], Loss: 0.886876106262207\n",
      "Validation: Epoch [8], Batch [25/938], Loss: 0.9988137483596802\n",
      "Validation: Epoch [8], Batch [26/938], Loss: 0.725702166557312\n",
      "Validation: Epoch [8], Batch [27/938], Loss: 0.9271078109741211\n",
      "Validation: Epoch [8], Batch [28/938], Loss: 0.8382154107093811\n",
      "Validation: Epoch [8], Batch [29/938], Loss: 0.8026140928268433\n",
      "Validation: Epoch [8], Batch [30/938], Loss: 0.5418524146080017\n",
      "Validation: Epoch [8], Batch [31/938], Loss: 0.8710677027702332\n",
      "Validation: Epoch [8], Batch [32/938], Loss: 0.6801495552062988\n",
      "Validation: Epoch [8], Batch [33/938], Loss: 1.0460577011108398\n",
      "Validation: Epoch [8], Batch [34/938], Loss: 0.8358618021011353\n",
      "Validation: Epoch [8], Batch [35/938], Loss: 0.7851534485816956\n",
      "Validation: Epoch [8], Batch [36/938], Loss: 0.8545694351196289\n",
      "Validation: Epoch [8], Batch [37/938], Loss: 1.185684084892273\n",
      "Validation: Epoch [8], Batch [38/938], Loss: 0.6462912559509277\n",
      "Validation: Epoch [8], Batch [39/938], Loss: 0.7748680114746094\n",
      "Validation: Epoch [8], Batch [40/938], Loss: 0.8954472541809082\n",
      "Validation: Epoch [8], Batch [41/938], Loss: 1.0593770742416382\n",
      "Validation: Epoch [8], Batch [42/938], Loss: 0.7817226648330688\n",
      "Validation: Epoch [8], Batch [43/938], Loss: 1.0279200077056885\n",
      "Validation: Epoch [8], Batch [44/938], Loss: 0.8438589572906494\n",
      "Validation: Epoch [8], Batch [45/938], Loss: 0.8714085817337036\n",
      "Validation: Epoch [8], Batch [46/938], Loss: 0.859097957611084\n",
      "Validation: Epoch [8], Batch [47/938], Loss: 0.7909821271896362\n",
      "Validation: Epoch [8], Batch [48/938], Loss: 0.8272489905357361\n",
      "Validation: Epoch [8], Batch [49/938], Loss: 0.7187673449516296\n",
      "Validation: Epoch [8], Batch [50/938], Loss: 0.8045414686203003\n",
      "Validation: Epoch [8], Batch [51/938], Loss: 0.8024694323539734\n",
      "Validation: Epoch [8], Batch [52/938], Loss: 0.9228538870811462\n",
      "Validation: Epoch [8], Batch [53/938], Loss: 0.8732658624649048\n",
      "Validation: Epoch [8], Batch [54/938], Loss: 1.0041648149490356\n",
      "Validation: Epoch [8], Batch [55/938], Loss: 0.947067379951477\n",
      "Validation: Epoch [8], Batch [56/938], Loss: 1.0057554244995117\n",
      "Validation: Epoch [8], Batch [57/938], Loss: 0.8605029582977295\n",
      "Validation: Epoch [8], Batch [58/938], Loss: 0.7796875238418579\n",
      "Validation: Epoch [8], Batch [59/938], Loss: 0.9088276624679565\n",
      "Validation: Epoch [8], Batch [60/938], Loss: 0.8437316417694092\n",
      "Validation: Epoch [8], Batch [61/938], Loss: 0.8374477028846741\n",
      "Validation: Epoch [8], Batch [62/938], Loss: 0.8244664072990417\n",
      "Validation: Epoch [8], Batch [63/938], Loss: 0.7344830632209778\n",
      "Validation: Epoch [8], Batch [64/938], Loss: 0.8084858655929565\n",
      "Validation: Epoch [8], Batch [65/938], Loss: 0.7870213985443115\n",
      "Validation: Epoch [8], Batch [66/938], Loss: 0.843277096748352\n",
      "Validation: Epoch [8], Batch [67/938], Loss: 0.8301587104797363\n",
      "Validation: Epoch [8], Batch [68/938], Loss: 0.9426155090332031\n",
      "Validation: Epoch [8], Batch [69/938], Loss: 0.9293997287750244\n",
      "Validation: Epoch [8], Batch [70/938], Loss: 0.7387911081314087\n",
      "Validation: Epoch [8], Batch [71/938], Loss: 1.0015794038772583\n",
      "Validation: Epoch [8], Batch [72/938], Loss: 0.9382321238517761\n",
      "Validation: Epoch [8], Batch [73/938], Loss: 0.731708288192749\n",
      "Validation: Epoch [8], Batch [74/938], Loss: 0.7107194066047668\n",
      "Validation: Epoch [8], Batch [75/938], Loss: 0.8908873796463013\n",
      "Validation: Epoch [8], Batch [76/938], Loss: 0.6861335635185242\n",
      "Validation: Epoch [8], Batch [77/938], Loss: 0.7735023498535156\n",
      "Validation: Epoch [8], Batch [78/938], Loss: 1.0513393878936768\n",
      "Validation: Epoch [8], Batch [79/938], Loss: 0.7934448719024658\n",
      "Validation: Epoch [8], Batch [80/938], Loss: 0.9877306222915649\n",
      "Validation: Epoch [8], Batch [81/938], Loss: 0.7328542470932007\n",
      "Validation: Epoch [8], Batch [82/938], Loss: 0.9085972309112549\n",
      "Validation: Epoch [8], Batch [83/938], Loss: 0.9307073950767517\n",
      "Validation: Epoch [8], Batch [84/938], Loss: 0.9562528133392334\n",
      "Validation: Epoch [8], Batch [85/938], Loss: 0.700954020023346\n",
      "Validation: Epoch [8], Batch [86/938], Loss: 1.0633256435394287\n",
      "Validation: Epoch [8], Batch [87/938], Loss: 0.7614288330078125\n",
      "Validation: Epoch [8], Batch [88/938], Loss: 0.9606395363807678\n",
      "Validation: Epoch [8], Batch [89/938], Loss: 0.8892013430595398\n",
      "Validation: Epoch [8], Batch [90/938], Loss: 1.042319893836975\n",
      "Validation: Epoch [8], Batch [91/938], Loss: 0.827368974685669\n",
      "Validation: Epoch [8], Batch [92/938], Loss: 1.0311479568481445\n",
      "Validation: Epoch [8], Batch [93/938], Loss: 1.0168757438659668\n",
      "Validation: Epoch [8], Batch [94/938], Loss: 0.854071319103241\n",
      "Validation: Epoch [8], Batch [95/938], Loss: 0.7858880162239075\n",
      "Validation: Epoch [8], Batch [96/938], Loss: 1.1185259819030762\n",
      "Validation: Epoch [8], Batch [97/938], Loss: 0.7184783816337585\n",
      "Validation: Epoch [8], Batch [98/938], Loss: 0.8119379281997681\n",
      "Validation: Epoch [8], Batch [99/938], Loss: 0.8801223635673523\n",
      "Validation: Epoch [8], Batch [100/938], Loss: 0.752101719379425\n",
      "Validation: Epoch [8], Batch [101/938], Loss: 0.9737995266914368\n",
      "Validation: Epoch [8], Batch [102/938], Loss: 0.9066118001937866\n",
      "Validation: Epoch [8], Batch [103/938], Loss: 1.0584872961044312\n",
      "Validation: Epoch [8], Batch [104/938], Loss: 0.8126876354217529\n",
      "Validation: Epoch [8], Batch [105/938], Loss: 0.7045259475708008\n",
      "Validation: Epoch [8], Batch [106/938], Loss: 0.7651273012161255\n",
      "Validation: Epoch [8], Batch [107/938], Loss: 0.7838497161865234\n",
      "Validation: Epoch [8], Batch [108/938], Loss: 1.2322834730148315\n",
      "Validation: Epoch [8], Batch [109/938], Loss: 0.8937515020370483\n",
      "Validation: Epoch [8], Batch [110/938], Loss: 0.8415086269378662\n",
      "Validation: Epoch [8], Batch [111/938], Loss: 0.6685872077941895\n",
      "Validation: Epoch [8], Batch [112/938], Loss: 1.0843433141708374\n",
      "Validation: Epoch [8], Batch [113/938], Loss: 0.9036917686462402\n",
      "Validation: Epoch [8], Batch [114/938], Loss: 0.966654360294342\n",
      "Validation: Epoch [8], Batch [115/938], Loss: 0.9997382164001465\n",
      "Validation: Epoch [8], Batch [116/938], Loss: 0.7743949890136719\n",
      "Validation: Epoch [8], Batch [117/938], Loss: 0.9168750643730164\n",
      "Validation: Epoch [8], Batch [118/938], Loss: 0.9408192038536072\n",
      "Validation: Epoch [8], Batch [119/938], Loss: 0.7940462827682495\n",
      "Validation: Epoch [8], Batch [120/938], Loss: 1.211312174797058\n",
      "Validation: Epoch [8], Batch [121/938], Loss: 0.8993632793426514\n",
      "Validation: Epoch [8], Batch [122/938], Loss: 0.8491684198379517\n",
      "Validation: Epoch [8], Batch [123/938], Loss: 0.8579312562942505\n",
      "Validation: Epoch [8], Batch [124/938], Loss: 0.916487455368042\n",
      "Validation: Epoch [8], Batch [125/938], Loss: 0.6966395378112793\n",
      "Validation: Epoch [8], Batch [126/938], Loss: 0.6813059449195862\n",
      "Validation: Epoch [8], Batch [127/938], Loss: 1.0124329328536987\n",
      "Validation: Epoch [8], Batch [128/938], Loss: 0.6742614507675171\n",
      "Validation: Epoch [8], Batch [129/938], Loss: 0.8528264164924622\n",
      "Validation: Epoch [8], Batch [130/938], Loss: 0.8865888118743896\n",
      "Validation: Epoch [8], Batch [131/938], Loss: 0.8245786428451538\n",
      "Validation: Epoch [8], Batch [132/938], Loss: 1.130550503730774\n",
      "Validation: Epoch [8], Batch [133/938], Loss: 0.684542715549469\n",
      "Validation: Epoch [8], Batch [134/938], Loss: 0.7672879099845886\n",
      "Validation: Epoch [8], Batch [135/938], Loss: 0.9469966888427734\n",
      "Validation: Epoch [8], Batch [136/938], Loss: 0.723197340965271\n",
      "Validation: Epoch [8], Batch [137/938], Loss: 0.7033193707466125\n",
      "Validation: Epoch [8], Batch [138/938], Loss: 0.816326916217804\n",
      "Validation: Epoch [8], Batch [139/938], Loss: 0.7642459273338318\n",
      "Validation: Epoch [8], Batch [140/938], Loss: 0.9486421346664429\n",
      "Validation: Epoch [8], Batch [141/938], Loss: 0.9091723561286926\n",
      "Validation: Epoch [8], Batch [142/938], Loss: 0.8130378723144531\n",
      "Validation: Epoch [8], Batch [143/938], Loss: 0.8720799684524536\n",
      "Validation: Epoch [8], Batch [144/938], Loss: 0.7563305497169495\n",
      "Validation: Epoch [8], Batch [145/938], Loss: 0.6855965256690979\n",
      "Validation: Epoch [8], Batch [146/938], Loss: 0.7694442868232727\n",
      "Validation: Epoch [8], Batch [147/938], Loss: 0.8992109894752502\n",
      "Validation: Epoch [8], Batch [148/938], Loss: 0.9798960089683533\n",
      "Validation: Epoch [8], Batch [149/938], Loss: 0.8684182167053223\n",
      "Validation: Epoch [8], Batch [150/938], Loss: 0.7589282989501953\n",
      "Validation: Epoch [8], Batch [151/938], Loss: 0.8468549251556396\n",
      "Validation: Epoch [8], Batch [152/938], Loss: 0.6569886207580566\n",
      "Validation: Epoch [8], Batch [153/938], Loss: 1.0590115785598755\n",
      "Validation: Epoch [8], Batch [154/938], Loss: 1.0908344984054565\n",
      "Validation: Epoch [8], Batch [155/938], Loss: 0.6643921136856079\n",
      "Validation: Epoch [8], Batch [156/938], Loss: 0.8595976233482361\n",
      "Validation: Epoch [8], Batch [157/938], Loss: 0.7548103928565979\n",
      "Validation: Epoch [8], Batch [158/938], Loss: 0.8447985053062439\n",
      "Validation: Epoch [8], Batch [159/938], Loss: 0.8252854347229004\n",
      "Validation: Epoch [8], Batch [160/938], Loss: 0.7363636493682861\n",
      "Validation: Epoch [8], Batch [161/938], Loss: 0.6046497225761414\n",
      "Validation: Epoch [8], Batch [162/938], Loss: 1.004845142364502\n",
      "Validation: Epoch [8], Batch [163/938], Loss: 0.9684746265411377\n",
      "Validation: Epoch [8], Batch [164/938], Loss: 0.7673726677894592\n",
      "Validation: Epoch [8], Batch [165/938], Loss: 0.9359697699546814\n",
      "Validation: Epoch [8], Batch [166/938], Loss: 1.0010783672332764\n",
      "Validation: Epoch [8], Batch [167/938], Loss: 0.8113471269607544\n",
      "Validation: Epoch [8], Batch [168/938], Loss: 1.0363974571228027\n",
      "Validation: Epoch [8], Batch [169/938], Loss: 0.6891787052154541\n",
      "Validation: Epoch [8], Batch [170/938], Loss: 0.7849373817443848\n",
      "Validation: Epoch [8], Batch [171/938], Loss: 0.7468640804290771\n",
      "Validation: Epoch [8], Batch [172/938], Loss: 0.8184552788734436\n",
      "Validation: Epoch [8], Batch [173/938], Loss: 0.8090304136276245\n",
      "Validation: Epoch [8], Batch [174/938], Loss: 0.9499320983886719\n",
      "Validation: Epoch [8], Batch [175/938], Loss: 0.9652712345123291\n",
      "Validation: Epoch [8], Batch [176/938], Loss: 0.7981014251708984\n",
      "Validation: Epoch [8], Batch [177/938], Loss: 0.8033578395843506\n",
      "Validation: Epoch [8], Batch [178/938], Loss: 1.0230937004089355\n",
      "Validation: Epoch [8], Batch [179/938], Loss: 1.1203972101211548\n",
      "Validation: Epoch [8], Batch [180/938], Loss: 0.8556429147720337\n",
      "Validation: Epoch [8], Batch [181/938], Loss: 0.6995956897735596\n",
      "Validation: Epoch [8], Batch [182/938], Loss: 0.704535722732544\n",
      "Validation: Epoch [8], Batch [183/938], Loss: 0.821183443069458\n",
      "Validation: Epoch [8], Batch [184/938], Loss: 0.5696032643318176\n",
      "Validation: Epoch [8], Batch [185/938], Loss: 0.8190328478813171\n",
      "Validation: Epoch [8], Batch [186/938], Loss: 0.7072489857673645\n",
      "Validation: Epoch [8], Batch [187/938], Loss: 1.0106630325317383\n",
      "Validation: Epoch [8], Batch [188/938], Loss: 0.7298001050949097\n",
      "Validation: Epoch [8], Batch [189/938], Loss: 0.8251683712005615\n",
      "Validation: Epoch [8], Batch [190/938], Loss: 0.904630184173584\n",
      "Validation: Epoch [8], Batch [191/938], Loss: 0.8861382007598877\n",
      "Validation: Epoch [8], Batch [192/938], Loss: 0.8518247604370117\n",
      "Validation: Epoch [8], Batch [193/938], Loss: 0.8067277073860168\n",
      "Validation: Epoch [8], Batch [194/938], Loss: 0.9424652457237244\n",
      "Validation: Epoch [8], Batch [195/938], Loss: 0.8354308605194092\n",
      "Validation: Epoch [8], Batch [196/938], Loss: 0.5902040600776672\n",
      "Validation: Epoch [8], Batch [197/938], Loss: 1.025187611579895\n",
      "Validation: Epoch [8], Batch [198/938], Loss: 0.6365571618080139\n",
      "Validation: Epoch [8], Batch [199/938], Loss: 0.8610642552375793\n",
      "Validation: Epoch [8], Batch [200/938], Loss: 0.7066068649291992\n",
      "Validation: Epoch [8], Batch [201/938], Loss: 0.8608280420303345\n",
      "Validation: Epoch [8], Batch [202/938], Loss: 0.7573200464248657\n",
      "Validation: Epoch [8], Batch [203/938], Loss: 0.8303074836730957\n",
      "Validation: Epoch [8], Batch [204/938], Loss: 0.5524252653121948\n",
      "Validation: Epoch [8], Batch [205/938], Loss: 0.8670846223831177\n",
      "Validation: Epoch [8], Batch [206/938], Loss: 0.8879705667495728\n",
      "Validation: Epoch [8], Batch [207/938], Loss: 0.9374041557312012\n",
      "Validation: Epoch [8], Batch [208/938], Loss: 0.8712040185928345\n",
      "Validation: Epoch [8], Batch [209/938], Loss: 0.8134331107139587\n",
      "Validation: Epoch [8], Batch [210/938], Loss: 0.5925490856170654\n",
      "Validation: Epoch [8], Batch [211/938], Loss: 0.6725410223007202\n",
      "Validation: Epoch [8], Batch [212/938], Loss: 1.1648821830749512\n",
      "Validation: Epoch [8], Batch [213/938], Loss: 0.8545207381248474\n",
      "Validation: Epoch [8], Batch [214/938], Loss: 1.025850772857666\n",
      "Validation: Epoch [8], Batch [215/938], Loss: 1.1635851860046387\n",
      "Validation: Epoch [8], Batch [216/938], Loss: 0.7159075140953064\n",
      "Validation: Epoch [8], Batch [217/938], Loss: 1.0514799356460571\n",
      "Validation: Epoch [8], Batch [218/938], Loss: 0.7534682154655457\n",
      "Validation: Epoch [8], Batch [219/938], Loss: 0.7568671703338623\n",
      "Validation: Epoch [8], Batch [220/938], Loss: 0.8247178792953491\n",
      "Validation: Epoch [8], Batch [221/938], Loss: 0.7042089700698853\n",
      "Validation: Epoch [8], Batch [222/938], Loss: 0.7174575328826904\n",
      "Validation: Epoch [8], Batch [223/938], Loss: 0.9920139312744141\n",
      "Validation: Epoch [8], Batch [224/938], Loss: 0.5678633451461792\n",
      "Validation: Epoch [8], Batch [225/938], Loss: 0.7982413172721863\n",
      "Validation: Epoch [8], Batch [226/938], Loss: 0.82566899061203\n",
      "Validation: Epoch [8], Batch [227/938], Loss: 0.8280873894691467\n",
      "Validation: Epoch [8], Batch [228/938], Loss: 0.5769603252410889\n",
      "Validation: Epoch [8], Batch [229/938], Loss: 0.9272167086601257\n",
      "Validation: Epoch [8], Batch [230/938], Loss: 0.8757286071777344\n",
      "Validation: Epoch [8], Batch [231/938], Loss: 0.8646844625473022\n",
      "Validation: Epoch [8], Batch [232/938], Loss: 0.7355561852455139\n",
      "Validation: Epoch [8], Batch [233/938], Loss: 0.9833601713180542\n",
      "Validation: Epoch [8], Batch [234/938], Loss: 0.8103148341178894\n",
      "Validation: Epoch [8], Batch [235/938], Loss: 0.788349449634552\n",
      "Validation: Epoch [8], Batch [236/938], Loss: 0.8140137195587158\n",
      "Validation: Epoch [8], Batch [237/938], Loss: 0.6770106554031372\n",
      "Validation: Epoch [8], Batch [238/938], Loss: 0.8901355862617493\n",
      "Validation: Epoch [8], Batch [239/938], Loss: 0.9492520689964294\n",
      "Validation: Epoch [8], Batch [240/938], Loss: 0.582425057888031\n",
      "Validation: Epoch [8], Batch [241/938], Loss: 0.779122531414032\n",
      "Validation: Epoch [8], Batch [242/938], Loss: 0.6811832189559937\n",
      "Validation: Epoch [8], Batch [243/938], Loss: 0.9281030297279358\n",
      "Validation: Epoch [8], Batch [244/938], Loss: 0.7851569056510925\n",
      "Validation: Epoch [8], Batch [245/938], Loss: 0.9105705618858337\n",
      "Validation: Epoch [8], Batch [246/938], Loss: 0.7325375080108643\n",
      "Validation: Epoch [8], Batch [247/938], Loss: 0.7774348258972168\n",
      "Validation: Epoch [8], Batch [248/938], Loss: 0.8485814332962036\n",
      "Validation: Epoch [8], Batch [249/938], Loss: 0.9588557481765747\n",
      "Validation: Epoch [8], Batch [250/938], Loss: 0.7674013376235962\n",
      "Validation: Epoch [8], Batch [251/938], Loss: 0.7749724388122559\n",
      "Validation: Epoch [8], Batch [252/938], Loss: 0.771813154220581\n",
      "Validation: Epoch [8], Batch [253/938], Loss: 0.739241361618042\n",
      "Validation: Epoch [8], Batch [254/938], Loss: 0.646388053894043\n",
      "Validation: Epoch [8], Batch [255/938], Loss: 0.9313421249389648\n",
      "Validation: Epoch [8], Batch [256/938], Loss: 0.8840515613555908\n",
      "Validation: Epoch [8], Batch [257/938], Loss: 0.6819967031478882\n",
      "Validation: Epoch [8], Batch [258/938], Loss: 0.8982640504837036\n",
      "Validation: Epoch [8], Batch [259/938], Loss: 0.6755757331848145\n",
      "Validation: Epoch [8], Batch [260/938], Loss: 1.0595108270645142\n",
      "Validation: Epoch [8], Batch [261/938], Loss: 0.6703886985778809\n",
      "Validation: Epoch [8], Batch [262/938], Loss: 1.058096170425415\n",
      "Validation: Epoch [8], Batch [263/938], Loss: 0.8909722566604614\n",
      "Validation: Epoch [8], Batch [264/938], Loss: 1.0552139282226562\n",
      "Validation: Epoch [8], Batch [265/938], Loss: 0.7635595202445984\n",
      "Validation: Epoch [8], Batch [266/938], Loss: 0.8628917932510376\n",
      "Validation: Epoch [8], Batch [267/938], Loss: 0.992385983467102\n",
      "Validation: Epoch [8], Batch [268/938], Loss: 0.8704351782798767\n",
      "Validation: Epoch [8], Batch [269/938], Loss: 0.857787549495697\n",
      "Validation: Epoch [8], Batch [270/938], Loss: 0.7672979235649109\n",
      "Validation: Epoch [8], Batch [271/938], Loss: 0.8640114665031433\n",
      "Validation: Epoch [8], Batch [272/938], Loss: 0.9002399444580078\n",
      "Validation: Epoch [8], Batch [273/938], Loss: 0.7931662201881409\n",
      "Validation: Epoch [8], Batch [274/938], Loss: 0.8266862034797668\n",
      "Validation: Epoch [8], Batch [275/938], Loss: 0.8948095440864563\n",
      "Validation: Epoch [8], Batch [276/938], Loss: 0.8323373794555664\n",
      "Validation: Epoch [8], Batch [277/938], Loss: 0.7556943297386169\n",
      "Validation: Epoch [8], Batch [278/938], Loss: 1.1303331851959229\n",
      "Validation: Epoch [8], Batch [279/938], Loss: 1.0121420621871948\n",
      "Validation: Epoch [8], Batch [280/938], Loss: 1.0007376670837402\n",
      "Validation: Epoch [8], Batch [281/938], Loss: 1.0242347717285156\n",
      "Validation: Epoch [8], Batch [282/938], Loss: 0.7946146130561829\n",
      "Validation: Epoch [8], Batch [283/938], Loss: 0.8468371033668518\n",
      "Validation: Epoch [8], Batch [284/938], Loss: 0.8953438401222229\n",
      "Validation: Epoch [8], Batch [285/938], Loss: 0.8159360885620117\n",
      "Validation: Epoch [8], Batch [286/938], Loss: 1.0729970932006836\n",
      "Validation: Epoch [8], Batch [287/938], Loss: 0.8655956983566284\n",
      "Validation: Epoch [8], Batch [288/938], Loss: 0.7819653153419495\n",
      "Validation: Epoch [8], Batch [289/938], Loss: 1.1991299390792847\n",
      "Validation: Epoch [8], Batch [290/938], Loss: 0.7743090391159058\n",
      "Validation: Epoch [8], Batch [291/938], Loss: 0.8149580955505371\n",
      "Validation: Epoch [8], Batch [292/938], Loss: 0.6628206968307495\n",
      "Validation: Epoch [8], Batch [293/938], Loss: 1.1716229915618896\n",
      "Validation: Epoch [8], Batch [294/938], Loss: 0.9822030067443848\n",
      "Validation: Epoch [8], Batch [295/938], Loss: 0.7299842238426208\n",
      "Validation: Epoch [8], Batch [296/938], Loss: 0.668764591217041\n",
      "Validation: Epoch [8], Batch [297/938], Loss: 1.0987441539764404\n",
      "Validation: Epoch [8], Batch [298/938], Loss: 0.9725788831710815\n",
      "Validation: Epoch [8], Batch [299/938], Loss: 1.0254230499267578\n",
      "Validation: Epoch [8], Batch [300/938], Loss: 0.6846031546592712\n",
      "Validation: Epoch [8], Batch [301/938], Loss: 0.8378359079360962\n",
      "Validation: Epoch [8], Batch [302/938], Loss: 0.6459909677505493\n",
      "Validation: Epoch [8], Batch [303/938], Loss: 0.8671684265136719\n",
      "Validation: Epoch [8], Batch [304/938], Loss: 0.7243651747703552\n",
      "Validation: Epoch [8], Batch [305/938], Loss: 0.5754807591438293\n",
      "Validation: Epoch [8], Batch [306/938], Loss: 0.9360798597335815\n",
      "Validation: Epoch [8], Batch [307/938], Loss: 0.7611969709396362\n",
      "Validation: Epoch [8], Batch [308/938], Loss: 0.7918395400047302\n",
      "Validation: Epoch [8], Batch [309/938], Loss: 1.001985788345337\n",
      "Validation: Epoch [8], Batch [310/938], Loss: 0.754776656627655\n",
      "Validation: Epoch [8], Batch [311/938], Loss: 0.9158471822738647\n",
      "Validation: Epoch [8], Batch [312/938], Loss: 0.8832752108573914\n",
      "Validation: Epoch [8], Batch [313/938], Loss: 0.9323276281356812\n",
      "Validation: Epoch [8], Batch [314/938], Loss: 0.5744277238845825\n",
      "Validation: Epoch [8], Batch [315/938], Loss: 0.8881028890609741\n",
      "Validation: Epoch [8], Batch [316/938], Loss: 0.9406459331512451\n",
      "Validation: Epoch [8], Batch [317/938], Loss: 0.8436991572380066\n",
      "Validation: Epoch [8], Batch [318/938], Loss: 0.7485867142677307\n",
      "Validation: Epoch [8], Batch [319/938], Loss: 0.7264178991317749\n",
      "Validation: Epoch [8], Batch [320/938], Loss: 0.8594364523887634\n",
      "Validation: Epoch [8], Batch [321/938], Loss: 0.7209509611129761\n",
      "Validation: Epoch [8], Batch [322/938], Loss: 0.8318644165992737\n",
      "Validation: Epoch [8], Batch [323/938], Loss: 0.7866888642311096\n",
      "Validation: Epoch [8], Batch [324/938], Loss: 0.9496922492980957\n",
      "Validation: Epoch [8], Batch [325/938], Loss: 0.8661577701568604\n",
      "Validation: Epoch [8], Batch [326/938], Loss: 0.8432644009590149\n",
      "Validation: Epoch [8], Batch [327/938], Loss: 1.0027610063552856\n",
      "Validation: Epoch [8], Batch [328/938], Loss: 0.8522013425827026\n",
      "Validation: Epoch [8], Batch [329/938], Loss: 0.9010515809059143\n",
      "Validation: Epoch [8], Batch [330/938], Loss: 0.8117192983627319\n",
      "Validation: Epoch [8], Batch [331/938], Loss: 0.712002158164978\n",
      "Validation: Epoch [8], Batch [332/938], Loss: 0.777758777141571\n",
      "Validation: Epoch [8], Batch [333/938], Loss: 0.6260362267494202\n",
      "Validation: Epoch [8], Batch [334/938], Loss: 0.916945219039917\n",
      "Validation: Epoch [8], Batch [335/938], Loss: 0.8924607634544373\n",
      "Validation: Epoch [8], Batch [336/938], Loss: 1.0822727680206299\n",
      "Validation: Epoch [8], Batch [337/938], Loss: 0.7884539365768433\n",
      "Validation: Epoch [8], Batch [338/938], Loss: 0.7705947160720825\n",
      "Validation: Epoch [8], Batch [339/938], Loss: 0.9074242115020752\n",
      "Validation: Epoch [8], Batch [340/938], Loss: 1.0674490928649902\n",
      "Validation: Epoch [8], Batch [341/938], Loss: 1.0575320720672607\n",
      "Validation: Epoch [8], Batch [342/938], Loss: 0.6950922012329102\n",
      "Validation: Epoch [8], Batch [343/938], Loss: 0.8267220258712769\n",
      "Validation: Epoch [8], Batch [344/938], Loss: 0.7793128490447998\n",
      "Validation: Epoch [8], Batch [345/938], Loss: 0.9163546562194824\n",
      "Validation: Epoch [8], Batch [346/938], Loss: 0.9263047575950623\n",
      "Validation: Epoch [8], Batch [347/938], Loss: 0.8576657772064209\n",
      "Validation: Epoch [8], Batch [348/938], Loss: 0.9057439565658569\n",
      "Validation: Epoch [8], Batch [349/938], Loss: 0.7768095135688782\n",
      "Validation: Epoch [8], Batch [350/938], Loss: 0.8971850275993347\n",
      "Validation: Epoch [8], Batch [351/938], Loss: 0.9815634489059448\n",
      "Validation: Epoch [8], Batch [352/938], Loss: 0.8088352084159851\n",
      "Validation: Epoch [8], Batch [353/938], Loss: 0.7785391807556152\n",
      "Validation: Epoch [8], Batch [354/938], Loss: 0.9797606468200684\n",
      "Validation: Epoch [8], Batch [355/938], Loss: 0.7650764584541321\n",
      "Validation: Epoch [8], Batch [356/938], Loss: 0.9843328595161438\n",
      "Validation: Epoch [8], Batch [357/938], Loss: 0.7042956352233887\n",
      "Validation: Epoch [8], Batch [358/938], Loss: 0.6488412618637085\n",
      "Validation: Epoch [8], Batch [359/938], Loss: 0.8497317433357239\n",
      "Validation: Epoch [8], Batch [360/938], Loss: 0.7203819751739502\n",
      "Validation: Epoch [8], Batch [361/938], Loss: 0.7983633875846863\n",
      "Validation: Epoch [8], Batch [362/938], Loss: 0.7665750980377197\n",
      "Validation: Epoch [8], Batch [363/938], Loss: 0.895500898361206\n",
      "Validation: Epoch [8], Batch [364/938], Loss: 0.9875253438949585\n",
      "Validation: Epoch [8], Batch [365/938], Loss: 0.8668460845947266\n",
      "Validation: Epoch [8], Batch [366/938], Loss: 0.8638187646865845\n",
      "Validation: Epoch [8], Batch [367/938], Loss: 0.7808532118797302\n",
      "Validation: Epoch [8], Batch [368/938], Loss: 0.814254879951477\n",
      "Validation: Epoch [8], Batch [369/938], Loss: 0.7247737646102905\n",
      "Validation: Epoch [8], Batch [370/938], Loss: 0.9030434489250183\n",
      "Validation: Epoch [8], Batch [371/938], Loss: 0.6434726715087891\n",
      "Validation: Epoch [8], Batch [372/938], Loss: 0.7487419843673706\n",
      "Validation: Epoch [8], Batch [373/938], Loss: 0.7518503665924072\n",
      "Validation: Epoch [8], Batch [374/938], Loss: 0.7864542603492737\n",
      "Validation: Epoch [8], Batch [375/938], Loss: 0.6560083031654358\n",
      "Validation: Epoch [8], Batch [376/938], Loss: 0.7668564319610596\n",
      "Validation: Epoch [8], Batch [377/938], Loss: 0.9496448636054993\n",
      "Validation: Epoch [8], Batch [378/938], Loss: 0.9694687128067017\n",
      "Validation: Epoch [8], Batch [379/938], Loss: 0.712705671787262\n",
      "Validation: Epoch [8], Batch [380/938], Loss: 1.081600546836853\n",
      "Validation: Epoch [8], Batch [381/938], Loss: 0.8841651678085327\n",
      "Validation: Epoch [8], Batch [382/938], Loss: 0.7948209047317505\n",
      "Validation: Epoch [8], Batch [383/938], Loss: 1.0222574472427368\n",
      "Validation: Epoch [8], Batch [384/938], Loss: 0.9353980422019958\n",
      "Validation: Epoch [8], Batch [385/938], Loss: 1.139194369316101\n",
      "Validation: Epoch [8], Batch [386/938], Loss: 0.8782334327697754\n",
      "Validation: Epoch [8], Batch [387/938], Loss: 0.7953611612319946\n",
      "Validation: Epoch [8], Batch [388/938], Loss: 0.8555834293365479\n",
      "Validation: Epoch [8], Batch [389/938], Loss: 0.7528550624847412\n",
      "Validation: Epoch [8], Batch [390/938], Loss: 0.7671027183532715\n",
      "Validation: Epoch [8], Batch [391/938], Loss: 0.7450584173202515\n",
      "Validation: Epoch [8], Batch [392/938], Loss: 0.8120319843292236\n",
      "Validation: Epoch [8], Batch [393/938], Loss: 0.6319084763526917\n",
      "Validation: Epoch [8], Batch [394/938], Loss: 0.6214185357093811\n",
      "Validation: Epoch [8], Batch [395/938], Loss: 0.670897364616394\n",
      "Validation: Epoch [8], Batch [396/938], Loss: 0.7992042899131775\n",
      "Validation: Epoch [8], Batch [397/938], Loss: 0.5533567070960999\n",
      "Validation: Epoch [8], Batch [398/938], Loss: 0.7680474519729614\n",
      "Validation: Epoch [8], Batch [399/938], Loss: 0.797156035900116\n",
      "Validation: Epoch [8], Batch [400/938], Loss: 0.8669521808624268\n",
      "Validation: Epoch [8], Batch [401/938], Loss: 0.6860638856887817\n",
      "Validation: Epoch [8], Batch [402/938], Loss: 0.883281946182251\n",
      "Validation: Epoch [8], Batch [403/938], Loss: 0.7450705170631409\n",
      "Validation: Epoch [8], Batch [404/938], Loss: 0.8093242645263672\n",
      "Validation: Epoch [8], Batch [405/938], Loss: 0.7564904093742371\n",
      "Validation: Epoch [8], Batch [406/938], Loss: 0.7910376787185669\n",
      "Validation: Epoch [8], Batch [407/938], Loss: 0.7818203568458557\n",
      "Validation: Epoch [8], Batch [408/938], Loss: 0.8201316595077515\n",
      "Validation: Epoch [8], Batch [409/938], Loss: 0.9045364856719971\n",
      "Validation: Epoch [8], Batch [410/938], Loss: 1.0319401025772095\n",
      "Validation: Epoch [8], Batch [411/938], Loss: 0.8315497636795044\n",
      "Validation: Epoch [8], Batch [412/938], Loss: 0.8536602258682251\n",
      "Validation: Epoch [8], Batch [413/938], Loss: 1.0072205066680908\n",
      "Validation: Epoch [8], Batch [414/938], Loss: 0.7926074266433716\n",
      "Validation: Epoch [8], Batch [415/938], Loss: 0.8943809270858765\n",
      "Validation: Epoch [8], Batch [416/938], Loss: 0.6988276243209839\n",
      "Validation: Epoch [8], Batch [417/938], Loss: 0.7485284805297852\n",
      "Validation: Epoch [8], Batch [418/938], Loss: 0.8372699022293091\n",
      "Validation: Epoch [8], Batch [419/938], Loss: 0.9468371272087097\n",
      "Validation: Epoch [8], Batch [420/938], Loss: 0.7256000638008118\n",
      "Validation: Epoch [8], Batch [421/938], Loss: 0.778510332107544\n",
      "Validation: Epoch [8], Batch [422/938], Loss: 0.9436285495758057\n",
      "Validation: Epoch [8], Batch [423/938], Loss: 0.853649914264679\n",
      "Validation: Epoch [8], Batch [424/938], Loss: 0.8475585579872131\n",
      "Validation: Epoch [8], Batch [425/938], Loss: 0.588058590888977\n",
      "Validation: Epoch [8], Batch [426/938], Loss: 1.0172539949417114\n",
      "Validation: Epoch [8], Batch [427/938], Loss: 0.9637107849121094\n",
      "Validation: Epoch [8], Batch [428/938], Loss: 0.9305018186569214\n",
      "Validation: Epoch [8], Batch [429/938], Loss: 0.9038109183311462\n",
      "Validation: Epoch [8], Batch [430/938], Loss: 0.894439160823822\n",
      "Validation: Epoch [8], Batch [431/938], Loss: 0.7904936075210571\n",
      "Validation: Epoch [8], Batch [432/938], Loss: 0.5777294039726257\n",
      "Validation: Epoch [8], Batch [433/938], Loss: 0.690994381904602\n",
      "Validation: Epoch [8], Batch [434/938], Loss: 1.0556997060775757\n",
      "Validation: Epoch [8], Batch [435/938], Loss: 0.893128514289856\n",
      "Validation: Epoch [8], Batch [436/938], Loss: 0.7119191884994507\n",
      "Validation: Epoch [8], Batch [437/938], Loss: 0.7435610890388489\n",
      "Validation: Epoch [8], Batch [438/938], Loss: 0.8214511275291443\n",
      "Validation: Epoch [8], Batch [439/938], Loss: 0.9930023550987244\n",
      "Validation: Epoch [8], Batch [440/938], Loss: 0.9514582753181458\n",
      "Validation: Epoch [8], Batch [441/938], Loss: 0.8619785904884338\n",
      "Validation: Epoch [8], Batch [442/938], Loss: 0.7678087949752808\n",
      "Validation: Epoch [8], Batch [443/938], Loss: 0.8500266671180725\n",
      "Validation: Epoch [8], Batch [444/938], Loss: 0.9969523549079895\n",
      "Validation: Epoch [8], Batch [445/938], Loss: 0.8663010597229004\n",
      "Validation: Epoch [8], Batch [446/938], Loss: 0.9047912359237671\n",
      "Validation: Epoch [8], Batch [447/938], Loss: 0.8076809644699097\n",
      "Validation: Epoch [8], Batch [448/938], Loss: 0.7858003973960876\n",
      "Validation: Epoch [8], Batch [449/938], Loss: 0.7689651846885681\n",
      "Validation: Epoch [8], Batch [450/938], Loss: 0.8231356143951416\n",
      "Validation: Epoch [8], Batch [451/938], Loss: 0.8146817088127136\n",
      "Validation: Epoch [8], Batch [452/938], Loss: 0.6276352405548096\n",
      "Validation: Epoch [8], Batch [453/938], Loss: 0.8511093258857727\n",
      "Validation: Epoch [8], Batch [454/938], Loss: 0.7650351524353027\n",
      "Validation: Epoch [8], Batch [455/938], Loss: 1.0169564485549927\n",
      "Validation: Epoch [8], Batch [456/938], Loss: 0.912491500377655\n",
      "Validation: Epoch [8], Batch [457/938], Loss: 0.8483825922012329\n",
      "Validation: Epoch [8], Batch [458/938], Loss: 0.5636919140815735\n",
      "Validation: Epoch [8], Batch [459/938], Loss: 0.8771213889122009\n",
      "Validation: Epoch [8], Batch [460/938], Loss: 0.742922306060791\n",
      "Validation: Epoch [8], Batch [461/938], Loss: 0.732677161693573\n",
      "Validation: Epoch [8], Batch [462/938], Loss: 0.77947598695755\n",
      "Validation: Epoch [8], Batch [463/938], Loss: 0.6502786874771118\n",
      "Validation: Epoch [8], Batch [464/938], Loss: 0.9220178127288818\n",
      "Validation: Epoch [8], Batch [465/938], Loss: 0.9157885313034058\n",
      "Validation: Epoch [8], Batch [466/938], Loss: 0.7471591234207153\n",
      "Validation: Epoch [8], Batch [467/938], Loss: 0.8334967494010925\n",
      "Validation: Epoch [8], Batch [468/938], Loss: 0.8801451921463013\n",
      "Validation: Epoch [8], Batch [469/938], Loss: 0.9822782278060913\n",
      "Validation: Epoch [8], Batch [470/938], Loss: 0.8608000874519348\n",
      "Validation: Epoch [8], Batch [471/938], Loss: 1.0452696084976196\n",
      "Validation: Epoch [8], Batch [472/938], Loss: 0.9529374241828918\n",
      "Validation: Epoch [8], Batch [473/938], Loss: 0.653270959854126\n",
      "Validation: Epoch [8], Batch [474/938], Loss: 0.7755727767944336\n",
      "Validation: Epoch [8], Batch [475/938], Loss: 0.7685467600822449\n",
      "Validation: Epoch [8], Batch [476/938], Loss: 0.8400461673736572\n",
      "Validation: Epoch [8], Batch [477/938], Loss: 0.8098340034484863\n",
      "Validation: Epoch [8], Batch [478/938], Loss: 0.857761561870575\n",
      "Validation: Epoch [8], Batch [479/938], Loss: 0.8302021026611328\n",
      "Validation: Epoch [8], Batch [480/938], Loss: 0.8649089336395264\n",
      "Validation: Epoch [8], Batch [481/938], Loss: 0.6199486255645752\n",
      "Validation: Epoch [8], Batch [482/938], Loss: 0.9723895788192749\n",
      "Validation: Epoch [8], Batch [483/938], Loss: 0.9007682800292969\n",
      "Validation: Epoch [8], Batch [484/938], Loss: 0.7669379711151123\n",
      "Validation: Epoch [8], Batch [485/938], Loss: 0.9370066523551941\n",
      "Validation: Epoch [8], Batch [486/938], Loss: 1.1253478527069092\n",
      "Validation: Epoch [8], Batch [487/938], Loss: 0.9959965944290161\n",
      "Validation: Epoch [8], Batch [488/938], Loss: 0.8905543088912964\n",
      "Validation: Epoch [8], Batch [489/938], Loss: 0.7342984676361084\n",
      "Validation: Epoch [8], Batch [490/938], Loss: 0.7550718784332275\n",
      "Validation: Epoch [8], Batch [491/938], Loss: 0.9798853397369385\n",
      "Validation: Epoch [8], Batch [492/938], Loss: 0.8998137712478638\n",
      "Validation: Epoch [8], Batch [493/938], Loss: 0.7626150846481323\n",
      "Validation: Epoch [8], Batch [494/938], Loss: 0.7111238837242126\n",
      "Validation: Epoch [8], Batch [495/938], Loss: 0.7777490019798279\n",
      "Validation: Epoch [8], Batch [496/938], Loss: 0.7590252757072449\n",
      "Validation: Epoch [8], Batch [497/938], Loss: 0.6418217420578003\n",
      "Validation: Epoch [8], Batch [498/938], Loss: 0.5890899896621704\n",
      "Validation: Epoch [8], Batch [499/938], Loss: 0.7177202105522156\n",
      "Validation: Epoch [8], Batch [500/938], Loss: 0.7757734060287476\n",
      "Validation: Epoch [8], Batch [501/938], Loss: 0.9205915331840515\n",
      "Validation: Epoch [8], Batch [502/938], Loss: 0.7868845462799072\n",
      "Validation: Epoch [8], Batch [503/938], Loss: 0.7233192920684814\n",
      "Validation: Epoch [8], Batch [504/938], Loss: 0.6676349639892578\n",
      "Validation: Epoch [8], Batch [505/938], Loss: 0.9225631952285767\n",
      "Validation: Epoch [8], Batch [506/938], Loss: 0.7531372904777527\n",
      "Validation: Epoch [8], Batch [507/938], Loss: 0.6547555923461914\n",
      "Validation: Epoch [8], Batch [508/938], Loss: 1.0625211000442505\n",
      "Validation: Epoch [8], Batch [509/938], Loss: 0.9101383686065674\n",
      "Validation: Epoch [8], Batch [510/938], Loss: 0.8535809516906738\n",
      "Validation: Epoch [8], Batch [511/938], Loss: 0.8597186803817749\n",
      "Validation: Epoch [8], Batch [512/938], Loss: 0.7174860239028931\n",
      "Validation: Epoch [8], Batch [513/938], Loss: 0.6397016644477844\n",
      "Validation: Epoch [8], Batch [514/938], Loss: 0.8366283178329468\n",
      "Validation: Epoch [8], Batch [515/938], Loss: 0.7602904438972473\n",
      "Validation: Epoch [8], Batch [516/938], Loss: 0.8647010326385498\n",
      "Validation: Epoch [8], Batch [517/938], Loss: 0.6148586273193359\n",
      "Validation: Epoch [8], Batch [518/938], Loss: 1.1713306903839111\n",
      "Validation: Epoch [8], Batch [519/938], Loss: 0.8245079517364502\n",
      "Validation: Epoch [8], Batch [520/938], Loss: 0.7351619005203247\n",
      "Validation: Epoch [8], Batch [521/938], Loss: 0.7382925748825073\n",
      "Validation: Epoch [8], Batch [522/938], Loss: 0.8387163281440735\n",
      "Validation: Epoch [8], Batch [523/938], Loss: 0.8193325996398926\n",
      "Validation: Epoch [8], Batch [524/938], Loss: 0.7748425602912903\n",
      "Validation: Epoch [8], Batch [525/938], Loss: 0.6670154333114624\n",
      "Validation: Epoch [8], Batch [526/938], Loss: 0.9449618458747864\n",
      "Validation: Epoch [8], Batch [527/938], Loss: 0.8878723382949829\n",
      "Validation: Epoch [8], Batch [528/938], Loss: 0.6197447776794434\n",
      "Validation: Epoch [8], Batch [529/938], Loss: 0.705178439617157\n",
      "Validation: Epoch [8], Batch [530/938], Loss: 0.7008731365203857\n",
      "Validation: Epoch [8], Batch [531/938], Loss: 0.998850405216217\n",
      "Validation: Epoch [8], Batch [532/938], Loss: 0.825463593006134\n",
      "Validation: Epoch [8], Batch [533/938], Loss: 1.068374514579773\n",
      "Validation: Epoch [8], Batch [534/938], Loss: 0.9560034275054932\n",
      "Validation: Epoch [8], Batch [535/938], Loss: 1.0026501417160034\n",
      "Validation: Epoch [8], Batch [536/938], Loss: 0.7728668451309204\n",
      "Validation: Epoch [8], Batch [537/938], Loss: 0.8191844820976257\n",
      "Validation: Epoch [8], Batch [538/938], Loss: 0.986967146396637\n",
      "Validation: Epoch [8], Batch [539/938], Loss: 0.8502596616744995\n",
      "Validation: Epoch [8], Batch [540/938], Loss: 0.8733608722686768\n",
      "Validation: Epoch [8], Batch [541/938], Loss: 0.8622627258300781\n",
      "Validation: Epoch [8], Batch [542/938], Loss: 0.7886276245117188\n",
      "Validation: Epoch [8], Batch [543/938], Loss: 1.0295627117156982\n",
      "Validation: Epoch [8], Batch [544/938], Loss: 0.8950353860855103\n",
      "Validation: Epoch [8], Batch [545/938], Loss: 0.8403022289276123\n",
      "Validation: Epoch [8], Batch [546/938], Loss: 0.7020523548126221\n",
      "Validation: Epoch [8], Batch [547/938], Loss: 0.8830428719520569\n",
      "Validation: Epoch [8], Batch [548/938], Loss: 0.9605727791786194\n",
      "Validation: Epoch [8], Batch [549/938], Loss: 0.8500258922576904\n",
      "Validation: Epoch [8], Batch [550/938], Loss: 0.8984089493751526\n",
      "Validation: Epoch [8], Batch [551/938], Loss: 0.645912766456604\n",
      "Validation: Epoch [8], Batch [552/938], Loss: 0.901445209980011\n",
      "Validation: Epoch [8], Batch [553/938], Loss: 0.8741283416748047\n",
      "Validation: Epoch [8], Batch [554/938], Loss: 0.890174925327301\n",
      "Validation: Epoch [8], Batch [555/938], Loss: 0.7738993167877197\n",
      "Validation: Epoch [8], Batch [556/938], Loss: 0.7018856406211853\n",
      "Validation: Epoch [8], Batch [557/938], Loss: 0.9219420552253723\n",
      "Validation: Epoch [8], Batch [558/938], Loss: 0.757895827293396\n",
      "Validation: Epoch [8], Batch [559/938], Loss: 0.6245965361595154\n",
      "Validation: Epoch [8], Batch [560/938], Loss: 0.5859447717666626\n",
      "Validation: Epoch [8], Batch [561/938], Loss: 0.7461690902709961\n",
      "Validation: Epoch [8], Batch [562/938], Loss: 0.9524336457252502\n",
      "Validation: Epoch [8], Batch [563/938], Loss: 0.7581532597541809\n",
      "Validation: Epoch [8], Batch [564/938], Loss: 0.860639214515686\n",
      "Validation: Epoch [8], Batch [565/938], Loss: 1.006467580795288\n",
      "Validation: Epoch [8], Batch [566/938], Loss: 0.9207571744918823\n",
      "Validation: Epoch [8], Batch [567/938], Loss: 1.060248613357544\n",
      "Validation: Epoch [8], Batch [568/938], Loss: 0.7859758734703064\n",
      "Validation: Epoch [8], Batch [569/938], Loss: 0.7624099254608154\n",
      "Validation: Epoch [8], Batch [570/938], Loss: 0.7248705625534058\n",
      "Validation: Epoch [8], Batch [571/938], Loss: 0.7206768989562988\n",
      "Validation: Epoch [8], Batch [572/938], Loss: 0.7184158563613892\n",
      "Validation: Epoch [8], Batch [573/938], Loss: 0.722069263458252\n",
      "Validation: Epoch [8], Batch [574/938], Loss: 0.9235854148864746\n",
      "Validation: Epoch [8], Batch [575/938], Loss: 0.8719459176063538\n",
      "Validation: Epoch [8], Batch [576/938], Loss: 0.8580647706985474\n",
      "Validation: Epoch [8], Batch [577/938], Loss: 0.7372198700904846\n",
      "Validation: Epoch [8], Batch [578/938], Loss: 0.8164088129997253\n",
      "Validation: Epoch [8], Batch [579/938], Loss: 0.8662434816360474\n",
      "Validation: Epoch [8], Batch [580/938], Loss: 0.7120404243469238\n",
      "Validation: Epoch [8], Batch [581/938], Loss: 0.9321187138557434\n",
      "Validation: Epoch [8], Batch [582/938], Loss: 1.057492971420288\n",
      "Validation: Epoch [8], Batch [583/938], Loss: 1.0768777132034302\n",
      "Validation: Epoch [8], Batch [584/938], Loss: 0.8015031218528748\n",
      "Validation: Epoch [8], Batch [585/938], Loss: 0.7307456731796265\n",
      "Validation: Epoch [8], Batch [586/938], Loss: 0.7655581831932068\n",
      "Validation: Epoch [8], Batch [587/938], Loss: 0.9124643206596375\n",
      "Validation: Epoch [8], Batch [588/938], Loss: 0.913849949836731\n",
      "Validation: Epoch [8], Batch [589/938], Loss: 0.8824500441551208\n",
      "Validation: Epoch [8], Batch [590/938], Loss: 0.8730522990226746\n",
      "Validation: Epoch [8], Batch [591/938], Loss: 0.920251727104187\n",
      "Validation: Epoch [8], Batch [592/938], Loss: 0.9597845077514648\n",
      "Validation: Epoch [8], Batch [593/938], Loss: 1.1021312475204468\n",
      "Validation: Epoch [8], Batch [594/938], Loss: 0.9637371897697449\n",
      "Validation: Epoch [8], Batch [595/938], Loss: 0.790158748626709\n",
      "Validation: Epoch [8], Batch [596/938], Loss: 0.8164476156234741\n",
      "Validation: Epoch [8], Batch [597/938], Loss: 0.9035828709602356\n",
      "Validation: Epoch [8], Batch [598/938], Loss: 0.7363225817680359\n",
      "Validation: Epoch [8], Batch [599/938], Loss: 0.8517023324966431\n",
      "Validation: Epoch [8], Batch [600/938], Loss: 0.7465043663978577\n",
      "Validation: Epoch [8], Batch [601/938], Loss: 0.8798246383666992\n",
      "Validation: Epoch [8], Batch [602/938], Loss: 1.067713975906372\n",
      "Validation: Epoch [8], Batch [603/938], Loss: 1.082779049873352\n",
      "Validation: Epoch [8], Batch [604/938], Loss: 0.633377194404602\n",
      "Validation: Epoch [8], Batch [605/938], Loss: 0.7347414493560791\n",
      "Validation: Epoch [8], Batch [606/938], Loss: 0.9290765523910522\n",
      "Validation: Epoch [8], Batch [607/938], Loss: 0.5021476745605469\n",
      "Validation: Epoch [8], Batch [608/938], Loss: 0.8847153782844543\n",
      "Validation: Epoch [8], Batch [609/938], Loss: 0.7524736523628235\n",
      "Validation: Epoch [8], Batch [610/938], Loss: 0.9636409878730774\n",
      "Validation: Epoch [8], Batch [611/938], Loss: 0.8314908742904663\n",
      "Validation: Epoch [8], Batch [612/938], Loss: 1.099841594696045\n",
      "Validation: Epoch [8], Batch [613/938], Loss: 0.9227972626686096\n",
      "Validation: Epoch [8], Batch [614/938], Loss: 0.8340327739715576\n",
      "Validation: Epoch [8], Batch [615/938], Loss: 0.8762851357460022\n",
      "Validation: Epoch [8], Batch [616/938], Loss: 0.8255926370620728\n",
      "Validation: Epoch [8], Batch [617/938], Loss: 0.7497035264968872\n",
      "Validation: Epoch [8], Batch [618/938], Loss: 1.0382249355316162\n",
      "Validation: Epoch [8], Batch [619/938], Loss: 0.7404917478561401\n",
      "Validation: Epoch [8], Batch [620/938], Loss: 0.8901618123054504\n",
      "Validation: Epoch [8], Batch [621/938], Loss: 0.7654737234115601\n",
      "Validation: Epoch [8], Batch [622/938], Loss: 0.8350613713264465\n",
      "Validation: Epoch [8], Batch [623/938], Loss: 0.8559648990631104\n",
      "Validation: Epoch [8], Batch [624/938], Loss: 0.7528288960456848\n",
      "Validation: Epoch [8], Batch [625/938], Loss: 0.9299856424331665\n",
      "Validation: Epoch [8], Batch [626/938], Loss: 0.7909584045410156\n",
      "Validation: Epoch [8], Batch [627/938], Loss: 0.8043761849403381\n",
      "Validation: Epoch [8], Batch [628/938], Loss: 0.6952223777770996\n",
      "Validation: Epoch [8], Batch [629/938], Loss: 0.9295543432235718\n",
      "Validation: Epoch [8], Batch [630/938], Loss: 0.7810565233230591\n",
      "Validation: Epoch [8], Batch [631/938], Loss: 0.8740224242210388\n",
      "Validation: Epoch [8], Batch [632/938], Loss: 0.9647241234779358\n",
      "Validation: Epoch [8], Batch [633/938], Loss: 0.9248405694961548\n",
      "Validation: Epoch [8], Batch [634/938], Loss: 0.8198337554931641\n",
      "Validation: Epoch [8], Batch [635/938], Loss: 0.8229091763496399\n",
      "Validation: Epoch [8], Batch [636/938], Loss: 0.8299046754837036\n",
      "Validation: Epoch [8], Batch [637/938], Loss: 0.8182759284973145\n",
      "Validation: Epoch [8], Batch [638/938], Loss: 1.0665093660354614\n",
      "Validation: Epoch [8], Batch [639/938], Loss: 0.7950191497802734\n",
      "Validation: Epoch [8], Batch [640/938], Loss: 0.8031731247901917\n",
      "Validation: Epoch [8], Batch [641/938], Loss: 0.7667242288589478\n",
      "Validation: Epoch [8], Batch [642/938], Loss: 0.8052701354026794\n",
      "Validation: Epoch [8], Batch [643/938], Loss: 0.6643840074539185\n",
      "Validation: Epoch [8], Batch [644/938], Loss: 0.9754222631454468\n",
      "Validation: Epoch [8], Batch [645/938], Loss: 0.9101307392120361\n",
      "Validation: Epoch [8], Batch [646/938], Loss: 0.59837806224823\n",
      "Validation: Epoch [8], Batch [647/938], Loss: 1.0067678689956665\n",
      "Validation: Epoch [8], Batch [648/938], Loss: 0.9219743013381958\n",
      "Validation: Epoch [8], Batch [649/938], Loss: 0.8220753073692322\n",
      "Validation: Epoch [8], Batch [650/938], Loss: 0.8622276186943054\n",
      "Validation: Epoch [8], Batch [651/938], Loss: 0.6608219742774963\n",
      "Validation: Epoch [8], Batch [652/938], Loss: 0.7551108598709106\n",
      "Validation: Epoch [8], Batch [653/938], Loss: 0.8437929153442383\n",
      "Validation: Epoch [8], Batch [654/938], Loss: 1.2700717449188232\n",
      "Validation: Epoch [8], Batch [655/938], Loss: 0.9539995789527893\n",
      "Validation: Epoch [8], Batch [656/938], Loss: 0.7900886535644531\n",
      "Validation: Epoch [8], Batch [657/938], Loss: 0.618318498134613\n",
      "Validation: Epoch [8], Batch [658/938], Loss: 0.7869224548339844\n",
      "Validation: Epoch [8], Batch [659/938], Loss: 0.8029683232307434\n",
      "Validation: Epoch [8], Batch [660/938], Loss: 0.7611939907073975\n",
      "Validation: Epoch [8], Batch [661/938], Loss: 0.7212909460067749\n",
      "Validation: Epoch [8], Batch [662/938], Loss: 1.0058245658874512\n",
      "Validation: Epoch [8], Batch [663/938], Loss: 0.7318922281265259\n",
      "Validation: Epoch [8], Batch [664/938], Loss: 0.9943118691444397\n",
      "Validation: Epoch [8], Batch [665/938], Loss: 0.5092546939849854\n",
      "Validation: Epoch [8], Batch [666/938], Loss: 0.6006790995597839\n",
      "Validation: Epoch [8], Batch [667/938], Loss: 0.7764949798583984\n",
      "Validation: Epoch [8], Batch [668/938], Loss: 1.0051308870315552\n",
      "Validation: Epoch [8], Batch [669/938], Loss: 0.8907691836357117\n",
      "Validation: Epoch [8], Batch [670/938], Loss: 0.6895701885223389\n",
      "Validation: Epoch [8], Batch [671/938], Loss: 1.198967695236206\n",
      "Validation: Epoch [8], Batch [672/938], Loss: 1.0097026824951172\n",
      "Validation: Epoch [8], Batch [673/938], Loss: 0.8264703750610352\n",
      "Validation: Epoch [8], Batch [674/938], Loss: 0.9230931997299194\n",
      "Validation: Epoch [8], Batch [675/938], Loss: 1.0229030847549438\n",
      "Validation: Epoch [8], Batch [676/938], Loss: 1.1895756721496582\n",
      "Validation: Epoch [8], Batch [677/938], Loss: 0.9670010209083557\n",
      "Validation: Epoch [8], Batch [678/938], Loss: 0.8812719583511353\n",
      "Validation: Epoch [8], Batch [679/938], Loss: 0.7673951387405396\n",
      "Validation: Epoch [8], Batch [680/938], Loss: 0.762542724609375\n",
      "Validation: Epoch [8], Batch [681/938], Loss: 0.5532822608947754\n",
      "Validation: Epoch [8], Batch [682/938], Loss: 0.8911701440811157\n",
      "Validation: Epoch [8], Batch [683/938], Loss: 0.9248204827308655\n",
      "Validation: Epoch [8], Batch [684/938], Loss: 0.7771341800689697\n",
      "Validation: Epoch [8], Batch [685/938], Loss: 0.8621212244033813\n",
      "Validation: Epoch [8], Batch [686/938], Loss: 0.8024278283119202\n",
      "Validation: Epoch [8], Batch [687/938], Loss: 0.5948203206062317\n",
      "Validation: Epoch [8], Batch [688/938], Loss: 0.5764093995094299\n",
      "Validation: Epoch [8], Batch [689/938], Loss: 0.6819096803665161\n",
      "Validation: Epoch [8], Batch [690/938], Loss: 0.6643580198287964\n",
      "Validation: Epoch [8], Batch [691/938], Loss: 0.6985602974891663\n",
      "Validation: Epoch [8], Batch [692/938], Loss: 0.8294416069984436\n",
      "Validation: Epoch [8], Batch [693/938], Loss: 1.0210846662521362\n",
      "Validation: Epoch [8], Batch [694/938], Loss: 0.8611975908279419\n",
      "Validation: Epoch [8], Batch [695/938], Loss: 0.8546823263168335\n",
      "Validation: Epoch [8], Batch [696/938], Loss: 1.0370030403137207\n",
      "Validation: Epoch [8], Batch [697/938], Loss: 0.969072699546814\n",
      "Validation: Epoch [8], Batch [698/938], Loss: 0.7405210733413696\n",
      "Validation: Epoch [8], Batch [699/938], Loss: 0.8435249328613281\n",
      "Validation: Epoch [8], Batch [700/938], Loss: 0.8473642468452454\n",
      "Validation: Epoch [8], Batch [701/938], Loss: 1.1572870016098022\n",
      "Validation: Epoch [8], Batch [702/938], Loss: 0.707115650177002\n",
      "Validation: Epoch [8], Batch [703/938], Loss: 0.901348352432251\n",
      "Validation: Epoch [8], Batch [704/938], Loss: 0.7397406697273254\n",
      "Validation: Epoch [8], Batch [705/938], Loss: 0.9030076265335083\n",
      "Validation: Epoch [8], Batch [706/938], Loss: 0.9866812229156494\n",
      "Validation: Epoch [8], Batch [707/938], Loss: 1.2292413711547852\n",
      "Validation: Epoch [8], Batch [708/938], Loss: 0.9664457440376282\n",
      "Validation: Epoch [8], Batch [709/938], Loss: 0.9435077905654907\n",
      "Validation: Epoch [8], Batch [710/938], Loss: 0.7705948948860168\n",
      "Validation: Epoch [8], Batch [711/938], Loss: 0.9647796750068665\n",
      "Validation: Epoch [8], Batch [712/938], Loss: 0.8025063276290894\n",
      "Validation: Epoch [8], Batch [713/938], Loss: 0.9895422458648682\n",
      "Validation: Epoch [8], Batch [714/938], Loss: 0.7277084589004517\n",
      "Validation: Epoch [8], Batch [715/938], Loss: 1.1843016147613525\n",
      "Validation: Epoch [8], Batch [716/938], Loss: 0.7366345524787903\n",
      "Validation: Epoch [8], Batch [717/938], Loss: 0.8513787388801575\n",
      "Validation: Epoch [8], Batch [718/938], Loss: 0.703482449054718\n",
      "Validation: Epoch [8], Batch [719/938], Loss: 0.5325735807418823\n",
      "Validation: Epoch [8], Batch [720/938], Loss: 0.7151505351066589\n",
      "Validation: Epoch [8], Batch [721/938], Loss: 0.9333350658416748\n",
      "Validation: Epoch [8], Batch [722/938], Loss: 0.982552170753479\n",
      "Validation: Epoch [8], Batch [723/938], Loss: 0.8165384531021118\n",
      "Validation: Epoch [8], Batch [724/938], Loss: 1.08673095703125\n",
      "Validation: Epoch [8], Batch [725/938], Loss: 0.6806339621543884\n",
      "Validation: Epoch [8], Batch [726/938], Loss: 0.936443567276001\n",
      "Validation: Epoch [8], Batch [727/938], Loss: 0.8484774827957153\n",
      "Validation: Epoch [8], Batch [728/938], Loss: 0.6999853253364563\n",
      "Validation: Epoch [8], Batch [729/938], Loss: 0.9787588715553284\n",
      "Validation: Epoch [8], Batch [730/938], Loss: 0.7569019794464111\n",
      "Validation: Epoch [8], Batch [731/938], Loss: 0.8120788335800171\n",
      "Validation: Epoch [8], Batch [732/938], Loss: 0.6080194711685181\n",
      "Validation: Epoch [8], Batch [733/938], Loss: 0.8550556302070618\n",
      "Validation: Epoch [8], Batch [734/938], Loss: 0.7422826290130615\n",
      "Validation: Epoch [8], Batch [735/938], Loss: 0.7549751996994019\n",
      "Validation: Epoch [8], Batch [736/938], Loss: 0.8793380260467529\n",
      "Validation: Epoch [8], Batch [737/938], Loss: 1.150481939315796\n",
      "Validation: Epoch [8], Batch [738/938], Loss: 0.8265029788017273\n",
      "Validation: Epoch [8], Batch [739/938], Loss: 0.7647678852081299\n",
      "Validation: Epoch [8], Batch [740/938], Loss: 0.8423130512237549\n",
      "Validation: Epoch [8], Batch [741/938], Loss: 0.6886798739433289\n",
      "Validation: Epoch [8], Batch [742/938], Loss: 0.6676380634307861\n",
      "Validation: Epoch [8], Batch [743/938], Loss: 0.7744734883308411\n",
      "Validation: Epoch [8], Batch [744/938], Loss: 0.5801568627357483\n",
      "Validation: Epoch [8], Batch [745/938], Loss: 0.7997341752052307\n",
      "Validation: Epoch [8], Batch [746/938], Loss: 0.7420787811279297\n",
      "Validation: Epoch [8], Batch [747/938], Loss: 0.742464542388916\n",
      "Validation: Epoch [8], Batch [748/938], Loss: 0.7564377784729004\n",
      "Validation: Epoch [8], Batch [749/938], Loss: 0.7122370004653931\n",
      "Validation: Epoch [8], Batch [750/938], Loss: 0.8103159070014954\n",
      "Validation: Epoch [8], Batch [751/938], Loss: 0.7692670226097107\n",
      "Validation: Epoch [8], Batch [752/938], Loss: 0.8074044585227966\n",
      "Validation: Epoch [8], Batch [753/938], Loss: 0.8922985792160034\n",
      "Validation: Epoch [8], Batch [754/938], Loss: 0.783473789691925\n",
      "Validation: Epoch [8], Batch [755/938], Loss: 0.9882347583770752\n",
      "Validation: Epoch [8], Batch [756/938], Loss: 0.8529231548309326\n",
      "Validation: Epoch [8], Batch [757/938], Loss: 0.8088129162788391\n",
      "Validation: Epoch [8], Batch [758/938], Loss: 0.6478495597839355\n",
      "Validation: Epoch [8], Batch [759/938], Loss: 0.7746865153312683\n",
      "Validation: Epoch [8], Batch [760/938], Loss: 0.8115984797477722\n",
      "Validation: Epoch [8], Batch [761/938], Loss: 0.7106470465660095\n",
      "Validation: Epoch [8], Batch [762/938], Loss: 0.950393795967102\n",
      "Validation: Epoch [8], Batch [763/938], Loss: 0.7467007637023926\n",
      "Validation: Epoch [8], Batch [764/938], Loss: 0.7919106483459473\n",
      "Validation: Epoch [8], Batch [765/938], Loss: 0.7919099926948547\n",
      "Validation: Epoch [8], Batch [766/938], Loss: 0.790298581123352\n",
      "Validation: Epoch [8], Batch [767/938], Loss: 0.8062424659729004\n",
      "Validation: Epoch [8], Batch [768/938], Loss: 1.1583268642425537\n",
      "Validation: Epoch [8], Batch [769/938], Loss: 0.6878368854522705\n",
      "Validation: Epoch [8], Batch [770/938], Loss: 0.7648276686668396\n",
      "Validation: Epoch [8], Batch [771/938], Loss: 0.867353081703186\n",
      "Validation: Epoch [8], Batch [772/938], Loss: 0.8006467819213867\n",
      "Validation: Epoch [8], Batch [773/938], Loss: 0.8231437802314758\n",
      "Validation: Epoch [8], Batch [774/938], Loss: 0.8567790389060974\n",
      "Validation: Epoch [8], Batch [775/938], Loss: 0.8815892934799194\n",
      "Validation: Epoch [8], Batch [776/938], Loss: 0.7830561399459839\n",
      "Validation: Epoch [8], Batch [777/938], Loss: 0.8517111539840698\n",
      "Validation: Epoch [8], Batch [778/938], Loss: 0.9783750176429749\n",
      "Validation: Epoch [8], Batch [779/938], Loss: 0.8212928771972656\n",
      "Validation: Epoch [8], Batch [780/938], Loss: 0.717555582523346\n",
      "Validation: Epoch [8], Batch [781/938], Loss: 0.9108485579490662\n",
      "Validation: Epoch [8], Batch [782/938], Loss: 0.8137835264205933\n",
      "Validation: Epoch [8], Batch [783/938], Loss: 0.5842005610466003\n",
      "Validation: Epoch [8], Batch [784/938], Loss: 0.6627922058105469\n",
      "Validation: Epoch [8], Batch [785/938], Loss: 0.6613807082176208\n",
      "Validation: Epoch [8], Batch [786/938], Loss: 0.7417835593223572\n",
      "Validation: Epoch [8], Batch [787/938], Loss: 0.8546444177627563\n",
      "Validation: Epoch [8], Batch [788/938], Loss: 0.7855380773544312\n",
      "Validation: Epoch [8], Batch [789/938], Loss: 0.9431684613227844\n",
      "Validation: Epoch [8], Batch [790/938], Loss: 1.0528695583343506\n",
      "Validation: Epoch [8], Batch [791/938], Loss: 0.6474466323852539\n",
      "Validation: Epoch [8], Batch [792/938], Loss: 1.0065901279449463\n",
      "Validation: Epoch [8], Batch [793/938], Loss: 0.8173165917396545\n",
      "Validation: Epoch [8], Batch [794/938], Loss: 0.7593041062355042\n",
      "Validation: Epoch [8], Batch [795/938], Loss: 0.9360544085502625\n",
      "Validation: Epoch [8], Batch [796/938], Loss: 0.8458272218704224\n",
      "Validation: Epoch [8], Batch [797/938], Loss: 0.7093939185142517\n",
      "Validation: Epoch [8], Batch [798/938], Loss: 1.146475076675415\n",
      "Validation: Epoch [8], Batch [799/938], Loss: 0.8182107210159302\n",
      "Validation: Epoch [8], Batch [800/938], Loss: 0.7994590997695923\n",
      "Validation: Epoch [8], Batch [801/938], Loss: 0.9131220579147339\n",
      "Validation: Epoch [8], Batch [802/938], Loss: 0.7462098598480225\n",
      "Validation: Epoch [8], Batch [803/938], Loss: 0.7878773212432861\n",
      "Validation: Epoch [8], Batch [804/938], Loss: 0.865019679069519\n",
      "Validation: Epoch [8], Batch [805/938], Loss: 0.8234126567840576\n",
      "Validation: Epoch [8], Batch [806/938], Loss: 0.7169306874275208\n",
      "Validation: Epoch [8], Batch [807/938], Loss: 0.6160328388214111\n",
      "Validation: Epoch [8], Batch [808/938], Loss: 0.8113824725151062\n",
      "Validation: Epoch [8], Batch [809/938], Loss: 0.9965085387229919\n",
      "Validation: Epoch [8], Batch [810/938], Loss: 0.6947475671768188\n",
      "Validation: Epoch [8], Batch [811/938], Loss: 0.7236183881759644\n",
      "Validation: Epoch [8], Batch [812/938], Loss: 1.0444284677505493\n",
      "Validation: Epoch [8], Batch [813/938], Loss: 1.1524419784545898\n",
      "Validation: Epoch [8], Batch [814/938], Loss: 0.5877253413200378\n",
      "Validation: Epoch [8], Batch [815/938], Loss: 0.8707601428031921\n",
      "Validation: Epoch [8], Batch [816/938], Loss: 0.7619673609733582\n",
      "Validation: Epoch [8], Batch [817/938], Loss: 0.8476965427398682\n",
      "Validation: Epoch [8], Batch [818/938], Loss: 0.8299318552017212\n",
      "Validation: Epoch [8], Batch [819/938], Loss: 0.7431526780128479\n",
      "Validation: Epoch [8], Batch [820/938], Loss: 0.608610987663269\n",
      "Validation: Epoch [8], Batch [821/938], Loss: 0.7374389171600342\n",
      "Validation: Epoch [8], Batch [822/938], Loss: 0.8027374744415283\n",
      "Validation: Epoch [8], Batch [823/938], Loss: 0.8204097747802734\n",
      "Validation: Epoch [8], Batch [824/938], Loss: 0.8692231178283691\n",
      "Validation: Epoch [8], Batch [825/938], Loss: 0.8747575879096985\n",
      "Validation: Epoch [8], Batch [826/938], Loss: 0.6761715412139893\n",
      "Validation: Epoch [8], Batch [827/938], Loss: 0.7325910925865173\n",
      "Validation: Epoch [8], Batch [828/938], Loss: 0.7042672634124756\n",
      "Validation: Epoch [8], Batch [829/938], Loss: 1.0362493991851807\n",
      "Validation: Epoch [8], Batch [830/938], Loss: 1.0366899967193604\n",
      "Validation: Epoch [8], Batch [831/938], Loss: 0.8255212903022766\n",
      "Validation: Epoch [8], Batch [832/938], Loss: 0.9276117086410522\n",
      "Validation: Epoch [8], Batch [833/938], Loss: 0.7715644836425781\n",
      "Validation: Epoch [8], Batch [834/938], Loss: 0.7253704071044922\n",
      "Validation: Epoch [8], Batch [835/938], Loss: 0.686346173286438\n",
      "Validation: Epoch [8], Batch [836/938], Loss: 0.7219071388244629\n",
      "Validation: Epoch [8], Batch [837/938], Loss: 0.8126453161239624\n",
      "Validation: Epoch [8], Batch [838/938], Loss: 0.7689468860626221\n",
      "Validation: Epoch [8], Batch [839/938], Loss: 1.0000369548797607\n",
      "Validation: Epoch [8], Batch [840/938], Loss: 0.9268569946289062\n",
      "Validation: Epoch [8], Batch [841/938], Loss: 1.053227186203003\n",
      "Validation: Epoch [8], Batch [842/938], Loss: 0.7963722348213196\n",
      "Validation: Epoch [8], Batch [843/938], Loss: 0.6773127317428589\n",
      "Validation: Epoch [8], Batch [844/938], Loss: 0.9774206280708313\n",
      "Validation: Epoch [8], Batch [845/938], Loss: 0.8417152166366577\n",
      "Validation: Epoch [8], Batch [846/938], Loss: 0.9719814658164978\n",
      "Validation: Epoch [8], Batch [847/938], Loss: 1.011224627494812\n",
      "Validation: Epoch [8], Batch [848/938], Loss: 0.8724719882011414\n",
      "Validation: Epoch [8], Batch [849/938], Loss: 0.8404144048690796\n",
      "Validation: Epoch [8], Batch [850/938], Loss: 0.9279351234436035\n",
      "Validation: Epoch [8], Batch [851/938], Loss: 0.8691913485527039\n",
      "Validation: Epoch [8], Batch [852/938], Loss: 1.0823426246643066\n",
      "Validation: Epoch [8], Batch [853/938], Loss: 0.9068895578384399\n",
      "Validation: Epoch [8], Batch [854/938], Loss: 0.9400335550308228\n",
      "Validation: Epoch [8], Batch [855/938], Loss: 0.8628578186035156\n",
      "Validation: Epoch [8], Batch [856/938], Loss: 0.9683089256286621\n",
      "Validation: Epoch [8], Batch [857/938], Loss: 0.8215673565864563\n",
      "Validation: Epoch [8], Batch [858/938], Loss: 0.924176037311554\n",
      "Validation: Epoch [8], Batch [859/938], Loss: 0.7625973224639893\n",
      "Validation: Epoch [8], Batch [860/938], Loss: 0.8627433180809021\n",
      "Validation: Epoch [8], Batch [861/938], Loss: 0.7586784958839417\n",
      "Validation: Epoch [8], Batch [862/938], Loss: 0.8179787993431091\n",
      "Validation: Epoch [8], Batch [863/938], Loss: 0.9055641889572144\n",
      "Validation: Epoch [8], Batch [864/938], Loss: 0.8731701374053955\n",
      "Validation: Epoch [8], Batch [865/938], Loss: 1.003280758857727\n",
      "Validation: Epoch [8], Batch [866/938], Loss: 0.6942636370658875\n",
      "Validation: Epoch [8], Batch [867/938], Loss: 0.8174248933792114\n",
      "Validation: Epoch [8], Batch [868/938], Loss: 0.7474117279052734\n",
      "Validation: Epoch [8], Batch [869/938], Loss: 0.9539384245872498\n",
      "Validation: Epoch [8], Batch [870/938], Loss: 0.8836507797241211\n",
      "Validation: Epoch [8], Batch [871/938], Loss: 0.7521185874938965\n",
      "Validation: Epoch [8], Batch [872/938], Loss: 0.6833691596984863\n",
      "Validation: Epoch [8], Batch [873/938], Loss: 0.7001550197601318\n",
      "Validation: Epoch [8], Batch [874/938], Loss: 0.8582400679588318\n",
      "Validation: Epoch [8], Batch [875/938], Loss: 0.7428399920463562\n",
      "Validation: Epoch [8], Batch [876/938], Loss: 0.9505587220191956\n",
      "Validation: Epoch [8], Batch [877/938], Loss: 0.9754172563552856\n",
      "Validation: Epoch [8], Batch [878/938], Loss: 0.6694821715354919\n",
      "Validation: Epoch [8], Batch [879/938], Loss: 0.7331571578979492\n",
      "Validation: Epoch [8], Batch [880/938], Loss: 0.8965722322463989\n",
      "Validation: Epoch [8], Batch [881/938], Loss: 0.8708459138870239\n",
      "Validation: Epoch [8], Batch [882/938], Loss: 0.9222860336303711\n",
      "Validation: Epoch [8], Batch [883/938], Loss: 0.9382258653640747\n",
      "Validation: Epoch [8], Batch [884/938], Loss: 0.8915427327156067\n",
      "Validation: Epoch [8], Batch [885/938], Loss: 0.7584658861160278\n",
      "Validation: Epoch [8], Batch [886/938], Loss: 0.7857987284660339\n",
      "Validation: Epoch [8], Batch [887/938], Loss: 0.7366660833358765\n",
      "Validation: Epoch [8], Batch [888/938], Loss: 0.6375970840454102\n",
      "Validation: Epoch [8], Batch [889/938], Loss: 0.6376107335090637\n",
      "Validation: Epoch [8], Batch [890/938], Loss: 0.6503497362136841\n",
      "Validation: Epoch [8], Batch [891/938], Loss: 0.9736514687538147\n",
      "Validation: Epoch [8], Batch [892/938], Loss: 0.7591508626937866\n",
      "Validation: Epoch [8], Batch [893/938], Loss: 0.7139458060264587\n",
      "Validation: Epoch [8], Batch [894/938], Loss: 0.9833110570907593\n",
      "Validation: Epoch [8], Batch [895/938], Loss: 0.8951084613800049\n",
      "Validation: Epoch [8], Batch [896/938], Loss: 0.7463536262512207\n",
      "Validation: Epoch [8], Batch [897/938], Loss: 0.9874895215034485\n",
      "Validation: Epoch [8], Batch [898/938], Loss: 0.7846637964248657\n",
      "Validation: Epoch [8], Batch [899/938], Loss: 0.8500202894210815\n",
      "Validation: Epoch [8], Batch [900/938], Loss: 0.9290308356285095\n",
      "Validation: Epoch [8], Batch [901/938], Loss: 0.6521691083908081\n",
      "Validation: Epoch [8], Batch [902/938], Loss: 1.020449161529541\n",
      "Validation: Epoch [8], Batch [903/938], Loss: 0.9461743831634521\n",
      "Validation: Epoch [8], Batch [904/938], Loss: 0.9461655020713806\n",
      "Validation: Epoch [8], Batch [905/938], Loss: 0.7847773432731628\n",
      "Validation: Epoch [8], Batch [906/938], Loss: 0.5886633992195129\n",
      "Validation: Epoch [8], Batch [907/938], Loss: 1.1542161703109741\n",
      "Validation: Epoch [8], Batch [908/938], Loss: 0.6906726360321045\n",
      "Validation: Epoch [8], Batch [909/938], Loss: 0.7542060017585754\n",
      "Validation: Epoch [8], Batch [910/938], Loss: 0.8490373492240906\n",
      "Validation: Epoch [8], Batch [911/938], Loss: 0.8479870557785034\n",
      "Validation: Epoch [8], Batch [912/938], Loss: 0.7667273283004761\n",
      "Validation: Epoch [8], Batch [913/938], Loss: 0.9136505722999573\n",
      "Validation: Epoch [8], Batch [914/938], Loss: 0.910001277923584\n",
      "Validation: Epoch [8], Batch [915/938], Loss: 0.7741073369979858\n",
      "Validation: Epoch [8], Batch [916/938], Loss: 0.7785711884498596\n",
      "Validation: Epoch [8], Batch [917/938], Loss: 0.784909725189209\n",
      "Validation: Epoch [8], Batch [918/938], Loss: 0.9083067178726196\n",
      "Validation: Epoch [8], Batch [919/938], Loss: 1.0307587385177612\n",
      "Validation: Epoch [8], Batch [920/938], Loss: 0.8132999539375305\n",
      "Validation: Epoch [8], Batch [921/938], Loss: 0.8591783046722412\n",
      "Validation: Epoch [8], Batch [922/938], Loss: 1.0457382202148438\n",
      "Validation: Epoch [8], Batch [923/938], Loss: 0.8407394289970398\n",
      "Validation: Epoch [8], Batch [924/938], Loss: 0.7343353033065796\n",
      "Validation: Epoch [8], Batch [925/938], Loss: 0.8850561380386353\n",
      "Validation: Epoch [8], Batch [926/938], Loss: 0.6818128228187561\n",
      "Validation: Epoch [8], Batch [927/938], Loss: 0.7212629318237305\n",
      "Validation: Epoch [8], Batch [928/938], Loss: 0.9793224334716797\n",
      "Validation: Epoch [8], Batch [929/938], Loss: 1.044222116470337\n",
      "Validation: Epoch [8], Batch [930/938], Loss: 0.8373174667358398\n",
      "Validation: Epoch [8], Batch [931/938], Loss: 0.9271772503852844\n",
      "Validation: Epoch [8], Batch [932/938], Loss: 0.7424606680870056\n",
      "Validation: Epoch [8], Batch [933/938], Loss: 0.7137961387634277\n",
      "Validation: Epoch [8], Batch [934/938], Loss: 0.8174207806587219\n",
      "Validation: Epoch [8], Batch [935/938], Loss: 0.735475480556488\n",
      "Validation: Epoch [8], Batch [936/938], Loss: 0.6500093936920166\n",
      "Validation: Epoch [8], Batch [937/938], Loss: 0.7143186330795288\n",
      "Validation: Epoch [8], Batch [938/938], Loss: 0.7242440581321716\n",
      "Accuracy of test set: 0.7165666666666667\n",
      "Train: Epoch [9], Batch [1/938], Loss: 1.127928614616394\n",
      "Train: Epoch [9], Batch [2/938], Loss: 0.8199506402015686\n",
      "Train: Epoch [9], Batch [3/938], Loss: 0.9146609902381897\n",
      "Train: Epoch [9], Batch [4/938], Loss: 0.843259871006012\n",
      "Train: Epoch [9], Batch [5/938], Loss: 0.8241240382194519\n",
      "Train: Epoch [9], Batch [6/938], Loss: 0.5831524133682251\n",
      "Train: Epoch [9], Batch [7/938], Loss: 0.7080679535865784\n",
      "Train: Epoch [9], Batch [8/938], Loss: 0.8131681680679321\n",
      "Train: Epoch [9], Batch [9/938], Loss: 0.9157209992408752\n",
      "Train: Epoch [9], Batch [10/938], Loss: 0.7480037212371826\n",
      "Train: Epoch [9], Batch [11/938], Loss: 0.9255620837211609\n",
      "Train: Epoch [9], Batch [12/938], Loss: 0.8553857207298279\n",
      "Train: Epoch [9], Batch [13/938], Loss: 0.8943943381309509\n",
      "Train: Epoch [9], Batch [14/938], Loss: 0.8063552379608154\n",
      "Train: Epoch [9], Batch [15/938], Loss: 0.8904293775558472\n",
      "Train: Epoch [9], Batch [16/938], Loss: 0.8907312750816345\n",
      "Train: Epoch [9], Batch [17/938], Loss: 0.8593472242355347\n",
      "Train: Epoch [9], Batch [18/938], Loss: 0.9668619632720947\n",
      "Train: Epoch [9], Batch [19/938], Loss: 0.6473161578178406\n",
      "Train: Epoch [9], Batch [20/938], Loss: 0.9999629855155945\n",
      "Train: Epoch [9], Batch [21/938], Loss: 0.8501510620117188\n",
      "Train: Epoch [9], Batch [22/938], Loss: 0.963263988494873\n",
      "Train: Epoch [9], Batch [23/938], Loss: 0.6781335473060608\n",
      "Train: Epoch [9], Batch [24/938], Loss: 0.5925692915916443\n",
      "Train: Epoch [9], Batch [25/938], Loss: 0.6430435180664062\n",
      "Train: Epoch [9], Batch [26/938], Loss: 0.774554967880249\n",
      "Train: Epoch [9], Batch [27/938], Loss: 0.6027651429176331\n",
      "Train: Epoch [9], Batch [28/938], Loss: 0.8073933720588684\n",
      "Train: Epoch [9], Batch [29/938], Loss: 0.6876872181892395\n",
      "Train: Epoch [9], Batch [30/938], Loss: 0.5148125886917114\n",
      "Train: Epoch [9], Batch [31/938], Loss: 0.9746158719062805\n",
      "Train: Epoch [9], Batch [32/938], Loss: 0.8560070395469666\n",
      "Train: Epoch [9], Batch [33/938], Loss: 0.7463639974594116\n",
      "Train: Epoch [9], Batch [34/938], Loss: 0.8223071694374084\n",
      "Train: Epoch [9], Batch [35/938], Loss: 0.6988462209701538\n",
      "Train: Epoch [9], Batch [36/938], Loss: 0.6570941805839539\n",
      "Train: Epoch [9], Batch [37/938], Loss: 0.8575525283813477\n",
      "Train: Epoch [9], Batch [38/938], Loss: 1.1186926364898682\n",
      "Train: Epoch [9], Batch [39/938], Loss: 0.7487866282463074\n",
      "Train: Epoch [9], Batch [40/938], Loss: 0.7225897312164307\n",
      "Train: Epoch [9], Batch [41/938], Loss: 0.6913617253303528\n",
      "Train: Epoch [9], Batch [42/938], Loss: 0.9915658235549927\n",
      "Train: Epoch [9], Batch [43/938], Loss: 0.8102157115936279\n",
      "Train: Epoch [9], Batch [44/938], Loss: 0.6954275965690613\n",
      "Train: Epoch [9], Batch [45/938], Loss: 0.7216446995735168\n",
      "Train: Epoch [9], Batch [46/938], Loss: 0.7645145654678345\n",
      "Train: Epoch [9], Batch [47/938], Loss: 0.6550265550613403\n",
      "Train: Epoch [9], Batch [48/938], Loss: 0.8353614211082458\n",
      "Train: Epoch [9], Batch [49/938], Loss: 0.9852501749992371\n",
      "Train: Epoch [9], Batch [50/938], Loss: 0.8577381372451782\n",
      "Train: Epoch [9], Batch [51/938], Loss: 1.042880892753601\n",
      "Train: Epoch [9], Batch [52/938], Loss: 0.8874017596244812\n",
      "Train: Epoch [9], Batch [53/938], Loss: 0.8237288594245911\n",
      "Train: Epoch [9], Batch [54/938], Loss: 0.9338613152503967\n",
      "Train: Epoch [9], Batch [55/938], Loss: 0.9586023688316345\n",
      "Train: Epoch [9], Batch [56/938], Loss: 0.8320837616920471\n",
      "Train: Epoch [9], Batch [57/938], Loss: 0.8782843351364136\n",
      "Train: Epoch [9], Batch [58/938], Loss: 0.7037066221237183\n",
      "Train: Epoch [9], Batch [59/938], Loss: 0.8313901424407959\n",
      "Train: Epoch [9], Batch [60/938], Loss: 0.9299581050872803\n",
      "Train: Epoch [9], Batch [61/938], Loss: 0.8522058725357056\n",
      "Train: Epoch [9], Batch [62/938], Loss: 0.7309622764587402\n",
      "Train: Epoch [9], Batch [63/938], Loss: 0.7878254055976868\n",
      "Train: Epoch [9], Batch [64/938], Loss: 0.8749592304229736\n",
      "Train: Epoch [9], Batch [65/938], Loss: 0.6570146679878235\n",
      "Train: Epoch [9], Batch [66/938], Loss: 1.056639552116394\n",
      "Train: Epoch [9], Batch [67/938], Loss: 0.770727276802063\n",
      "Train: Epoch [9], Batch [68/938], Loss: 0.6578055024147034\n",
      "Train: Epoch [9], Batch [69/938], Loss: 0.5248844027519226\n",
      "Train: Epoch [9], Batch [70/938], Loss: 0.7859023809432983\n",
      "Train: Epoch [9], Batch [71/938], Loss: 0.73238605260849\n",
      "Train: Epoch [9], Batch [72/938], Loss: 0.800870954990387\n",
      "Train: Epoch [9], Batch [73/938], Loss: 0.7821855545043945\n",
      "Train: Epoch [9], Batch [74/938], Loss: 0.9152501225471497\n",
      "Train: Epoch [9], Batch [75/938], Loss: 0.770647406578064\n",
      "Train: Epoch [9], Batch [76/938], Loss: 1.0035791397094727\n",
      "Train: Epoch [9], Batch [77/938], Loss: 0.7086640000343323\n",
      "Train: Epoch [9], Batch [78/938], Loss: 0.7942928075790405\n",
      "Train: Epoch [9], Batch [79/938], Loss: 0.6612991094589233\n",
      "Train: Epoch [9], Batch [80/938], Loss: 0.6839693188667297\n",
      "Train: Epoch [9], Batch [81/938], Loss: 0.7848652601242065\n",
      "Train: Epoch [9], Batch [82/938], Loss: 0.9667586088180542\n",
      "Train: Epoch [9], Batch [83/938], Loss: 0.8981991410255432\n",
      "Train: Epoch [9], Batch [84/938], Loss: 0.821290910243988\n",
      "Train: Epoch [9], Batch [85/938], Loss: 0.9755595922470093\n",
      "Train: Epoch [9], Batch [86/938], Loss: 0.957718014717102\n",
      "Train: Epoch [9], Batch [87/938], Loss: 0.9667397737503052\n",
      "Train: Epoch [9], Batch [88/938], Loss: 0.8220436573028564\n",
      "Train: Epoch [9], Batch [89/938], Loss: 0.9941598773002625\n",
      "Train: Epoch [9], Batch [90/938], Loss: 0.7733309268951416\n",
      "Train: Epoch [9], Batch [91/938], Loss: 0.7311350703239441\n",
      "Train: Epoch [9], Batch [92/938], Loss: 0.7376320362091064\n",
      "Train: Epoch [9], Batch [93/938], Loss: 0.6298258304595947\n",
      "Train: Epoch [9], Batch [94/938], Loss: 0.8612311482429504\n",
      "Train: Epoch [9], Batch [95/938], Loss: 0.8440678119659424\n",
      "Train: Epoch [9], Batch [96/938], Loss: 0.8546786904335022\n",
      "Train: Epoch [9], Batch [97/938], Loss: 0.6493484973907471\n",
      "Train: Epoch [9], Batch [98/938], Loss: 0.9124449491500854\n",
      "Train: Epoch [9], Batch [99/938], Loss: 0.7682594656944275\n",
      "Train: Epoch [9], Batch [100/938], Loss: 1.0127061605453491\n",
      "Train: Epoch [9], Batch [101/938], Loss: 0.8889250159263611\n",
      "Train: Epoch [9], Batch [102/938], Loss: 0.8947904109954834\n",
      "Train: Epoch [9], Batch [103/938], Loss: 0.7539671063423157\n",
      "Train: Epoch [9], Batch [104/938], Loss: 0.8491364121437073\n",
      "Train: Epoch [9], Batch [105/938], Loss: 0.9993536472320557\n",
      "Train: Epoch [9], Batch [106/938], Loss: 0.7879789471626282\n",
      "Train: Epoch [9], Batch [107/938], Loss: 0.6427741050720215\n",
      "Train: Epoch [9], Batch [108/938], Loss: 0.783191442489624\n",
      "Train: Epoch [9], Batch [109/938], Loss: 0.901100754737854\n",
      "Train: Epoch [9], Batch [110/938], Loss: 0.9264582991600037\n",
      "Train: Epoch [9], Batch [111/938], Loss: 0.683225691318512\n",
      "Train: Epoch [9], Batch [112/938], Loss: 0.8081392049789429\n",
      "Train: Epoch [9], Batch [113/938], Loss: 0.9850958585739136\n",
      "Train: Epoch [9], Batch [114/938], Loss: 1.015626311302185\n",
      "Train: Epoch [9], Batch [115/938], Loss: 1.0308985710144043\n",
      "Train: Epoch [9], Batch [116/938], Loss: 0.8012952208518982\n",
      "Train: Epoch [9], Batch [117/938], Loss: 0.7793651819229126\n",
      "Train: Epoch [9], Batch [118/938], Loss: 0.8651378750801086\n",
      "Train: Epoch [9], Batch [119/938], Loss: 0.7515737414360046\n",
      "Train: Epoch [9], Batch [120/938], Loss: 0.6606237888336182\n",
      "Train: Epoch [9], Batch [121/938], Loss: 0.8770507574081421\n",
      "Train: Epoch [9], Batch [122/938], Loss: 0.9139671921730042\n",
      "Train: Epoch [9], Batch [123/938], Loss: 1.211918592453003\n",
      "Train: Epoch [9], Batch [124/938], Loss: 0.7379873991012573\n",
      "Train: Epoch [9], Batch [125/938], Loss: 0.7608886957168579\n",
      "Train: Epoch [9], Batch [126/938], Loss: 0.8519822359085083\n",
      "Train: Epoch [9], Batch [127/938], Loss: 0.9708901643753052\n",
      "Train: Epoch [9], Batch [128/938], Loss: 0.8581651449203491\n",
      "Train: Epoch [9], Batch [129/938], Loss: 1.027293086051941\n",
      "Train: Epoch [9], Batch [130/938], Loss: 0.855028510093689\n",
      "Train: Epoch [9], Batch [131/938], Loss: 0.8342711925506592\n",
      "Train: Epoch [9], Batch [132/938], Loss: 0.7197909951210022\n",
      "Train: Epoch [9], Batch [133/938], Loss: 0.7439718246459961\n",
      "Train: Epoch [9], Batch [134/938], Loss: 0.79036945104599\n",
      "Train: Epoch [9], Batch [135/938], Loss: 0.6283224821090698\n",
      "Train: Epoch [9], Batch [136/938], Loss: 0.8031220436096191\n",
      "Train: Epoch [9], Batch [137/938], Loss: 0.973915696144104\n",
      "Train: Epoch [9], Batch [138/938], Loss: 1.005119800567627\n",
      "Train: Epoch [9], Batch [139/938], Loss: 0.8443719148635864\n",
      "Train: Epoch [9], Batch [140/938], Loss: 0.7881881594657898\n",
      "Train: Epoch [9], Batch [141/938], Loss: 0.7535775899887085\n",
      "Train: Epoch [9], Batch [142/938], Loss: 0.7978705167770386\n",
      "Train: Epoch [9], Batch [143/938], Loss: 0.9464268684387207\n",
      "Train: Epoch [9], Batch [144/938], Loss: 0.7185655236244202\n",
      "Train: Epoch [9], Batch [145/938], Loss: 0.9528313875198364\n",
      "Train: Epoch [9], Batch [146/938], Loss: 0.970396101474762\n",
      "Train: Epoch [9], Batch [147/938], Loss: 0.776971161365509\n",
      "Train: Epoch [9], Batch [148/938], Loss: 0.5935917496681213\n",
      "Train: Epoch [9], Batch [149/938], Loss: 0.9370232820510864\n",
      "Train: Epoch [9], Batch [150/938], Loss: 0.6527673602104187\n",
      "Train: Epoch [9], Batch [151/938], Loss: 0.8828442692756653\n",
      "Train: Epoch [9], Batch [152/938], Loss: 0.8394805192947388\n",
      "Train: Epoch [9], Batch [153/938], Loss: 0.8424280285835266\n",
      "Train: Epoch [9], Batch [154/938], Loss: 0.7881597876548767\n",
      "Train: Epoch [9], Batch [155/938], Loss: 0.8757718205451965\n",
      "Train: Epoch [9], Batch [156/938], Loss: 0.7277116775512695\n",
      "Train: Epoch [9], Batch [157/938], Loss: 0.7485311031341553\n",
      "Train: Epoch [9], Batch [158/938], Loss: 0.8515597581863403\n",
      "Train: Epoch [9], Batch [159/938], Loss: 0.7682591676712036\n",
      "Train: Epoch [9], Batch [160/938], Loss: 0.6880608201026917\n",
      "Train: Epoch [9], Batch [161/938], Loss: 0.7805954217910767\n",
      "Train: Epoch [9], Batch [162/938], Loss: 0.7903501391410828\n",
      "Train: Epoch [9], Batch [163/938], Loss: 0.8292738199234009\n",
      "Train: Epoch [9], Batch [164/938], Loss: 0.9274231195449829\n",
      "Train: Epoch [9], Batch [165/938], Loss: 0.7380779981613159\n",
      "Train: Epoch [9], Batch [166/938], Loss: 0.8321022987365723\n",
      "Train: Epoch [9], Batch [167/938], Loss: 0.9957607984542847\n",
      "Train: Epoch [9], Batch [168/938], Loss: 0.8889697790145874\n",
      "Train: Epoch [9], Batch [169/938], Loss: 0.9126825928688049\n",
      "Train: Epoch [9], Batch [170/938], Loss: 0.921218991279602\n",
      "Train: Epoch [9], Batch [171/938], Loss: 0.8689706325531006\n",
      "Train: Epoch [9], Batch [172/938], Loss: 0.6390186548233032\n",
      "Train: Epoch [9], Batch [173/938], Loss: 0.8467431664466858\n",
      "Train: Epoch [9], Batch [174/938], Loss: 0.726389467716217\n",
      "Train: Epoch [9], Batch [175/938], Loss: 1.0000691413879395\n",
      "Train: Epoch [9], Batch [176/938], Loss: 0.6947446465492249\n",
      "Train: Epoch [9], Batch [177/938], Loss: 1.0120928287506104\n",
      "Train: Epoch [9], Batch [178/938], Loss: 1.016383409500122\n",
      "Train: Epoch [9], Batch [179/938], Loss: 0.949360191822052\n",
      "Train: Epoch [9], Batch [180/938], Loss: 0.7042144536972046\n",
      "Train: Epoch [9], Batch [181/938], Loss: 1.0988672971725464\n",
      "Train: Epoch [9], Batch [182/938], Loss: 0.8337744474411011\n",
      "Train: Epoch [9], Batch [183/938], Loss: 0.7596394419670105\n",
      "Train: Epoch [9], Batch [184/938], Loss: 0.8438287973403931\n",
      "Train: Epoch [9], Batch [185/938], Loss: 1.032099962234497\n",
      "Train: Epoch [9], Batch [186/938], Loss: 0.7191462516784668\n",
      "Train: Epoch [9], Batch [187/938], Loss: 0.7549692988395691\n",
      "Train: Epoch [9], Batch [188/938], Loss: 0.6145601272583008\n",
      "Train: Epoch [9], Batch [189/938], Loss: 0.8010876178741455\n",
      "Train: Epoch [9], Batch [190/938], Loss: 0.6452082991600037\n",
      "Train: Epoch [9], Batch [191/938], Loss: 0.8249520659446716\n",
      "Train: Epoch [9], Batch [192/938], Loss: 0.9539660215377808\n",
      "Train: Epoch [9], Batch [193/938], Loss: 0.565717875957489\n",
      "Train: Epoch [9], Batch [194/938], Loss: 0.9907145500183105\n",
      "Train: Epoch [9], Batch [195/938], Loss: 0.8336336612701416\n",
      "Train: Epoch [9], Batch [196/938], Loss: 0.8636513352394104\n",
      "Train: Epoch [9], Batch [197/938], Loss: 0.9028059244155884\n",
      "Train: Epoch [9], Batch [198/938], Loss: 0.9210736751556396\n",
      "Train: Epoch [9], Batch [199/938], Loss: 0.909841001033783\n",
      "Train: Epoch [9], Batch [200/938], Loss: 0.7793286442756653\n",
      "Train: Epoch [9], Batch [201/938], Loss: 0.5781312584877014\n",
      "Train: Epoch [9], Batch [202/938], Loss: 0.6518595218658447\n",
      "Train: Epoch [9], Batch [203/938], Loss: 0.8072072267532349\n",
      "Train: Epoch [9], Batch [204/938], Loss: 0.9072784185409546\n",
      "Train: Epoch [9], Batch [205/938], Loss: 0.8811457753181458\n",
      "Train: Epoch [9], Batch [206/938], Loss: 0.8071616291999817\n",
      "Train: Epoch [9], Batch [207/938], Loss: 0.8433914184570312\n",
      "Train: Epoch [9], Batch [208/938], Loss: 0.7327553033828735\n",
      "Train: Epoch [9], Batch [209/938], Loss: 0.7171791791915894\n",
      "Train: Epoch [9], Batch [210/938], Loss: 0.7997877597808838\n",
      "Train: Epoch [9], Batch [211/938], Loss: 0.6710638403892517\n",
      "Train: Epoch [9], Batch [212/938], Loss: 0.7048985958099365\n",
      "Train: Epoch [9], Batch [213/938], Loss: 0.8536606431007385\n",
      "Train: Epoch [9], Batch [214/938], Loss: 0.8229554891586304\n",
      "Train: Epoch [9], Batch [215/938], Loss: 0.6538099050521851\n",
      "Train: Epoch [9], Batch [216/938], Loss: 0.6802718043327332\n",
      "Train: Epoch [9], Batch [217/938], Loss: 0.9344486594200134\n",
      "Train: Epoch [9], Batch [218/938], Loss: 0.8411762118339539\n",
      "Train: Epoch [9], Batch [219/938], Loss: 0.717420220375061\n",
      "Train: Epoch [9], Batch [220/938], Loss: 0.5079707503318787\n",
      "Train: Epoch [9], Batch [221/938], Loss: 0.73891681432724\n",
      "Train: Epoch [9], Batch [222/938], Loss: 0.9034745693206787\n",
      "Train: Epoch [9], Batch [223/938], Loss: 0.7899360656738281\n",
      "Train: Epoch [9], Batch [224/938], Loss: 1.0903033018112183\n",
      "Train: Epoch [9], Batch [225/938], Loss: 0.9255714416503906\n",
      "Train: Epoch [9], Batch [226/938], Loss: 0.7602712512016296\n",
      "Train: Epoch [9], Batch [227/938], Loss: 1.0104748010635376\n",
      "Train: Epoch [9], Batch [228/938], Loss: 0.8931674957275391\n",
      "Train: Epoch [9], Batch [229/938], Loss: 0.8846794962882996\n",
      "Train: Epoch [9], Batch [230/938], Loss: 0.862136721611023\n",
      "Train: Epoch [9], Batch [231/938], Loss: 0.6798973083496094\n",
      "Train: Epoch [9], Batch [232/938], Loss: 0.7141268253326416\n",
      "Train: Epoch [9], Batch [233/938], Loss: 0.8297041058540344\n",
      "Train: Epoch [9], Batch [234/938], Loss: 0.804601788520813\n",
      "Train: Epoch [9], Batch [235/938], Loss: 0.7888500094413757\n",
      "Train: Epoch [9], Batch [236/938], Loss: 0.9586857557296753\n",
      "Train: Epoch [9], Batch [237/938], Loss: 0.7572412490844727\n",
      "Train: Epoch [9], Batch [238/938], Loss: 0.739018976688385\n",
      "Train: Epoch [9], Batch [239/938], Loss: 0.7468195557594299\n",
      "Train: Epoch [9], Batch [240/938], Loss: 0.9349063038825989\n",
      "Train: Epoch [9], Batch [241/938], Loss: 0.5853772163391113\n",
      "Train: Epoch [9], Batch [242/938], Loss: 0.9802145957946777\n",
      "Train: Epoch [9], Batch [243/938], Loss: 0.949607253074646\n",
      "Train: Epoch [9], Batch [244/938], Loss: 0.8366186618804932\n",
      "Train: Epoch [9], Batch [245/938], Loss: 0.693840503692627\n",
      "Train: Epoch [9], Batch [246/938], Loss: 0.9809975028038025\n",
      "Train: Epoch [9], Batch [247/938], Loss: 0.8283700346946716\n",
      "Train: Epoch [9], Batch [248/938], Loss: 0.8349612951278687\n",
      "Train: Epoch [9], Batch [249/938], Loss: 0.7275640964508057\n",
      "Train: Epoch [9], Batch [250/938], Loss: 0.8984510898590088\n",
      "Train: Epoch [9], Batch [251/938], Loss: 0.8268660306930542\n",
      "Train: Epoch [9], Batch [252/938], Loss: 0.7266831994056702\n",
      "Train: Epoch [9], Batch [253/938], Loss: 0.7482464909553528\n",
      "Train: Epoch [9], Batch [254/938], Loss: 0.6427892446517944\n",
      "Train: Epoch [9], Batch [255/938], Loss: 0.8990307450294495\n",
      "Train: Epoch [9], Batch [256/938], Loss: 0.8032576441764832\n",
      "Train: Epoch [9], Batch [257/938], Loss: 0.6067348122596741\n",
      "Train: Epoch [9], Batch [258/938], Loss: 0.9558224678039551\n",
      "Train: Epoch [9], Batch [259/938], Loss: 1.066631555557251\n",
      "Train: Epoch [9], Batch [260/938], Loss: 0.9384796619415283\n",
      "Train: Epoch [9], Batch [261/938], Loss: 0.8587881922721863\n",
      "Train: Epoch [9], Batch [262/938], Loss: 0.9253572821617126\n",
      "Train: Epoch [9], Batch [263/938], Loss: 0.8096186518669128\n",
      "Train: Epoch [9], Batch [264/938], Loss: 0.767909049987793\n",
      "Train: Epoch [9], Batch [265/938], Loss: 0.7198821902275085\n",
      "Train: Epoch [9], Batch [266/938], Loss: 0.6299580931663513\n",
      "Train: Epoch [9], Batch [267/938], Loss: 0.6100514531135559\n",
      "Train: Epoch [9], Batch [268/938], Loss: 0.7503533363342285\n",
      "Train: Epoch [9], Batch [269/938], Loss: 0.7968397736549377\n",
      "Train: Epoch [9], Batch [270/938], Loss: 0.9822288751602173\n",
      "Train: Epoch [9], Batch [271/938], Loss: 0.9377498626708984\n",
      "Train: Epoch [9], Batch [272/938], Loss: 0.7627910375595093\n",
      "Train: Epoch [9], Batch [273/938], Loss: 0.9938391447067261\n",
      "Train: Epoch [9], Batch [274/938], Loss: 0.7956211566925049\n",
      "Train: Epoch [9], Batch [275/938], Loss: 1.1049457788467407\n",
      "Train: Epoch [9], Batch [276/938], Loss: 0.7387375235557556\n",
      "Train: Epoch [9], Batch [277/938], Loss: 0.8729104995727539\n",
      "Train: Epoch [9], Batch [278/938], Loss: 0.8565148115158081\n",
      "Train: Epoch [9], Batch [279/938], Loss: 0.7643170952796936\n",
      "Train: Epoch [9], Batch [280/938], Loss: 0.9590842127799988\n",
      "Train: Epoch [9], Batch [281/938], Loss: 0.8825429677963257\n",
      "Train: Epoch [9], Batch [282/938], Loss: 0.7980386018753052\n",
      "Train: Epoch [9], Batch [283/938], Loss: 0.7281304001808167\n",
      "Train: Epoch [9], Batch [284/938], Loss: 0.8358879685401917\n",
      "Train: Epoch [9], Batch [285/938], Loss: 0.6057130694389343\n",
      "Train: Epoch [9], Batch [286/938], Loss: 0.6580723524093628\n",
      "Train: Epoch [9], Batch [287/938], Loss: 0.698584258556366\n",
      "Train: Epoch [9], Batch [288/938], Loss: 0.9832489490509033\n",
      "Train: Epoch [9], Batch [289/938], Loss: 0.6486817002296448\n",
      "Train: Epoch [9], Batch [290/938], Loss: 0.6341572999954224\n",
      "Train: Epoch [9], Batch [291/938], Loss: 0.620707094669342\n",
      "Train: Epoch [9], Batch [292/938], Loss: 0.8132664561271667\n",
      "Train: Epoch [9], Batch [293/938], Loss: 0.7534775733947754\n",
      "Train: Epoch [9], Batch [294/938], Loss: 0.7651306986808777\n",
      "Train: Epoch [9], Batch [295/938], Loss: 0.9509245157241821\n",
      "Train: Epoch [9], Batch [296/938], Loss: 0.730056881904602\n",
      "Train: Epoch [9], Batch [297/938], Loss: 0.9174937009811401\n",
      "Train: Epoch [9], Batch [298/938], Loss: 0.6412540674209595\n",
      "Train: Epoch [9], Batch [299/938], Loss: 0.8676612377166748\n",
      "Train: Epoch [9], Batch [300/938], Loss: 1.1573013067245483\n",
      "Train: Epoch [9], Batch [301/938], Loss: 0.9750024080276489\n",
      "Train: Epoch [9], Batch [302/938], Loss: 0.676913321018219\n",
      "Train: Epoch [9], Batch [303/938], Loss: 0.6953731775283813\n",
      "Train: Epoch [9], Batch [304/938], Loss: 0.7608185410499573\n",
      "Train: Epoch [9], Batch [305/938], Loss: 0.7856537699699402\n",
      "Train: Epoch [9], Batch [306/938], Loss: 0.7841910123825073\n",
      "Train: Epoch [9], Batch [307/938], Loss: 0.7953001856803894\n",
      "Train: Epoch [9], Batch [308/938], Loss: 0.9699748754501343\n",
      "Train: Epoch [9], Batch [309/938], Loss: 0.9668397307395935\n",
      "Train: Epoch [9], Batch [310/938], Loss: 0.6898698210716248\n",
      "Train: Epoch [9], Batch [311/938], Loss: 0.803853452205658\n",
      "Train: Epoch [9], Batch [312/938], Loss: 0.7526620030403137\n",
      "Train: Epoch [9], Batch [313/938], Loss: 1.0271638631820679\n",
      "Train: Epoch [9], Batch [314/938], Loss: 0.9431872367858887\n",
      "Train: Epoch [9], Batch [315/938], Loss: 0.902298629283905\n",
      "Train: Epoch [9], Batch [316/938], Loss: 0.8181129693984985\n",
      "Train: Epoch [9], Batch [317/938], Loss: 0.9020540714263916\n",
      "Train: Epoch [9], Batch [318/938], Loss: 0.7280985713005066\n",
      "Train: Epoch [9], Batch [319/938], Loss: 0.8209612965583801\n",
      "Train: Epoch [9], Batch [320/938], Loss: 0.864629328250885\n",
      "Train: Epoch [9], Batch [321/938], Loss: 1.0072875022888184\n",
      "Train: Epoch [9], Batch [322/938], Loss: 0.7855217456817627\n",
      "Train: Epoch [9], Batch [323/938], Loss: 0.7476550340652466\n",
      "Train: Epoch [9], Batch [324/938], Loss: 1.0853755474090576\n",
      "Train: Epoch [9], Batch [325/938], Loss: 0.9327194690704346\n",
      "Train: Epoch [9], Batch [326/938], Loss: 0.9965329170227051\n",
      "Train: Epoch [9], Batch [327/938], Loss: 1.0050572156906128\n",
      "Train: Epoch [9], Batch [328/938], Loss: 0.6865804195404053\n",
      "Train: Epoch [9], Batch [329/938], Loss: 0.6462505459785461\n",
      "Train: Epoch [9], Batch [330/938], Loss: 0.8965020179748535\n",
      "Train: Epoch [9], Batch [331/938], Loss: 0.8169482946395874\n",
      "Train: Epoch [9], Batch [332/938], Loss: 0.7826206088066101\n",
      "Train: Epoch [9], Batch [333/938], Loss: 0.7263856530189514\n",
      "Train: Epoch [9], Batch [334/938], Loss: 0.7132288217544556\n",
      "Train: Epoch [9], Batch [335/938], Loss: 0.5567147731781006\n",
      "Train: Epoch [9], Batch [336/938], Loss: 0.8498920202255249\n",
      "Train: Epoch [9], Batch [337/938], Loss: 1.0049934387207031\n",
      "Train: Epoch [9], Batch [338/938], Loss: 0.6696946620941162\n",
      "Train: Epoch [9], Batch [339/938], Loss: 1.0266854763031006\n",
      "Train: Epoch [9], Batch [340/938], Loss: 0.9453620910644531\n",
      "Train: Epoch [9], Batch [341/938], Loss: 1.0370233058929443\n",
      "Train: Epoch [9], Batch [342/938], Loss: 0.9307122230529785\n",
      "Train: Epoch [9], Batch [343/938], Loss: 0.6913794875144958\n",
      "Train: Epoch [9], Batch [344/938], Loss: 0.834376335144043\n",
      "Train: Epoch [9], Batch [345/938], Loss: 0.6512025594711304\n",
      "Train: Epoch [9], Batch [346/938], Loss: 0.859300434589386\n",
      "Train: Epoch [9], Batch [347/938], Loss: 0.7548264265060425\n",
      "Train: Epoch [9], Batch [348/938], Loss: 0.8586744666099548\n",
      "Train: Epoch [9], Batch [349/938], Loss: 0.9577062726020813\n",
      "Train: Epoch [9], Batch [350/938], Loss: 0.7269304990768433\n",
      "Train: Epoch [9], Batch [351/938], Loss: 0.9286025762557983\n",
      "Train: Epoch [9], Batch [352/938], Loss: 0.7652230262756348\n",
      "Train: Epoch [9], Batch [353/938], Loss: 0.9472059011459351\n",
      "Train: Epoch [9], Batch [354/938], Loss: 0.9129906892776489\n",
      "Train: Epoch [9], Batch [355/938], Loss: 0.818436324596405\n",
      "Train: Epoch [9], Batch [356/938], Loss: 1.0906258821487427\n",
      "Train: Epoch [9], Batch [357/938], Loss: 0.8193517327308655\n",
      "Train: Epoch [9], Batch [358/938], Loss: 0.6542271375656128\n",
      "Train: Epoch [9], Batch [359/938], Loss: 0.9209362268447876\n",
      "Train: Epoch [9], Batch [360/938], Loss: 0.8926050066947937\n",
      "Train: Epoch [9], Batch [361/938], Loss: 0.7353931665420532\n",
      "Train: Epoch [9], Batch [362/938], Loss: 0.6143521666526794\n",
      "Train: Epoch [9], Batch [363/938], Loss: 0.9381595849990845\n",
      "Train: Epoch [9], Batch [364/938], Loss: 0.7536246180534363\n",
      "Train: Epoch [9], Batch [365/938], Loss: 0.8423850536346436\n",
      "Train: Epoch [9], Batch [366/938], Loss: 0.6271418333053589\n",
      "Train: Epoch [9], Batch [367/938], Loss: 0.7786802053451538\n",
      "Train: Epoch [9], Batch [368/938], Loss: 0.7678499221801758\n",
      "Train: Epoch [9], Batch [369/938], Loss: 0.777751088142395\n",
      "Train: Epoch [9], Batch [370/938], Loss: 0.7451117038726807\n",
      "Train: Epoch [9], Batch [371/938], Loss: 0.8328115344047546\n",
      "Train: Epoch [9], Batch [372/938], Loss: 0.9918212294578552\n",
      "Train: Epoch [9], Batch [373/938], Loss: 0.8718186020851135\n",
      "Train: Epoch [9], Batch [374/938], Loss: 0.8182723522186279\n",
      "Train: Epoch [9], Batch [375/938], Loss: 0.654188334941864\n",
      "Train: Epoch [9], Batch [376/938], Loss: 0.7994639873504639\n",
      "Train: Epoch [9], Batch [377/938], Loss: 0.6357011795043945\n",
      "Train: Epoch [9], Batch [378/938], Loss: 0.7966799736022949\n",
      "Train: Epoch [9], Batch [379/938], Loss: 0.9477086067199707\n",
      "Train: Epoch [9], Batch [380/938], Loss: 0.6443681716918945\n",
      "Train: Epoch [9], Batch [381/938], Loss: 0.8432917594909668\n",
      "Train: Epoch [9], Batch [382/938], Loss: 0.8804610371589661\n",
      "Train: Epoch [9], Batch [383/938], Loss: 0.6880155205726624\n",
      "Train: Epoch [9], Batch [384/938], Loss: 0.8563593029975891\n",
      "Train: Epoch [9], Batch [385/938], Loss: 0.9598191976547241\n",
      "Train: Epoch [9], Batch [386/938], Loss: 0.8242573738098145\n",
      "Train: Epoch [9], Batch [387/938], Loss: 0.9186720848083496\n",
      "Train: Epoch [9], Batch [388/938], Loss: 0.9786764979362488\n",
      "Train: Epoch [9], Batch [389/938], Loss: 0.9009166955947876\n",
      "Train: Epoch [9], Batch [390/938], Loss: 0.6759939193725586\n",
      "Train: Epoch [9], Batch [391/938], Loss: 0.9181963801383972\n",
      "Train: Epoch [9], Batch [392/938], Loss: 0.8393427729606628\n",
      "Train: Epoch [9], Batch [393/938], Loss: 0.616089403629303\n",
      "Train: Epoch [9], Batch [394/938], Loss: 0.7279306650161743\n",
      "Train: Epoch [9], Batch [395/938], Loss: 0.7763046622276306\n",
      "Train: Epoch [9], Batch [396/938], Loss: 0.9511523246765137\n",
      "Train: Epoch [9], Batch [397/938], Loss: 0.9451640248298645\n",
      "Train: Epoch [9], Batch [398/938], Loss: 0.9333987236022949\n",
      "Train: Epoch [9], Batch [399/938], Loss: 0.8665779829025269\n",
      "Train: Epoch [9], Batch [400/938], Loss: 0.8984233140945435\n",
      "Train: Epoch [9], Batch [401/938], Loss: 0.8716073036193848\n",
      "Train: Epoch [9], Batch [402/938], Loss: 0.7955197095870972\n",
      "Train: Epoch [9], Batch [403/938], Loss: 0.7810735106468201\n",
      "Train: Epoch [9], Batch [404/938], Loss: 0.742260754108429\n",
      "Train: Epoch [9], Batch [405/938], Loss: 0.7608506679534912\n",
      "Train: Epoch [9], Batch [406/938], Loss: 0.6162029504776001\n",
      "Train: Epoch [9], Batch [407/938], Loss: 0.8500762581825256\n",
      "Train: Epoch [9], Batch [408/938], Loss: 0.6144888997077942\n",
      "Train: Epoch [9], Batch [409/938], Loss: 1.1207892894744873\n",
      "Train: Epoch [9], Batch [410/938], Loss: 0.7327914237976074\n",
      "Train: Epoch [9], Batch [411/938], Loss: 1.0529589653015137\n",
      "Train: Epoch [9], Batch [412/938], Loss: 0.5815082788467407\n",
      "Train: Epoch [9], Batch [413/938], Loss: 0.8214074373245239\n",
      "Train: Epoch [9], Batch [414/938], Loss: 0.7600624561309814\n",
      "Train: Epoch [9], Batch [415/938], Loss: 0.9740662574768066\n",
      "Train: Epoch [9], Batch [416/938], Loss: 0.934411346912384\n",
      "Train: Epoch [9], Batch [417/938], Loss: 0.820602297782898\n",
      "Train: Epoch [9], Batch [418/938], Loss: 0.4650575518608093\n",
      "Train: Epoch [9], Batch [419/938], Loss: 0.9728837013244629\n",
      "Train: Epoch [9], Batch [420/938], Loss: 0.7390070557594299\n",
      "Train: Epoch [9], Batch [421/938], Loss: 0.7497456669807434\n",
      "Train: Epoch [9], Batch [422/938], Loss: 0.6643860936164856\n",
      "Train: Epoch [9], Batch [423/938], Loss: 0.7745471596717834\n",
      "Train: Epoch [9], Batch [424/938], Loss: 0.5994174480438232\n",
      "Train: Epoch [9], Batch [425/938], Loss: 0.7764644622802734\n",
      "Train: Epoch [9], Batch [426/938], Loss: 0.852432906627655\n",
      "Train: Epoch [9], Batch [427/938], Loss: 0.7663064002990723\n",
      "Train: Epoch [9], Batch [428/938], Loss: 0.7521362900733948\n",
      "Train: Epoch [9], Batch [429/938], Loss: 0.6760192513465881\n",
      "Train: Epoch [9], Batch [430/938], Loss: 0.8086656928062439\n",
      "Train: Epoch [9], Batch [431/938], Loss: 0.8743036985397339\n",
      "Train: Epoch [9], Batch [432/938], Loss: 0.8107983469963074\n",
      "Train: Epoch [9], Batch [433/938], Loss: 1.0332682132720947\n",
      "Train: Epoch [9], Batch [434/938], Loss: 0.7434634566307068\n",
      "Train: Epoch [9], Batch [435/938], Loss: 1.0118261575698853\n",
      "Train: Epoch [9], Batch [436/938], Loss: 0.5578614473342896\n",
      "Train: Epoch [9], Batch [437/938], Loss: 0.7620786428451538\n",
      "Train: Epoch [9], Batch [438/938], Loss: 0.6294525861740112\n",
      "Train: Epoch [9], Batch [439/938], Loss: 0.7288026809692383\n",
      "Train: Epoch [9], Batch [440/938], Loss: 0.6865013241767883\n",
      "Train: Epoch [9], Batch [441/938], Loss: 0.8217677474021912\n",
      "Train: Epoch [9], Batch [442/938], Loss: 1.0062921047210693\n",
      "Train: Epoch [9], Batch [443/938], Loss: 0.960221529006958\n",
      "Train: Epoch [9], Batch [444/938], Loss: 0.842803955078125\n",
      "Train: Epoch [9], Batch [445/938], Loss: 1.0112804174423218\n",
      "Train: Epoch [9], Batch [446/938], Loss: 0.8046443462371826\n",
      "Train: Epoch [9], Batch [447/938], Loss: 0.5938282608985901\n",
      "Train: Epoch [9], Batch [448/938], Loss: 0.6851097345352173\n",
      "Train: Epoch [9], Batch [449/938], Loss: 0.719035267829895\n",
      "Train: Epoch [9], Batch [450/938], Loss: 0.7259397506713867\n",
      "Train: Epoch [9], Batch [451/938], Loss: 0.5553228259086609\n",
      "Train: Epoch [9], Batch [452/938], Loss: 0.8418598771095276\n",
      "Train: Epoch [9], Batch [453/938], Loss: 0.9652475714683533\n",
      "Train: Epoch [9], Batch [454/938], Loss: 0.9375842809677124\n",
      "Train: Epoch [9], Batch [455/938], Loss: 0.5884578824043274\n",
      "Train: Epoch [9], Batch [456/938], Loss: 0.6448644399642944\n",
      "Train: Epoch [9], Batch [457/938], Loss: 1.1745526790618896\n",
      "Train: Epoch [9], Batch [458/938], Loss: 0.8505505919456482\n",
      "Train: Epoch [9], Batch [459/938], Loss: 1.0008565187454224\n",
      "Train: Epoch [9], Batch [460/938], Loss: 0.707522451877594\n",
      "Train: Epoch [9], Batch [461/938], Loss: 0.7157148122787476\n",
      "Train: Epoch [9], Batch [462/938], Loss: 0.971290111541748\n",
      "Train: Epoch [9], Batch [463/938], Loss: 0.8324518799781799\n",
      "Train: Epoch [9], Batch [464/938], Loss: 0.6319897174835205\n",
      "Train: Epoch [9], Batch [465/938], Loss: 0.7006433010101318\n",
      "Train: Epoch [9], Batch [466/938], Loss: 0.724445641040802\n",
      "Train: Epoch [9], Batch [467/938], Loss: 0.9554723501205444\n",
      "Train: Epoch [9], Batch [468/938], Loss: 0.6760333180427551\n",
      "Train: Epoch [9], Batch [469/938], Loss: 0.72822505235672\n",
      "Train: Epoch [9], Batch [470/938], Loss: 0.7232439517974854\n",
      "Train: Epoch [9], Batch [471/938], Loss: 0.7730966806411743\n",
      "Train: Epoch [9], Batch [472/938], Loss: 0.9141885042190552\n",
      "Train: Epoch [9], Batch [473/938], Loss: 0.7336558699607849\n",
      "Train: Epoch [9], Batch [474/938], Loss: 0.8404445648193359\n",
      "Train: Epoch [9], Batch [475/938], Loss: 0.7187938690185547\n",
      "Train: Epoch [9], Batch [476/938], Loss: 0.7445723414421082\n",
      "Train: Epoch [9], Batch [477/938], Loss: 0.7796575427055359\n",
      "Train: Epoch [9], Batch [478/938], Loss: 0.826252818107605\n",
      "Train: Epoch [9], Batch [479/938], Loss: 0.8097769618034363\n",
      "Train: Epoch [9], Batch [480/938], Loss: 0.7891747355461121\n",
      "Train: Epoch [9], Batch [481/938], Loss: 0.6989385485649109\n",
      "Train: Epoch [9], Batch [482/938], Loss: 0.6076321005821228\n",
      "Train: Epoch [9], Batch [483/938], Loss: 0.8156100511550903\n",
      "Train: Epoch [9], Batch [484/938], Loss: 0.6741172671318054\n",
      "Train: Epoch [9], Batch [485/938], Loss: 0.8280439376831055\n",
      "Train: Epoch [9], Batch [486/938], Loss: 0.6241402626037598\n",
      "Train: Epoch [9], Batch [487/938], Loss: 0.8990074396133423\n",
      "Train: Epoch [9], Batch [488/938], Loss: 0.9661910533905029\n",
      "Train: Epoch [9], Batch [489/938], Loss: 0.6482650637626648\n",
      "Train: Epoch [9], Batch [490/938], Loss: 1.4677927494049072\n",
      "Train: Epoch [9], Batch [491/938], Loss: 1.0120426416397095\n",
      "Train: Epoch [9], Batch [492/938], Loss: 0.8107647895812988\n",
      "Train: Epoch [9], Batch [493/938], Loss: 0.7649033069610596\n",
      "Train: Epoch [9], Batch [494/938], Loss: 0.816588282585144\n",
      "Train: Epoch [9], Batch [495/938], Loss: 0.6539785265922546\n",
      "Train: Epoch [9], Batch [496/938], Loss: 0.7512710690498352\n",
      "Train: Epoch [9], Batch [497/938], Loss: 0.8547847270965576\n",
      "Train: Epoch [9], Batch [498/938], Loss: 0.8787450194358826\n",
      "Train: Epoch [9], Batch [499/938], Loss: 0.8922209143638611\n",
      "Train: Epoch [9], Batch [500/938], Loss: 0.6847657561302185\n",
      "Train: Epoch [9], Batch [501/938], Loss: 0.9927135109901428\n",
      "Train: Epoch [9], Batch [502/938], Loss: 0.8136815428733826\n",
      "Train: Epoch [9], Batch [503/938], Loss: 0.8646345734596252\n",
      "Train: Epoch [9], Batch [504/938], Loss: 0.7397794723510742\n",
      "Train: Epoch [9], Batch [505/938], Loss: 0.7826047539710999\n",
      "Train: Epoch [9], Batch [506/938], Loss: 0.6840866804122925\n",
      "Train: Epoch [9], Batch [507/938], Loss: 0.8188462853431702\n",
      "Train: Epoch [9], Batch [508/938], Loss: 0.8833876848220825\n",
      "Train: Epoch [9], Batch [509/938], Loss: 0.8883665800094604\n",
      "Train: Epoch [9], Batch [510/938], Loss: 0.8747048377990723\n",
      "Train: Epoch [9], Batch [511/938], Loss: 0.7910455465316772\n",
      "Train: Epoch [9], Batch [512/938], Loss: 0.6892238259315491\n",
      "Train: Epoch [9], Batch [513/938], Loss: 0.895490825176239\n",
      "Train: Epoch [9], Batch [514/938], Loss: 0.8343868851661682\n",
      "Train: Epoch [9], Batch [515/938], Loss: 0.8295058012008667\n",
      "Train: Epoch [9], Batch [516/938], Loss: 0.7313663363456726\n",
      "Train: Epoch [9], Batch [517/938], Loss: 0.826748788356781\n",
      "Train: Epoch [9], Batch [518/938], Loss: 0.8858197927474976\n",
      "Train: Epoch [9], Batch [519/938], Loss: 0.9213557243347168\n",
      "Train: Epoch [9], Batch [520/938], Loss: 0.7768454551696777\n",
      "Train: Epoch [9], Batch [521/938], Loss: 0.8968566656112671\n",
      "Train: Epoch [9], Batch [522/938], Loss: 0.6108708381652832\n",
      "Train: Epoch [9], Batch [523/938], Loss: 0.6627913117408752\n",
      "Train: Epoch [9], Batch [524/938], Loss: 0.7190316319465637\n",
      "Train: Epoch [9], Batch [525/938], Loss: 0.7937672138214111\n",
      "Train: Epoch [9], Batch [526/938], Loss: 0.8244536519050598\n",
      "Train: Epoch [9], Batch [527/938], Loss: 1.0509699583053589\n",
      "Train: Epoch [9], Batch [528/938], Loss: 0.6804846525192261\n",
      "Train: Epoch [9], Batch [529/938], Loss: 0.9085372090339661\n",
      "Train: Epoch [9], Batch [530/938], Loss: 0.9934598207473755\n",
      "Train: Epoch [9], Batch [531/938], Loss: 0.7234375476837158\n",
      "Train: Epoch [9], Batch [532/938], Loss: 0.7339076399803162\n",
      "Train: Epoch [9], Batch [533/938], Loss: 0.8154399394989014\n",
      "Train: Epoch [9], Batch [534/938], Loss: 0.6531829833984375\n",
      "Train: Epoch [9], Batch [535/938], Loss: 0.6382389068603516\n",
      "Train: Epoch [9], Batch [536/938], Loss: 0.7051019072532654\n",
      "Train: Epoch [9], Batch [537/938], Loss: 0.5401121377944946\n",
      "Train: Epoch [9], Batch [538/938], Loss: 0.7476984858512878\n",
      "Train: Epoch [9], Batch [539/938], Loss: 0.7958709001541138\n",
      "Train: Epoch [9], Batch [540/938], Loss: 0.9722766876220703\n",
      "Train: Epoch [9], Batch [541/938], Loss: 0.6869010329246521\n",
      "Train: Epoch [9], Batch [542/938], Loss: 0.8098993301391602\n",
      "Train: Epoch [9], Batch [543/938], Loss: 0.9585118293762207\n",
      "Train: Epoch [9], Batch [544/938], Loss: 0.7887122631072998\n",
      "Train: Epoch [9], Batch [545/938], Loss: 0.7570417523384094\n",
      "Train: Epoch [9], Batch [546/938], Loss: 0.7437711358070374\n",
      "Train: Epoch [9], Batch [547/938], Loss: 0.841082751750946\n",
      "Train: Epoch [9], Batch [548/938], Loss: 0.8895210027694702\n",
      "Train: Epoch [9], Batch [549/938], Loss: 0.7264204025268555\n",
      "Train: Epoch [9], Batch [550/938], Loss: 0.8132078051567078\n",
      "Train: Epoch [9], Batch [551/938], Loss: 0.8796449303627014\n",
      "Train: Epoch [9], Batch [552/938], Loss: 0.9242737293243408\n",
      "Train: Epoch [9], Batch [553/938], Loss: 0.9416124820709229\n",
      "Train: Epoch [9], Batch [554/938], Loss: 0.621066689491272\n",
      "Train: Epoch [9], Batch [555/938], Loss: 0.9279704093933105\n",
      "Train: Epoch [9], Batch [556/938], Loss: 0.8788862228393555\n",
      "Train: Epoch [9], Batch [557/938], Loss: 0.8932572603225708\n",
      "Train: Epoch [9], Batch [558/938], Loss: 0.8224906921386719\n",
      "Train: Epoch [9], Batch [559/938], Loss: 1.0112838745117188\n",
      "Train: Epoch [9], Batch [560/938], Loss: 1.0089428424835205\n",
      "Train: Epoch [9], Batch [561/938], Loss: 0.8302685022354126\n",
      "Train: Epoch [9], Batch [562/938], Loss: 0.7243404984474182\n",
      "Train: Epoch [9], Batch [563/938], Loss: 0.8000316619873047\n",
      "Train: Epoch [9], Batch [564/938], Loss: 0.7799805402755737\n",
      "Train: Epoch [9], Batch [565/938], Loss: 0.650146484375\n",
      "Train: Epoch [9], Batch [566/938], Loss: 0.6249650716781616\n",
      "Train: Epoch [9], Batch [567/938], Loss: 0.7745027542114258\n",
      "Train: Epoch [9], Batch [568/938], Loss: 0.6287246942520142\n",
      "Train: Epoch [9], Batch [569/938], Loss: 0.9307206869125366\n",
      "Train: Epoch [9], Batch [570/938], Loss: 0.7540413737297058\n",
      "Train: Epoch [9], Batch [571/938], Loss: 0.8032832741737366\n",
      "Train: Epoch [9], Batch [572/938], Loss: 0.9357764720916748\n",
      "Train: Epoch [9], Batch [573/938], Loss: 0.872869610786438\n",
      "Train: Epoch [9], Batch [574/938], Loss: 0.8226557970046997\n",
      "Train: Epoch [9], Batch [575/938], Loss: 0.6730931401252747\n",
      "Train: Epoch [9], Batch [576/938], Loss: 0.8428031206130981\n",
      "Train: Epoch [9], Batch [577/938], Loss: 0.6555441617965698\n",
      "Train: Epoch [9], Batch [578/938], Loss: 0.5148429870605469\n",
      "Train: Epoch [9], Batch [579/938], Loss: 0.8830321431159973\n",
      "Train: Epoch [9], Batch [580/938], Loss: 0.7744475603103638\n",
      "Train: Epoch [9], Batch [581/938], Loss: 0.8356285095214844\n",
      "Train: Epoch [9], Batch [582/938], Loss: 0.7941871881484985\n",
      "Train: Epoch [9], Batch [583/938], Loss: 0.9151169061660767\n",
      "Train: Epoch [9], Batch [584/938], Loss: 0.7620965242385864\n",
      "Train: Epoch [9], Batch [585/938], Loss: 0.7819660902023315\n",
      "Train: Epoch [9], Batch [586/938], Loss: 0.6699323654174805\n",
      "Train: Epoch [9], Batch [587/938], Loss: 0.7799288630485535\n",
      "Train: Epoch [9], Batch [588/938], Loss: 0.9008631706237793\n",
      "Train: Epoch [9], Batch [589/938], Loss: 0.6325268149375916\n",
      "Train: Epoch [9], Batch [590/938], Loss: 0.7464908957481384\n",
      "Train: Epoch [9], Batch [591/938], Loss: 0.8464336395263672\n",
      "Train: Epoch [9], Batch [592/938], Loss: 0.8498244285583496\n",
      "Train: Epoch [9], Batch [593/938], Loss: 0.7525991797447205\n",
      "Train: Epoch [9], Batch [594/938], Loss: 0.7782319784164429\n",
      "Train: Epoch [9], Batch [595/938], Loss: 0.506964921951294\n",
      "Train: Epoch [9], Batch [596/938], Loss: 0.8323649168014526\n",
      "Train: Epoch [9], Batch [597/938], Loss: 0.904584527015686\n",
      "Train: Epoch [9], Batch [598/938], Loss: 1.0053706169128418\n",
      "Train: Epoch [9], Batch [599/938], Loss: 0.7000136375427246\n",
      "Train: Epoch [9], Batch [600/938], Loss: 0.881660521030426\n",
      "Train: Epoch [9], Batch [601/938], Loss: 0.7157189846038818\n",
      "Train: Epoch [9], Batch [602/938], Loss: 0.6306067705154419\n",
      "Train: Epoch [9], Batch [603/938], Loss: 0.6284767985343933\n",
      "Train: Epoch [9], Batch [604/938], Loss: 0.886061429977417\n",
      "Train: Epoch [9], Batch [605/938], Loss: 0.9037495851516724\n",
      "Train: Epoch [9], Batch [606/938], Loss: 0.7026379108428955\n",
      "Train: Epoch [9], Batch [607/938], Loss: 0.9344035387039185\n",
      "Train: Epoch [9], Batch [608/938], Loss: 0.762040376663208\n",
      "Train: Epoch [9], Batch [609/938], Loss: 0.8512032628059387\n",
      "Train: Epoch [9], Batch [610/938], Loss: 0.770060122013092\n",
      "Train: Epoch [9], Batch [611/938], Loss: 0.7251441478729248\n",
      "Train: Epoch [9], Batch [612/938], Loss: 0.7610692977905273\n",
      "Train: Epoch [9], Batch [613/938], Loss: 0.8626768589019775\n",
      "Train: Epoch [9], Batch [614/938], Loss: 0.9295802712440491\n",
      "Train: Epoch [9], Batch [615/938], Loss: 0.8909200429916382\n",
      "Train: Epoch [9], Batch [616/938], Loss: 0.597928524017334\n",
      "Train: Epoch [9], Batch [617/938], Loss: 0.7702637314796448\n",
      "Train: Epoch [9], Batch [618/938], Loss: 0.7655019760131836\n",
      "Train: Epoch [9], Batch [619/938], Loss: 0.7831442356109619\n",
      "Train: Epoch [9], Batch [620/938], Loss: 0.6257674694061279\n",
      "Train: Epoch [9], Batch [621/938], Loss: 0.7783942818641663\n",
      "Train: Epoch [9], Batch [622/938], Loss: 0.8850513100624084\n",
      "Train: Epoch [9], Batch [623/938], Loss: 0.7417919635772705\n",
      "Train: Epoch [9], Batch [624/938], Loss: 0.7911198139190674\n",
      "Train: Epoch [9], Batch [625/938], Loss: 0.7706565856933594\n",
      "Train: Epoch [9], Batch [626/938], Loss: 0.7644926905632019\n",
      "Train: Epoch [9], Batch [627/938], Loss: 0.6712929010391235\n",
      "Train: Epoch [9], Batch [628/938], Loss: 0.5853174328804016\n",
      "Train: Epoch [9], Batch [629/938], Loss: 0.8581018447875977\n",
      "Train: Epoch [9], Batch [630/938], Loss: 1.0594316720962524\n",
      "Train: Epoch [9], Batch [631/938], Loss: 0.7497086524963379\n",
      "Train: Epoch [9], Batch [632/938], Loss: 1.0494132041931152\n",
      "Train: Epoch [9], Batch [633/938], Loss: 0.7181627750396729\n",
      "Train: Epoch [9], Batch [634/938], Loss: 0.5837030410766602\n",
      "Train: Epoch [9], Batch [635/938], Loss: 1.0201232433319092\n",
      "Train: Epoch [9], Batch [636/938], Loss: 0.9415812492370605\n",
      "Train: Epoch [9], Batch [637/938], Loss: 0.735002338886261\n",
      "Train: Epoch [9], Batch [638/938], Loss: 0.7027623057365417\n",
      "Train: Epoch [9], Batch [639/938], Loss: 0.5266984105110168\n",
      "Train: Epoch [9], Batch [640/938], Loss: 0.7774448990821838\n",
      "Train: Epoch [9], Batch [641/938], Loss: 0.8442267179489136\n",
      "Train: Epoch [9], Batch [642/938], Loss: 0.8118854761123657\n",
      "Train: Epoch [9], Batch [643/938], Loss: 0.7409217953681946\n",
      "Train: Epoch [9], Batch [644/938], Loss: 1.015129804611206\n",
      "Train: Epoch [9], Batch [645/938], Loss: 0.7606703042984009\n",
      "Train: Epoch [9], Batch [646/938], Loss: 0.837382435798645\n",
      "Train: Epoch [9], Batch [647/938], Loss: 0.7916841506958008\n",
      "Train: Epoch [9], Batch [648/938], Loss: 0.9153834581375122\n",
      "Train: Epoch [9], Batch [649/938], Loss: 0.6332463026046753\n",
      "Train: Epoch [9], Batch [650/938], Loss: 0.8331019282341003\n",
      "Train: Epoch [9], Batch [651/938], Loss: 0.8670198917388916\n",
      "Train: Epoch [9], Batch [652/938], Loss: 1.1480796337127686\n",
      "Train: Epoch [9], Batch [653/938], Loss: 0.9202055335044861\n",
      "Train: Epoch [9], Batch [654/938], Loss: 0.6940113306045532\n",
      "Train: Epoch [9], Batch [655/938], Loss: 0.8266668319702148\n",
      "Train: Epoch [9], Batch [656/938], Loss: 0.9371049404144287\n",
      "Train: Epoch [9], Batch [657/938], Loss: 0.9048802852630615\n",
      "Train: Epoch [9], Batch [658/938], Loss: 0.706204354763031\n",
      "Train: Epoch [9], Batch [659/938], Loss: 0.7612106204032898\n",
      "Train: Epoch [9], Batch [660/938], Loss: 1.0106487274169922\n",
      "Train: Epoch [9], Batch [661/938], Loss: 0.8661053776741028\n",
      "Train: Epoch [9], Batch [662/938], Loss: 0.7912110686302185\n",
      "Train: Epoch [9], Batch [663/938], Loss: 0.8903342485427856\n",
      "Train: Epoch [9], Batch [664/938], Loss: 0.847957193851471\n",
      "Train: Epoch [9], Batch [665/938], Loss: 0.8488826751708984\n",
      "Train: Epoch [9], Batch [666/938], Loss: 0.8407910466194153\n",
      "Train: Epoch [9], Batch [667/938], Loss: 0.9799140095710754\n",
      "Train: Epoch [9], Batch [668/938], Loss: 0.7170259356498718\n",
      "Train: Epoch [9], Batch [669/938], Loss: 0.9282746911048889\n",
      "Train: Epoch [9], Batch [670/938], Loss: 0.9456637501716614\n",
      "Train: Epoch [9], Batch [671/938], Loss: 0.837228536605835\n",
      "Train: Epoch [9], Batch [672/938], Loss: 0.7371084094047546\n",
      "Train: Epoch [9], Batch [673/938], Loss: 0.8390666246414185\n",
      "Train: Epoch [9], Batch [674/938], Loss: 0.8912500143051147\n",
      "Train: Epoch [9], Batch [675/938], Loss: 0.8066653609275818\n",
      "Train: Epoch [9], Batch [676/938], Loss: 0.7408187389373779\n",
      "Train: Epoch [9], Batch [677/938], Loss: 0.6075594425201416\n",
      "Train: Epoch [9], Batch [678/938], Loss: 0.6821722388267517\n",
      "Train: Epoch [9], Batch [679/938], Loss: 0.5467516779899597\n",
      "Train: Epoch [9], Batch [680/938], Loss: 1.0441620349884033\n",
      "Train: Epoch [9], Batch [681/938], Loss: 0.7548253536224365\n",
      "Train: Epoch [9], Batch [682/938], Loss: 0.5403062701225281\n",
      "Train: Epoch [9], Batch [683/938], Loss: 1.1456639766693115\n",
      "Train: Epoch [9], Batch [684/938], Loss: 0.8625337481498718\n",
      "Train: Epoch [9], Batch [685/938], Loss: 0.7009881138801575\n",
      "Train: Epoch [9], Batch [686/938], Loss: 0.7067368626594543\n",
      "Train: Epoch [9], Batch [687/938], Loss: 0.8787416219711304\n",
      "Train: Epoch [9], Batch [688/938], Loss: 0.5992838144302368\n",
      "Train: Epoch [9], Batch [689/938], Loss: 0.8445959091186523\n",
      "Train: Epoch [9], Batch [690/938], Loss: 0.9924359321594238\n",
      "Train: Epoch [9], Batch [691/938], Loss: 0.8214040398597717\n",
      "Train: Epoch [9], Batch [692/938], Loss: 0.8322173953056335\n",
      "Train: Epoch [9], Batch [693/938], Loss: 0.7757174372673035\n",
      "Train: Epoch [9], Batch [694/938], Loss: 0.7108252644538879\n",
      "Train: Epoch [9], Batch [695/938], Loss: 0.9241989850997925\n",
      "Train: Epoch [9], Batch [696/938], Loss: 0.587320864200592\n",
      "Train: Epoch [9], Batch [697/938], Loss: 0.8603276610374451\n",
      "Train: Epoch [9], Batch [698/938], Loss: 0.8285496830940247\n",
      "Train: Epoch [9], Batch [699/938], Loss: 0.7042827010154724\n",
      "Train: Epoch [9], Batch [700/938], Loss: 0.940394401550293\n",
      "Train: Epoch [9], Batch [701/938], Loss: 0.7240957021713257\n",
      "Train: Epoch [9], Batch [702/938], Loss: 0.6706307530403137\n",
      "Train: Epoch [9], Batch [703/938], Loss: 0.8588613867759705\n",
      "Train: Epoch [9], Batch [704/938], Loss: 0.8074497580528259\n",
      "Train: Epoch [9], Batch [705/938], Loss: 0.7799199223518372\n",
      "Train: Epoch [9], Batch [706/938], Loss: 0.7766251564025879\n",
      "Train: Epoch [9], Batch [707/938], Loss: 0.8912948966026306\n",
      "Train: Epoch [9], Batch [708/938], Loss: 1.0420719385147095\n",
      "Train: Epoch [9], Batch [709/938], Loss: 0.8691744208335876\n",
      "Train: Epoch [9], Batch [710/938], Loss: 0.6910394430160522\n",
      "Train: Epoch [9], Batch [711/938], Loss: 0.6986716389656067\n",
      "Train: Epoch [9], Batch [712/938], Loss: 0.7178338766098022\n",
      "Train: Epoch [9], Batch [713/938], Loss: 0.9699314832687378\n",
      "Train: Epoch [9], Batch [714/938], Loss: 0.6752774119377136\n",
      "Train: Epoch [9], Batch [715/938], Loss: 0.8506935238838196\n",
      "Train: Epoch [9], Batch [716/938], Loss: 0.5403542518615723\n",
      "Train: Epoch [9], Batch [717/938], Loss: 0.9624940156936646\n",
      "Train: Epoch [9], Batch [718/938], Loss: 1.0515867471694946\n",
      "Train: Epoch [9], Batch [719/938], Loss: 0.805029034614563\n",
      "Train: Epoch [9], Batch [720/938], Loss: 0.6715288758277893\n",
      "Train: Epoch [9], Batch [721/938], Loss: 0.8024070262908936\n",
      "Train: Epoch [9], Batch [722/938], Loss: 0.7178472280502319\n",
      "Train: Epoch [9], Batch [723/938], Loss: 0.9205875992774963\n",
      "Train: Epoch [9], Batch [724/938], Loss: 0.9469983577728271\n",
      "Train: Epoch [9], Batch [725/938], Loss: 0.9772287607192993\n",
      "Train: Epoch [9], Batch [726/938], Loss: 0.8342071175575256\n",
      "Train: Epoch [9], Batch [727/938], Loss: 0.789386510848999\n",
      "Train: Epoch [9], Batch [728/938], Loss: 0.5178200006484985\n",
      "Train: Epoch [9], Batch [729/938], Loss: 0.7684727907180786\n",
      "Train: Epoch [9], Batch [730/938], Loss: 0.7238057255744934\n",
      "Train: Epoch [9], Batch [731/938], Loss: 0.6454728245735168\n",
      "Train: Epoch [9], Batch [732/938], Loss: 0.724990963935852\n",
      "Train: Epoch [9], Batch [733/938], Loss: 0.6963818073272705\n",
      "Train: Epoch [9], Batch [734/938], Loss: 0.8312423825263977\n",
      "Train: Epoch [9], Batch [735/938], Loss: 0.9173059463500977\n",
      "Train: Epoch [9], Batch [736/938], Loss: 0.8470296859741211\n",
      "Train: Epoch [9], Batch [737/938], Loss: 0.8669339418411255\n",
      "Train: Epoch [9], Batch [738/938], Loss: 0.7119817137718201\n",
      "Train: Epoch [9], Batch [739/938], Loss: 0.8403880596160889\n",
      "Train: Epoch [9], Batch [740/938], Loss: 0.6536660194396973\n",
      "Train: Epoch [9], Batch [741/938], Loss: 0.9128776788711548\n",
      "Train: Epoch [9], Batch [742/938], Loss: 0.6167867183685303\n",
      "Train: Epoch [9], Batch [743/938], Loss: 0.9529132843017578\n",
      "Train: Epoch [9], Batch [744/938], Loss: 0.7149608135223389\n",
      "Train: Epoch [9], Batch [745/938], Loss: 0.5993677973747253\n",
      "Train: Epoch [9], Batch [746/938], Loss: 0.9909175038337708\n",
      "Train: Epoch [9], Batch [747/938], Loss: 0.7223503589630127\n",
      "Train: Epoch [9], Batch [748/938], Loss: 0.7473800182342529\n",
      "Train: Epoch [9], Batch [749/938], Loss: 0.7749662399291992\n",
      "Train: Epoch [9], Batch [750/938], Loss: 0.7251827716827393\n",
      "Train: Epoch [9], Batch [751/938], Loss: 0.5879795551300049\n",
      "Train: Epoch [9], Batch [752/938], Loss: 0.9137825965881348\n",
      "Train: Epoch [9], Batch [753/938], Loss: 0.6501801609992981\n",
      "Train: Epoch [9], Batch [754/938], Loss: 0.7136603593826294\n",
      "Train: Epoch [9], Batch [755/938], Loss: 0.6536950469017029\n",
      "Train: Epoch [9], Batch [756/938], Loss: 0.621769905090332\n",
      "Train: Epoch [9], Batch [757/938], Loss: 0.7408781051635742\n",
      "Train: Epoch [9], Batch [758/938], Loss: 0.8667888641357422\n",
      "Train: Epoch [9], Batch [759/938], Loss: 0.9836323261260986\n",
      "Train: Epoch [9], Batch [760/938], Loss: 1.0960197448730469\n",
      "Train: Epoch [9], Batch [761/938], Loss: 0.687597393989563\n",
      "Train: Epoch [9], Batch [762/938], Loss: 0.90610271692276\n",
      "Train: Epoch [9], Batch [763/938], Loss: 1.2798268795013428\n",
      "Train: Epoch [9], Batch [764/938], Loss: 0.7707016468048096\n",
      "Train: Epoch [9], Batch [765/938], Loss: 0.8584175109863281\n",
      "Train: Epoch [9], Batch [766/938], Loss: 1.0912309885025024\n",
      "Train: Epoch [9], Batch [767/938], Loss: 0.5711873769760132\n",
      "Train: Epoch [9], Batch [768/938], Loss: 1.0608506202697754\n",
      "Train: Epoch [9], Batch [769/938], Loss: 0.8132469654083252\n",
      "Train: Epoch [9], Batch [770/938], Loss: 0.9753476977348328\n",
      "Train: Epoch [9], Batch [771/938], Loss: 0.7208864688873291\n",
      "Train: Epoch [9], Batch [772/938], Loss: 0.6173573732376099\n",
      "Train: Epoch [9], Batch [773/938], Loss: 0.7649419903755188\n",
      "Train: Epoch [9], Batch [774/938], Loss: 0.8300333023071289\n",
      "Train: Epoch [9], Batch [775/938], Loss: 0.7614629864692688\n",
      "Train: Epoch [9], Batch [776/938], Loss: 0.7557919025421143\n",
      "Train: Epoch [9], Batch [777/938], Loss: 0.654315710067749\n",
      "Train: Epoch [9], Batch [778/938], Loss: 0.666226327419281\n",
      "Train: Epoch [9], Batch [779/938], Loss: 0.9527574181556702\n",
      "Train: Epoch [9], Batch [780/938], Loss: 0.7970397472381592\n",
      "Train: Epoch [9], Batch [781/938], Loss: 0.8541422486305237\n",
      "Train: Epoch [9], Batch [782/938], Loss: 0.6099804043769836\n",
      "Train: Epoch [9], Batch [783/938], Loss: 0.6581209301948547\n",
      "Train: Epoch [9], Batch [784/938], Loss: 0.6846644282341003\n",
      "Train: Epoch [9], Batch [785/938], Loss: 0.5535767078399658\n",
      "Train: Epoch [9], Batch [786/938], Loss: 0.7475402355194092\n",
      "Train: Epoch [9], Batch [787/938], Loss: 0.7217563390731812\n",
      "Train: Epoch [9], Batch [788/938], Loss: 0.6879128813743591\n",
      "Train: Epoch [9], Batch [789/938], Loss: 0.9002219438552856\n",
      "Train: Epoch [9], Batch [790/938], Loss: 0.6249512434005737\n",
      "Train: Epoch [9], Batch [791/938], Loss: 1.1044827699661255\n",
      "Train: Epoch [9], Batch [792/938], Loss: 0.7467649579048157\n",
      "Train: Epoch [9], Batch [793/938], Loss: 0.7595711350440979\n",
      "Train: Epoch [9], Batch [794/938], Loss: 0.7734801173210144\n",
      "Train: Epoch [9], Batch [795/938], Loss: 0.7527075409889221\n",
      "Train: Epoch [9], Batch [796/938], Loss: 1.053565502166748\n",
      "Train: Epoch [9], Batch [797/938], Loss: 0.9977131485939026\n",
      "Train: Epoch [9], Batch [798/938], Loss: 0.9690438508987427\n",
      "Train: Epoch [9], Batch [799/938], Loss: 0.6911518573760986\n",
      "Train: Epoch [9], Batch [800/938], Loss: 0.9465947151184082\n",
      "Train: Epoch [9], Batch [801/938], Loss: 0.622592031955719\n",
      "Train: Epoch [9], Batch [802/938], Loss: 0.8697596788406372\n",
      "Train: Epoch [9], Batch [803/938], Loss: 0.7447385191917419\n",
      "Train: Epoch [9], Batch [804/938], Loss: 1.0974247455596924\n",
      "Train: Epoch [9], Batch [805/938], Loss: 0.8320233821868896\n",
      "Train: Epoch [9], Batch [806/938], Loss: 0.9816199541091919\n",
      "Train: Epoch [9], Batch [807/938], Loss: 0.9968022108078003\n",
      "Train: Epoch [9], Batch [808/938], Loss: 0.6524814963340759\n",
      "Train: Epoch [9], Batch [809/938], Loss: 0.9005429744720459\n",
      "Train: Epoch [9], Batch [810/938], Loss: 1.0328847169876099\n",
      "Train: Epoch [9], Batch [811/938], Loss: 1.0103633403778076\n",
      "Train: Epoch [9], Batch [812/938], Loss: 0.9707226753234863\n",
      "Train: Epoch [9], Batch [813/938], Loss: 0.6562933921813965\n",
      "Train: Epoch [9], Batch [814/938], Loss: 1.1013091802597046\n",
      "Train: Epoch [9], Batch [815/938], Loss: 0.9416952729225159\n",
      "Train: Epoch [9], Batch [816/938], Loss: 0.7523096799850464\n",
      "Train: Epoch [9], Batch [817/938], Loss: 0.7494880557060242\n",
      "Train: Epoch [9], Batch [818/938], Loss: 0.7212240695953369\n",
      "Train: Epoch [9], Batch [819/938], Loss: 0.9720536470413208\n",
      "Train: Epoch [9], Batch [820/938], Loss: 0.6524592638015747\n",
      "Train: Epoch [9], Batch [821/938], Loss: 0.6713408827781677\n",
      "Train: Epoch [9], Batch [822/938], Loss: 0.7567579746246338\n",
      "Train: Epoch [9], Batch [823/938], Loss: 0.9475717544555664\n",
      "Train: Epoch [9], Batch [824/938], Loss: 0.860254168510437\n",
      "Train: Epoch [9], Batch [825/938], Loss: 0.7300446629524231\n",
      "Train: Epoch [9], Batch [826/938], Loss: 0.8942517042160034\n",
      "Train: Epoch [9], Batch [827/938], Loss: 0.6276354193687439\n",
      "Train: Epoch [9], Batch [828/938], Loss: 0.7054523229598999\n",
      "Train: Epoch [9], Batch [829/938], Loss: 0.6573840975761414\n",
      "Train: Epoch [9], Batch [830/938], Loss: 0.987108588218689\n",
      "Train: Epoch [9], Batch [831/938], Loss: 0.6963852643966675\n",
      "Train: Epoch [9], Batch [832/938], Loss: 1.0437699556350708\n",
      "Train: Epoch [9], Batch [833/938], Loss: 0.5171294212341309\n",
      "Train: Epoch [9], Batch [834/938], Loss: 0.90107262134552\n",
      "Train: Epoch [9], Batch [835/938], Loss: 0.9485794305801392\n",
      "Train: Epoch [9], Batch [836/938], Loss: 0.7828715443611145\n",
      "Train: Epoch [9], Batch [837/938], Loss: 0.9524093270301819\n",
      "Train: Epoch [9], Batch [838/938], Loss: 0.7986903190612793\n",
      "Train: Epoch [9], Batch [839/938], Loss: 0.8808698654174805\n",
      "Train: Epoch [9], Batch [840/938], Loss: 0.7994400262832642\n",
      "Train: Epoch [9], Batch [841/938], Loss: 0.7677281498908997\n",
      "Train: Epoch [9], Batch [842/938], Loss: 1.0018271207809448\n",
      "Train: Epoch [9], Batch [843/938], Loss: 0.980611264705658\n",
      "Train: Epoch [9], Batch [844/938], Loss: 0.7828295230865479\n",
      "Train: Epoch [9], Batch [845/938], Loss: 0.5925207138061523\n",
      "Train: Epoch [9], Batch [846/938], Loss: 0.8053471446037292\n",
      "Train: Epoch [9], Batch [847/938], Loss: 0.6861938834190369\n",
      "Train: Epoch [9], Batch [848/938], Loss: 0.5788336396217346\n",
      "Train: Epoch [9], Batch [849/938], Loss: 0.7010551691055298\n",
      "Train: Epoch [9], Batch [850/938], Loss: 1.1383899450302124\n",
      "Train: Epoch [9], Batch [851/938], Loss: 0.8255923986434937\n",
      "Train: Epoch [9], Batch [852/938], Loss: 0.7263166904449463\n",
      "Train: Epoch [9], Batch [853/938], Loss: 0.7033623456954956\n",
      "Train: Epoch [9], Batch [854/938], Loss: 0.8780782222747803\n",
      "Train: Epoch [9], Batch [855/938], Loss: 0.6903338432312012\n",
      "Train: Epoch [9], Batch [856/938], Loss: 1.1594620943069458\n",
      "Train: Epoch [9], Batch [857/938], Loss: 0.9069253206253052\n",
      "Train: Epoch [9], Batch [858/938], Loss: 0.8107403516769409\n",
      "Train: Epoch [9], Batch [859/938], Loss: 0.619593620300293\n",
      "Train: Epoch [9], Batch [860/938], Loss: 0.8624382019042969\n",
      "Train: Epoch [9], Batch [861/938], Loss: 0.8847545385360718\n",
      "Train: Epoch [9], Batch [862/938], Loss: 0.9196721315383911\n",
      "Train: Epoch [9], Batch [863/938], Loss: 0.7829498052597046\n",
      "Train: Epoch [9], Batch [864/938], Loss: 0.7412928342819214\n",
      "Train: Epoch [9], Batch [865/938], Loss: 0.498745322227478\n",
      "Train: Epoch [9], Batch [866/938], Loss: 1.013717770576477\n",
      "Train: Epoch [9], Batch [867/938], Loss: 0.738536536693573\n",
      "Train: Epoch [9], Batch [868/938], Loss: 0.9244815111160278\n",
      "Train: Epoch [9], Batch [869/938], Loss: 0.6407699584960938\n",
      "Train: Epoch [9], Batch [870/938], Loss: 1.031394600868225\n",
      "Train: Epoch [9], Batch [871/938], Loss: 0.46186721324920654\n",
      "Train: Epoch [9], Batch [872/938], Loss: 0.7644740343093872\n",
      "Train: Epoch [9], Batch [873/938], Loss: 0.6334265470504761\n",
      "Train: Epoch [9], Batch [874/938], Loss: 0.8826243877410889\n",
      "Train: Epoch [9], Batch [875/938], Loss: 0.8528910279273987\n",
      "Train: Epoch [9], Batch [876/938], Loss: 0.9097760319709778\n",
      "Train: Epoch [9], Batch [877/938], Loss: 0.7995007634162903\n",
      "Train: Epoch [9], Batch [878/938], Loss: 0.6835679411888123\n",
      "Train: Epoch [9], Batch [879/938], Loss: 0.725456953048706\n",
      "Train: Epoch [9], Batch [880/938], Loss: 0.8086076974868774\n",
      "Train: Epoch [9], Batch [881/938], Loss: 0.7367497086524963\n",
      "Train: Epoch [9], Batch [882/938], Loss: 0.978268027305603\n",
      "Train: Epoch [9], Batch [883/938], Loss: 0.9471054077148438\n",
      "Train: Epoch [9], Batch [884/938], Loss: 0.6741328239440918\n",
      "Train: Epoch [9], Batch [885/938], Loss: 0.837522566318512\n",
      "Train: Epoch [9], Batch [886/938], Loss: 0.777504026889801\n",
      "Train: Epoch [9], Batch [887/938], Loss: 0.5983477234840393\n",
      "Train: Epoch [9], Batch [888/938], Loss: 0.704970121383667\n",
      "Train: Epoch [9], Batch [889/938], Loss: 1.0659185647964478\n",
      "Train: Epoch [9], Batch [890/938], Loss: 0.8571852445602417\n",
      "Train: Epoch [9], Batch [891/938], Loss: 1.0196154117584229\n",
      "Train: Epoch [9], Batch [892/938], Loss: 0.9155542254447937\n",
      "Train: Epoch [9], Batch [893/938], Loss: 0.7746255397796631\n",
      "Train: Epoch [9], Batch [894/938], Loss: 0.6320167779922485\n",
      "Train: Epoch [9], Batch [895/938], Loss: 0.8098211884498596\n",
      "Train: Epoch [9], Batch [896/938], Loss: 0.8003625273704529\n",
      "Train: Epoch [9], Batch [897/938], Loss: 0.8274027109146118\n",
      "Train: Epoch [9], Batch [898/938], Loss: 0.8170713186264038\n",
      "Train: Epoch [9], Batch [899/938], Loss: 0.6676068902015686\n",
      "Train: Epoch [9], Batch [900/938], Loss: 0.8058565258979797\n",
      "Train: Epoch [9], Batch [901/938], Loss: 0.5640215277671814\n",
      "Train: Epoch [9], Batch [902/938], Loss: 0.9148005843162537\n",
      "Train: Epoch [9], Batch [903/938], Loss: 0.7452549934387207\n",
      "Train: Epoch [9], Batch [904/938], Loss: 0.9410521984100342\n",
      "Train: Epoch [9], Batch [905/938], Loss: 0.902947723865509\n",
      "Train: Epoch [9], Batch [906/938], Loss: 0.7260656356811523\n",
      "Train: Epoch [9], Batch [907/938], Loss: 0.7116183042526245\n",
      "Train: Epoch [9], Batch [908/938], Loss: 0.761524498462677\n",
      "Train: Epoch [9], Batch [909/938], Loss: 0.823371410369873\n",
      "Train: Epoch [9], Batch [910/938], Loss: 1.0521913766860962\n",
      "Train: Epoch [9], Batch [911/938], Loss: 0.7226911783218384\n",
      "Train: Epoch [9], Batch [912/938], Loss: 0.7409353256225586\n",
      "Train: Epoch [9], Batch [913/938], Loss: 0.9313787221908569\n",
      "Train: Epoch [9], Batch [914/938], Loss: 0.8998172283172607\n",
      "Train: Epoch [9], Batch [915/938], Loss: 0.7840480804443359\n",
      "Train: Epoch [9], Batch [916/938], Loss: 0.9221259355545044\n",
      "Train: Epoch [9], Batch [917/938], Loss: 0.7256351709365845\n",
      "Train: Epoch [9], Batch [918/938], Loss: 0.665060818195343\n",
      "Train: Epoch [9], Batch [919/938], Loss: 0.8963385224342346\n",
      "Train: Epoch [9], Batch [920/938], Loss: 0.8113358020782471\n",
      "Train: Epoch [9], Batch [921/938], Loss: 0.8023134469985962\n",
      "Train: Epoch [9], Batch [922/938], Loss: 0.6280885934829712\n",
      "Train: Epoch [9], Batch [923/938], Loss: 0.9357579350471497\n",
      "Train: Epoch [9], Batch [924/938], Loss: 0.7329142689704895\n",
      "Train: Epoch [9], Batch [925/938], Loss: 0.973137378692627\n",
      "Train: Epoch [9], Batch [926/938], Loss: 0.7214785814285278\n",
      "Train: Epoch [9], Batch [927/938], Loss: 0.8958067297935486\n",
      "Train: Epoch [9], Batch [928/938], Loss: 1.0734587907791138\n",
      "Train: Epoch [9], Batch [929/938], Loss: 0.7362269163131714\n",
      "Train: Epoch [9], Batch [930/938], Loss: 0.6378394961357117\n",
      "Train: Epoch [9], Batch [931/938], Loss: 0.838594913482666\n",
      "Train: Epoch [9], Batch [932/938], Loss: 0.7545918226242065\n",
      "Train: Epoch [9], Batch [933/938], Loss: 0.6776889562606812\n",
      "Train: Epoch [9], Batch [934/938], Loss: 0.7886462807655334\n",
      "Train: Epoch [9], Batch [935/938], Loss: 1.0174181461334229\n",
      "Train: Epoch [9], Batch [936/938], Loss: 0.8700152635574341\n",
      "Train: Epoch [9], Batch [937/938], Loss: 0.6383654475212097\n",
      "Train: Epoch [9], Batch [938/938], Loss: 0.7790639400482178\n",
      "Accuracy of train set: 0.7352833333333333\n",
      "Validation: Epoch [9], Batch [1/938], Loss: 0.7901424765586853\n",
      "Validation: Epoch [9], Batch [2/938], Loss: 0.8255698680877686\n",
      "Validation: Epoch [9], Batch [3/938], Loss: 0.8661155104637146\n",
      "Validation: Epoch [9], Batch [4/938], Loss: 0.7265009880065918\n",
      "Validation: Epoch [9], Batch [5/938], Loss: 0.934809148311615\n",
      "Validation: Epoch [9], Batch [6/938], Loss: 0.783828854560852\n",
      "Validation: Epoch [9], Batch [7/938], Loss: 0.7009494304656982\n",
      "Validation: Epoch [9], Batch [8/938], Loss: 0.8770138025283813\n",
      "Validation: Epoch [9], Batch [9/938], Loss: 1.0255005359649658\n",
      "Validation: Epoch [9], Batch [10/938], Loss: 0.882499098777771\n",
      "Validation: Epoch [9], Batch [11/938], Loss: 0.6994637250900269\n",
      "Validation: Epoch [9], Batch [12/938], Loss: 0.9900100231170654\n",
      "Validation: Epoch [9], Batch [13/938], Loss: 0.796066403388977\n",
      "Validation: Epoch [9], Batch [14/938], Loss: 1.1141411066055298\n",
      "Validation: Epoch [9], Batch [15/938], Loss: 0.7806592583656311\n",
      "Validation: Epoch [9], Batch [16/938], Loss: 1.0781370401382446\n",
      "Validation: Epoch [9], Batch [17/938], Loss: 0.9803910255432129\n",
      "Validation: Epoch [9], Batch [18/938], Loss: 0.7457178831100464\n",
      "Validation: Epoch [9], Batch [19/938], Loss: 0.8156076669692993\n",
      "Validation: Epoch [9], Batch [20/938], Loss: 0.547478973865509\n",
      "Validation: Epoch [9], Batch [21/938], Loss: 0.861628532409668\n",
      "Validation: Epoch [9], Batch [22/938], Loss: 0.828532338142395\n",
      "Validation: Epoch [9], Batch [23/938], Loss: 1.0736256837844849\n",
      "Validation: Epoch [9], Batch [24/938], Loss: 0.7391482591629028\n",
      "Validation: Epoch [9], Batch [25/938], Loss: 0.9039921164512634\n",
      "Validation: Epoch [9], Batch [26/938], Loss: 0.7896832227706909\n",
      "Validation: Epoch [9], Batch [27/938], Loss: 0.9312143325805664\n",
      "Validation: Epoch [9], Batch [28/938], Loss: 0.6947075128555298\n",
      "Validation: Epoch [9], Batch [29/938], Loss: 0.59718257188797\n",
      "Validation: Epoch [9], Batch [30/938], Loss: 0.8387792110443115\n",
      "Validation: Epoch [9], Batch [31/938], Loss: 0.9330305457115173\n",
      "Validation: Epoch [9], Batch [32/938], Loss: 0.7302042245864868\n",
      "Validation: Epoch [9], Batch [33/938], Loss: 0.6960059404373169\n",
      "Validation: Epoch [9], Batch [34/938], Loss: 0.6927739977836609\n",
      "Validation: Epoch [9], Batch [35/938], Loss: 0.8482035398483276\n",
      "Validation: Epoch [9], Batch [36/938], Loss: 0.6617028713226318\n",
      "Validation: Epoch [9], Batch [37/938], Loss: 0.7670125961303711\n",
      "Validation: Epoch [9], Batch [38/938], Loss: 0.9771491289138794\n",
      "Validation: Epoch [9], Batch [39/938], Loss: 0.7083939909934998\n",
      "Validation: Epoch [9], Batch [40/938], Loss: 0.7375913858413696\n",
      "Validation: Epoch [9], Batch [41/938], Loss: 0.8917752504348755\n",
      "Validation: Epoch [9], Batch [42/938], Loss: 0.9509039521217346\n",
      "Validation: Epoch [9], Batch [43/938], Loss: 0.809114933013916\n",
      "Validation: Epoch [9], Batch [44/938], Loss: 0.7455739974975586\n",
      "Validation: Epoch [9], Batch [45/938], Loss: 0.776611864566803\n",
      "Validation: Epoch [9], Batch [46/938], Loss: 0.9586900472640991\n",
      "Validation: Epoch [9], Batch [47/938], Loss: 0.7058458924293518\n",
      "Validation: Epoch [9], Batch [48/938], Loss: 1.004716157913208\n",
      "Validation: Epoch [9], Batch [49/938], Loss: 0.8104822635650635\n",
      "Validation: Epoch [9], Batch [50/938], Loss: 0.9246047735214233\n",
      "Validation: Epoch [9], Batch [51/938], Loss: 0.8242059946060181\n",
      "Validation: Epoch [9], Batch [52/938], Loss: 0.6858533620834351\n",
      "Validation: Epoch [9], Batch [53/938], Loss: 0.9825196266174316\n",
      "Validation: Epoch [9], Batch [54/938], Loss: 0.9451223611831665\n",
      "Validation: Epoch [9], Batch [55/938], Loss: 0.7412095069885254\n",
      "Validation: Epoch [9], Batch [56/938], Loss: 0.8952794075012207\n",
      "Validation: Epoch [9], Batch [57/938], Loss: 0.6916934847831726\n",
      "Validation: Epoch [9], Batch [58/938], Loss: 0.8363938927650452\n",
      "Validation: Epoch [9], Batch [59/938], Loss: 0.7438046932220459\n",
      "Validation: Epoch [9], Batch [60/938], Loss: 0.839011013507843\n",
      "Validation: Epoch [9], Batch [61/938], Loss: 0.9935636520385742\n",
      "Validation: Epoch [9], Batch [62/938], Loss: 0.8067303895950317\n",
      "Validation: Epoch [9], Batch [63/938], Loss: 0.82920902967453\n",
      "Validation: Epoch [9], Batch [64/938], Loss: 0.6826881170272827\n",
      "Validation: Epoch [9], Batch [65/938], Loss: 0.8697470426559448\n",
      "Validation: Epoch [9], Batch [66/938], Loss: 0.8611136674880981\n",
      "Validation: Epoch [9], Batch [67/938], Loss: 0.8499430418014526\n",
      "Validation: Epoch [9], Batch [68/938], Loss: 0.8272677063941956\n",
      "Validation: Epoch [9], Batch [69/938], Loss: 0.9037590622901917\n",
      "Validation: Epoch [9], Batch [70/938], Loss: 0.6594737768173218\n",
      "Validation: Epoch [9], Batch [71/938], Loss: 0.6665140390396118\n",
      "Validation: Epoch [9], Batch [72/938], Loss: 1.0635807514190674\n",
      "Validation: Epoch [9], Batch [73/938], Loss: 0.9072344303131104\n",
      "Validation: Epoch [9], Batch [74/938], Loss: 0.9224306344985962\n",
      "Validation: Epoch [9], Batch [75/938], Loss: 0.6402656435966492\n",
      "Validation: Epoch [9], Batch [76/938], Loss: 0.7689694762229919\n",
      "Validation: Epoch [9], Batch [77/938], Loss: 0.6231251955032349\n",
      "Validation: Epoch [9], Batch [78/938], Loss: 0.7666574120521545\n",
      "Validation: Epoch [9], Batch [79/938], Loss: 0.6373214721679688\n",
      "Validation: Epoch [9], Batch [80/938], Loss: 1.1201900243759155\n",
      "Validation: Epoch [9], Batch [81/938], Loss: 0.5490735769271851\n",
      "Validation: Epoch [9], Batch [82/938], Loss: 0.6214768886566162\n",
      "Validation: Epoch [9], Batch [83/938], Loss: 1.0559296607971191\n",
      "Validation: Epoch [9], Batch [84/938], Loss: 1.2288299798965454\n",
      "Validation: Epoch [9], Batch [85/938], Loss: 0.5372592806816101\n",
      "Validation: Epoch [9], Batch [86/938], Loss: 0.7645392417907715\n",
      "Validation: Epoch [9], Batch [87/938], Loss: 0.8484808802604675\n",
      "Validation: Epoch [9], Batch [88/938], Loss: 1.3138169050216675\n",
      "Validation: Epoch [9], Batch [89/938], Loss: 0.6868322491645813\n",
      "Validation: Epoch [9], Batch [90/938], Loss: 1.031561255455017\n",
      "Validation: Epoch [9], Batch [91/938], Loss: 0.9539824724197388\n",
      "Validation: Epoch [9], Batch [92/938], Loss: 1.1528303623199463\n",
      "Validation: Epoch [9], Batch [93/938], Loss: 0.8301032185554504\n",
      "Validation: Epoch [9], Batch [94/938], Loss: 0.8743319511413574\n",
      "Validation: Epoch [9], Batch [95/938], Loss: 0.9427891373634338\n",
      "Validation: Epoch [9], Batch [96/938], Loss: 0.6951957941055298\n",
      "Validation: Epoch [9], Batch [97/938], Loss: 0.6092707514762878\n",
      "Validation: Epoch [9], Batch [98/938], Loss: 0.6852825284004211\n",
      "Validation: Epoch [9], Batch [99/938], Loss: 0.8595614433288574\n",
      "Validation: Epoch [9], Batch [100/938], Loss: 0.7972343564033508\n",
      "Validation: Epoch [9], Batch [101/938], Loss: 0.7459624409675598\n",
      "Validation: Epoch [9], Batch [102/938], Loss: 0.8157575130462646\n",
      "Validation: Epoch [9], Batch [103/938], Loss: 0.8350955247879028\n",
      "Validation: Epoch [9], Batch [104/938], Loss: 0.8630916476249695\n",
      "Validation: Epoch [9], Batch [105/938], Loss: 0.8599676489830017\n",
      "Validation: Epoch [9], Batch [106/938], Loss: 1.0621691942214966\n",
      "Validation: Epoch [9], Batch [107/938], Loss: 0.6987595558166504\n",
      "Validation: Epoch [9], Batch [108/938], Loss: 0.5932233929634094\n",
      "Validation: Epoch [9], Batch [109/938], Loss: 0.8709108829498291\n",
      "Validation: Epoch [9], Batch [110/938], Loss: 0.9083114266395569\n",
      "Validation: Epoch [9], Batch [111/938], Loss: 0.8545986413955688\n",
      "Validation: Epoch [9], Batch [112/938], Loss: 0.9433776140213013\n",
      "Validation: Epoch [9], Batch [113/938], Loss: 0.7818235754966736\n",
      "Validation: Epoch [9], Batch [114/938], Loss: 0.6084517240524292\n",
      "Validation: Epoch [9], Batch [115/938], Loss: 0.6773483157157898\n",
      "Validation: Epoch [9], Batch [116/938], Loss: 0.49435240030288696\n",
      "Validation: Epoch [9], Batch [117/938], Loss: 1.0164299011230469\n",
      "Validation: Epoch [9], Batch [118/938], Loss: 0.8061994314193726\n",
      "Validation: Epoch [9], Batch [119/938], Loss: 0.8257397413253784\n",
      "Validation: Epoch [9], Batch [120/938], Loss: 0.649782657623291\n",
      "Validation: Epoch [9], Batch [121/938], Loss: 0.7083896398544312\n",
      "Validation: Epoch [9], Batch [122/938], Loss: 0.8884463310241699\n",
      "Validation: Epoch [9], Batch [123/938], Loss: 1.1471991539001465\n",
      "Validation: Epoch [9], Batch [124/938], Loss: 0.8768665790557861\n",
      "Validation: Epoch [9], Batch [125/938], Loss: 0.8586559295654297\n",
      "Validation: Epoch [9], Batch [126/938], Loss: 0.787489652633667\n",
      "Validation: Epoch [9], Batch [127/938], Loss: 0.629612386226654\n",
      "Validation: Epoch [9], Batch [128/938], Loss: 0.8005068302154541\n",
      "Validation: Epoch [9], Batch [129/938], Loss: 1.0364238023757935\n",
      "Validation: Epoch [9], Batch [130/938], Loss: 0.7511943578720093\n",
      "Validation: Epoch [9], Batch [131/938], Loss: 0.7898179888725281\n",
      "Validation: Epoch [9], Batch [132/938], Loss: 0.89460289478302\n",
      "Validation: Epoch [9], Batch [133/938], Loss: 0.5745004415512085\n",
      "Validation: Epoch [9], Batch [134/938], Loss: 0.8691791892051697\n",
      "Validation: Epoch [9], Batch [135/938], Loss: 0.6708523631095886\n",
      "Validation: Epoch [9], Batch [136/938], Loss: 0.9108578562736511\n",
      "Validation: Epoch [9], Batch [137/938], Loss: 0.8764033317565918\n",
      "Validation: Epoch [9], Batch [138/938], Loss: 0.8326654434204102\n",
      "Validation: Epoch [9], Batch [139/938], Loss: 1.0094127655029297\n",
      "Validation: Epoch [9], Batch [140/938], Loss: 0.8134774565696716\n",
      "Validation: Epoch [9], Batch [141/938], Loss: 0.6975902318954468\n",
      "Validation: Epoch [9], Batch [142/938], Loss: 0.7956483960151672\n",
      "Validation: Epoch [9], Batch [143/938], Loss: 0.7231633067131042\n",
      "Validation: Epoch [9], Batch [144/938], Loss: 0.6880447268486023\n",
      "Validation: Epoch [9], Batch [145/938], Loss: 0.9109246730804443\n",
      "Validation: Epoch [9], Batch [146/938], Loss: 0.6621160507202148\n",
      "Validation: Epoch [9], Batch [147/938], Loss: 0.7383896112442017\n",
      "Validation: Epoch [9], Batch [148/938], Loss: 0.7378641963005066\n",
      "Validation: Epoch [9], Batch [149/938], Loss: 0.6763657331466675\n",
      "Validation: Epoch [9], Batch [150/938], Loss: 0.852057933807373\n",
      "Validation: Epoch [9], Batch [151/938], Loss: 0.8337048888206482\n",
      "Validation: Epoch [9], Batch [152/938], Loss: 0.6688264608383179\n",
      "Validation: Epoch [9], Batch [153/938], Loss: 0.6440010666847229\n",
      "Validation: Epoch [9], Batch [154/938], Loss: 0.5765063166618347\n",
      "Validation: Epoch [9], Batch [155/938], Loss: 0.8193055987358093\n",
      "Validation: Epoch [9], Batch [156/938], Loss: 0.8714989423751831\n",
      "Validation: Epoch [9], Batch [157/938], Loss: 0.6288880109786987\n",
      "Validation: Epoch [9], Batch [158/938], Loss: 1.0320749282836914\n",
      "Validation: Epoch [9], Batch [159/938], Loss: 0.918178379535675\n",
      "Validation: Epoch [9], Batch [160/938], Loss: 0.6649867296218872\n",
      "Validation: Epoch [9], Batch [161/938], Loss: 0.7529337406158447\n",
      "Validation: Epoch [9], Batch [162/938], Loss: 0.7664089798927307\n",
      "Validation: Epoch [9], Batch [163/938], Loss: 0.7974085211753845\n",
      "Validation: Epoch [9], Batch [164/938], Loss: 1.0557175874710083\n",
      "Validation: Epoch [9], Batch [165/938], Loss: 0.6568962335586548\n",
      "Validation: Epoch [9], Batch [166/938], Loss: 0.8136628270149231\n",
      "Validation: Epoch [9], Batch [167/938], Loss: 0.6456893682479858\n",
      "Validation: Epoch [9], Batch [168/938], Loss: 0.8040951490402222\n",
      "Validation: Epoch [9], Batch [169/938], Loss: 0.8093143701553345\n",
      "Validation: Epoch [9], Batch [170/938], Loss: 0.7896202802658081\n",
      "Validation: Epoch [9], Batch [171/938], Loss: 1.071289300918579\n",
      "Validation: Epoch [9], Batch [172/938], Loss: 1.075670838356018\n",
      "Validation: Epoch [9], Batch [173/938], Loss: 0.8670183420181274\n",
      "Validation: Epoch [9], Batch [174/938], Loss: 0.9308980703353882\n",
      "Validation: Epoch [9], Batch [175/938], Loss: 0.7374317646026611\n",
      "Validation: Epoch [9], Batch [176/938], Loss: 0.566910982131958\n",
      "Validation: Epoch [9], Batch [177/938], Loss: 0.9499028921127319\n",
      "Validation: Epoch [9], Batch [178/938], Loss: 0.7163740396499634\n",
      "Validation: Epoch [9], Batch [179/938], Loss: 0.5263924598693848\n",
      "Validation: Epoch [9], Batch [180/938], Loss: 0.7474586963653564\n",
      "Validation: Epoch [9], Batch [181/938], Loss: 0.7751047015190125\n",
      "Validation: Epoch [9], Batch [182/938], Loss: 0.7287731766700745\n",
      "Validation: Epoch [9], Batch [183/938], Loss: 0.9897933602333069\n",
      "Validation: Epoch [9], Batch [184/938], Loss: 0.7657379508018494\n",
      "Validation: Epoch [9], Batch [185/938], Loss: 0.8220367431640625\n",
      "Validation: Epoch [9], Batch [186/938], Loss: 0.6957592368125916\n",
      "Validation: Epoch [9], Batch [187/938], Loss: 0.8892592191696167\n",
      "Validation: Epoch [9], Batch [188/938], Loss: 0.8077717423439026\n",
      "Validation: Epoch [9], Batch [189/938], Loss: 0.6263777017593384\n",
      "Validation: Epoch [9], Batch [190/938], Loss: 0.8192995190620422\n",
      "Validation: Epoch [9], Batch [191/938], Loss: 0.7693015933036804\n",
      "Validation: Epoch [9], Batch [192/938], Loss: 0.7708654999732971\n",
      "Validation: Epoch [9], Batch [193/938], Loss: 0.9231206774711609\n",
      "Validation: Epoch [9], Batch [194/938], Loss: 0.932483434677124\n",
      "Validation: Epoch [9], Batch [195/938], Loss: 0.9818257689476013\n",
      "Validation: Epoch [9], Batch [196/938], Loss: 0.8123838901519775\n",
      "Validation: Epoch [9], Batch [197/938], Loss: 1.0118224620819092\n",
      "Validation: Epoch [9], Batch [198/938], Loss: 0.9467524290084839\n",
      "Validation: Epoch [9], Batch [199/938], Loss: 0.6912445425987244\n",
      "Validation: Epoch [9], Batch [200/938], Loss: 0.8212142586708069\n",
      "Validation: Epoch [9], Batch [201/938], Loss: 0.7249988317489624\n",
      "Validation: Epoch [9], Batch [202/938], Loss: 0.8145193457603455\n",
      "Validation: Epoch [9], Batch [203/938], Loss: 0.6193253397941589\n",
      "Validation: Epoch [9], Batch [204/938], Loss: 0.7236375212669373\n",
      "Validation: Epoch [9], Batch [205/938], Loss: 0.7857291102409363\n",
      "Validation: Epoch [9], Batch [206/938], Loss: 0.71019446849823\n",
      "Validation: Epoch [9], Batch [207/938], Loss: 0.853240966796875\n",
      "Validation: Epoch [9], Batch [208/938], Loss: 0.739553689956665\n",
      "Validation: Epoch [9], Batch [209/938], Loss: 0.776695728302002\n",
      "Validation: Epoch [9], Batch [210/938], Loss: 0.7279238104820251\n",
      "Validation: Epoch [9], Batch [211/938], Loss: 0.7756028175354004\n",
      "Validation: Epoch [9], Batch [212/938], Loss: 0.9902502298355103\n",
      "Validation: Epoch [9], Batch [213/938], Loss: 0.7694593071937561\n",
      "Validation: Epoch [9], Batch [214/938], Loss: 0.7349840402603149\n",
      "Validation: Epoch [9], Batch [215/938], Loss: 0.5413575768470764\n",
      "Validation: Epoch [9], Batch [216/938], Loss: 0.726524829864502\n",
      "Validation: Epoch [9], Batch [217/938], Loss: 0.46827927231788635\n",
      "Validation: Epoch [9], Batch [218/938], Loss: 0.7929222583770752\n",
      "Validation: Epoch [9], Batch [219/938], Loss: 0.8415071368217468\n",
      "Validation: Epoch [9], Batch [220/938], Loss: 0.9285340905189514\n",
      "Validation: Epoch [9], Batch [221/938], Loss: 0.7419589161872864\n",
      "Validation: Epoch [9], Batch [222/938], Loss: 0.8051401376724243\n",
      "Validation: Epoch [9], Batch [223/938], Loss: 0.8004021644592285\n",
      "Validation: Epoch [9], Batch [224/938], Loss: 0.6986835598945618\n",
      "Validation: Epoch [9], Batch [225/938], Loss: 0.9296815395355225\n",
      "Validation: Epoch [9], Batch [226/938], Loss: 0.9475541710853577\n",
      "Validation: Epoch [9], Batch [227/938], Loss: 0.8238492608070374\n",
      "Validation: Epoch [9], Batch [228/938], Loss: 0.7752391695976257\n",
      "Validation: Epoch [9], Batch [229/938], Loss: 0.7609481811523438\n",
      "Validation: Epoch [9], Batch [230/938], Loss: 0.9242997765541077\n",
      "Validation: Epoch [9], Batch [231/938], Loss: 1.0254570245742798\n",
      "Validation: Epoch [9], Batch [232/938], Loss: 0.8082970976829529\n",
      "Validation: Epoch [9], Batch [233/938], Loss: 0.8437235355377197\n",
      "Validation: Epoch [9], Batch [234/938], Loss: 0.8325636982917786\n",
      "Validation: Epoch [9], Batch [235/938], Loss: 0.7070826888084412\n",
      "Validation: Epoch [9], Batch [236/938], Loss: 0.6995494365692139\n",
      "Validation: Epoch [9], Batch [237/938], Loss: 0.6963551640510559\n",
      "Validation: Epoch [9], Batch [238/938], Loss: 0.6439850330352783\n",
      "Validation: Epoch [9], Batch [239/938], Loss: 0.505530595779419\n",
      "Validation: Epoch [9], Batch [240/938], Loss: 0.8188052177429199\n",
      "Validation: Epoch [9], Batch [241/938], Loss: 0.6456224322319031\n",
      "Validation: Epoch [9], Batch [242/938], Loss: 0.8396997451782227\n",
      "Validation: Epoch [9], Batch [243/938], Loss: 0.9337577223777771\n",
      "Validation: Epoch [9], Batch [244/938], Loss: 0.6726365089416504\n",
      "Validation: Epoch [9], Batch [245/938], Loss: 0.7135883569717407\n",
      "Validation: Epoch [9], Batch [246/938], Loss: 0.893581748008728\n",
      "Validation: Epoch [9], Batch [247/938], Loss: 0.59311443567276\n",
      "Validation: Epoch [9], Batch [248/938], Loss: 0.8709721565246582\n",
      "Validation: Epoch [9], Batch [249/938], Loss: 0.7818631529808044\n",
      "Validation: Epoch [9], Batch [250/938], Loss: 0.6943976879119873\n",
      "Validation: Epoch [9], Batch [251/938], Loss: 0.772503137588501\n",
      "Validation: Epoch [9], Batch [252/938], Loss: 0.9064176678657532\n",
      "Validation: Epoch [9], Batch [253/938], Loss: 0.7428492903709412\n",
      "Validation: Epoch [9], Batch [254/938], Loss: 0.6083641648292542\n",
      "Validation: Epoch [9], Batch [255/938], Loss: 0.9066378474235535\n",
      "Validation: Epoch [9], Batch [256/938], Loss: 0.6517906785011292\n",
      "Validation: Epoch [9], Batch [257/938], Loss: 0.6644387245178223\n",
      "Validation: Epoch [9], Batch [258/938], Loss: 0.7372066974639893\n",
      "Validation: Epoch [9], Batch [259/938], Loss: 0.9552251100540161\n",
      "Validation: Epoch [9], Batch [260/938], Loss: 1.032261610031128\n",
      "Validation: Epoch [9], Batch [261/938], Loss: 1.0457265377044678\n",
      "Validation: Epoch [9], Batch [262/938], Loss: 0.8984843492507935\n",
      "Validation: Epoch [9], Batch [263/938], Loss: 0.7961037755012512\n",
      "Validation: Epoch [9], Batch [264/938], Loss: 0.7368481159210205\n",
      "Validation: Epoch [9], Batch [265/938], Loss: 0.8555207252502441\n",
      "Validation: Epoch [9], Batch [266/938], Loss: 1.009867548942566\n",
      "Validation: Epoch [9], Batch [267/938], Loss: 0.8159230351448059\n",
      "Validation: Epoch [9], Batch [268/938], Loss: 0.9183337092399597\n",
      "Validation: Epoch [9], Batch [269/938], Loss: 0.9757859110832214\n",
      "Validation: Epoch [9], Batch [270/938], Loss: 0.7647293210029602\n",
      "Validation: Epoch [9], Batch [271/938], Loss: 0.8059719800949097\n",
      "Validation: Epoch [9], Batch [272/938], Loss: 0.5731654763221741\n",
      "Validation: Epoch [9], Batch [273/938], Loss: 0.8669572472572327\n",
      "Validation: Epoch [9], Batch [274/938], Loss: 0.6599411964416504\n",
      "Validation: Epoch [9], Batch [275/938], Loss: 0.9419999718666077\n",
      "Validation: Epoch [9], Batch [276/938], Loss: 0.7491072416305542\n",
      "Validation: Epoch [9], Batch [277/938], Loss: 0.5307236313819885\n",
      "Validation: Epoch [9], Batch [278/938], Loss: 1.0398138761520386\n",
      "Validation: Epoch [9], Batch [279/938], Loss: 0.809909462928772\n",
      "Validation: Epoch [9], Batch [280/938], Loss: 0.6830886006355286\n",
      "Validation: Epoch [9], Batch [281/938], Loss: 0.7797624468803406\n",
      "Validation: Epoch [9], Batch [282/938], Loss: 0.763810932636261\n",
      "Validation: Epoch [9], Batch [283/938], Loss: 0.803528904914856\n",
      "Validation: Epoch [9], Batch [284/938], Loss: 0.8794357776641846\n",
      "Validation: Epoch [9], Batch [285/938], Loss: 0.7776788473129272\n",
      "Validation: Epoch [9], Batch [286/938], Loss: 0.8787534832954407\n",
      "Validation: Epoch [9], Batch [287/938], Loss: 0.7564892172813416\n",
      "Validation: Epoch [9], Batch [288/938], Loss: 0.735219419002533\n",
      "Validation: Epoch [9], Batch [289/938], Loss: 0.6871626377105713\n",
      "Validation: Epoch [9], Batch [290/938], Loss: 0.8077023029327393\n",
      "Validation: Epoch [9], Batch [291/938], Loss: 0.7615113258361816\n",
      "Validation: Epoch [9], Batch [292/938], Loss: 0.617970883846283\n",
      "Validation: Epoch [9], Batch [293/938], Loss: 1.04276704788208\n",
      "Validation: Epoch [9], Batch [294/938], Loss: 0.8791164755821228\n",
      "Validation: Epoch [9], Batch [295/938], Loss: 0.7363166213035583\n",
      "Validation: Epoch [9], Batch [296/938], Loss: 0.9150667190551758\n",
      "Validation: Epoch [9], Batch [297/938], Loss: 0.8254440426826477\n",
      "Validation: Epoch [9], Batch [298/938], Loss: 0.6090818643569946\n",
      "Validation: Epoch [9], Batch [299/938], Loss: 0.8911543488502502\n",
      "Validation: Epoch [9], Batch [300/938], Loss: 0.7915270924568176\n",
      "Validation: Epoch [9], Batch [301/938], Loss: 0.6262854933738708\n",
      "Validation: Epoch [9], Batch [302/938], Loss: 0.7879896759986877\n",
      "Validation: Epoch [9], Batch [303/938], Loss: 0.815204381942749\n",
      "Validation: Epoch [9], Batch [304/938], Loss: 1.016686201095581\n",
      "Validation: Epoch [9], Batch [305/938], Loss: 0.7357040643692017\n",
      "Validation: Epoch [9], Batch [306/938], Loss: 0.8932473659515381\n",
      "Validation: Epoch [9], Batch [307/938], Loss: 0.8007493019104004\n",
      "Validation: Epoch [9], Batch [308/938], Loss: 0.764575719833374\n",
      "Validation: Epoch [9], Batch [309/938], Loss: 0.928626537322998\n",
      "Validation: Epoch [9], Batch [310/938], Loss: 0.7813951969146729\n",
      "Validation: Epoch [9], Batch [311/938], Loss: 0.8021572828292847\n",
      "Validation: Epoch [9], Batch [312/938], Loss: 1.156965732574463\n",
      "Validation: Epoch [9], Batch [313/938], Loss: 0.8984802961349487\n",
      "Validation: Epoch [9], Batch [314/938], Loss: 0.8149736523628235\n",
      "Validation: Epoch [9], Batch [315/938], Loss: 0.8634538054466248\n",
      "Validation: Epoch [9], Batch [316/938], Loss: 0.8955949544906616\n",
      "Validation: Epoch [9], Batch [317/938], Loss: 0.9845377206802368\n",
      "Validation: Epoch [9], Batch [318/938], Loss: 0.523389458656311\n",
      "Validation: Epoch [9], Batch [319/938], Loss: 0.6801660656929016\n",
      "Validation: Epoch [9], Batch [320/938], Loss: 0.8261665105819702\n",
      "Validation: Epoch [9], Batch [321/938], Loss: 0.592892050743103\n",
      "Validation: Epoch [9], Batch [322/938], Loss: 0.5550127029418945\n",
      "Validation: Epoch [9], Batch [323/938], Loss: 0.9054076075553894\n",
      "Validation: Epoch [9], Batch [324/938], Loss: 0.6778905987739563\n",
      "Validation: Epoch [9], Batch [325/938], Loss: 0.7980329990386963\n",
      "Validation: Epoch [9], Batch [326/938], Loss: 0.9243834614753723\n",
      "Validation: Epoch [9], Batch [327/938], Loss: 0.8402513265609741\n",
      "Validation: Epoch [9], Batch [328/938], Loss: 1.1611647605895996\n",
      "Validation: Epoch [9], Batch [329/938], Loss: 0.6316139698028564\n",
      "Validation: Epoch [9], Batch [330/938], Loss: 0.7259446382522583\n",
      "Validation: Epoch [9], Batch [331/938], Loss: 0.6595422029495239\n",
      "Validation: Epoch [9], Batch [332/938], Loss: 0.6581761240959167\n",
      "Validation: Epoch [9], Batch [333/938], Loss: 0.664992094039917\n",
      "Validation: Epoch [9], Batch [334/938], Loss: 0.7218371033668518\n",
      "Validation: Epoch [9], Batch [335/938], Loss: 0.9671363830566406\n",
      "Validation: Epoch [9], Batch [336/938], Loss: 0.754465639591217\n",
      "Validation: Epoch [9], Batch [337/938], Loss: 1.0417286157608032\n",
      "Validation: Epoch [9], Batch [338/938], Loss: 0.6938531398773193\n",
      "Validation: Epoch [9], Batch [339/938], Loss: 0.890935480594635\n",
      "Validation: Epoch [9], Batch [340/938], Loss: 0.7306678295135498\n",
      "Validation: Epoch [9], Batch [341/938], Loss: 0.7843138575553894\n",
      "Validation: Epoch [9], Batch [342/938], Loss: 0.9712796211242676\n",
      "Validation: Epoch [9], Batch [343/938], Loss: 0.662800133228302\n",
      "Validation: Epoch [9], Batch [344/938], Loss: 0.8423721194267273\n",
      "Validation: Epoch [9], Batch [345/938], Loss: 0.6916424036026001\n",
      "Validation: Epoch [9], Batch [346/938], Loss: 0.8018841743469238\n",
      "Validation: Epoch [9], Batch [347/938], Loss: 0.7508274912834167\n",
      "Validation: Epoch [9], Batch [348/938], Loss: 0.9548261165618896\n",
      "Validation: Epoch [9], Batch [349/938], Loss: 0.678558349609375\n",
      "Validation: Epoch [9], Batch [350/938], Loss: 0.9923686981201172\n",
      "Validation: Epoch [9], Batch [351/938], Loss: 1.0106614828109741\n",
      "Validation: Epoch [9], Batch [352/938], Loss: 0.926367461681366\n",
      "Validation: Epoch [9], Batch [353/938], Loss: 0.7452404499053955\n",
      "Validation: Epoch [9], Batch [354/938], Loss: 0.6821640133857727\n",
      "Validation: Epoch [9], Batch [355/938], Loss: 0.8730676770210266\n",
      "Validation: Epoch [9], Batch [356/938], Loss: 0.8259595036506653\n",
      "Validation: Epoch [9], Batch [357/938], Loss: 0.7271334528923035\n",
      "Validation: Epoch [9], Batch [358/938], Loss: 0.9492888450622559\n",
      "Validation: Epoch [9], Batch [359/938], Loss: 0.757340669631958\n",
      "Validation: Epoch [9], Batch [360/938], Loss: 0.8121201395988464\n",
      "Validation: Epoch [9], Batch [361/938], Loss: 1.0108450651168823\n",
      "Validation: Epoch [9], Batch [362/938], Loss: 1.0250312089920044\n",
      "Validation: Epoch [9], Batch [363/938], Loss: 0.8222748041152954\n",
      "Validation: Epoch [9], Batch [364/938], Loss: 0.6769315600395203\n",
      "Validation: Epoch [9], Batch [365/938], Loss: 0.840260922908783\n",
      "Validation: Epoch [9], Batch [366/938], Loss: 0.8769956231117249\n",
      "Validation: Epoch [9], Batch [367/938], Loss: 0.7538292407989502\n",
      "Validation: Epoch [9], Batch [368/938], Loss: 0.8793994188308716\n",
      "Validation: Epoch [9], Batch [369/938], Loss: 0.7911926507949829\n",
      "Validation: Epoch [9], Batch [370/938], Loss: 0.8764944076538086\n",
      "Validation: Epoch [9], Batch [371/938], Loss: 0.7829364538192749\n",
      "Validation: Epoch [9], Batch [372/938], Loss: 0.9522014260292053\n",
      "Validation: Epoch [9], Batch [373/938], Loss: 0.9432290196418762\n",
      "Validation: Epoch [9], Batch [374/938], Loss: 0.7209998965263367\n",
      "Validation: Epoch [9], Batch [375/938], Loss: 0.5849956274032593\n",
      "Validation: Epoch [9], Batch [376/938], Loss: 0.8842357993125916\n",
      "Validation: Epoch [9], Batch [377/938], Loss: 0.6127076148986816\n",
      "Validation: Epoch [9], Batch [378/938], Loss: 0.7844517827033997\n",
      "Validation: Epoch [9], Batch [379/938], Loss: 0.7030701637268066\n",
      "Validation: Epoch [9], Batch [380/938], Loss: 0.6681107878684998\n",
      "Validation: Epoch [9], Batch [381/938], Loss: 0.8831897974014282\n",
      "Validation: Epoch [9], Batch [382/938], Loss: 0.93986576795578\n",
      "Validation: Epoch [9], Batch [383/938], Loss: 0.6317334175109863\n",
      "Validation: Epoch [9], Batch [384/938], Loss: 0.5988746285438538\n",
      "Validation: Epoch [9], Batch [385/938], Loss: 0.8110606074333191\n",
      "Validation: Epoch [9], Batch [386/938], Loss: 0.5973384380340576\n",
      "Validation: Epoch [9], Batch [387/938], Loss: 0.773756742477417\n",
      "Validation: Epoch [9], Batch [388/938], Loss: 0.9841654300689697\n",
      "Validation: Epoch [9], Batch [389/938], Loss: 0.9508559107780457\n",
      "Validation: Epoch [9], Batch [390/938], Loss: 0.6904997229576111\n",
      "Validation: Epoch [9], Batch [391/938], Loss: 0.6487054228782654\n",
      "Validation: Epoch [9], Batch [392/938], Loss: 0.733904242515564\n",
      "Validation: Epoch [9], Batch [393/938], Loss: 0.7807446718215942\n",
      "Validation: Epoch [9], Batch [394/938], Loss: 0.8797102570533752\n",
      "Validation: Epoch [9], Batch [395/938], Loss: 0.49137258529663086\n",
      "Validation: Epoch [9], Batch [396/938], Loss: 0.6421830058097839\n",
      "Validation: Epoch [9], Batch [397/938], Loss: 0.6337085366249084\n",
      "Validation: Epoch [9], Batch [398/938], Loss: 0.9188486337661743\n",
      "Validation: Epoch [9], Batch [399/938], Loss: 0.8537119626998901\n",
      "Validation: Epoch [9], Batch [400/938], Loss: 0.6287297010421753\n",
      "Validation: Epoch [9], Batch [401/938], Loss: 1.1591590642929077\n",
      "Validation: Epoch [9], Batch [402/938], Loss: 0.7650090456008911\n",
      "Validation: Epoch [9], Batch [403/938], Loss: 1.0137287378311157\n",
      "Validation: Epoch [9], Batch [404/938], Loss: 0.7904341816902161\n",
      "Validation: Epoch [9], Batch [405/938], Loss: 0.6619316339492798\n",
      "Validation: Epoch [9], Batch [406/938], Loss: 0.6527634263038635\n",
      "Validation: Epoch [9], Batch [407/938], Loss: 0.7097157835960388\n",
      "Validation: Epoch [9], Batch [408/938], Loss: 0.6059154868125916\n",
      "Validation: Epoch [9], Batch [409/938], Loss: 0.7311210632324219\n",
      "Validation: Epoch [9], Batch [410/938], Loss: 0.7058993577957153\n",
      "Validation: Epoch [9], Batch [411/938], Loss: 0.5809074640274048\n",
      "Validation: Epoch [9], Batch [412/938], Loss: 0.917367696762085\n",
      "Validation: Epoch [9], Batch [413/938], Loss: 0.7718992233276367\n",
      "Validation: Epoch [9], Batch [414/938], Loss: 0.8773685693740845\n",
      "Validation: Epoch [9], Batch [415/938], Loss: 0.7187281847000122\n",
      "Validation: Epoch [9], Batch [416/938], Loss: 0.8503344058990479\n",
      "Validation: Epoch [9], Batch [417/938], Loss: 0.8190212845802307\n",
      "Validation: Epoch [9], Batch [418/938], Loss: 0.6700025200843811\n",
      "Validation: Epoch [9], Batch [419/938], Loss: 0.6107994318008423\n",
      "Validation: Epoch [9], Batch [420/938], Loss: 0.8405676484107971\n",
      "Validation: Epoch [9], Batch [421/938], Loss: 0.7778661847114563\n",
      "Validation: Epoch [9], Batch [422/938], Loss: 0.7289780974388123\n",
      "Validation: Epoch [9], Batch [423/938], Loss: 0.6796167492866516\n",
      "Validation: Epoch [9], Batch [424/938], Loss: 1.0140622854232788\n",
      "Validation: Epoch [9], Batch [425/938], Loss: 1.022756814956665\n",
      "Validation: Epoch [9], Batch [426/938], Loss: 0.8217263221740723\n",
      "Validation: Epoch [9], Batch [427/938], Loss: 0.7553598880767822\n",
      "Validation: Epoch [9], Batch [428/938], Loss: 0.7581186890602112\n",
      "Validation: Epoch [9], Batch [429/938], Loss: 1.0591371059417725\n",
      "Validation: Epoch [9], Batch [430/938], Loss: 0.9192634224891663\n",
      "Validation: Epoch [9], Batch [431/938], Loss: 0.7134548425674438\n",
      "Validation: Epoch [9], Batch [432/938], Loss: 1.0516610145568848\n",
      "Validation: Epoch [9], Batch [433/938], Loss: 0.960790753364563\n",
      "Validation: Epoch [9], Batch [434/938], Loss: 0.9102098941802979\n",
      "Validation: Epoch [9], Batch [435/938], Loss: 0.9973493814468384\n",
      "Validation: Epoch [9], Batch [436/938], Loss: 0.8899053335189819\n",
      "Validation: Epoch [9], Batch [437/938], Loss: 0.8441392779350281\n",
      "Validation: Epoch [9], Batch [438/938], Loss: 0.9684180617332458\n",
      "Validation: Epoch [9], Batch [439/938], Loss: 0.791968047618866\n",
      "Validation: Epoch [9], Batch [440/938], Loss: 0.863957941532135\n",
      "Validation: Epoch [9], Batch [441/938], Loss: 0.7701089978218079\n",
      "Validation: Epoch [9], Batch [442/938], Loss: 0.7265637516975403\n",
      "Validation: Epoch [9], Batch [443/938], Loss: 0.5437697768211365\n",
      "Validation: Epoch [9], Batch [444/938], Loss: 1.0102531909942627\n",
      "Validation: Epoch [9], Batch [445/938], Loss: 0.7525472640991211\n",
      "Validation: Epoch [9], Batch [446/938], Loss: 0.8618223667144775\n",
      "Validation: Epoch [9], Batch [447/938], Loss: 0.7449183464050293\n",
      "Validation: Epoch [9], Batch [448/938], Loss: 0.9422671794891357\n",
      "Validation: Epoch [9], Batch [449/938], Loss: 0.8844790458679199\n",
      "Validation: Epoch [9], Batch [450/938], Loss: 0.9332064986228943\n",
      "Validation: Epoch [9], Batch [451/938], Loss: 0.7005698084831238\n",
      "Validation: Epoch [9], Batch [452/938], Loss: 0.8875014781951904\n",
      "Validation: Epoch [9], Batch [453/938], Loss: 0.8902949690818787\n",
      "Validation: Epoch [9], Batch [454/938], Loss: 0.8920908570289612\n",
      "Validation: Epoch [9], Batch [455/938], Loss: 0.6490184664726257\n",
      "Validation: Epoch [9], Batch [456/938], Loss: 0.9098451733589172\n",
      "Validation: Epoch [9], Batch [457/938], Loss: 0.7457672357559204\n",
      "Validation: Epoch [9], Batch [458/938], Loss: 0.8557007312774658\n",
      "Validation: Epoch [9], Batch [459/938], Loss: 1.0136994123458862\n",
      "Validation: Epoch [9], Batch [460/938], Loss: 0.8366150856018066\n",
      "Validation: Epoch [9], Batch [461/938], Loss: 0.74381023645401\n",
      "Validation: Epoch [9], Batch [462/938], Loss: 0.7410792112350464\n",
      "Validation: Epoch [9], Batch [463/938], Loss: 0.7756922841072083\n",
      "Validation: Epoch [9], Batch [464/938], Loss: 0.7719010710716248\n",
      "Validation: Epoch [9], Batch [465/938], Loss: 0.6605950593948364\n",
      "Validation: Epoch [9], Batch [466/938], Loss: 0.9175381064414978\n",
      "Validation: Epoch [9], Batch [467/938], Loss: 0.8523181676864624\n",
      "Validation: Epoch [9], Batch [468/938], Loss: 0.8690986633300781\n",
      "Validation: Epoch [9], Batch [469/938], Loss: 0.7981739640235901\n",
      "Validation: Epoch [9], Batch [470/938], Loss: 0.781512975692749\n",
      "Validation: Epoch [9], Batch [471/938], Loss: 0.6630427837371826\n",
      "Validation: Epoch [9], Batch [472/938], Loss: 0.9086176156997681\n",
      "Validation: Epoch [9], Batch [473/938], Loss: 0.7589343786239624\n",
      "Validation: Epoch [9], Batch [474/938], Loss: 0.8378903865814209\n",
      "Validation: Epoch [9], Batch [475/938], Loss: 0.668586015701294\n",
      "Validation: Epoch [9], Batch [476/938], Loss: 0.8501184582710266\n",
      "Validation: Epoch [9], Batch [477/938], Loss: 0.8044389486312866\n",
      "Validation: Epoch [9], Batch [478/938], Loss: 0.7313758134841919\n",
      "Validation: Epoch [9], Batch [479/938], Loss: 0.6396626234054565\n",
      "Validation: Epoch [9], Batch [480/938], Loss: 0.8201809525489807\n",
      "Validation: Epoch [9], Batch [481/938], Loss: 0.6072747707366943\n",
      "Validation: Epoch [9], Batch [482/938], Loss: 0.6671724915504456\n",
      "Validation: Epoch [9], Batch [483/938], Loss: 0.8413967490196228\n",
      "Validation: Epoch [9], Batch [484/938], Loss: 0.8614253401756287\n",
      "Validation: Epoch [9], Batch [485/938], Loss: 0.8319966793060303\n",
      "Validation: Epoch [9], Batch [486/938], Loss: 0.8965705633163452\n",
      "Validation: Epoch [9], Batch [487/938], Loss: 1.0580158233642578\n",
      "Validation: Epoch [9], Batch [488/938], Loss: 0.9019457697868347\n",
      "Validation: Epoch [9], Batch [489/938], Loss: 0.7586643099784851\n",
      "Validation: Epoch [9], Batch [490/938], Loss: 0.6980898380279541\n",
      "Validation: Epoch [9], Batch [491/938], Loss: 0.7130043506622314\n",
      "Validation: Epoch [9], Batch [492/938], Loss: 0.7460604906082153\n",
      "Validation: Epoch [9], Batch [493/938], Loss: 0.9444217681884766\n",
      "Validation: Epoch [9], Batch [494/938], Loss: 0.826179563999176\n",
      "Validation: Epoch [9], Batch [495/938], Loss: 0.7811282873153687\n",
      "Validation: Epoch [9], Batch [496/938], Loss: 0.8336845636367798\n",
      "Validation: Epoch [9], Batch [497/938], Loss: 0.7746462821960449\n",
      "Validation: Epoch [9], Batch [498/938], Loss: 0.8102327585220337\n",
      "Validation: Epoch [9], Batch [499/938], Loss: 0.8216322064399719\n",
      "Validation: Epoch [9], Batch [500/938], Loss: 0.6934603452682495\n",
      "Validation: Epoch [9], Batch [501/938], Loss: 0.8394260406494141\n",
      "Validation: Epoch [9], Batch [502/938], Loss: 0.8197008371353149\n",
      "Validation: Epoch [9], Batch [503/938], Loss: 0.8117021918296814\n",
      "Validation: Epoch [9], Batch [504/938], Loss: 0.8607460260391235\n",
      "Validation: Epoch [9], Batch [505/938], Loss: 0.8484737873077393\n",
      "Validation: Epoch [9], Batch [506/938], Loss: 0.7342920899391174\n",
      "Validation: Epoch [9], Batch [507/938], Loss: 0.8597463369369507\n",
      "Validation: Epoch [9], Batch [508/938], Loss: 1.0141533613204956\n",
      "Validation: Epoch [9], Batch [509/938], Loss: 1.1085751056671143\n",
      "Validation: Epoch [9], Batch [510/938], Loss: 0.6419074535369873\n",
      "Validation: Epoch [9], Batch [511/938], Loss: 0.7157078981399536\n",
      "Validation: Epoch [9], Batch [512/938], Loss: 0.6013070344924927\n",
      "Validation: Epoch [9], Batch [513/938], Loss: 0.870213508605957\n",
      "Validation: Epoch [9], Batch [514/938], Loss: 0.7057488560676575\n",
      "Validation: Epoch [9], Batch [515/938], Loss: 0.7777882814407349\n",
      "Validation: Epoch [9], Batch [516/938], Loss: 0.6188516020774841\n",
      "Validation: Epoch [9], Batch [517/938], Loss: 0.9659159183502197\n",
      "Validation: Epoch [9], Batch [518/938], Loss: 0.8340082168579102\n",
      "Validation: Epoch [9], Batch [519/938], Loss: 0.9642972946166992\n",
      "Validation: Epoch [9], Batch [520/938], Loss: 0.6567912101745605\n",
      "Validation: Epoch [9], Batch [521/938], Loss: 0.9131554365158081\n",
      "Validation: Epoch [9], Batch [522/938], Loss: 0.7682268023490906\n",
      "Validation: Epoch [9], Batch [523/938], Loss: 0.7648361325263977\n",
      "Validation: Epoch [9], Batch [524/938], Loss: 0.5546892881393433\n",
      "Validation: Epoch [9], Batch [525/938], Loss: 0.8166331052780151\n",
      "Validation: Epoch [9], Batch [526/938], Loss: 0.7293307781219482\n",
      "Validation: Epoch [9], Batch [527/938], Loss: 0.674750030040741\n",
      "Validation: Epoch [9], Batch [528/938], Loss: 0.6715549230575562\n",
      "Validation: Epoch [9], Batch [529/938], Loss: 0.7220026850700378\n",
      "Validation: Epoch [9], Batch [530/938], Loss: 0.9239466190338135\n",
      "Validation: Epoch [9], Batch [531/938], Loss: 0.7586002945899963\n",
      "Validation: Epoch [9], Batch [532/938], Loss: 0.8433870673179626\n",
      "Validation: Epoch [9], Batch [533/938], Loss: 0.5865933299064636\n",
      "Validation: Epoch [9], Batch [534/938], Loss: 1.0192358493804932\n",
      "Validation: Epoch [9], Batch [535/938], Loss: 0.7871881723403931\n",
      "Validation: Epoch [9], Batch [536/938], Loss: 0.7446820735931396\n",
      "Validation: Epoch [9], Batch [537/938], Loss: 1.1092021465301514\n",
      "Validation: Epoch [9], Batch [538/938], Loss: 0.6934218406677246\n",
      "Validation: Epoch [9], Batch [539/938], Loss: 0.7305748462677002\n",
      "Validation: Epoch [9], Batch [540/938], Loss: 0.8236554265022278\n",
      "Validation: Epoch [9], Batch [541/938], Loss: 0.8071464896202087\n",
      "Validation: Epoch [9], Batch [542/938], Loss: 0.7920647859573364\n",
      "Validation: Epoch [9], Batch [543/938], Loss: 0.7651011943817139\n",
      "Validation: Epoch [9], Batch [544/938], Loss: 0.804098904132843\n",
      "Validation: Epoch [9], Batch [545/938], Loss: 0.7769858241081238\n",
      "Validation: Epoch [9], Batch [546/938], Loss: 0.947347104549408\n",
      "Validation: Epoch [9], Batch [547/938], Loss: 0.9512554407119751\n",
      "Validation: Epoch [9], Batch [548/938], Loss: 0.7315961718559265\n",
      "Validation: Epoch [9], Batch [549/938], Loss: 0.6827842593193054\n",
      "Validation: Epoch [9], Batch [550/938], Loss: 0.5538684129714966\n",
      "Validation: Epoch [9], Batch [551/938], Loss: 0.857184648513794\n",
      "Validation: Epoch [9], Batch [552/938], Loss: 0.5696809887886047\n",
      "Validation: Epoch [9], Batch [553/938], Loss: 0.8230292201042175\n",
      "Validation: Epoch [9], Batch [554/938], Loss: 0.9838380217552185\n",
      "Validation: Epoch [9], Batch [555/938], Loss: 0.6534703373908997\n",
      "Validation: Epoch [9], Batch [556/938], Loss: 0.8925926089286804\n",
      "Validation: Epoch [9], Batch [557/938], Loss: 0.7584201097488403\n",
      "Validation: Epoch [9], Batch [558/938], Loss: 1.023278832435608\n",
      "Validation: Epoch [9], Batch [559/938], Loss: 0.8257662057876587\n",
      "Validation: Epoch [9], Batch [560/938], Loss: 0.9001408219337463\n",
      "Validation: Epoch [9], Batch [561/938], Loss: 0.976466178894043\n",
      "Validation: Epoch [9], Batch [562/938], Loss: 0.681678831577301\n",
      "Validation: Epoch [9], Batch [563/938], Loss: 0.9356774687767029\n",
      "Validation: Epoch [9], Batch [564/938], Loss: 0.6196112632751465\n",
      "Validation: Epoch [9], Batch [565/938], Loss: 0.8652094602584839\n",
      "Validation: Epoch [9], Batch [566/938], Loss: 0.7377862334251404\n",
      "Validation: Epoch [9], Batch [567/938], Loss: 0.6386393308639526\n",
      "Validation: Epoch [9], Batch [568/938], Loss: 0.7452901005744934\n",
      "Validation: Epoch [9], Batch [569/938], Loss: 0.7451941967010498\n",
      "Validation: Epoch [9], Batch [570/938], Loss: 0.8920884728431702\n",
      "Validation: Epoch [9], Batch [571/938], Loss: 1.0478453636169434\n",
      "Validation: Epoch [9], Batch [572/938], Loss: 0.9245803356170654\n",
      "Validation: Epoch [9], Batch [573/938], Loss: 0.788251519203186\n",
      "Validation: Epoch [9], Batch [574/938], Loss: 1.0896471738815308\n",
      "Validation: Epoch [9], Batch [575/938], Loss: 0.7005813717842102\n",
      "Validation: Epoch [9], Batch [576/938], Loss: 0.7620207071304321\n",
      "Validation: Epoch [9], Batch [577/938], Loss: 0.7980587482452393\n",
      "Validation: Epoch [9], Batch [578/938], Loss: 0.5337428450584412\n",
      "Validation: Epoch [9], Batch [579/938], Loss: 0.8584578037261963\n",
      "Validation: Epoch [9], Batch [580/938], Loss: 0.8289651870727539\n",
      "Validation: Epoch [9], Batch [581/938], Loss: 0.7832174301147461\n",
      "Validation: Epoch [9], Batch [582/938], Loss: 0.6816019415855408\n",
      "Validation: Epoch [9], Batch [583/938], Loss: 0.639365017414093\n",
      "Validation: Epoch [9], Batch [584/938], Loss: 0.8087007403373718\n",
      "Validation: Epoch [9], Batch [585/938], Loss: 0.7656912207603455\n",
      "Validation: Epoch [9], Batch [586/938], Loss: 0.6192406415939331\n",
      "Validation: Epoch [9], Batch [587/938], Loss: 0.6029903888702393\n",
      "Validation: Epoch [9], Batch [588/938], Loss: 0.7857174873352051\n",
      "Validation: Epoch [9], Batch [589/938], Loss: 0.9583443403244019\n",
      "Validation: Epoch [9], Batch [590/938], Loss: 0.7466927766799927\n",
      "Validation: Epoch [9], Batch [591/938], Loss: 0.7459030747413635\n",
      "Validation: Epoch [9], Batch [592/938], Loss: 0.8315414786338806\n",
      "Validation: Epoch [9], Batch [593/938], Loss: 0.741702675819397\n",
      "Validation: Epoch [9], Batch [594/938], Loss: 0.7208812236785889\n",
      "Validation: Epoch [9], Batch [595/938], Loss: 0.8242948651313782\n",
      "Validation: Epoch [9], Batch [596/938], Loss: 1.0012519359588623\n",
      "Validation: Epoch [9], Batch [597/938], Loss: 0.7538080215454102\n",
      "Validation: Epoch [9], Batch [598/938], Loss: 0.7405434846878052\n",
      "Validation: Epoch [9], Batch [599/938], Loss: 0.716204822063446\n",
      "Validation: Epoch [9], Batch [600/938], Loss: 0.7876663208007812\n",
      "Validation: Epoch [9], Batch [601/938], Loss: 0.9443538188934326\n",
      "Validation: Epoch [9], Batch [602/938], Loss: 0.8175559639930725\n",
      "Validation: Epoch [9], Batch [603/938], Loss: 0.5886010527610779\n",
      "Validation: Epoch [9], Batch [604/938], Loss: 0.9521401524543762\n",
      "Validation: Epoch [9], Batch [605/938], Loss: 0.9367753267288208\n",
      "Validation: Epoch [9], Batch [606/938], Loss: 0.6277482509613037\n",
      "Validation: Epoch [9], Batch [607/938], Loss: 0.7379778623580933\n",
      "Validation: Epoch [9], Batch [608/938], Loss: 0.8154345750808716\n",
      "Validation: Epoch [9], Batch [609/938], Loss: 0.8732382655143738\n",
      "Validation: Epoch [9], Batch [610/938], Loss: 0.9415262341499329\n",
      "Validation: Epoch [9], Batch [611/938], Loss: 0.868175208568573\n",
      "Validation: Epoch [9], Batch [612/938], Loss: 1.1989107131958008\n",
      "Validation: Epoch [9], Batch [613/938], Loss: 0.8414808511734009\n",
      "Validation: Epoch [9], Batch [614/938], Loss: 0.9279527068138123\n",
      "Validation: Epoch [9], Batch [615/938], Loss: 0.9458556771278381\n",
      "Validation: Epoch [9], Batch [616/938], Loss: 0.8229235410690308\n",
      "Validation: Epoch [9], Batch [617/938], Loss: 0.8330230712890625\n",
      "Validation: Epoch [9], Batch [618/938], Loss: 0.7454060912132263\n",
      "Validation: Epoch [9], Batch [619/938], Loss: 0.7777329087257385\n",
      "Validation: Epoch [9], Batch [620/938], Loss: 1.0375957489013672\n",
      "Validation: Epoch [9], Batch [621/938], Loss: 0.8245248198509216\n",
      "Validation: Epoch [9], Batch [622/938], Loss: 0.9066413044929504\n",
      "Validation: Epoch [9], Batch [623/938], Loss: 0.778715193271637\n",
      "Validation: Epoch [9], Batch [624/938], Loss: 0.8253793120384216\n",
      "Validation: Epoch [9], Batch [625/938], Loss: 0.49115312099456787\n",
      "Validation: Epoch [9], Batch [626/938], Loss: 0.5846770405769348\n",
      "Validation: Epoch [9], Batch [627/938], Loss: 0.6594883799552917\n",
      "Validation: Epoch [9], Batch [628/938], Loss: 1.004607081413269\n",
      "Validation: Epoch [9], Batch [629/938], Loss: 0.7808963656425476\n",
      "Validation: Epoch [9], Batch [630/938], Loss: 0.7929074168205261\n",
      "Validation: Epoch [9], Batch [631/938], Loss: 0.8385924100875854\n",
      "Validation: Epoch [9], Batch [632/938], Loss: 0.7788386940956116\n",
      "Validation: Epoch [9], Batch [633/938], Loss: 0.7614771723747253\n",
      "Validation: Epoch [9], Batch [634/938], Loss: 0.8105051517486572\n",
      "Validation: Epoch [9], Batch [635/938], Loss: 0.8979899287223816\n",
      "Validation: Epoch [9], Batch [636/938], Loss: 0.82703697681427\n",
      "Validation: Epoch [9], Batch [637/938], Loss: 0.742219090461731\n",
      "Validation: Epoch [9], Batch [638/938], Loss: 1.0360137224197388\n",
      "Validation: Epoch [9], Batch [639/938], Loss: 1.0056695938110352\n",
      "Validation: Epoch [9], Batch [640/938], Loss: 0.930584192276001\n",
      "Validation: Epoch [9], Batch [641/938], Loss: 0.9195884466171265\n",
      "Validation: Epoch [9], Batch [642/938], Loss: 0.9005486965179443\n",
      "Validation: Epoch [9], Batch [643/938], Loss: 0.6135889887809753\n",
      "Validation: Epoch [9], Batch [644/938], Loss: 0.7249971628189087\n",
      "Validation: Epoch [9], Batch [645/938], Loss: 0.8127821683883667\n",
      "Validation: Epoch [9], Batch [646/938], Loss: 1.1575143337249756\n",
      "Validation: Epoch [9], Batch [647/938], Loss: 0.8366146683692932\n",
      "Validation: Epoch [9], Batch [648/938], Loss: 0.6896616220474243\n",
      "Validation: Epoch [9], Batch [649/938], Loss: 0.8883659243583679\n",
      "Validation: Epoch [9], Batch [650/938], Loss: 0.6185747385025024\n",
      "Validation: Epoch [9], Batch [651/938], Loss: 0.8332476615905762\n",
      "Validation: Epoch [9], Batch [652/938], Loss: 0.536439061164856\n",
      "Validation: Epoch [9], Batch [653/938], Loss: 0.6850304007530212\n",
      "Validation: Epoch [9], Batch [654/938], Loss: 0.7671851515769958\n",
      "Validation: Epoch [9], Batch [655/938], Loss: 0.8329150676727295\n",
      "Validation: Epoch [9], Batch [656/938], Loss: 0.7713600397109985\n",
      "Validation: Epoch [9], Batch [657/938], Loss: 0.7515771985054016\n",
      "Validation: Epoch [9], Batch [658/938], Loss: 0.7746825814247131\n",
      "Validation: Epoch [9], Batch [659/938], Loss: 0.910839855670929\n",
      "Validation: Epoch [9], Batch [660/938], Loss: 1.0024166107177734\n",
      "Validation: Epoch [9], Batch [661/938], Loss: 0.9070060849189758\n",
      "Validation: Epoch [9], Batch [662/938], Loss: 0.6057957410812378\n",
      "Validation: Epoch [9], Batch [663/938], Loss: 0.6846241354942322\n",
      "Validation: Epoch [9], Batch [664/938], Loss: 0.932664155960083\n",
      "Validation: Epoch [9], Batch [665/938], Loss: 0.47069409489631653\n",
      "Validation: Epoch [9], Batch [666/938], Loss: 0.8587320446968079\n",
      "Validation: Epoch [9], Batch [667/938], Loss: 1.0344243049621582\n",
      "Validation: Epoch [9], Batch [668/938], Loss: 0.9171380996704102\n",
      "Validation: Epoch [9], Batch [669/938], Loss: 1.0420254468917847\n",
      "Validation: Epoch [9], Batch [670/938], Loss: 0.7692203521728516\n",
      "Validation: Epoch [9], Batch [671/938], Loss: 1.1003435850143433\n",
      "Validation: Epoch [9], Batch [672/938], Loss: 0.8040368556976318\n",
      "Validation: Epoch [9], Batch [673/938], Loss: 1.0602855682373047\n",
      "Validation: Epoch [9], Batch [674/938], Loss: 0.6351951956748962\n",
      "Validation: Epoch [9], Batch [675/938], Loss: 0.8949940204620361\n",
      "Validation: Epoch [9], Batch [676/938], Loss: 0.7451854944229126\n",
      "Validation: Epoch [9], Batch [677/938], Loss: 1.0389498472213745\n",
      "Validation: Epoch [9], Batch [678/938], Loss: 0.8100327253341675\n",
      "Validation: Epoch [9], Batch [679/938], Loss: 0.7223005294799805\n",
      "Validation: Epoch [9], Batch [680/938], Loss: 0.8027802109718323\n",
      "Validation: Epoch [9], Batch [681/938], Loss: 0.8028373122215271\n",
      "Validation: Epoch [9], Batch [682/938], Loss: 0.6236100792884827\n",
      "Validation: Epoch [9], Batch [683/938], Loss: 0.7552695870399475\n",
      "Validation: Epoch [9], Batch [684/938], Loss: 0.8378534913063049\n",
      "Validation: Epoch [9], Batch [685/938], Loss: 0.8146334886550903\n",
      "Validation: Epoch [9], Batch [686/938], Loss: 0.8989434242248535\n",
      "Validation: Epoch [9], Batch [687/938], Loss: 0.7168580889701843\n",
      "Validation: Epoch [9], Batch [688/938], Loss: 0.7097852230072021\n",
      "Validation: Epoch [9], Batch [689/938], Loss: 0.7368181347846985\n",
      "Validation: Epoch [9], Batch [690/938], Loss: 0.8502051830291748\n",
      "Validation: Epoch [9], Batch [691/938], Loss: 0.760137677192688\n",
      "Validation: Epoch [9], Batch [692/938], Loss: 0.6872310638427734\n",
      "Validation: Epoch [9], Batch [693/938], Loss: 0.8306112289428711\n",
      "Validation: Epoch [9], Batch [694/938], Loss: 0.8005289435386658\n",
      "Validation: Epoch [9], Batch [695/938], Loss: 0.9442312717437744\n",
      "Validation: Epoch [9], Batch [696/938], Loss: 0.7078938484191895\n",
      "Validation: Epoch [9], Batch [697/938], Loss: 0.6530471444129944\n",
      "Validation: Epoch [9], Batch [698/938], Loss: 1.135318398475647\n",
      "Validation: Epoch [9], Batch [699/938], Loss: 0.8691768646240234\n",
      "Validation: Epoch [9], Batch [700/938], Loss: 0.9004772305488586\n",
      "Validation: Epoch [9], Batch [701/938], Loss: 0.7814132571220398\n",
      "Validation: Epoch [9], Batch [702/938], Loss: 0.8236826658248901\n",
      "Validation: Epoch [9], Batch [703/938], Loss: 0.8583014607429504\n",
      "Validation: Epoch [9], Batch [704/938], Loss: 1.1492104530334473\n",
      "Validation: Epoch [9], Batch [705/938], Loss: 0.8627493977546692\n",
      "Validation: Epoch [9], Batch [706/938], Loss: 0.7832733988761902\n",
      "Validation: Epoch [9], Batch [707/938], Loss: 0.7985160946846008\n",
      "Validation: Epoch [9], Batch [708/938], Loss: 0.8457534909248352\n",
      "Validation: Epoch [9], Batch [709/938], Loss: 0.9262261390686035\n",
      "Validation: Epoch [9], Batch [710/938], Loss: 1.0370721817016602\n",
      "Validation: Epoch [9], Batch [711/938], Loss: 0.727956235408783\n",
      "Validation: Epoch [9], Batch [712/938], Loss: 0.8576167225837708\n",
      "Validation: Epoch [9], Batch [713/938], Loss: 0.8356085419654846\n",
      "Validation: Epoch [9], Batch [714/938], Loss: 0.7742511630058289\n",
      "Validation: Epoch [9], Batch [715/938], Loss: 0.8612123131752014\n",
      "Validation: Epoch [9], Batch [716/938], Loss: 0.9023101925849915\n",
      "Validation: Epoch [9], Batch [717/938], Loss: 0.826240599155426\n",
      "Validation: Epoch [9], Batch [718/938], Loss: 0.568953812122345\n",
      "Validation: Epoch [9], Batch [719/938], Loss: 0.7524076104164124\n",
      "Validation: Epoch [9], Batch [720/938], Loss: 0.723914623260498\n",
      "Validation: Epoch [9], Batch [721/938], Loss: 0.7746440172195435\n",
      "Validation: Epoch [9], Batch [722/938], Loss: 1.0204439163208008\n",
      "Validation: Epoch [9], Batch [723/938], Loss: 0.6267220377922058\n",
      "Validation: Epoch [9], Batch [724/938], Loss: 1.0512917041778564\n",
      "Validation: Epoch [9], Batch [725/938], Loss: 0.7485624551773071\n",
      "Validation: Epoch [9], Batch [726/938], Loss: 0.7596388459205627\n",
      "Validation: Epoch [9], Batch [727/938], Loss: 0.6992879509925842\n",
      "Validation: Epoch [9], Batch [728/938], Loss: 0.9489272236824036\n",
      "Validation: Epoch [9], Batch [729/938], Loss: 0.790481686592102\n",
      "Validation: Epoch [9], Batch [730/938], Loss: 0.7981588244438171\n",
      "Validation: Epoch [9], Batch [731/938], Loss: 0.9787297248840332\n",
      "Validation: Epoch [9], Batch [732/938], Loss: 0.724197268486023\n",
      "Validation: Epoch [9], Batch [733/938], Loss: 0.8406142592430115\n",
      "Validation: Epoch [9], Batch [734/938], Loss: 0.646896481513977\n",
      "Validation: Epoch [9], Batch [735/938], Loss: 0.8975598812103271\n",
      "Validation: Epoch [9], Batch [736/938], Loss: 0.943209171295166\n",
      "Validation: Epoch [9], Batch [737/938], Loss: 0.6202201843261719\n",
      "Validation: Epoch [9], Batch [738/938], Loss: 0.7670523524284363\n",
      "Validation: Epoch [9], Batch [739/938], Loss: 0.9162819981575012\n",
      "Validation: Epoch [9], Batch [740/938], Loss: 0.8421214818954468\n",
      "Validation: Epoch [9], Batch [741/938], Loss: 0.9193668961524963\n",
      "Validation: Epoch [9], Batch [742/938], Loss: 0.8329170942306519\n",
      "Validation: Epoch [9], Batch [743/938], Loss: 0.43107742071151733\n",
      "Validation: Epoch [9], Batch [744/938], Loss: 0.6661208271980286\n",
      "Validation: Epoch [9], Batch [745/938], Loss: 0.7172314524650574\n",
      "Validation: Epoch [9], Batch [746/938], Loss: 0.61298668384552\n",
      "Validation: Epoch [9], Batch [747/938], Loss: 0.96519935131073\n",
      "Validation: Epoch [9], Batch [748/938], Loss: 0.7853007316589355\n",
      "Validation: Epoch [9], Batch [749/938], Loss: 0.7764663696289062\n",
      "Validation: Epoch [9], Batch [750/938], Loss: 0.8708511590957642\n",
      "Validation: Epoch [9], Batch [751/938], Loss: 0.8622806668281555\n",
      "Validation: Epoch [9], Batch [752/938], Loss: 0.7301895022392273\n",
      "Validation: Epoch [9], Batch [753/938], Loss: 0.9108011722564697\n",
      "Validation: Epoch [9], Batch [754/938], Loss: 0.8221703767776489\n",
      "Validation: Epoch [9], Batch [755/938], Loss: 0.7756605744361877\n",
      "Validation: Epoch [9], Batch [756/938], Loss: 0.8654033541679382\n",
      "Validation: Epoch [9], Batch [757/938], Loss: 0.8635574579238892\n",
      "Validation: Epoch [9], Batch [758/938], Loss: 0.7309096455574036\n",
      "Validation: Epoch [9], Batch [759/938], Loss: 0.868990957736969\n",
      "Validation: Epoch [9], Batch [760/938], Loss: 0.7057009935379028\n",
      "Validation: Epoch [9], Batch [761/938], Loss: 0.6163477897644043\n",
      "Validation: Epoch [9], Batch [762/938], Loss: 0.814710795879364\n",
      "Validation: Epoch [9], Batch [763/938], Loss: 1.0781606435775757\n",
      "Validation: Epoch [9], Batch [764/938], Loss: 1.032488226890564\n",
      "Validation: Epoch [9], Batch [765/938], Loss: 0.6226045489311218\n",
      "Validation: Epoch [9], Batch [766/938], Loss: 1.0107449293136597\n",
      "Validation: Epoch [9], Batch [767/938], Loss: 0.8957785367965698\n",
      "Validation: Epoch [9], Batch [768/938], Loss: 0.8051214218139648\n",
      "Validation: Epoch [9], Batch [769/938], Loss: 0.8049908876419067\n",
      "Validation: Epoch [9], Batch [770/938], Loss: 0.9638895988464355\n",
      "Validation: Epoch [9], Batch [771/938], Loss: 0.48885127902030945\n",
      "Validation: Epoch [9], Batch [772/938], Loss: 0.6576129198074341\n",
      "Validation: Epoch [9], Batch [773/938], Loss: 0.8546401262283325\n",
      "Validation: Epoch [9], Batch [774/938], Loss: 0.6764313578605652\n",
      "Validation: Epoch [9], Batch [775/938], Loss: 0.8998774290084839\n",
      "Validation: Epoch [9], Batch [776/938], Loss: 0.9539440870285034\n",
      "Validation: Epoch [9], Batch [777/938], Loss: 0.8383187055587769\n",
      "Validation: Epoch [9], Batch [778/938], Loss: 0.8909775614738464\n",
      "Validation: Epoch [9], Batch [779/938], Loss: 0.7216217517852783\n",
      "Validation: Epoch [9], Batch [780/938], Loss: 0.8478032350540161\n",
      "Validation: Epoch [9], Batch [781/938], Loss: 0.5795783400535583\n",
      "Validation: Epoch [9], Batch [782/938], Loss: 1.1916848421096802\n",
      "Validation: Epoch [9], Batch [783/938], Loss: 0.7135464549064636\n",
      "Validation: Epoch [9], Batch [784/938], Loss: 0.7935929298400879\n",
      "Validation: Epoch [9], Batch [785/938], Loss: 0.661087155342102\n",
      "Validation: Epoch [9], Batch [786/938], Loss: 0.6725367307662964\n",
      "Validation: Epoch [9], Batch [787/938], Loss: 0.7190405130386353\n",
      "Validation: Epoch [9], Batch [788/938], Loss: 0.8753553032875061\n",
      "Validation: Epoch [9], Batch [789/938], Loss: 0.7770900130271912\n",
      "Validation: Epoch [9], Batch [790/938], Loss: 0.7433234453201294\n",
      "Validation: Epoch [9], Batch [791/938], Loss: 0.7792752981185913\n",
      "Validation: Epoch [9], Batch [792/938], Loss: 0.7289584875106812\n",
      "Validation: Epoch [9], Batch [793/938], Loss: 0.8765811920166016\n",
      "Validation: Epoch [9], Batch [794/938], Loss: 0.6525457501411438\n",
      "Validation: Epoch [9], Batch [795/938], Loss: 0.6143895387649536\n",
      "Validation: Epoch [9], Batch [796/938], Loss: 0.6685924530029297\n",
      "Validation: Epoch [9], Batch [797/938], Loss: 0.7888248562812805\n",
      "Validation: Epoch [9], Batch [798/938], Loss: 0.7970379590988159\n",
      "Validation: Epoch [9], Batch [799/938], Loss: 0.9696961641311646\n",
      "Validation: Epoch [9], Batch [800/938], Loss: 0.5392380356788635\n",
      "Validation: Epoch [9], Batch [801/938], Loss: 0.9964736700057983\n",
      "Validation: Epoch [9], Batch [802/938], Loss: 0.8032266497612\n",
      "Validation: Epoch [9], Batch [803/938], Loss: 0.6929938793182373\n",
      "Validation: Epoch [9], Batch [804/938], Loss: 0.9291743636131287\n",
      "Validation: Epoch [9], Batch [805/938], Loss: 0.8081684112548828\n",
      "Validation: Epoch [9], Batch [806/938], Loss: 0.5820485949516296\n",
      "Validation: Epoch [9], Batch [807/938], Loss: 0.8185614347457886\n",
      "Validation: Epoch [9], Batch [808/938], Loss: 0.8623996376991272\n",
      "Validation: Epoch [9], Batch [809/938], Loss: 0.6479588747024536\n",
      "Validation: Epoch [9], Batch [810/938], Loss: 0.8528168797492981\n",
      "Validation: Epoch [9], Batch [811/938], Loss: 0.8962342739105225\n",
      "Validation: Epoch [9], Batch [812/938], Loss: 0.8056319952011108\n",
      "Validation: Epoch [9], Batch [813/938], Loss: 0.8375588655471802\n",
      "Validation: Epoch [9], Batch [814/938], Loss: 0.8453713059425354\n",
      "Validation: Epoch [9], Batch [815/938], Loss: 0.8994859457015991\n",
      "Validation: Epoch [9], Batch [816/938], Loss: 0.467727929353714\n",
      "Validation: Epoch [9], Batch [817/938], Loss: 0.7236481308937073\n",
      "Validation: Epoch [9], Batch [818/938], Loss: 0.6444944143295288\n",
      "Validation: Epoch [9], Batch [819/938], Loss: 0.783814549446106\n",
      "Validation: Epoch [9], Batch [820/938], Loss: 0.7355818748474121\n",
      "Validation: Epoch [9], Batch [821/938], Loss: 1.0019054412841797\n",
      "Validation: Epoch [9], Batch [822/938], Loss: 0.8686774373054504\n",
      "Validation: Epoch [9], Batch [823/938], Loss: 0.7036094069480896\n",
      "Validation: Epoch [9], Batch [824/938], Loss: 1.054258108139038\n",
      "Validation: Epoch [9], Batch [825/938], Loss: 1.0061296224594116\n",
      "Validation: Epoch [9], Batch [826/938], Loss: 0.9557680487632751\n",
      "Validation: Epoch [9], Batch [827/938], Loss: 0.7194311022758484\n",
      "Validation: Epoch [9], Batch [828/938], Loss: 0.6545501947402954\n",
      "Validation: Epoch [9], Batch [829/938], Loss: 0.8406065702438354\n",
      "Validation: Epoch [9], Batch [830/938], Loss: 0.7993824481964111\n",
      "Validation: Epoch [9], Batch [831/938], Loss: 0.6484590172767639\n",
      "Validation: Epoch [9], Batch [832/938], Loss: 0.8841624855995178\n",
      "Validation: Epoch [9], Batch [833/938], Loss: 0.82539302110672\n",
      "Validation: Epoch [9], Batch [834/938], Loss: 0.9238548278808594\n",
      "Validation: Epoch [9], Batch [835/938], Loss: 0.8291171789169312\n",
      "Validation: Epoch [9], Batch [836/938], Loss: 1.0028434991836548\n",
      "Validation: Epoch [9], Batch [837/938], Loss: 0.6873498558998108\n",
      "Validation: Epoch [9], Batch [838/938], Loss: 0.6900380849838257\n",
      "Validation: Epoch [9], Batch [839/938], Loss: 0.7934975624084473\n",
      "Validation: Epoch [9], Batch [840/938], Loss: 0.5890258550643921\n",
      "Validation: Epoch [9], Batch [841/938], Loss: 0.8476755619049072\n",
      "Validation: Epoch [9], Batch [842/938], Loss: 0.9783873558044434\n",
      "Validation: Epoch [9], Batch [843/938], Loss: 0.5749926567077637\n",
      "Validation: Epoch [9], Batch [844/938], Loss: 0.904686450958252\n",
      "Validation: Epoch [9], Batch [845/938], Loss: 0.957726240158081\n",
      "Validation: Epoch [9], Batch [846/938], Loss: 0.731267511844635\n",
      "Validation: Epoch [9], Batch [847/938], Loss: 0.686057984828949\n",
      "Validation: Epoch [9], Batch [848/938], Loss: 0.7633360028266907\n",
      "Validation: Epoch [9], Batch [849/938], Loss: 0.6892474293708801\n",
      "Validation: Epoch [9], Batch [850/938], Loss: 0.7418252825737\n",
      "Validation: Epoch [9], Batch [851/938], Loss: 0.7787163853645325\n",
      "Validation: Epoch [9], Batch [852/938], Loss: 1.0129156112670898\n",
      "Validation: Epoch [9], Batch [853/938], Loss: 0.980758786201477\n",
      "Validation: Epoch [9], Batch [854/938], Loss: 0.7794985175132751\n",
      "Validation: Epoch [9], Batch [855/938], Loss: 0.6877909898757935\n",
      "Validation: Epoch [9], Batch [856/938], Loss: 0.6143325567245483\n",
      "Validation: Epoch [9], Batch [857/938], Loss: 0.7196851372718811\n",
      "Validation: Epoch [9], Batch [858/938], Loss: 0.9833107590675354\n",
      "Validation: Epoch [9], Batch [859/938], Loss: 0.9506350755691528\n",
      "Validation: Epoch [9], Batch [860/938], Loss: 0.8036940097808838\n",
      "Validation: Epoch [9], Batch [861/938], Loss: 1.089341640472412\n",
      "Validation: Epoch [9], Batch [862/938], Loss: 0.7551969289779663\n",
      "Validation: Epoch [9], Batch [863/938], Loss: 0.8868059515953064\n",
      "Validation: Epoch [9], Batch [864/938], Loss: 0.9990296363830566\n",
      "Validation: Epoch [9], Batch [865/938], Loss: 0.6459376215934753\n",
      "Validation: Epoch [9], Batch [866/938], Loss: 0.8856207728385925\n",
      "Validation: Epoch [9], Batch [867/938], Loss: 0.7024832963943481\n",
      "Validation: Epoch [9], Batch [868/938], Loss: 0.68232661485672\n",
      "Validation: Epoch [9], Batch [869/938], Loss: 1.1706585884094238\n",
      "Validation: Epoch [9], Batch [870/938], Loss: 0.7089303731918335\n",
      "Validation: Epoch [9], Batch [871/938], Loss: 0.7547707557678223\n",
      "Validation: Epoch [9], Batch [872/938], Loss: 0.7213755249977112\n",
      "Validation: Epoch [9], Batch [873/938], Loss: 0.6316134929656982\n",
      "Validation: Epoch [9], Batch [874/938], Loss: 1.1034363508224487\n",
      "Validation: Epoch [9], Batch [875/938], Loss: 0.518358051776886\n",
      "Validation: Epoch [9], Batch [876/938], Loss: 1.0155179500579834\n",
      "Validation: Epoch [9], Batch [877/938], Loss: 1.0557570457458496\n",
      "Validation: Epoch [9], Batch [878/938], Loss: 0.7987989187240601\n",
      "Validation: Epoch [9], Batch [879/938], Loss: 0.923564076423645\n",
      "Validation: Epoch [9], Batch [880/938], Loss: 1.1052812337875366\n",
      "Validation: Epoch [9], Batch [881/938], Loss: 0.7738574743270874\n",
      "Validation: Epoch [9], Batch [882/938], Loss: 0.8858075737953186\n",
      "Validation: Epoch [9], Batch [883/938], Loss: 0.599240243434906\n",
      "Validation: Epoch [9], Batch [884/938], Loss: 0.711134135723114\n",
      "Validation: Epoch [9], Batch [885/938], Loss: 0.8551981449127197\n",
      "Validation: Epoch [9], Batch [886/938], Loss: 0.8978854417800903\n",
      "Validation: Epoch [9], Batch [887/938], Loss: 0.9479947090148926\n",
      "Validation: Epoch [9], Batch [888/938], Loss: 0.8416805863380432\n",
      "Validation: Epoch [9], Batch [889/938], Loss: 0.7704585194587708\n",
      "Validation: Epoch [9], Batch [890/938], Loss: 0.6686834692955017\n",
      "Validation: Epoch [9], Batch [891/938], Loss: 0.6411710977554321\n",
      "Validation: Epoch [9], Batch [892/938], Loss: 0.7458546161651611\n",
      "Validation: Epoch [9], Batch [893/938], Loss: 0.7448674440383911\n",
      "Validation: Epoch [9], Batch [894/938], Loss: 0.5467824339866638\n",
      "Validation: Epoch [9], Batch [895/938], Loss: 0.6417891383171082\n",
      "Validation: Epoch [9], Batch [896/938], Loss: 0.9680564403533936\n",
      "Validation: Epoch [9], Batch [897/938], Loss: 0.8937492966651917\n",
      "Validation: Epoch [9], Batch [898/938], Loss: 0.7025834321975708\n",
      "Validation: Epoch [9], Batch [899/938], Loss: 0.8199259638786316\n",
      "Validation: Epoch [9], Batch [900/938], Loss: 0.6868808269500732\n",
      "Validation: Epoch [9], Batch [901/938], Loss: 0.7780556082725525\n",
      "Validation: Epoch [9], Batch [902/938], Loss: 0.8767164945602417\n",
      "Validation: Epoch [9], Batch [903/938], Loss: 0.9101705551147461\n",
      "Validation: Epoch [9], Batch [904/938], Loss: 0.7785066366195679\n",
      "Validation: Epoch [9], Batch [905/938], Loss: 0.58545982837677\n",
      "Validation: Epoch [9], Batch [906/938], Loss: 0.682269811630249\n",
      "Validation: Epoch [9], Batch [907/938], Loss: 0.7521812915802002\n",
      "Validation: Epoch [9], Batch [908/938], Loss: 0.5252271890640259\n",
      "Validation: Epoch [9], Batch [909/938], Loss: 0.6273716688156128\n",
      "Validation: Epoch [9], Batch [910/938], Loss: 0.7158617377281189\n",
      "Validation: Epoch [9], Batch [911/938], Loss: 0.7188128232955933\n",
      "Validation: Epoch [9], Batch [912/938], Loss: 1.0207833051681519\n",
      "Validation: Epoch [9], Batch [913/938], Loss: 0.896196722984314\n",
      "Validation: Epoch [9], Batch [914/938], Loss: 0.8386930823326111\n",
      "Validation: Epoch [9], Batch [915/938], Loss: 0.7211046814918518\n",
      "Validation: Epoch [9], Batch [916/938], Loss: 0.7288909554481506\n",
      "Validation: Epoch [9], Batch [917/938], Loss: 0.7886966466903687\n",
      "Validation: Epoch [9], Batch [918/938], Loss: 0.691252589225769\n",
      "Validation: Epoch [9], Batch [919/938], Loss: 0.7277858257293701\n",
      "Validation: Epoch [9], Batch [920/938], Loss: 0.6100037097930908\n",
      "Validation: Epoch [9], Batch [921/938], Loss: 0.9066951274871826\n",
      "Validation: Epoch [9], Batch [922/938], Loss: 0.691279947757721\n",
      "Validation: Epoch [9], Batch [923/938], Loss: 0.9563761353492737\n",
      "Validation: Epoch [9], Batch [924/938], Loss: 0.6964987516403198\n",
      "Validation: Epoch [9], Batch [925/938], Loss: 0.6384032964706421\n",
      "Validation: Epoch [9], Batch [926/938], Loss: 0.9288575649261475\n",
      "Validation: Epoch [9], Batch [927/938], Loss: 0.8590459227561951\n",
      "Validation: Epoch [9], Batch [928/938], Loss: 0.8960033655166626\n",
      "Validation: Epoch [9], Batch [929/938], Loss: 0.6498666405677795\n",
      "Validation: Epoch [9], Batch [930/938], Loss: 1.0016382932662964\n",
      "Validation: Epoch [9], Batch [931/938], Loss: 0.8132215738296509\n",
      "Validation: Epoch [9], Batch [932/938], Loss: 0.7999616861343384\n",
      "Validation: Epoch [9], Batch [933/938], Loss: 0.6601937413215637\n",
      "Validation: Epoch [9], Batch [934/938], Loss: 0.9392852783203125\n",
      "Validation: Epoch [9], Batch [935/938], Loss: 0.9700140357017517\n",
      "Validation: Epoch [9], Batch [936/938], Loss: 0.9114404320716858\n",
      "Validation: Epoch [9], Batch [937/938], Loss: 0.6758151054382324\n",
      "Validation: Epoch [9], Batch [938/938], Loss: 1.4028810262680054\n",
      "Accuracy of test set: 0.73105\n",
      "Train: Epoch [10], Batch [1/938], Loss: 0.595960259437561\n",
      "Train: Epoch [10], Batch [2/938], Loss: 0.7935241460800171\n",
      "Train: Epoch [10], Batch [3/938], Loss: 0.7685505151748657\n",
      "Train: Epoch [10], Batch [4/938], Loss: 0.6342437863349915\n",
      "Train: Epoch [10], Batch [5/938], Loss: 0.964987576007843\n",
      "Train: Epoch [10], Batch [6/938], Loss: 0.7803749442100525\n",
      "Train: Epoch [10], Batch [7/938], Loss: 0.8034841418266296\n",
      "Train: Epoch [10], Batch [8/938], Loss: 0.7405045628547668\n",
      "Train: Epoch [10], Batch [9/938], Loss: 0.8539184331893921\n",
      "Train: Epoch [10], Batch [10/938], Loss: 0.5658965110778809\n",
      "Train: Epoch [10], Batch [11/938], Loss: 1.1051325798034668\n",
      "Train: Epoch [10], Batch [12/938], Loss: 0.9088752865791321\n",
      "Train: Epoch [10], Batch [13/938], Loss: 0.8826621770858765\n",
      "Train: Epoch [10], Batch [14/938], Loss: 0.9215261340141296\n",
      "Train: Epoch [10], Batch [15/938], Loss: 0.7829191088676453\n",
      "Train: Epoch [10], Batch [16/938], Loss: 0.6662724018096924\n",
      "Train: Epoch [10], Batch [17/938], Loss: 0.8179017901420593\n",
      "Train: Epoch [10], Batch [18/938], Loss: 0.746896505355835\n",
      "Train: Epoch [10], Batch [19/938], Loss: 0.8970027565956116\n",
      "Train: Epoch [10], Batch [20/938], Loss: 0.8484888076782227\n",
      "Train: Epoch [10], Batch [21/938], Loss: 0.6967930793762207\n",
      "Train: Epoch [10], Batch [22/938], Loss: 0.9346187114715576\n",
      "Train: Epoch [10], Batch [23/938], Loss: 0.5648692846298218\n",
      "Train: Epoch [10], Batch [24/938], Loss: 0.916541337966919\n",
      "Train: Epoch [10], Batch [25/938], Loss: 0.6290037631988525\n",
      "Train: Epoch [10], Batch [26/938], Loss: 0.6494205594062805\n",
      "Train: Epoch [10], Batch [27/938], Loss: 0.6425539255142212\n",
      "Train: Epoch [10], Batch [28/938], Loss: 0.9544559717178345\n",
      "Train: Epoch [10], Batch [29/938], Loss: 0.8496838808059692\n",
      "Train: Epoch [10], Batch [30/938], Loss: 0.9088441729545593\n",
      "Train: Epoch [10], Batch [31/938], Loss: 0.6733763217926025\n",
      "Train: Epoch [10], Batch [32/938], Loss: 0.7714557647705078\n",
      "Train: Epoch [10], Batch [33/938], Loss: 0.957854151725769\n",
      "Train: Epoch [10], Batch [34/938], Loss: 0.8224400281906128\n",
      "Train: Epoch [10], Batch [35/938], Loss: 0.936692476272583\n",
      "Train: Epoch [10], Batch [36/938], Loss: 0.6561469435691833\n",
      "Train: Epoch [10], Batch [37/938], Loss: 0.8730321526527405\n",
      "Train: Epoch [10], Batch [38/938], Loss: 0.7256552577018738\n",
      "Train: Epoch [10], Batch [39/938], Loss: 0.8008323907852173\n",
      "Train: Epoch [10], Batch [40/938], Loss: 0.7027560472488403\n",
      "Train: Epoch [10], Batch [41/938], Loss: 0.818711519241333\n",
      "Train: Epoch [10], Batch [42/938], Loss: 0.8792734146118164\n",
      "Train: Epoch [10], Batch [43/938], Loss: 0.9163089990615845\n",
      "Train: Epoch [10], Batch [44/938], Loss: 0.5767300724983215\n",
      "Train: Epoch [10], Batch [45/938], Loss: 0.8273038864135742\n",
      "Train: Epoch [10], Batch [46/938], Loss: 0.8252061605453491\n",
      "Train: Epoch [10], Batch [47/938], Loss: 0.6904013156890869\n",
      "Train: Epoch [10], Batch [48/938], Loss: 0.9751729965209961\n",
      "Train: Epoch [10], Batch [49/938], Loss: 0.8625110983848572\n",
      "Train: Epoch [10], Batch [50/938], Loss: 0.7257125377655029\n",
      "Train: Epoch [10], Batch [51/938], Loss: 0.8241041898727417\n",
      "Train: Epoch [10], Batch [52/938], Loss: 0.87685626745224\n",
      "Train: Epoch [10], Batch [53/938], Loss: 0.818122386932373\n",
      "Train: Epoch [10], Batch [54/938], Loss: 0.7527680397033691\n",
      "Train: Epoch [10], Batch [55/938], Loss: 0.7797476649284363\n",
      "Train: Epoch [10], Batch [56/938], Loss: 0.6892185807228088\n",
      "Train: Epoch [10], Batch [57/938], Loss: 0.6220511794090271\n",
      "Train: Epoch [10], Batch [58/938], Loss: 0.8944649696350098\n",
      "Train: Epoch [10], Batch [59/938], Loss: 0.9453558921813965\n",
      "Train: Epoch [10], Batch [60/938], Loss: 0.9431068301200867\n",
      "Train: Epoch [10], Batch [61/938], Loss: 0.6944603323936462\n",
      "Train: Epoch [10], Batch [62/938], Loss: 0.8134190440177917\n",
      "Train: Epoch [10], Batch [63/938], Loss: 0.7604955434799194\n",
      "Train: Epoch [10], Batch [64/938], Loss: 0.6949622631072998\n",
      "Train: Epoch [10], Batch [65/938], Loss: 0.595604419708252\n",
      "Train: Epoch [10], Batch [66/938], Loss: 0.6617425084114075\n",
      "Train: Epoch [10], Batch [67/938], Loss: 0.7699253559112549\n",
      "Train: Epoch [10], Batch [68/938], Loss: 0.8308236002922058\n",
      "Train: Epoch [10], Batch [69/938], Loss: 0.7299314141273499\n",
      "Train: Epoch [10], Batch [70/938], Loss: 0.7432572245597839\n",
      "Train: Epoch [10], Batch [71/938], Loss: 0.603482186794281\n",
      "Train: Epoch [10], Batch [72/938], Loss: 0.7896405458450317\n",
      "Train: Epoch [10], Batch [73/938], Loss: 0.7214597463607788\n",
      "Train: Epoch [10], Batch [74/938], Loss: 0.6512064933776855\n",
      "Train: Epoch [10], Batch [75/938], Loss: 0.7094822525978088\n",
      "Train: Epoch [10], Batch [76/938], Loss: 0.7765153646469116\n",
      "Train: Epoch [10], Batch [77/938], Loss: 0.7594285607337952\n",
      "Train: Epoch [10], Batch [78/938], Loss: 0.7582841515541077\n",
      "Train: Epoch [10], Batch [79/938], Loss: 0.7835561037063599\n",
      "Train: Epoch [10], Batch [80/938], Loss: 0.7115585803985596\n",
      "Train: Epoch [10], Batch [81/938], Loss: 0.7771399021148682\n",
      "Train: Epoch [10], Batch [82/938], Loss: 0.7927870154380798\n",
      "Train: Epoch [10], Batch [83/938], Loss: 0.6847745776176453\n",
      "Train: Epoch [10], Batch [84/938], Loss: 0.9609043598175049\n",
      "Train: Epoch [10], Batch [85/938], Loss: 0.8522224426269531\n",
      "Train: Epoch [10], Batch [86/938], Loss: 0.6878892779350281\n",
      "Train: Epoch [10], Batch [87/938], Loss: 0.5819967985153198\n",
      "Train: Epoch [10], Batch [88/938], Loss: 0.7672485709190369\n",
      "Train: Epoch [10], Batch [89/938], Loss: 0.47830915451049805\n",
      "Train: Epoch [10], Batch [90/938], Loss: 0.793641984462738\n",
      "Train: Epoch [10], Batch [91/938], Loss: 0.720643937587738\n",
      "Train: Epoch [10], Batch [92/938], Loss: 0.746396541595459\n",
      "Train: Epoch [10], Batch [93/938], Loss: 0.7618372440338135\n",
      "Train: Epoch [10], Batch [94/938], Loss: 0.6044386625289917\n",
      "Train: Epoch [10], Batch [95/938], Loss: 0.8453531861305237\n",
      "Train: Epoch [10], Batch [96/938], Loss: 0.5904657244682312\n",
      "Train: Epoch [10], Batch [97/938], Loss: 0.808284342288971\n",
      "Train: Epoch [10], Batch [98/938], Loss: 0.8488339185714722\n",
      "Train: Epoch [10], Batch [99/938], Loss: 1.0420047044754028\n",
      "Train: Epoch [10], Batch [100/938], Loss: 0.758354127407074\n",
      "Train: Epoch [10], Batch [101/938], Loss: 0.8877150416374207\n",
      "Train: Epoch [10], Batch [102/938], Loss: 0.9997900724411011\n",
      "Train: Epoch [10], Batch [103/938], Loss: 0.7708166837692261\n",
      "Train: Epoch [10], Batch [104/938], Loss: 0.7885248064994812\n",
      "Train: Epoch [10], Batch [105/938], Loss: 0.6075013279914856\n",
      "Train: Epoch [10], Batch [106/938], Loss: 0.6614305377006531\n",
      "Train: Epoch [10], Batch [107/938], Loss: 0.7313950061798096\n",
      "Train: Epoch [10], Batch [108/938], Loss: 0.7831709384918213\n",
      "Train: Epoch [10], Batch [109/938], Loss: 1.123518943786621\n",
      "Train: Epoch [10], Batch [110/938], Loss: 0.8439297676086426\n",
      "Train: Epoch [10], Batch [111/938], Loss: 0.8396527767181396\n",
      "Train: Epoch [10], Batch [112/938], Loss: 0.8249930143356323\n",
      "Train: Epoch [10], Batch [113/938], Loss: 0.8602153658866882\n",
      "Train: Epoch [10], Batch [114/938], Loss: 0.9198670983314514\n",
      "Train: Epoch [10], Batch [115/938], Loss: 0.7416292428970337\n",
      "Train: Epoch [10], Batch [116/938], Loss: 0.8782711029052734\n",
      "Train: Epoch [10], Batch [117/938], Loss: 0.7002811431884766\n",
      "Train: Epoch [10], Batch [118/938], Loss: 0.7755258679389954\n",
      "Train: Epoch [10], Batch [119/938], Loss: 0.801591694355011\n",
      "Train: Epoch [10], Batch [120/938], Loss: 0.7583801746368408\n",
      "Train: Epoch [10], Batch [121/938], Loss: 0.4799432158470154\n",
      "Train: Epoch [10], Batch [122/938], Loss: 0.7907534241676331\n",
      "Train: Epoch [10], Batch [123/938], Loss: 0.8656251430511475\n",
      "Train: Epoch [10], Batch [124/938], Loss: 0.7797212600708008\n",
      "Train: Epoch [10], Batch [125/938], Loss: 0.7808405756950378\n",
      "Train: Epoch [10], Batch [126/938], Loss: 0.8332154154777527\n",
      "Train: Epoch [10], Batch [127/938], Loss: 0.708564281463623\n",
      "Train: Epoch [10], Batch [128/938], Loss: 0.5841459035873413\n",
      "Train: Epoch [10], Batch [129/938], Loss: 0.8512879610061646\n",
      "Train: Epoch [10], Batch [130/938], Loss: 0.7445257902145386\n",
      "Train: Epoch [10], Batch [131/938], Loss: 0.7692428231239319\n",
      "Train: Epoch [10], Batch [132/938], Loss: 0.5891866683959961\n",
      "Train: Epoch [10], Batch [133/938], Loss: 0.7413883805274963\n",
      "Train: Epoch [10], Batch [134/938], Loss: 0.7320034503936768\n",
      "Train: Epoch [10], Batch [135/938], Loss: 0.9281079769134521\n",
      "Train: Epoch [10], Batch [136/938], Loss: 0.7340958118438721\n",
      "Train: Epoch [10], Batch [137/938], Loss: 0.927399218082428\n",
      "Train: Epoch [10], Batch [138/938], Loss: 0.7686696648597717\n",
      "Train: Epoch [10], Batch [139/938], Loss: 0.6357581615447998\n",
      "Train: Epoch [10], Batch [140/938], Loss: 0.8119746446609497\n",
      "Train: Epoch [10], Batch [141/938], Loss: 0.8080610036849976\n",
      "Train: Epoch [10], Batch [142/938], Loss: 0.7793851494789124\n",
      "Train: Epoch [10], Batch [143/938], Loss: 0.7240308523178101\n",
      "Train: Epoch [10], Batch [144/938], Loss: 0.898320734500885\n",
      "Train: Epoch [10], Batch [145/938], Loss: 1.1749416589736938\n",
      "Train: Epoch [10], Batch [146/938], Loss: 0.6761781573295593\n",
      "Train: Epoch [10], Batch [147/938], Loss: 0.6527719497680664\n",
      "Train: Epoch [10], Batch [148/938], Loss: 0.931597113609314\n",
      "Train: Epoch [10], Batch [149/938], Loss: 0.8349800705909729\n",
      "Train: Epoch [10], Batch [150/938], Loss: 1.2583353519439697\n",
      "Train: Epoch [10], Batch [151/938], Loss: 0.6426831483840942\n",
      "Train: Epoch [10], Batch [152/938], Loss: 0.726969838142395\n",
      "Train: Epoch [10], Batch [153/938], Loss: 0.6824053525924683\n",
      "Train: Epoch [10], Batch [154/938], Loss: 0.8026836514472961\n",
      "Train: Epoch [10], Batch [155/938], Loss: 0.6958292126655579\n",
      "Train: Epoch [10], Batch [156/938], Loss: 1.1350176334381104\n",
      "Train: Epoch [10], Batch [157/938], Loss: 0.6693763136863708\n",
      "Train: Epoch [10], Batch [158/938], Loss: 0.8423490524291992\n",
      "Train: Epoch [10], Batch [159/938], Loss: 0.9838098883628845\n",
      "Train: Epoch [10], Batch [160/938], Loss: 0.5663869976997375\n",
      "Train: Epoch [10], Batch [161/938], Loss: 0.8186086416244507\n",
      "Train: Epoch [10], Batch [162/938], Loss: 0.8246698975563049\n",
      "Train: Epoch [10], Batch [163/938], Loss: 0.5769747495651245\n",
      "Train: Epoch [10], Batch [164/938], Loss: 1.0466575622558594\n",
      "Train: Epoch [10], Batch [165/938], Loss: 0.7964527010917664\n",
      "Train: Epoch [10], Batch [166/938], Loss: 0.9989451766014099\n",
      "Train: Epoch [10], Batch [167/938], Loss: 0.762900710105896\n",
      "Train: Epoch [10], Batch [168/938], Loss: 0.7051663398742676\n",
      "Train: Epoch [10], Batch [169/938], Loss: 0.8182737231254578\n",
      "Train: Epoch [10], Batch [170/938], Loss: 0.994508683681488\n",
      "Train: Epoch [10], Batch [171/938], Loss: 0.8129105567932129\n",
      "Train: Epoch [10], Batch [172/938], Loss: 1.043837308883667\n",
      "Train: Epoch [10], Batch [173/938], Loss: 0.9280439019203186\n",
      "Train: Epoch [10], Batch [174/938], Loss: 0.5971274375915527\n",
      "Train: Epoch [10], Batch [175/938], Loss: 0.9098815321922302\n",
      "Train: Epoch [10], Batch [176/938], Loss: 0.5276389122009277\n",
      "Train: Epoch [10], Batch [177/938], Loss: 0.5336167812347412\n",
      "Train: Epoch [10], Batch [178/938], Loss: 0.684935450553894\n",
      "Train: Epoch [10], Batch [179/938], Loss: 0.7482405304908752\n",
      "Train: Epoch [10], Batch [180/938], Loss: 0.6784833669662476\n",
      "Train: Epoch [10], Batch [181/938], Loss: 0.7510466575622559\n",
      "Train: Epoch [10], Batch [182/938], Loss: 0.7651870846748352\n",
      "Train: Epoch [10], Batch [183/938], Loss: 0.7961943745613098\n",
      "Train: Epoch [10], Batch [184/938], Loss: 0.600497841835022\n",
      "Train: Epoch [10], Batch [185/938], Loss: 0.9576305150985718\n",
      "Train: Epoch [10], Batch [186/938], Loss: 0.785598635673523\n",
      "Train: Epoch [10], Batch [187/938], Loss: 0.8439428806304932\n",
      "Train: Epoch [10], Batch [188/938], Loss: 0.8502244353294373\n",
      "Train: Epoch [10], Batch [189/938], Loss: 0.7794879078865051\n",
      "Train: Epoch [10], Batch [190/938], Loss: 0.9203558564186096\n",
      "Train: Epoch [10], Batch [191/938], Loss: 0.5366635918617249\n",
      "Train: Epoch [10], Batch [192/938], Loss: 0.6324558854103088\n",
      "Train: Epoch [10], Batch [193/938], Loss: 0.8116207718849182\n",
      "Train: Epoch [10], Batch [194/938], Loss: 0.9123609066009521\n",
      "Train: Epoch [10], Batch [195/938], Loss: 0.7240413427352905\n",
      "Train: Epoch [10], Batch [196/938], Loss: 1.1337206363677979\n",
      "Train: Epoch [10], Batch [197/938], Loss: 0.7294106483459473\n",
      "Train: Epoch [10], Batch [198/938], Loss: 0.6910001635551453\n",
      "Train: Epoch [10], Batch [199/938], Loss: 0.8361346125602722\n",
      "Train: Epoch [10], Batch [200/938], Loss: 0.9169830679893494\n",
      "Train: Epoch [10], Batch [201/938], Loss: 0.8167718052864075\n",
      "Train: Epoch [10], Batch [202/938], Loss: 0.7576267123222351\n",
      "Train: Epoch [10], Batch [203/938], Loss: 0.7562432885169983\n",
      "Train: Epoch [10], Batch [204/938], Loss: 0.9837080836296082\n",
      "Train: Epoch [10], Batch [205/938], Loss: 0.8620022535324097\n",
      "Train: Epoch [10], Batch [206/938], Loss: 0.9627989530563354\n",
      "Train: Epoch [10], Batch [207/938], Loss: 0.8783314824104309\n",
      "Train: Epoch [10], Batch [208/938], Loss: 0.701473593711853\n",
      "Train: Epoch [10], Batch [209/938], Loss: 0.7111961841583252\n",
      "Train: Epoch [10], Batch [210/938], Loss: 0.8144960999488831\n",
      "Train: Epoch [10], Batch [211/938], Loss: 0.7751582860946655\n",
      "Train: Epoch [10], Batch [212/938], Loss: 0.9307070970535278\n",
      "Train: Epoch [10], Batch [213/938], Loss: 0.9863469004631042\n",
      "Train: Epoch [10], Batch [214/938], Loss: 0.7865068912506104\n",
      "Train: Epoch [10], Batch [215/938], Loss: 0.7974122166633606\n",
      "Train: Epoch [10], Batch [216/938], Loss: 0.6579569578170776\n",
      "Train: Epoch [10], Batch [217/938], Loss: 0.7007859945297241\n",
      "Train: Epoch [10], Batch [218/938], Loss: 0.9171290397644043\n",
      "Train: Epoch [10], Batch [219/938], Loss: 0.6841671466827393\n",
      "Train: Epoch [10], Batch [220/938], Loss: 0.909285843372345\n",
      "Train: Epoch [10], Batch [221/938], Loss: 0.7653264403343201\n",
      "Train: Epoch [10], Batch [222/938], Loss: 0.9419673085212708\n",
      "Train: Epoch [10], Batch [223/938], Loss: 0.7224353551864624\n",
      "Train: Epoch [10], Batch [224/938], Loss: 0.7545349597930908\n",
      "Train: Epoch [10], Batch [225/938], Loss: 0.7924787402153015\n",
      "Train: Epoch [10], Batch [226/938], Loss: 0.7835744619369507\n",
      "Train: Epoch [10], Batch [227/938], Loss: 0.5546591877937317\n",
      "Train: Epoch [10], Batch [228/938], Loss: 0.6350268721580505\n",
      "Train: Epoch [10], Batch [229/938], Loss: 0.9785822033882141\n",
      "Train: Epoch [10], Batch [230/938], Loss: 0.9544723033905029\n",
      "Train: Epoch [10], Batch [231/938], Loss: 0.8450859189033508\n",
      "Train: Epoch [10], Batch [232/938], Loss: 0.9310857057571411\n",
      "Train: Epoch [10], Batch [233/938], Loss: 0.7078155279159546\n",
      "Train: Epoch [10], Batch [234/938], Loss: 0.9877262115478516\n",
      "Train: Epoch [10], Batch [235/938], Loss: 0.5710750222206116\n",
      "Train: Epoch [10], Batch [236/938], Loss: 0.555670976638794\n",
      "Train: Epoch [10], Batch [237/938], Loss: 1.0253089666366577\n",
      "Train: Epoch [10], Batch [238/938], Loss: 0.7756756544113159\n",
      "Train: Epoch [10], Batch [239/938], Loss: 0.8342685103416443\n",
      "Train: Epoch [10], Batch [240/938], Loss: 0.9078272581100464\n",
      "Train: Epoch [10], Batch [241/938], Loss: 0.9591944813728333\n",
      "Train: Epoch [10], Batch [242/938], Loss: 1.1419669389724731\n",
      "Train: Epoch [10], Batch [243/938], Loss: 0.7673956155776978\n",
      "Train: Epoch [10], Batch [244/938], Loss: 0.6781526803970337\n",
      "Train: Epoch [10], Batch [245/938], Loss: 0.9111043214797974\n",
      "Train: Epoch [10], Batch [246/938], Loss: 0.8614559173583984\n",
      "Train: Epoch [10], Batch [247/938], Loss: 0.7698788642883301\n",
      "Train: Epoch [10], Batch [248/938], Loss: 0.9174066781997681\n",
      "Train: Epoch [10], Batch [249/938], Loss: 0.6389298439025879\n",
      "Train: Epoch [10], Batch [250/938], Loss: 0.7545409798622131\n",
      "Train: Epoch [10], Batch [251/938], Loss: 1.0321121215820312\n",
      "Train: Epoch [10], Batch [252/938], Loss: 0.8465724587440491\n",
      "Train: Epoch [10], Batch [253/938], Loss: 0.7840005159378052\n",
      "Train: Epoch [10], Batch [254/938], Loss: 0.904720664024353\n",
      "Train: Epoch [10], Batch [255/938], Loss: 0.8127060532569885\n",
      "Train: Epoch [10], Batch [256/938], Loss: 0.5661394596099854\n",
      "Train: Epoch [10], Batch [257/938], Loss: 0.6779257655143738\n",
      "Train: Epoch [10], Batch [258/938], Loss: 0.6217409372329712\n",
      "Train: Epoch [10], Batch [259/938], Loss: 0.8512940406799316\n",
      "Train: Epoch [10], Batch [260/938], Loss: 0.7287560105323792\n",
      "Train: Epoch [10], Batch [261/938], Loss: 0.9834357500076294\n",
      "Train: Epoch [10], Batch [262/938], Loss: 0.7031970024108887\n",
      "Train: Epoch [10], Batch [263/938], Loss: 0.9172968864440918\n",
      "Train: Epoch [10], Batch [264/938], Loss: 0.6581548452377319\n",
      "Train: Epoch [10], Batch [265/938], Loss: 0.9758551120758057\n",
      "Train: Epoch [10], Batch [266/938], Loss: 0.734004557132721\n",
      "Train: Epoch [10], Batch [267/938], Loss: 0.8014572262763977\n",
      "Train: Epoch [10], Batch [268/938], Loss: 1.0005269050598145\n",
      "Train: Epoch [10], Batch [269/938], Loss: 0.9406429529190063\n",
      "Train: Epoch [10], Batch [270/938], Loss: 0.6900123357772827\n",
      "Train: Epoch [10], Batch [271/938], Loss: 0.7232844829559326\n",
      "Train: Epoch [10], Batch [272/938], Loss: 0.9027506113052368\n",
      "Train: Epoch [10], Batch [273/938], Loss: 0.941726803779602\n",
      "Train: Epoch [10], Batch [274/938], Loss: 0.8218162059783936\n",
      "Train: Epoch [10], Batch [275/938], Loss: 0.6951811909675598\n",
      "Train: Epoch [10], Batch [276/938], Loss: 0.8696675300598145\n",
      "Train: Epoch [10], Batch [277/938], Loss: 0.8228358030319214\n",
      "Train: Epoch [10], Batch [278/938], Loss: 0.8983480930328369\n",
      "Train: Epoch [10], Batch [279/938], Loss: 0.7486813068389893\n",
      "Train: Epoch [10], Batch [280/938], Loss: 0.7418792247772217\n",
      "Train: Epoch [10], Batch [281/938], Loss: 0.6707841157913208\n",
      "Train: Epoch [10], Batch [282/938], Loss: 0.9467478394508362\n",
      "Train: Epoch [10], Batch [283/938], Loss: 0.5261327028274536\n",
      "Train: Epoch [10], Batch [284/938], Loss: 0.8859318494796753\n",
      "Train: Epoch [10], Batch [285/938], Loss: 0.8772811889648438\n",
      "Train: Epoch [10], Batch [286/938], Loss: 0.7012035250663757\n",
      "Train: Epoch [10], Batch [287/938], Loss: 0.6138684749603271\n",
      "Train: Epoch [10], Batch [288/938], Loss: 0.8749955296516418\n",
      "Train: Epoch [10], Batch [289/938], Loss: 0.7137683033943176\n",
      "Train: Epoch [10], Batch [290/938], Loss: 0.696161150932312\n",
      "Train: Epoch [10], Batch [291/938], Loss: 0.5543297529220581\n",
      "Train: Epoch [10], Batch [292/938], Loss: 0.7814932465553284\n",
      "Train: Epoch [10], Batch [293/938], Loss: 0.5198515057563782\n",
      "Train: Epoch [10], Batch [294/938], Loss: 0.7227328419685364\n",
      "Train: Epoch [10], Batch [295/938], Loss: 0.6257668733596802\n",
      "Train: Epoch [10], Batch [296/938], Loss: 0.8097124695777893\n",
      "Train: Epoch [10], Batch [297/938], Loss: 0.7313178181648254\n",
      "Train: Epoch [10], Batch [298/938], Loss: 0.8935626745223999\n",
      "Train: Epoch [10], Batch [299/938], Loss: 0.9225688576698303\n",
      "Train: Epoch [10], Batch [300/938], Loss: 0.7737452387809753\n",
      "Train: Epoch [10], Batch [301/938], Loss: 0.6590917706489563\n",
      "Train: Epoch [10], Batch [302/938], Loss: 1.0773712396621704\n",
      "Train: Epoch [10], Batch [303/938], Loss: 1.0125560760498047\n",
      "Train: Epoch [10], Batch [304/938], Loss: 0.8205009698867798\n",
      "Train: Epoch [10], Batch [305/938], Loss: 0.6978384256362915\n",
      "Train: Epoch [10], Batch [306/938], Loss: 0.7414880990982056\n",
      "Train: Epoch [10], Batch [307/938], Loss: 0.8658959865570068\n",
      "Train: Epoch [10], Batch [308/938], Loss: 0.8102639317512512\n",
      "Train: Epoch [10], Batch [309/938], Loss: 0.8807207345962524\n",
      "Train: Epoch [10], Batch [310/938], Loss: 0.7190168499946594\n",
      "Train: Epoch [10], Batch [311/938], Loss: 0.5893048644065857\n",
      "Train: Epoch [10], Batch [312/938], Loss: 0.9957285523414612\n",
      "Train: Epoch [10], Batch [313/938], Loss: 0.8692291975021362\n",
      "Train: Epoch [10], Batch [314/938], Loss: 0.8487247824668884\n",
      "Train: Epoch [10], Batch [315/938], Loss: 0.7450844049453735\n",
      "Train: Epoch [10], Batch [316/938], Loss: 0.5137631893157959\n",
      "Train: Epoch [10], Batch [317/938], Loss: 0.5999982953071594\n",
      "Train: Epoch [10], Batch [318/938], Loss: 0.6167563796043396\n",
      "Train: Epoch [10], Batch [319/938], Loss: 0.6898813247680664\n",
      "Train: Epoch [10], Batch [320/938], Loss: 0.7232179641723633\n",
      "Train: Epoch [10], Batch [321/938], Loss: 0.7017704248428345\n",
      "Train: Epoch [10], Batch [322/938], Loss: 0.965109646320343\n",
      "Train: Epoch [10], Batch [323/938], Loss: 0.9044209718704224\n",
      "Train: Epoch [10], Batch [324/938], Loss: 0.8311451077461243\n",
      "Train: Epoch [10], Batch [325/938], Loss: 0.8043711185455322\n",
      "Train: Epoch [10], Batch [326/938], Loss: 0.9862436652183533\n",
      "Train: Epoch [10], Batch [327/938], Loss: 0.7236805558204651\n",
      "Train: Epoch [10], Batch [328/938], Loss: 0.6295867562294006\n",
      "Train: Epoch [10], Batch [329/938], Loss: 0.9978624582290649\n",
      "Train: Epoch [10], Batch [330/938], Loss: 0.7190844416618347\n",
      "Train: Epoch [10], Batch [331/938], Loss: 1.0180039405822754\n",
      "Train: Epoch [10], Batch [332/938], Loss: 0.9838851690292358\n",
      "Train: Epoch [10], Batch [333/938], Loss: 0.7092291712760925\n",
      "Train: Epoch [10], Batch [334/938], Loss: 0.7502433657646179\n",
      "Train: Epoch [10], Batch [335/938], Loss: 0.6547650694847107\n",
      "Train: Epoch [10], Batch [336/938], Loss: 0.8192330002784729\n",
      "Train: Epoch [10], Batch [337/938], Loss: 0.6800346970558167\n",
      "Train: Epoch [10], Batch [338/938], Loss: 0.7678682208061218\n",
      "Train: Epoch [10], Batch [339/938], Loss: 0.6776009202003479\n",
      "Train: Epoch [10], Batch [340/938], Loss: 0.7814585566520691\n",
      "Train: Epoch [10], Batch [341/938], Loss: 0.7365604639053345\n",
      "Train: Epoch [10], Batch [342/938], Loss: 0.9589054584503174\n",
      "Train: Epoch [10], Batch [343/938], Loss: 0.85211580991745\n",
      "Train: Epoch [10], Batch [344/938], Loss: 0.923318088054657\n",
      "Train: Epoch [10], Batch [345/938], Loss: 0.5581563711166382\n",
      "Train: Epoch [10], Batch [346/938], Loss: 1.0203667879104614\n",
      "Train: Epoch [10], Batch [347/938], Loss: 0.7110670804977417\n",
      "Train: Epoch [10], Batch [348/938], Loss: 0.6091464757919312\n",
      "Train: Epoch [10], Batch [349/938], Loss: 1.0032480955123901\n",
      "Train: Epoch [10], Batch [350/938], Loss: 0.9644743800163269\n",
      "Train: Epoch [10], Batch [351/938], Loss: 0.8442860841751099\n",
      "Train: Epoch [10], Batch [352/938], Loss: 0.6684458255767822\n",
      "Train: Epoch [10], Batch [353/938], Loss: 0.7507731914520264\n",
      "Train: Epoch [10], Batch [354/938], Loss: 0.8462547659873962\n",
      "Train: Epoch [10], Batch [355/938], Loss: 0.6432958245277405\n",
      "Train: Epoch [10], Batch [356/938], Loss: 0.7241525650024414\n",
      "Train: Epoch [10], Batch [357/938], Loss: 0.8033843040466309\n",
      "Train: Epoch [10], Batch [358/938], Loss: 0.9188582897186279\n",
      "Train: Epoch [10], Batch [359/938], Loss: 0.7898057699203491\n",
      "Train: Epoch [10], Batch [360/938], Loss: 0.7296270132064819\n",
      "Train: Epoch [10], Batch [361/938], Loss: 0.8604037165641785\n",
      "Train: Epoch [10], Batch [362/938], Loss: 0.6663966178894043\n",
      "Train: Epoch [10], Batch [363/938], Loss: 0.9666779637336731\n",
      "Train: Epoch [10], Batch [364/938], Loss: 0.6618006825447083\n",
      "Train: Epoch [10], Batch [365/938], Loss: 0.8262707591056824\n",
      "Train: Epoch [10], Batch [366/938], Loss: 0.9064844846725464\n",
      "Train: Epoch [10], Batch [367/938], Loss: 0.6745133399963379\n",
      "Train: Epoch [10], Batch [368/938], Loss: 0.8896040916442871\n",
      "Train: Epoch [10], Batch [369/938], Loss: 0.9864364266395569\n",
      "Train: Epoch [10], Batch [370/938], Loss: 0.9119764566421509\n",
      "Train: Epoch [10], Batch [371/938], Loss: 0.7245807647705078\n",
      "Train: Epoch [10], Batch [372/938], Loss: 0.9371458888053894\n",
      "Train: Epoch [10], Batch [373/938], Loss: 0.8150131702423096\n",
      "Train: Epoch [10], Batch [374/938], Loss: 0.931934654712677\n",
      "Train: Epoch [10], Batch [375/938], Loss: 0.5689428448677063\n",
      "Train: Epoch [10], Batch [376/938], Loss: 0.9965522885322571\n",
      "Train: Epoch [10], Batch [377/938], Loss: 0.8982895612716675\n",
      "Train: Epoch [10], Batch [378/938], Loss: 0.7692638039588928\n",
      "Train: Epoch [10], Batch [379/938], Loss: 0.8046616315841675\n",
      "Train: Epoch [10], Batch [380/938], Loss: 0.7585058808326721\n",
      "Train: Epoch [10], Batch [381/938], Loss: 0.8533406257629395\n",
      "Train: Epoch [10], Batch [382/938], Loss: 0.6762984395027161\n",
      "Train: Epoch [10], Batch [383/938], Loss: 0.6613673567771912\n",
      "Train: Epoch [10], Batch [384/938], Loss: 0.625620424747467\n",
      "Train: Epoch [10], Batch [385/938], Loss: 0.5978181958198547\n",
      "Train: Epoch [10], Batch [386/938], Loss: 0.8430165648460388\n",
      "Train: Epoch [10], Batch [387/938], Loss: 0.7287012338638306\n",
      "Train: Epoch [10], Batch [388/938], Loss: 0.700125515460968\n",
      "Train: Epoch [10], Batch [389/938], Loss: 1.0572115182876587\n",
      "Train: Epoch [10], Batch [390/938], Loss: 0.687872588634491\n",
      "Train: Epoch [10], Batch [391/938], Loss: 0.8483372926712036\n",
      "Train: Epoch [10], Batch [392/938], Loss: 0.8401320576667786\n",
      "Train: Epoch [10], Batch [393/938], Loss: 0.902675986289978\n",
      "Train: Epoch [10], Batch [394/938], Loss: 0.8841310739517212\n",
      "Train: Epoch [10], Batch [395/938], Loss: 0.8207426071166992\n",
      "Train: Epoch [10], Batch [396/938], Loss: 0.6680892705917358\n",
      "Train: Epoch [10], Batch [397/938], Loss: 0.8195640444755554\n",
      "Train: Epoch [10], Batch [398/938], Loss: 0.7182379961013794\n",
      "Train: Epoch [10], Batch [399/938], Loss: 0.8838078379631042\n",
      "Train: Epoch [10], Batch [400/938], Loss: 0.9216433167457581\n",
      "Train: Epoch [10], Batch [401/938], Loss: 0.8891865015029907\n",
      "Train: Epoch [10], Batch [402/938], Loss: 0.6067186594009399\n",
      "Train: Epoch [10], Batch [403/938], Loss: 0.6461710929870605\n",
      "Train: Epoch [10], Batch [404/938], Loss: 0.9689114093780518\n",
      "Train: Epoch [10], Batch [405/938], Loss: 0.8898465037345886\n",
      "Train: Epoch [10], Batch [406/938], Loss: 0.7827973365783691\n",
      "Train: Epoch [10], Batch [407/938], Loss: 0.7042190432548523\n",
      "Train: Epoch [10], Batch [408/938], Loss: 0.8817352056503296\n",
      "Train: Epoch [10], Batch [409/938], Loss: 0.5718238949775696\n",
      "Train: Epoch [10], Batch [410/938], Loss: 0.7515507936477661\n",
      "Train: Epoch [10], Batch [411/938], Loss: 0.8545181155204773\n",
      "Train: Epoch [10], Batch [412/938], Loss: 0.7263925075531006\n",
      "Train: Epoch [10], Batch [413/938], Loss: 0.7160943746566772\n",
      "Train: Epoch [10], Batch [414/938], Loss: 0.630215585231781\n",
      "Train: Epoch [10], Batch [415/938], Loss: 0.6206794381141663\n",
      "Train: Epoch [10], Batch [416/938], Loss: 0.8797857165336609\n",
      "Train: Epoch [10], Batch [417/938], Loss: 0.8206236362457275\n",
      "Train: Epoch [10], Batch [418/938], Loss: 0.9235044121742249\n",
      "Train: Epoch [10], Batch [419/938], Loss: 0.6485872268676758\n",
      "Train: Epoch [10], Batch [420/938], Loss: 0.9681797623634338\n",
      "Train: Epoch [10], Batch [421/938], Loss: 0.6797766089439392\n",
      "Train: Epoch [10], Batch [422/938], Loss: 0.5363180637359619\n",
      "Train: Epoch [10], Batch [423/938], Loss: 0.685408353805542\n",
      "Train: Epoch [10], Batch [424/938], Loss: 0.5769965648651123\n",
      "Train: Epoch [10], Batch [425/938], Loss: 0.6064270734786987\n",
      "Train: Epoch [10], Batch [426/938], Loss: 0.7795773148536682\n",
      "Train: Epoch [10], Batch [427/938], Loss: 0.5994070768356323\n",
      "Train: Epoch [10], Batch [428/938], Loss: 0.7862737774848938\n",
      "Train: Epoch [10], Batch [429/938], Loss: 0.771887481212616\n",
      "Train: Epoch [10], Batch [430/938], Loss: 0.7435941100120544\n",
      "Train: Epoch [10], Batch [431/938], Loss: 0.7777259945869446\n",
      "Train: Epoch [10], Batch [432/938], Loss: 1.1944596767425537\n",
      "Train: Epoch [10], Batch [433/938], Loss: 0.9521539211273193\n",
      "Train: Epoch [10], Batch [434/938], Loss: 0.8600298762321472\n",
      "Train: Epoch [10], Batch [435/938], Loss: 0.9241476655006409\n",
      "Train: Epoch [10], Batch [436/938], Loss: 0.5338026285171509\n",
      "Train: Epoch [10], Batch [437/938], Loss: 0.7968660593032837\n",
      "Train: Epoch [10], Batch [438/938], Loss: 0.8889150619506836\n",
      "Train: Epoch [10], Batch [439/938], Loss: 0.8077216148376465\n",
      "Train: Epoch [10], Batch [440/938], Loss: 0.6361127495765686\n",
      "Train: Epoch [10], Batch [441/938], Loss: 0.7706193327903748\n",
      "Train: Epoch [10], Batch [442/938], Loss: 0.6439567804336548\n",
      "Train: Epoch [10], Batch [443/938], Loss: 0.47568681836128235\n",
      "Train: Epoch [10], Batch [444/938], Loss: 0.7501170039176941\n",
      "Train: Epoch [10], Batch [445/938], Loss: 1.0223400592803955\n",
      "Train: Epoch [10], Batch [446/938], Loss: 0.9972113370895386\n",
      "Train: Epoch [10], Batch [447/938], Loss: 1.0712432861328125\n",
      "Train: Epoch [10], Batch [448/938], Loss: 0.6409359574317932\n",
      "Train: Epoch [10], Batch [449/938], Loss: 0.9231413006782532\n",
      "Train: Epoch [10], Batch [450/938], Loss: 0.824815034866333\n",
      "Train: Epoch [10], Batch [451/938], Loss: 0.7419496178627014\n",
      "Train: Epoch [10], Batch [452/938], Loss: 0.8555576205253601\n",
      "Train: Epoch [10], Batch [453/938], Loss: 0.882793664932251\n",
      "Train: Epoch [10], Batch [454/938], Loss: 0.8958091735839844\n",
      "Train: Epoch [10], Batch [455/938], Loss: 0.630329966545105\n",
      "Train: Epoch [10], Batch [456/938], Loss: 0.7387241125106812\n",
      "Train: Epoch [10], Batch [457/938], Loss: 0.5526560544967651\n",
      "Train: Epoch [10], Batch [458/938], Loss: 0.7312549352645874\n",
      "Train: Epoch [10], Batch [459/938], Loss: 0.8024716377258301\n",
      "Train: Epoch [10], Batch [460/938], Loss: 0.9219064116477966\n",
      "Train: Epoch [10], Batch [461/938], Loss: 0.6271448731422424\n",
      "Train: Epoch [10], Batch [462/938], Loss: 0.902489423751831\n",
      "Train: Epoch [10], Batch [463/938], Loss: 0.722671389579773\n",
      "Train: Epoch [10], Batch [464/938], Loss: 0.7323471903800964\n",
      "Train: Epoch [10], Batch [465/938], Loss: 0.541745662689209\n",
      "Train: Epoch [10], Batch [466/938], Loss: 0.7694319486618042\n",
      "Train: Epoch [10], Batch [467/938], Loss: 0.8933738470077515\n",
      "Train: Epoch [10], Batch [468/938], Loss: 0.5092425346374512\n",
      "Train: Epoch [10], Batch [469/938], Loss: 0.683506965637207\n",
      "Train: Epoch [10], Batch [470/938], Loss: 0.7441897392272949\n",
      "Train: Epoch [10], Batch [471/938], Loss: 0.5353676676750183\n",
      "Train: Epoch [10], Batch [472/938], Loss: 0.620242714881897\n",
      "Train: Epoch [10], Batch [473/938], Loss: 0.7741765975952148\n",
      "Train: Epoch [10], Batch [474/938], Loss: 0.7222490906715393\n",
      "Train: Epoch [10], Batch [475/938], Loss: 0.9468989372253418\n",
      "Train: Epoch [10], Batch [476/938], Loss: 0.7805384993553162\n",
      "Train: Epoch [10], Batch [477/938], Loss: 0.8171865940093994\n",
      "Train: Epoch [10], Batch [478/938], Loss: 0.7347924113273621\n",
      "Train: Epoch [10], Batch [479/938], Loss: 0.8488134145736694\n",
      "Train: Epoch [10], Batch [480/938], Loss: 0.7514667510986328\n",
      "Train: Epoch [10], Batch [481/938], Loss: 0.7372831702232361\n",
      "Train: Epoch [10], Batch [482/938], Loss: 0.8033095598220825\n",
      "Train: Epoch [10], Batch [483/938], Loss: 0.4904605448246002\n",
      "Train: Epoch [10], Batch [484/938], Loss: 0.9509227275848389\n",
      "Train: Epoch [10], Batch [485/938], Loss: 0.7737717628479004\n",
      "Train: Epoch [10], Batch [486/938], Loss: 0.6856407523155212\n",
      "Train: Epoch [10], Batch [487/938], Loss: 0.7299122214317322\n",
      "Train: Epoch [10], Batch [488/938], Loss: 0.9718336462974548\n",
      "Train: Epoch [10], Batch [489/938], Loss: 0.7743982076644897\n",
      "Train: Epoch [10], Batch [490/938], Loss: 0.8051724433898926\n",
      "Train: Epoch [10], Batch [491/938], Loss: 0.6445133090019226\n",
      "Train: Epoch [10], Batch [492/938], Loss: 0.7601005434989929\n",
      "Train: Epoch [10], Batch [493/938], Loss: 0.8273407220840454\n",
      "Train: Epoch [10], Batch [494/938], Loss: 0.8240903615951538\n",
      "Train: Epoch [10], Batch [495/938], Loss: 0.8270316123962402\n",
      "Train: Epoch [10], Batch [496/938], Loss: 0.6179267764091492\n",
      "Train: Epoch [10], Batch [497/938], Loss: 0.8316697478294373\n",
      "Train: Epoch [10], Batch [498/938], Loss: 0.6093428730964661\n",
      "Train: Epoch [10], Batch [499/938], Loss: 0.7464229464530945\n",
      "Train: Epoch [10], Batch [500/938], Loss: 0.9580735564231873\n",
      "Train: Epoch [10], Batch [501/938], Loss: 0.7959955930709839\n",
      "Train: Epoch [10], Batch [502/938], Loss: 0.5490261912345886\n",
      "Train: Epoch [10], Batch [503/938], Loss: 0.6819633841514587\n",
      "Train: Epoch [10], Batch [504/938], Loss: 0.620695948600769\n",
      "Train: Epoch [10], Batch [505/938], Loss: 0.8747316598892212\n",
      "Train: Epoch [10], Batch [506/938], Loss: 0.8712304830551147\n",
      "Train: Epoch [10], Batch [507/938], Loss: 0.8377161026000977\n",
      "Train: Epoch [10], Batch [508/938], Loss: 0.9923064708709717\n",
      "Train: Epoch [10], Batch [509/938], Loss: 0.7786299586296082\n",
      "Train: Epoch [10], Batch [510/938], Loss: 0.6524826288223267\n",
      "Train: Epoch [10], Batch [511/938], Loss: 0.8946340084075928\n",
      "Train: Epoch [10], Batch [512/938], Loss: 0.8853204250335693\n",
      "Train: Epoch [10], Batch [513/938], Loss: 0.686866044998169\n",
      "Train: Epoch [10], Batch [514/938], Loss: 0.900355875492096\n",
      "Train: Epoch [10], Batch [515/938], Loss: 0.737921416759491\n",
      "Train: Epoch [10], Batch [516/938], Loss: 0.6577221751213074\n",
      "Train: Epoch [10], Batch [517/938], Loss: 0.8800380825996399\n",
      "Train: Epoch [10], Batch [518/938], Loss: 1.005861520767212\n",
      "Train: Epoch [10], Batch [519/938], Loss: 0.7159410715103149\n",
      "Train: Epoch [10], Batch [520/938], Loss: 0.9044513702392578\n",
      "Train: Epoch [10], Batch [521/938], Loss: 0.8617361783981323\n",
      "Train: Epoch [10], Batch [522/938], Loss: 0.8096392154693604\n",
      "Train: Epoch [10], Batch [523/938], Loss: 0.8601707816123962\n",
      "Train: Epoch [10], Batch [524/938], Loss: 0.9305115938186646\n",
      "Train: Epoch [10], Batch [525/938], Loss: 0.7588735222816467\n",
      "Train: Epoch [10], Batch [526/938], Loss: 0.7946540117263794\n",
      "Train: Epoch [10], Batch [527/938], Loss: 0.8742793798446655\n",
      "Train: Epoch [10], Batch [528/938], Loss: 0.5753979086875916\n",
      "Train: Epoch [10], Batch [529/938], Loss: 0.5161470174789429\n",
      "Train: Epoch [10], Batch [530/938], Loss: 0.5084922909736633\n",
      "Train: Epoch [10], Batch [531/938], Loss: 0.7113670706748962\n",
      "Train: Epoch [10], Batch [532/938], Loss: 0.9372056722640991\n",
      "Train: Epoch [10], Batch [533/938], Loss: 0.5575326085090637\n",
      "Train: Epoch [10], Batch [534/938], Loss: 0.8089235424995422\n",
      "Train: Epoch [10], Batch [535/938], Loss: 1.0748926401138306\n",
      "Train: Epoch [10], Batch [536/938], Loss: 0.8816327452659607\n",
      "Train: Epoch [10], Batch [537/938], Loss: 0.6676985025405884\n",
      "Train: Epoch [10], Batch [538/938], Loss: 0.6745761036872864\n",
      "Train: Epoch [10], Batch [539/938], Loss: 0.982495903968811\n",
      "Train: Epoch [10], Batch [540/938], Loss: 0.6695008873939514\n",
      "Train: Epoch [10], Batch [541/938], Loss: 0.6096047163009644\n",
      "Train: Epoch [10], Batch [542/938], Loss: 0.7288990020751953\n",
      "Train: Epoch [10], Batch [543/938], Loss: 1.1348724365234375\n",
      "Train: Epoch [10], Batch [544/938], Loss: 0.5552594661712646\n",
      "Train: Epoch [10], Batch [545/938], Loss: 0.8407612442970276\n",
      "Train: Epoch [10], Batch [546/938], Loss: 0.7834227085113525\n",
      "Train: Epoch [10], Batch [547/938], Loss: 0.7627534866333008\n",
      "Train: Epoch [10], Batch [548/938], Loss: 0.7666665315628052\n",
      "Train: Epoch [10], Batch [549/938], Loss: 0.8872060179710388\n",
      "Train: Epoch [10], Batch [550/938], Loss: 0.8807121515274048\n",
      "Train: Epoch [10], Batch [551/938], Loss: 0.8739984631538391\n",
      "Train: Epoch [10], Batch [552/938], Loss: 0.9187688231468201\n",
      "Train: Epoch [10], Batch [553/938], Loss: 1.0133248567581177\n",
      "Train: Epoch [10], Batch [554/938], Loss: 0.7222079038619995\n",
      "Train: Epoch [10], Batch [555/938], Loss: 0.7268547415733337\n",
      "Train: Epoch [10], Batch [556/938], Loss: 0.732106626033783\n",
      "Train: Epoch [10], Batch [557/938], Loss: 0.767036497592926\n",
      "Train: Epoch [10], Batch [558/938], Loss: 0.6429625749588013\n",
      "Train: Epoch [10], Batch [559/938], Loss: 0.7831723093986511\n",
      "Train: Epoch [10], Batch [560/938], Loss: 0.9510211944580078\n",
      "Train: Epoch [10], Batch [561/938], Loss: 0.8137275576591492\n",
      "Train: Epoch [10], Batch [562/938], Loss: 0.8277931809425354\n",
      "Train: Epoch [10], Batch [563/938], Loss: 0.6432590484619141\n",
      "Train: Epoch [10], Batch [564/938], Loss: 0.7915905714035034\n",
      "Train: Epoch [10], Batch [565/938], Loss: 0.717162013053894\n",
      "Train: Epoch [10], Batch [566/938], Loss: 0.6200170516967773\n",
      "Train: Epoch [10], Batch [567/938], Loss: 0.7910311818122864\n",
      "Train: Epoch [10], Batch [568/938], Loss: 0.9525548815727234\n",
      "Train: Epoch [10], Batch [569/938], Loss: 1.0251373052597046\n",
      "Train: Epoch [10], Batch [570/938], Loss: 0.9116707444190979\n",
      "Train: Epoch [10], Batch [571/938], Loss: 0.8947340250015259\n",
      "Train: Epoch [10], Batch [572/938], Loss: 0.7788088321685791\n",
      "Train: Epoch [10], Batch [573/938], Loss: 0.8898600339889526\n",
      "Train: Epoch [10], Batch [574/938], Loss: 0.7964203357696533\n",
      "Train: Epoch [10], Batch [575/938], Loss: 0.7052270174026489\n",
      "Train: Epoch [10], Batch [576/938], Loss: 0.8300297260284424\n",
      "Train: Epoch [10], Batch [577/938], Loss: 0.6185176968574524\n",
      "Train: Epoch [10], Batch [578/938], Loss: 0.7410612106323242\n",
      "Train: Epoch [10], Batch [579/938], Loss: 0.8804982900619507\n",
      "Train: Epoch [10], Batch [580/938], Loss: 0.9738810658454895\n",
      "Train: Epoch [10], Batch [581/938], Loss: 0.643501877784729\n",
      "Train: Epoch [10], Batch [582/938], Loss: 0.7864792943000793\n",
      "Train: Epoch [10], Batch [583/938], Loss: 0.7752357721328735\n",
      "Train: Epoch [10], Batch [584/938], Loss: 0.9770398736000061\n",
      "Train: Epoch [10], Batch [585/938], Loss: 0.7196635603904724\n",
      "Train: Epoch [10], Batch [586/938], Loss: 0.9048209190368652\n",
      "Train: Epoch [10], Batch [587/938], Loss: 0.8604490756988525\n",
      "Train: Epoch [10], Batch [588/938], Loss: 0.6772772073745728\n",
      "Train: Epoch [10], Batch [589/938], Loss: 0.8592969179153442\n",
      "Train: Epoch [10], Batch [590/938], Loss: 0.8434298038482666\n",
      "Train: Epoch [10], Batch [591/938], Loss: 0.9182142019271851\n",
      "Train: Epoch [10], Batch [592/938], Loss: 0.7373066544532776\n",
      "Train: Epoch [10], Batch [593/938], Loss: 0.7923296093940735\n",
      "Train: Epoch [10], Batch [594/938], Loss: 0.7064741849899292\n",
      "Train: Epoch [10], Batch [595/938], Loss: 0.7315362691879272\n",
      "Train: Epoch [10], Batch [596/938], Loss: 0.7929701209068298\n",
      "Train: Epoch [10], Batch [597/938], Loss: 0.7607448697090149\n",
      "Train: Epoch [10], Batch [598/938], Loss: 0.7728812098503113\n",
      "Train: Epoch [10], Batch [599/938], Loss: 0.797004759311676\n",
      "Train: Epoch [10], Batch [600/938], Loss: 0.8160871863365173\n",
      "Train: Epoch [10], Batch [601/938], Loss: 0.9164437055587769\n",
      "Train: Epoch [10], Batch [602/938], Loss: 0.6530067920684814\n",
      "Train: Epoch [10], Batch [603/938], Loss: 0.587417483329773\n",
      "Train: Epoch [10], Batch [604/938], Loss: 0.6597142219543457\n",
      "Train: Epoch [10], Batch [605/938], Loss: 1.0000437498092651\n",
      "Train: Epoch [10], Batch [606/938], Loss: 0.7501568794250488\n",
      "Train: Epoch [10], Batch [607/938], Loss: 0.8309049606323242\n",
      "Train: Epoch [10], Batch [608/938], Loss: 1.2034460306167603\n",
      "Train: Epoch [10], Batch [609/938], Loss: 0.8728405237197876\n",
      "Train: Epoch [10], Batch [610/938], Loss: 0.5557346343994141\n",
      "Train: Epoch [10], Batch [611/938], Loss: 0.7168329954147339\n",
      "Train: Epoch [10], Batch [612/938], Loss: 0.7259004712104797\n",
      "Train: Epoch [10], Batch [613/938], Loss: 0.7423204779624939\n",
      "Train: Epoch [10], Batch [614/938], Loss: 0.8491237163543701\n",
      "Train: Epoch [10], Batch [615/938], Loss: 0.5259061455726624\n",
      "Train: Epoch [10], Batch [616/938], Loss: 1.0936663150787354\n",
      "Train: Epoch [10], Batch [617/938], Loss: 0.6870788931846619\n",
      "Train: Epoch [10], Batch [618/938], Loss: 0.6194579005241394\n",
      "Train: Epoch [10], Batch [619/938], Loss: 0.6138343811035156\n",
      "Train: Epoch [10], Batch [620/938], Loss: 0.8919371366500854\n",
      "Train: Epoch [10], Batch [621/938], Loss: 0.6086447238922119\n",
      "Train: Epoch [10], Batch [622/938], Loss: 0.7821311354637146\n",
      "Train: Epoch [10], Batch [623/938], Loss: 0.7371386289596558\n",
      "Train: Epoch [10], Batch [624/938], Loss: 0.6535555720329285\n",
      "Train: Epoch [10], Batch [625/938], Loss: 0.7662426829338074\n",
      "Train: Epoch [10], Batch [626/938], Loss: 0.8704297542572021\n",
      "Train: Epoch [10], Batch [627/938], Loss: 0.7962209582328796\n",
      "Train: Epoch [10], Batch [628/938], Loss: 0.7447633147239685\n",
      "Train: Epoch [10], Batch [629/938], Loss: 0.5876089334487915\n",
      "Train: Epoch [10], Batch [630/938], Loss: 0.8184574246406555\n",
      "Train: Epoch [10], Batch [631/938], Loss: 0.7119449377059937\n",
      "Train: Epoch [10], Batch [632/938], Loss: 0.872134268283844\n",
      "Train: Epoch [10], Batch [633/938], Loss: 0.6753666400909424\n",
      "Train: Epoch [10], Batch [634/938], Loss: 0.7916874885559082\n",
      "Train: Epoch [10], Batch [635/938], Loss: 0.8186118602752686\n",
      "Train: Epoch [10], Batch [636/938], Loss: 0.659862756729126\n",
      "Train: Epoch [10], Batch [637/938], Loss: 0.9791408777236938\n",
      "Train: Epoch [10], Batch [638/938], Loss: 0.5877569913864136\n",
      "Train: Epoch [10], Batch [639/938], Loss: 0.8961180448532104\n",
      "Train: Epoch [10], Batch [640/938], Loss: 0.6782543063163757\n",
      "Train: Epoch [10], Batch [641/938], Loss: 0.7467209696769714\n",
      "Train: Epoch [10], Batch [642/938], Loss: 0.6508853435516357\n",
      "Train: Epoch [10], Batch [643/938], Loss: 0.9332278370857239\n",
      "Train: Epoch [10], Batch [644/938], Loss: 0.7588009238243103\n",
      "Train: Epoch [10], Batch [645/938], Loss: 0.6996607780456543\n",
      "Train: Epoch [10], Batch [646/938], Loss: 0.8179950714111328\n",
      "Train: Epoch [10], Batch [647/938], Loss: 0.8563638925552368\n",
      "Train: Epoch [10], Batch [648/938], Loss: 0.5757940411567688\n",
      "Train: Epoch [10], Batch [649/938], Loss: 0.8854654431343079\n",
      "Train: Epoch [10], Batch [650/938], Loss: 0.6725799441337585\n",
      "Train: Epoch [10], Batch [651/938], Loss: 0.6867650151252747\n",
      "Train: Epoch [10], Batch [652/938], Loss: 0.8201208114624023\n",
      "Train: Epoch [10], Batch [653/938], Loss: 0.8039379715919495\n",
      "Train: Epoch [10], Batch [654/938], Loss: 0.6262455582618713\n",
      "Train: Epoch [10], Batch [655/938], Loss: 0.7984782457351685\n",
      "Train: Epoch [10], Batch [656/938], Loss: 0.9180270433425903\n",
      "Train: Epoch [10], Batch [657/938], Loss: 0.6640380024909973\n",
      "Train: Epoch [10], Batch [658/938], Loss: 0.8074408769607544\n",
      "Train: Epoch [10], Batch [659/938], Loss: 0.9322944283485413\n",
      "Train: Epoch [10], Batch [660/938], Loss: 0.881392240524292\n",
      "Train: Epoch [10], Batch [661/938], Loss: 0.7829218506813049\n",
      "Train: Epoch [10], Batch [662/938], Loss: 0.6899688839912415\n",
      "Train: Epoch [10], Batch [663/938], Loss: 0.6919321417808533\n",
      "Train: Epoch [10], Batch [664/938], Loss: 0.7098866105079651\n",
      "Train: Epoch [10], Batch [665/938], Loss: 0.7423385381698608\n",
      "Train: Epoch [10], Batch [666/938], Loss: 0.9939731359481812\n",
      "Train: Epoch [10], Batch [667/938], Loss: 0.7227171659469604\n",
      "Train: Epoch [10], Batch [668/938], Loss: 0.6924412250518799\n",
      "Train: Epoch [10], Batch [669/938], Loss: 0.5269004106521606\n",
      "Train: Epoch [10], Batch [670/938], Loss: 0.7772508859634399\n",
      "Train: Epoch [10], Batch [671/938], Loss: 0.47850680351257324\n",
      "Train: Epoch [10], Batch [672/938], Loss: 0.5817996263504028\n",
      "Train: Epoch [10], Batch [673/938], Loss: 0.6551042795181274\n",
      "Train: Epoch [10], Batch [674/938], Loss: 0.8451490998268127\n",
      "Train: Epoch [10], Batch [675/938], Loss: 0.8766801357269287\n",
      "Train: Epoch [10], Batch [676/938], Loss: 0.7849544882774353\n",
      "Train: Epoch [10], Batch [677/938], Loss: 0.9570684432983398\n",
      "Train: Epoch [10], Batch [678/938], Loss: 0.6721739768981934\n",
      "Train: Epoch [10], Batch [679/938], Loss: 0.6090472936630249\n",
      "Train: Epoch [10], Batch [680/938], Loss: 0.8770424127578735\n",
      "Train: Epoch [10], Batch [681/938], Loss: 0.6421323418617249\n",
      "Train: Epoch [10], Batch [682/938], Loss: 0.6283479928970337\n",
      "Train: Epoch [10], Batch [683/938], Loss: 0.7117524147033691\n",
      "Train: Epoch [10], Batch [684/938], Loss: 0.9505003690719604\n",
      "Train: Epoch [10], Batch [685/938], Loss: 0.8242233991622925\n",
      "Train: Epoch [10], Batch [686/938], Loss: 0.7930061221122742\n",
      "Train: Epoch [10], Batch [687/938], Loss: 0.8932831287384033\n",
      "Train: Epoch [10], Batch [688/938], Loss: 0.7101433277130127\n",
      "Train: Epoch [10], Batch [689/938], Loss: 0.8850237131118774\n",
      "Train: Epoch [10], Batch [690/938], Loss: 0.8360474109649658\n",
      "Train: Epoch [10], Batch [691/938], Loss: 0.6598159074783325\n",
      "Train: Epoch [10], Batch [692/938], Loss: 0.7679122090339661\n",
      "Train: Epoch [10], Batch [693/938], Loss: 0.7007665634155273\n",
      "Train: Epoch [10], Batch [694/938], Loss: 0.8543673753738403\n",
      "Train: Epoch [10], Batch [695/938], Loss: 0.7618710398674011\n",
      "Train: Epoch [10], Batch [696/938], Loss: 0.7222239971160889\n",
      "Train: Epoch [10], Batch [697/938], Loss: 0.7792408466339111\n",
      "Train: Epoch [10], Batch [698/938], Loss: 0.6545679569244385\n",
      "Train: Epoch [10], Batch [699/938], Loss: 0.7771981954574585\n",
      "Train: Epoch [10], Batch [700/938], Loss: 0.7764376997947693\n",
      "Train: Epoch [10], Batch [701/938], Loss: 0.7172051668167114\n",
      "Train: Epoch [10], Batch [702/938], Loss: 0.6339673399925232\n",
      "Train: Epoch [10], Batch [703/938], Loss: 1.0227123498916626\n",
      "Train: Epoch [10], Batch [704/938], Loss: 0.7170832753181458\n",
      "Train: Epoch [10], Batch [705/938], Loss: 0.8547192811965942\n",
      "Train: Epoch [10], Batch [706/938], Loss: 0.903758704662323\n",
      "Train: Epoch [10], Batch [707/938], Loss: 0.9679998755455017\n",
      "Train: Epoch [10], Batch [708/938], Loss: 0.769559383392334\n",
      "Train: Epoch [10], Batch [709/938], Loss: 0.796942949295044\n",
      "Train: Epoch [10], Batch [710/938], Loss: 0.8551008701324463\n",
      "Train: Epoch [10], Batch [711/938], Loss: 0.6715540885925293\n",
      "Train: Epoch [10], Batch [712/938], Loss: 0.72825026512146\n",
      "Train: Epoch [10], Batch [713/938], Loss: 0.7396848201751709\n",
      "Train: Epoch [10], Batch [714/938], Loss: 1.112238883972168\n",
      "Train: Epoch [10], Batch [715/938], Loss: 0.7183892726898193\n",
      "Train: Epoch [10], Batch [716/938], Loss: 0.9477861523628235\n",
      "Train: Epoch [10], Batch [717/938], Loss: 0.5485672354698181\n",
      "Train: Epoch [10], Batch [718/938], Loss: 0.9678405523300171\n",
      "Train: Epoch [10], Batch [719/938], Loss: 0.8073946237564087\n",
      "Train: Epoch [10], Batch [720/938], Loss: 0.7973688840866089\n",
      "Train: Epoch [10], Batch [721/938], Loss: 0.8744637966156006\n",
      "Train: Epoch [10], Batch [722/938], Loss: 0.9624842405319214\n",
      "Train: Epoch [10], Batch [723/938], Loss: 0.7821294069290161\n",
      "Train: Epoch [10], Batch [724/938], Loss: 0.7996969819068909\n",
      "Train: Epoch [10], Batch [725/938], Loss: 0.8462127447128296\n",
      "Train: Epoch [10], Batch [726/938], Loss: 0.8721174001693726\n",
      "Train: Epoch [10], Batch [727/938], Loss: 0.7136924266815186\n",
      "Train: Epoch [10], Batch [728/938], Loss: 0.736670196056366\n",
      "Train: Epoch [10], Batch [729/938], Loss: 0.7417209148406982\n",
      "Train: Epoch [10], Batch [730/938], Loss: 0.8333601355552673\n",
      "Train: Epoch [10], Batch [731/938], Loss: 0.7603163719177246\n",
      "Train: Epoch [10], Batch [732/938], Loss: 0.7508585453033447\n",
      "Train: Epoch [10], Batch [733/938], Loss: 0.8742172122001648\n",
      "Train: Epoch [10], Batch [734/938], Loss: 0.7293280959129333\n",
      "Train: Epoch [10], Batch [735/938], Loss: 0.8153209686279297\n",
      "Train: Epoch [10], Batch [736/938], Loss: 0.7585493326187134\n",
      "Train: Epoch [10], Batch [737/938], Loss: 0.7171398401260376\n",
      "Train: Epoch [10], Batch [738/938], Loss: 0.8242411613464355\n",
      "Train: Epoch [10], Batch [739/938], Loss: 0.7505773305892944\n",
      "Train: Epoch [10], Batch [740/938], Loss: 0.7829071283340454\n",
      "Train: Epoch [10], Batch [741/938], Loss: 0.8167370557785034\n",
      "Train: Epoch [10], Batch [742/938], Loss: 0.7768316864967346\n",
      "Train: Epoch [10], Batch [743/938], Loss: 0.736695408821106\n",
      "Train: Epoch [10], Batch [744/938], Loss: 0.9124072194099426\n",
      "Train: Epoch [10], Batch [745/938], Loss: 1.0551000833511353\n",
      "Train: Epoch [10], Batch [746/938], Loss: 0.8821327090263367\n",
      "Train: Epoch [10], Batch [747/938], Loss: 0.6974982023239136\n",
      "Train: Epoch [10], Batch [748/938], Loss: 0.7281707525253296\n",
      "Train: Epoch [10], Batch [749/938], Loss: 0.6754339933395386\n",
      "Train: Epoch [10], Batch [750/938], Loss: 0.7017728090286255\n",
      "Train: Epoch [10], Batch [751/938], Loss: 0.5883482098579407\n",
      "Train: Epoch [10], Batch [752/938], Loss: 0.7301974296569824\n",
      "Train: Epoch [10], Batch [753/938], Loss: 0.614578127861023\n",
      "Train: Epoch [10], Batch [754/938], Loss: 0.8029491901397705\n",
      "Train: Epoch [10], Batch [755/938], Loss: 0.8291122317314148\n",
      "Train: Epoch [10], Batch [756/938], Loss: 0.8453782796859741\n",
      "Train: Epoch [10], Batch [757/938], Loss: 0.8417251110076904\n",
      "Train: Epoch [10], Batch [758/938], Loss: 0.6995336413383484\n",
      "Train: Epoch [10], Batch [759/938], Loss: 1.0123251676559448\n",
      "Train: Epoch [10], Batch [760/938], Loss: 0.7503721117973328\n",
      "Train: Epoch [10], Batch [761/938], Loss: 0.793658971786499\n",
      "Train: Epoch [10], Batch [762/938], Loss: 0.6121143102645874\n",
      "Train: Epoch [10], Batch [763/938], Loss: 0.7190660834312439\n",
      "Train: Epoch [10], Batch [764/938], Loss: 0.8000972270965576\n",
      "Train: Epoch [10], Batch [765/938], Loss: 0.8987504243850708\n",
      "Train: Epoch [10], Batch [766/938], Loss: 0.6368407607078552\n",
      "Train: Epoch [10], Batch [767/938], Loss: 0.9759175181388855\n",
      "Train: Epoch [10], Batch [768/938], Loss: 0.643722653388977\n",
      "Train: Epoch [10], Batch [769/938], Loss: 0.6538276672363281\n",
      "Train: Epoch [10], Batch [770/938], Loss: 0.848974347114563\n",
      "Train: Epoch [10], Batch [771/938], Loss: 0.7360489368438721\n",
      "Train: Epoch [10], Batch [772/938], Loss: 0.9460073113441467\n",
      "Train: Epoch [10], Batch [773/938], Loss: 0.5776387453079224\n",
      "Train: Epoch [10], Batch [774/938], Loss: 0.7606523036956787\n",
      "Train: Epoch [10], Batch [775/938], Loss: 1.0131306648254395\n",
      "Train: Epoch [10], Batch [776/938], Loss: 0.8314462900161743\n",
      "Train: Epoch [10], Batch [777/938], Loss: 0.7616291642189026\n",
      "Train: Epoch [10], Batch [778/938], Loss: 0.705167293548584\n",
      "Train: Epoch [10], Batch [779/938], Loss: 0.7743287086486816\n",
      "Train: Epoch [10], Batch [780/938], Loss: 0.8290608525276184\n",
      "Train: Epoch [10], Batch [781/938], Loss: 0.88332599401474\n",
      "Train: Epoch [10], Batch [782/938], Loss: 1.016366958618164\n",
      "Train: Epoch [10], Batch [783/938], Loss: 0.7981767654418945\n",
      "Train: Epoch [10], Batch [784/938], Loss: 0.6773533821105957\n",
      "Train: Epoch [10], Batch [785/938], Loss: 0.6894773244857788\n",
      "Train: Epoch [10], Batch [786/938], Loss: 0.9630144834518433\n",
      "Train: Epoch [10], Batch [787/938], Loss: 0.6290137767791748\n",
      "Train: Epoch [10], Batch [788/938], Loss: 0.8742602467536926\n",
      "Train: Epoch [10], Batch [789/938], Loss: 0.6670237183570862\n",
      "Train: Epoch [10], Batch [790/938], Loss: 0.68650883436203\n",
      "Train: Epoch [10], Batch [791/938], Loss: 0.8598160743713379\n",
      "Train: Epoch [10], Batch [792/938], Loss: 0.8118520975112915\n",
      "Train: Epoch [10], Batch [793/938], Loss: 0.8801862597465515\n",
      "Train: Epoch [10], Batch [794/938], Loss: 0.9013534784317017\n",
      "Train: Epoch [10], Batch [795/938], Loss: 0.7710888385772705\n",
      "Train: Epoch [10], Batch [796/938], Loss: 0.7745782732963562\n",
      "Train: Epoch [10], Batch [797/938], Loss: 0.6827813386917114\n",
      "Train: Epoch [10], Batch [798/938], Loss: 0.7472423911094666\n",
      "Train: Epoch [10], Batch [799/938], Loss: 0.7587274312973022\n",
      "Train: Epoch [10], Batch [800/938], Loss: 0.8954238295555115\n",
      "Train: Epoch [10], Batch [801/938], Loss: 0.7332323789596558\n",
      "Train: Epoch [10], Batch [802/938], Loss: 0.7186481952667236\n",
      "Train: Epoch [10], Batch [803/938], Loss: 0.537287712097168\n",
      "Train: Epoch [10], Batch [804/938], Loss: 0.5118511319160461\n",
      "Train: Epoch [10], Batch [805/938], Loss: 0.6610049605369568\n",
      "Train: Epoch [10], Batch [806/938], Loss: 0.8074313402175903\n",
      "Train: Epoch [10], Batch [807/938], Loss: 0.8080453872680664\n",
      "Train: Epoch [10], Batch [808/938], Loss: 0.8304653167724609\n",
      "Train: Epoch [10], Batch [809/938], Loss: 0.7581027150154114\n",
      "Train: Epoch [10], Batch [810/938], Loss: 0.8732280731201172\n",
      "Train: Epoch [10], Batch [811/938], Loss: 0.7694251537322998\n",
      "Train: Epoch [10], Batch [812/938], Loss: 0.7595292329788208\n",
      "Train: Epoch [10], Batch [813/938], Loss: 1.040132761001587\n",
      "Train: Epoch [10], Batch [814/938], Loss: 0.9971137642860413\n",
      "Train: Epoch [10], Batch [815/938], Loss: 0.8708578944206238\n",
      "Train: Epoch [10], Batch [816/938], Loss: 0.8890573978424072\n",
      "Train: Epoch [10], Batch [817/938], Loss: 0.6043037176132202\n",
      "Train: Epoch [10], Batch [818/938], Loss: 0.6885203123092651\n",
      "Train: Epoch [10], Batch [819/938], Loss: 0.7842835187911987\n",
      "Train: Epoch [10], Batch [820/938], Loss: 0.8508332967758179\n",
      "Train: Epoch [10], Batch [821/938], Loss: 0.9513967633247375\n",
      "Train: Epoch [10], Batch [822/938], Loss: 0.6285021901130676\n",
      "Train: Epoch [10], Batch [823/938], Loss: 0.844232439994812\n",
      "Train: Epoch [10], Batch [824/938], Loss: 0.7904589176177979\n",
      "Train: Epoch [10], Batch [825/938], Loss: 0.674140453338623\n",
      "Train: Epoch [10], Batch [826/938], Loss: 0.78227299451828\n",
      "Train: Epoch [10], Batch [827/938], Loss: 0.6787071228027344\n",
      "Train: Epoch [10], Batch [828/938], Loss: 0.6802761554718018\n",
      "Train: Epoch [10], Batch [829/938], Loss: 0.7136648297309875\n",
      "Train: Epoch [10], Batch [830/938], Loss: 0.6703171730041504\n",
      "Train: Epoch [10], Batch [831/938], Loss: 0.6135309934616089\n",
      "Train: Epoch [10], Batch [832/938], Loss: 0.7818161249160767\n",
      "Train: Epoch [10], Batch [833/938], Loss: 0.7936551570892334\n",
      "Train: Epoch [10], Batch [834/938], Loss: 0.5316070318222046\n",
      "Train: Epoch [10], Batch [835/938], Loss: 0.9163833856582642\n",
      "Train: Epoch [10], Batch [836/938], Loss: 0.7512400150299072\n",
      "Train: Epoch [10], Batch [837/938], Loss: 1.0206694602966309\n",
      "Train: Epoch [10], Batch [838/938], Loss: 0.6868171691894531\n",
      "Train: Epoch [10], Batch [839/938], Loss: 0.6705181002616882\n",
      "Train: Epoch [10], Batch [840/938], Loss: 1.043462872505188\n",
      "Train: Epoch [10], Batch [841/938], Loss: 0.660618782043457\n",
      "Train: Epoch [10], Batch [842/938], Loss: 0.6629172563552856\n",
      "Train: Epoch [10], Batch [843/938], Loss: 0.600322425365448\n",
      "Train: Epoch [10], Batch [844/938], Loss: 0.7764652371406555\n",
      "Train: Epoch [10], Batch [845/938], Loss: 0.8167389035224915\n",
      "Train: Epoch [10], Batch [846/938], Loss: 0.704039454460144\n",
      "Train: Epoch [10], Batch [847/938], Loss: 0.6503860354423523\n",
      "Train: Epoch [10], Batch [848/938], Loss: 0.6320205330848694\n",
      "Train: Epoch [10], Batch [849/938], Loss: 0.7774310111999512\n",
      "Train: Epoch [10], Batch [850/938], Loss: 0.5983221530914307\n",
      "Train: Epoch [10], Batch [851/938], Loss: 0.720899224281311\n",
      "Train: Epoch [10], Batch [852/938], Loss: 1.0239845514297485\n",
      "Train: Epoch [10], Batch [853/938], Loss: 0.77462238073349\n",
      "Train: Epoch [10], Batch [854/938], Loss: 0.8149228096008301\n",
      "Train: Epoch [10], Batch [855/938], Loss: 0.4576738774776459\n",
      "Train: Epoch [10], Batch [856/938], Loss: 0.8545801043510437\n",
      "Train: Epoch [10], Batch [857/938], Loss: 0.8583817481994629\n",
      "Train: Epoch [10], Batch [858/938], Loss: 0.6338050365447998\n",
      "Train: Epoch [10], Batch [859/938], Loss: 0.7536477446556091\n",
      "Train: Epoch [10], Batch [860/938], Loss: 0.6825395226478577\n",
      "Train: Epoch [10], Batch [861/938], Loss: 0.8946146965026855\n",
      "Train: Epoch [10], Batch [862/938], Loss: 0.5952646136283875\n",
      "Train: Epoch [10], Batch [863/938], Loss: 0.7462974190711975\n",
      "Train: Epoch [10], Batch [864/938], Loss: 0.9518477916717529\n",
      "Train: Epoch [10], Batch [865/938], Loss: 0.8228650689125061\n",
      "Train: Epoch [10], Batch [866/938], Loss: 0.8462758660316467\n",
      "Train: Epoch [10], Batch [867/938], Loss: 0.5427157878875732\n",
      "Train: Epoch [10], Batch [868/938], Loss: 0.5026101469993591\n",
      "Train: Epoch [10], Batch [869/938], Loss: 0.5665640234947205\n",
      "Train: Epoch [10], Batch [870/938], Loss: 0.9154282212257385\n",
      "Train: Epoch [10], Batch [871/938], Loss: 0.6827664375305176\n",
      "Train: Epoch [10], Batch [872/938], Loss: 0.7268660068511963\n",
      "Train: Epoch [10], Batch [873/938], Loss: 0.9725615978240967\n",
      "Train: Epoch [10], Batch [874/938], Loss: 0.8389500975608826\n",
      "Train: Epoch [10], Batch [875/938], Loss: 0.7196866273880005\n",
      "Train: Epoch [10], Batch [876/938], Loss: 0.8632757663726807\n",
      "Train: Epoch [10], Batch [877/938], Loss: 0.8753817081451416\n",
      "Train: Epoch [10], Batch [878/938], Loss: 0.6133497953414917\n",
      "Train: Epoch [10], Batch [879/938], Loss: 0.8854371905326843\n",
      "Train: Epoch [10], Batch [880/938], Loss: 0.5153331756591797\n",
      "Train: Epoch [10], Batch [881/938], Loss: 0.7597570419311523\n",
      "Train: Epoch [10], Batch [882/938], Loss: 0.8115072250366211\n",
      "Train: Epoch [10], Batch [883/938], Loss: 0.9186118841171265\n",
      "Train: Epoch [10], Batch [884/938], Loss: 0.6366522312164307\n",
      "Train: Epoch [10], Batch [885/938], Loss: 1.1776683330535889\n",
      "Train: Epoch [10], Batch [886/938], Loss: 0.8403072953224182\n",
      "Train: Epoch [10], Batch [887/938], Loss: 0.6780127286911011\n",
      "Train: Epoch [10], Batch [888/938], Loss: 0.9916460514068604\n",
      "Train: Epoch [10], Batch [889/938], Loss: 0.8260051608085632\n",
      "Train: Epoch [10], Batch [890/938], Loss: 0.598507285118103\n",
      "Train: Epoch [10], Batch [891/938], Loss: 0.5718903541564941\n",
      "Train: Epoch [10], Batch [892/938], Loss: 0.7851829528808594\n",
      "Train: Epoch [10], Batch [893/938], Loss: 0.7471963763237\n",
      "Train: Epoch [10], Batch [894/938], Loss: 0.8462181091308594\n",
      "Train: Epoch [10], Batch [895/938], Loss: 0.7000123262405396\n",
      "Train: Epoch [10], Batch [896/938], Loss: 0.8986703753471375\n",
      "Train: Epoch [10], Batch [897/938], Loss: 0.5674578547477722\n",
      "Train: Epoch [10], Batch [898/938], Loss: 0.7740135788917542\n",
      "Train: Epoch [10], Batch [899/938], Loss: 0.9594976902008057\n",
      "Train: Epoch [10], Batch [900/938], Loss: 0.9510988593101501\n",
      "Train: Epoch [10], Batch [901/938], Loss: 0.7431520819664001\n",
      "Train: Epoch [10], Batch [902/938], Loss: 0.7922276854515076\n",
      "Train: Epoch [10], Batch [903/938], Loss: 0.8280404806137085\n",
      "Train: Epoch [10], Batch [904/938], Loss: 0.6909360289573669\n",
      "Train: Epoch [10], Batch [905/938], Loss: 0.9313121438026428\n",
      "Train: Epoch [10], Batch [906/938], Loss: 0.7498855590820312\n",
      "Train: Epoch [10], Batch [907/938], Loss: 0.704964280128479\n",
      "Train: Epoch [10], Batch [908/938], Loss: 0.8339833617210388\n",
      "Train: Epoch [10], Batch [909/938], Loss: 0.6682257652282715\n",
      "Train: Epoch [10], Batch [910/938], Loss: 0.54029780626297\n",
      "Train: Epoch [10], Batch [911/938], Loss: 0.6898693442344666\n",
      "Train: Epoch [10], Batch [912/938], Loss: 0.7065809369087219\n",
      "Train: Epoch [10], Batch [913/938], Loss: 0.5953989624977112\n",
      "Train: Epoch [10], Batch [914/938], Loss: 1.0280354022979736\n",
      "Train: Epoch [10], Batch [915/938], Loss: 0.792030394077301\n",
      "Train: Epoch [10], Batch [916/938], Loss: 0.6593148708343506\n",
      "Train: Epoch [10], Batch [917/938], Loss: 0.9328342080116272\n",
      "Train: Epoch [10], Batch [918/938], Loss: 0.954957902431488\n",
      "Train: Epoch [10], Batch [919/938], Loss: 0.9499033689498901\n",
      "Train: Epoch [10], Batch [920/938], Loss: 0.670245885848999\n",
      "Train: Epoch [10], Batch [921/938], Loss: 0.9527968168258667\n",
      "Train: Epoch [10], Batch [922/938], Loss: 0.7680493593215942\n",
      "Train: Epoch [10], Batch [923/938], Loss: 0.8558210134506226\n",
      "Train: Epoch [10], Batch [924/938], Loss: 0.6642278432846069\n",
      "Train: Epoch [10], Batch [925/938], Loss: 0.8633226752281189\n",
      "Train: Epoch [10], Batch [926/938], Loss: 0.8687881827354431\n",
      "Train: Epoch [10], Batch [927/938], Loss: 0.6472458243370056\n",
      "Train: Epoch [10], Batch [928/938], Loss: 0.746112048625946\n",
      "Train: Epoch [10], Batch [929/938], Loss: 0.7342090010643005\n",
      "Train: Epoch [10], Batch [930/938], Loss: 0.8742086291313171\n",
      "Train: Epoch [10], Batch [931/938], Loss: 0.6614869236946106\n",
      "Train: Epoch [10], Batch [932/938], Loss: 0.7663235068321228\n",
      "Train: Epoch [10], Batch [933/938], Loss: 0.7314382791519165\n",
      "Train: Epoch [10], Batch [934/938], Loss: 0.6537612676620483\n",
      "Train: Epoch [10], Batch [935/938], Loss: 0.7239788174629211\n",
      "Train: Epoch [10], Batch [936/938], Loss: 0.9066683053970337\n",
      "Train: Epoch [10], Batch [937/938], Loss: 0.7147939205169678\n",
      "Train: Epoch [10], Batch [938/938], Loss: 0.5189325213432312\n",
      "Accuracy of train set: 0.7515833333333334\n",
      "Validation: Epoch [10], Batch [1/938], Loss: 0.6973265409469604\n",
      "Validation: Epoch [10], Batch [2/938], Loss: 0.7395194172859192\n",
      "Validation: Epoch [10], Batch [3/938], Loss: 0.5870707631111145\n",
      "Validation: Epoch [10], Batch [4/938], Loss: 0.6753910183906555\n",
      "Validation: Epoch [10], Batch [5/938], Loss: 0.61738121509552\n",
      "Validation: Epoch [10], Batch [6/938], Loss: 0.7459959387779236\n",
      "Validation: Epoch [10], Batch [7/938], Loss: 0.9777982234954834\n",
      "Validation: Epoch [10], Batch [8/938], Loss: 0.830907940864563\n",
      "Validation: Epoch [10], Batch [9/938], Loss: 0.8366889953613281\n",
      "Validation: Epoch [10], Batch [10/938], Loss: 0.9089193940162659\n",
      "Validation: Epoch [10], Batch [11/938], Loss: 0.7570038437843323\n",
      "Validation: Epoch [10], Batch [12/938], Loss: 0.9630558490753174\n",
      "Validation: Epoch [10], Batch [13/938], Loss: 0.6484178304672241\n",
      "Validation: Epoch [10], Batch [14/938], Loss: 0.672274112701416\n",
      "Validation: Epoch [10], Batch [15/938], Loss: 0.866300642490387\n",
      "Validation: Epoch [10], Batch [16/938], Loss: 0.751678466796875\n",
      "Validation: Epoch [10], Batch [17/938], Loss: 0.8615207076072693\n",
      "Validation: Epoch [10], Batch [18/938], Loss: 0.7044563293457031\n",
      "Validation: Epoch [10], Batch [19/938], Loss: 0.9816149473190308\n",
      "Validation: Epoch [10], Batch [20/938], Loss: 0.6936386227607727\n",
      "Validation: Epoch [10], Batch [21/938], Loss: 0.6328402161598206\n",
      "Validation: Epoch [10], Batch [22/938], Loss: 1.0046849250793457\n",
      "Validation: Epoch [10], Batch [23/938], Loss: 0.7384622693061829\n",
      "Validation: Epoch [10], Batch [24/938], Loss: 0.663155734539032\n",
      "Validation: Epoch [10], Batch [25/938], Loss: 0.9816659092903137\n",
      "Validation: Epoch [10], Batch [26/938], Loss: 1.0206201076507568\n",
      "Validation: Epoch [10], Batch [27/938], Loss: 0.8521671295166016\n",
      "Validation: Epoch [10], Batch [28/938], Loss: 0.8378005623817444\n",
      "Validation: Epoch [10], Batch [29/938], Loss: 0.8860560655593872\n",
      "Validation: Epoch [10], Batch [30/938], Loss: 0.7716400623321533\n",
      "Validation: Epoch [10], Batch [31/938], Loss: 0.6544375419616699\n",
      "Validation: Epoch [10], Batch [32/938], Loss: 0.6399114727973938\n",
      "Validation: Epoch [10], Batch [33/938], Loss: 0.7486392855644226\n",
      "Validation: Epoch [10], Batch [34/938], Loss: 0.6434915661811829\n",
      "Validation: Epoch [10], Batch [35/938], Loss: 0.5543904900550842\n",
      "Validation: Epoch [10], Batch [36/938], Loss: 0.666149377822876\n",
      "Validation: Epoch [10], Batch [37/938], Loss: 0.8465092182159424\n",
      "Validation: Epoch [10], Batch [38/938], Loss: 1.002568244934082\n",
      "Validation: Epoch [10], Batch [39/938], Loss: 0.5946383476257324\n",
      "Validation: Epoch [10], Batch [40/938], Loss: 0.6376211047172546\n",
      "Validation: Epoch [10], Batch [41/938], Loss: 0.7121821045875549\n",
      "Validation: Epoch [10], Batch [42/938], Loss: 1.090919852256775\n",
      "Validation: Epoch [10], Batch [43/938], Loss: 0.8009089231491089\n",
      "Validation: Epoch [10], Batch [44/938], Loss: 1.0781223773956299\n",
      "Validation: Epoch [10], Batch [45/938], Loss: 0.5944711565971375\n",
      "Validation: Epoch [10], Batch [46/938], Loss: 0.8591657280921936\n",
      "Validation: Epoch [10], Batch [47/938], Loss: 0.7299087047576904\n",
      "Validation: Epoch [10], Batch [48/938], Loss: 0.5875310897827148\n",
      "Validation: Epoch [10], Batch [49/938], Loss: 0.8254953026771545\n",
      "Validation: Epoch [10], Batch [50/938], Loss: 0.8571513891220093\n",
      "Validation: Epoch [10], Batch [51/938], Loss: 0.8078817129135132\n",
      "Validation: Epoch [10], Batch [52/938], Loss: 0.8800403475761414\n",
      "Validation: Epoch [10], Batch [53/938], Loss: 0.9352708458900452\n",
      "Validation: Epoch [10], Batch [54/938], Loss: 0.5488892793655396\n",
      "Validation: Epoch [10], Batch [55/938], Loss: 0.6763841509819031\n",
      "Validation: Epoch [10], Batch [56/938], Loss: 1.0467331409454346\n",
      "Validation: Epoch [10], Batch [57/938], Loss: 0.8186327219009399\n",
      "Validation: Epoch [10], Batch [58/938], Loss: 1.0729728937149048\n",
      "Validation: Epoch [10], Batch [59/938], Loss: 0.9343087673187256\n",
      "Validation: Epoch [10], Batch [60/938], Loss: 0.799118161201477\n",
      "Validation: Epoch [10], Batch [61/938], Loss: 0.7988881468772888\n",
      "Validation: Epoch [10], Batch [62/938], Loss: 0.8289728164672852\n",
      "Validation: Epoch [10], Batch [63/938], Loss: 0.7729916572570801\n",
      "Validation: Epoch [10], Batch [64/938], Loss: 0.8347564935684204\n",
      "Validation: Epoch [10], Batch [65/938], Loss: 0.991748034954071\n",
      "Validation: Epoch [10], Batch [66/938], Loss: 0.941545844078064\n",
      "Validation: Epoch [10], Batch [67/938], Loss: 0.8909668326377869\n",
      "Validation: Epoch [10], Batch [68/938], Loss: 0.886239230632782\n",
      "Validation: Epoch [10], Batch [69/938], Loss: 0.629038393497467\n",
      "Validation: Epoch [10], Batch [70/938], Loss: 0.7908248901367188\n",
      "Validation: Epoch [10], Batch [71/938], Loss: 1.059377908706665\n",
      "Validation: Epoch [10], Batch [72/938], Loss: 1.0309549570083618\n",
      "Validation: Epoch [10], Batch [73/938], Loss: 1.1496042013168335\n",
      "Validation: Epoch [10], Batch [74/938], Loss: 0.5922463536262512\n",
      "Validation: Epoch [10], Batch [75/938], Loss: 0.6458796858787537\n",
      "Validation: Epoch [10], Batch [76/938], Loss: 1.1069434881210327\n",
      "Validation: Epoch [10], Batch [77/938], Loss: 0.7524377107620239\n",
      "Validation: Epoch [10], Batch [78/938], Loss: 0.6992277503013611\n",
      "Validation: Epoch [10], Batch [79/938], Loss: 0.6635854840278625\n",
      "Validation: Epoch [10], Batch [80/938], Loss: 0.6466540098190308\n",
      "Validation: Epoch [10], Batch [81/938], Loss: 0.8412317037582397\n",
      "Validation: Epoch [10], Batch [82/938], Loss: 0.7572027444839478\n",
      "Validation: Epoch [10], Batch [83/938], Loss: 0.7714889645576477\n",
      "Validation: Epoch [10], Batch [84/938], Loss: 0.817626953125\n",
      "Validation: Epoch [10], Batch [85/938], Loss: 0.6829832792282104\n",
      "Validation: Epoch [10], Batch [86/938], Loss: 0.5714986324310303\n",
      "Validation: Epoch [10], Batch [87/938], Loss: 0.9303514957427979\n",
      "Validation: Epoch [10], Batch [88/938], Loss: 0.9606517553329468\n",
      "Validation: Epoch [10], Batch [89/938], Loss: 1.1050361394882202\n",
      "Validation: Epoch [10], Batch [90/938], Loss: 0.869104266166687\n",
      "Validation: Epoch [10], Batch [91/938], Loss: 0.5384596586227417\n",
      "Validation: Epoch [10], Batch [92/938], Loss: 0.6757075786590576\n",
      "Validation: Epoch [10], Batch [93/938], Loss: 0.8756045699119568\n",
      "Validation: Epoch [10], Batch [94/938], Loss: 0.6297487020492554\n",
      "Validation: Epoch [10], Batch [95/938], Loss: 0.7254231572151184\n",
      "Validation: Epoch [10], Batch [96/938], Loss: 0.8654989004135132\n",
      "Validation: Epoch [10], Batch [97/938], Loss: 0.7967670559883118\n",
      "Validation: Epoch [10], Batch [98/938], Loss: 0.8539041876792908\n",
      "Validation: Epoch [10], Batch [99/938], Loss: 0.7407163381576538\n",
      "Validation: Epoch [10], Batch [100/938], Loss: 0.8315152525901794\n",
      "Validation: Epoch [10], Batch [101/938], Loss: 0.6734567284584045\n",
      "Validation: Epoch [10], Batch [102/938], Loss: 0.8481358885765076\n",
      "Validation: Epoch [10], Batch [103/938], Loss: 0.7188453674316406\n",
      "Validation: Epoch [10], Batch [104/938], Loss: 0.7162553071975708\n",
      "Validation: Epoch [10], Batch [105/938], Loss: 0.45656681060791016\n",
      "Validation: Epoch [10], Batch [106/938], Loss: 0.7000169157981873\n",
      "Validation: Epoch [10], Batch [107/938], Loss: 0.5831610560417175\n",
      "Validation: Epoch [10], Batch [108/938], Loss: 0.854738175868988\n",
      "Validation: Epoch [10], Batch [109/938], Loss: 0.7183473706245422\n",
      "Validation: Epoch [10], Batch [110/938], Loss: 0.6117767691612244\n",
      "Validation: Epoch [10], Batch [111/938], Loss: 0.642676591873169\n",
      "Validation: Epoch [10], Batch [112/938], Loss: 0.8532447814941406\n",
      "Validation: Epoch [10], Batch [113/938], Loss: 0.8318830728530884\n",
      "Validation: Epoch [10], Batch [114/938], Loss: 0.810772180557251\n",
      "Validation: Epoch [10], Batch [115/938], Loss: 0.9020825624465942\n",
      "Validation: Epoch [10], Batch [116/938], Loss: 0.6965310573577881\n",
      "Validation: Epoch [10], Batch [117/938], Loss: 0.7215859293937683\n",
      "Validation: Epoch [10], Batch [118/938], Loss: 0.5830502510070801\n",
      "Validation: Epoch [10], Batch [119/938], Loss: 0.9370408058166504\n",
      "Validation: Epoch [10], Batch [120/938], Loss: 0.7930241227149963\n",
      "Validation: Epoch [10], Batch [121/938], Loss: 0.7418726682662964\n",
      "Validation: Epoch [10], Batch [122/938], Loss: 0.9350751042366028\n",
      "Validation: Epoch [10], Batch [123/938], Loss: 0.6813334226608276\n",
      "Validation: Epoch [10], Batch [124/938], Loss: 0.7726473212242126\n",
      "Validation: Epoch [10], Batch [125/938], Loss: 0.7545560002326965\n",
      "Validation: Epoch [10], Batch [126/938], Loss: 0.8844400644302368\n",
      "Validation: Epoch [10], Batch [127/938], Loss: 0.8281793594360352\n",
      "Validation: Epoch [10], Batch [128/938], Loss: 0.8886646628379822\n",
      "Validation: Epoch [10], Batch [129/938], Loss: 0.7705245018005371\n",
      "Validation: Epoch [10], Batch [130/938], Loss: 0.6151586174964905\n",
      "Validation: Epoch [10], Batch [131/938], Loss: 0.7410105466842651\n",
      "Validation: Epoch [10], Batch [132/938], Loss: 0.7105746865272522\n",
      "Validation: Epoch [10], Batch [133/938], Loss: 0.6730934977531433\n",
      "Validation: Epoch [10], Batch [134/938], Loss: 0.7226588129997253\n",
      "Validation: Epoch [10], Batch [135/938], Loss: 0.6034232378005981\n",
      "Validation: Epoch [10], Batch [136/938], Loss: 0.783379316329956\n",
      "Validation: Epoch [10], Batch [137/938], Loss: 0.6152950525283813\n",
      "Validation: Epoch [10], Batch [138/938], Loss: 0.5999255180358887\n",
      "Validation: Epoch [10], Batch [139/938], Loss: 0.7220291495323181\n",
      "Validation: Epoch [10], Batch [140/938], Loss: 1.1160768270492554\n",
      "Validation: Epoch [10], Batch [141/938], Loss: 0.9341171383857727\n",
      "Validation: Epoch [10], Batch [142/938], Loss: 0.571309506893158\n",
      "Validation: Epoch [10], Batch [143/938], Loss: 1.1338458061218262\n",
      "Validation: Epoch [10], Batch [144/938], Loss: 0.8579009771347046\n",
      "Validation: Epoch [10], Batch [145/938], Loss: 0.8310394287109375\n",
      "Validation: Epoch [10], Batch [146/938], Loss: 0.849969744682312\n",
      "Validation: Epoch [10], Batch [147/938], Loss: 0.7361762523651123\n",
      "Validation: Epoch [10], Batch [148/938], Loss: 0.7982310056686401\n",
      "Validation: Epoch [10], Batch [149/938], Loss: 0.7789360284805298\n",
      "Validation: Epoch [10], Batch [150/938], Loss: 0.9170916080474854\n",
      "Validation: Epoch [10], Batch [151/938], Loss: 0.6964909434318542\n",
      "Validation: Epoch [10], Batch [152/938], Loss: 0.7061667442321777\n",
      "Validation: Epoch [10], Batch [153/938], Loss: 0.5724006295204163\n",
      "Validation: Epoch [10], Batch [154/938], Loss: 0.7199898362159729\n",
      "Validation: Epoch [10], Batch [155/938], Loss: 0.804702877998352\n",
      "Validation: Epoch [10], Batch [156/938], Loss: 0.8301205635070801\n",
      "Validation: Epoch [10], Batch [157/938], Loss: 1.0889654159545898\n",
      "Validation: Epoch [10], Batch [158/938], Loss: 0.7414202690124512\n",
      "Validation: Epoch [10], Batch [159/938], Loss: 0.7424246668815613\n",
      "Validation: Epoch [10], Batch [160/938], Loss: 0.6110617518424988\n",
      "Validation: Epoch [10], Batch [161/938], Loss: 0.7315933108329773\n",
      "Validation: Epoch [10], Batch [162/938], Loss: 0.8176077604293823\n",
      "Validation: Epoch [10], Batch [163/938], Loss: 1.0925836563110352\n",
      "Validation: Epoch [10], Batch [164/938], Loss: 0.5943925380706787\n",
      "Validation: Epoch [10], Batch [165/938], Loss: 0.8768948912620544\n",
      "Validation: Epoch [10], Batch [166/938], Loss: 0.681566059589386\n",
      "Validation: Epoch [10], Batch [167/938], Loss: 0.8531443476676941\n",
      "Validation: Epoch [10], Batch [168/938], Loss: 0.8722226023674011\n",
      "Validation: Epoch [10], Batch [169/938], Loss: 0.8552438020706177\n",
      "Validation: Epoch [10], Batch [170/938], Loss: 0.8817610144615173\n",
      "Validation: Epoch [10], Batch [171/938], Loss: 0.7998080849647522\n",
      "Validation: Epoch [10], Batch [172/938], Loss: 1.0987038612365723\n",
      "Validation: Epoch [10], Batch [173/938], Loss: 0.8867313265800476\n",
      "Validation: Epoch [10], Batch [174/938], Loss: 0.8175374865531921\n",
      "Validation: Epoch [10], Batch [175/938], Loss: 0.8222750425338745\n",
      "Validation: Epoch [10], Batch [176/938], Loss: 0.7375409603118896\n",
      "Validation: Epoch [10], Batch [177/938], Loss: 0.740033745765686\n",
      "Validation: Epoch [10], Batch [178/938], Loss: 1.1288845539093018\n",
      "Validation: Epoch [10], Batch [179/938], Loss: 0.7142956256866455\n",
      "Validation: Epoch [10], Batch [180/938], Loss: 0.7878120541572571\n",
      "Validation: Epoch [10], Batch [181/938], Loss: 0.7837212681770325\n",
      "Validation: Epoch [10], Batch [182/938], Loss: 0.8688347339630127\n",
      "Validation: Epoch [10], Batch [183/938], Loss: 0.8102295994758606\n",
      "Validation: Epoch [10], Batch [184/938], Loss: 0.8041918873786926\n",
      "Validation: Epoch [10], Batch [185/938], Loss: 0.7122641801834106\n",
      "Validation: Epoch [10], Batch [186/938], Loss: 0.8410599231719971\n",
      "Validation: Epoch [10], Batch [187/938], Loss: 0.7154904007911682\n",
      "Validation: Epoch [10], Batch [188/938], Loss: 1.0678387880325317\n",
      "Validation: Epoch [10], Batch [189/938], Loss: 0.996289849281311\n",
      "Validation: Epoch [10], Batch [190/938], Loss: 0.6534271240234375\n",
      "Validation: Epoch [10], Batch [191/938], Loss: 0.6952145099639893\n",
      "Validation: Epoch [10], Batch [192/938], Loss: 1.0202152729034424\n",
      "Validation: Epoch [10], Batch [193/938], Loss: 0.8783478736877441\n",
      "Validation: Epoch [10], Batch [194/938], Loss: 0.8206985592842102\n",
      "Validation: Epoch [10], Batch [195/938], Loss: 0.8487402200698853\n",
      "Validation: Epoch [10], Batch [196/938], Loss: 0.7789004445075989\n",
      "Validation: Epoch [10], Batch [197/938], Loss: 0.5369460582733154\n",
      "Validation: Epoch [10], Batch [198/938], Loss: 0.7467913627624512\n",
      "Validation: Epoch [10], Batch [199/938], Loss: 0.579643964767456\n",
      "Validation: Epoch [10], Batch [200/938], Loss: 1.0803699493408203\n",
      "Validation: Epoch [10], Batch [201/938], Loss: 0.6678150296211243\n",
      "Validation: Epoch [10], Batch [202/938], Loss: 0.5860140919685364\n",
      "Validation: Epoch [10], Batch [203/938], Loss: 0.6072133779525757\n",
      "Validation: Epoch [10], Batch [204/938], Loss: 0.9929254651069641\n",
      "Validation: Epoch [10], Batch [205/938], Loss: 0.8798508048057556\n",
      "Validation: Epoch [10], Batch [206/938], Loss: 0.8121777176856995\n",
      "Validation: Epoch [10], Batch [207/938], Loss: 0.9381939768791199\n",
      "Validation: Epoch [10], Batch [208/938], Loss: 0.7668589353561401\n",
      "Validation: Epoch [10], Batch [209/938], Loss: 0.774468183517456\n",
      "Validation: Epoch [10], Batch [210/938], Loss: 0.809181272983551\n",
      "Validation: Epoch [10], Batch [211/938], Loss: 0.5870100855827332\n",
      "Validation: Epoch [10], Batch [212/938], Loss: 0.9948619604110718\n",
      "Validation: Epoch [10], Batch [213/938], Loss: 0.6908491849899292\n",
      "Validation: Epoch [10], Batch [214/938], Loss: 0.896353006362915\n",
      "Validation: Epoch [10], Batch [215/938], Loss: 0.7063899040222168\n",
      "Validation: Epoch [10], Batch [216/938], Loss: 0.7300935983657837\n",
      "Validation: Epoch [10], Batch [217/938], Loss: 0.9974401593208313\n",
      "Validation: Epoch [10], Batch [218/938], Loss: 0.9610523581504822\n",
      "Validation: Epoch [10], Batch [219/938], Loss: 0.6473121047019958\n",
      "Validation: Epoch [10], Batch [220/938], Loss: 0.7038264274597168\n",
      "Validation: Epoch [10], Batch [221/938], Loss: 1.0870263576507568\n",
      "Validation: Epoch [10], Batch [222/938], Loss: 0.7642642259597778\n",
      "Validation: Epoch [10], Batch [223/938], Loss: 0.7060914039611816\n",
      "Validation: Epoch [10], Batch [224/938], Loss: 0.7242401242256165\n",
      "Validation: Epoch [10], Batch [225/938], Loss: 0.8350281715393066\n",
      "Validation: Epoch [10], Batch [226/938], Loss: 0.7312967777252197\n",
      "Validation: Epoch [10], Batch [227/938], Loss: 0.7042606472969055\n",
      "Validation: Epoch [10], Batch [228/938], Loss: 0.8060526251792908\n",
      "Validation: Epoch [10], Batch [229/938], Loss: 0.7535424828529358\n",
      "Validation: Epoch [10], Batch [230/938], Loss: 0.9358049631118774\n",
      "Validation: Epoch [10], Batch [231/938], Loss: 0.734475314617157\n",
      "Validation: Epoch [10], Batch [232/938], Loss: 0.8610126376152039\n",
      "Validation: Epoch [10], Batch [233/938], Loss: 0.9783179759979248\n",
      "Validation: Epoch [10], Batch [234/938], Loss: 0.89574134349823\n",
      "Validation: Epoch [10], Batch [235/938], Loss: 0.8633378744125366\n",
      "Validation: Epoch [10], Batch [236/938], Loss: 0.7405129671096802\n",
      "Validation: Epoch [10], Batch [237/938], Loss: 0.6407341361045837\n",
      "Validation: Epoch [10], Batch [238/938], Loss: 0.9721399545669556\n",
      "Validation: Epoch [10], Batch [239/938], Loss: 0.8106580376625061\n",
      "Validation: Epoch [10], Batch [240/938], Loss: 0.841239869594574\n",
      "Validation: Epoch [10], Batch [241/938], Loss: 0.4658117890357971\n",
      "Validation: Epoch [10], Batch [242/938], Loss: 0.9367798566818237\n",
      "Validation: Epoch [10], Batch [243/938], Loss: 0.8458162546157837\n",
      "Validation: Epoch [10], Batch [244/938], Loss: 0.6069530844688416\n",
      "Validation: Epoch [10], Batch [245/938], Loss: 0.7390167713165283\n",
      "Validation: Epoch [10], Batch [246/938], Loss: 0.895408570766449\n",
      "Validation: Epoch [10], Batch [247/938], Loss: 0.667829155921936\n",
      "Validation: Epoch [10], Batch [248/938], Loss: 0.7920838594436646\n",
      "Validation: Epoch [10], Batch [249/938], Loss: 0.6787599921226501\n",
      "Validation: Epoch [10], Batch [250/938], Loss: 0.9346956014633179\n",
      "Validation: Epoch [10], Batch [251/938], Loss: 0.6506341099739075\n",
      "Validation: Epoch [10], Batch [252/938], Loss: 0.6947020888328552\n",
      "Validation: Epoch [10], Batch [253/938], Loss: 1.0147253274917603\n",
      "Validation: Epoch [10], Batch [254/938], Loss: 0.9889537692070007\n",
      "Validation: Epoch [10], Batch [255/938], Loss: 1.056467056274414\n",
      "Validation: Epoch [10], Batch [256/938], Loss: 0.6720558404922485\n",
      "Validation: Epoch [10], Batch [257/938], Loss: 1.1210066080093384\n",
      "Validation: Epoch [10], Batch [258/938], Loss: 0.6053822636604309\n",
      "Validation: Epoch [10], Batch [259/938], Loss: 0.4841668903827667\n",
      "Validation: Epoch [10], Batch [260/938], Loss: 0.7528463006019592\n",
      "Validation: Epoch [10], Batch [261/938], Loss: 0.9801849722862244\n",
      "Validation: Epoch [10], Batch [262/938], Loss: 0.733301043510437\n",
      "Validation: Epoch [10], Batch [263/938], Loss: 0.8217745423316956\n",
      "Validation: Epoch [10], Batch [264/938], Loss: 0.906643271446228\n",
      "Validation: Epoch [10], Batch [265/938], Loss: 0.6646994352340698\n",
      "Validation: Epoch [10], Batch [266/938], Loss: 0.9594804048538208\n",
      "Validation: Epoch [10], Batch [267/938], Loss: 0.6592307686805725\n",
      "Validation: Epoch [10], Batch [268/938], Loss: 0.7156368494033813\n",
      "Validation: Epoch [10], Batch [269/938], Loss: 0.6976698040962219\n",
      "Validation: Epoch [10], Batch [270/938], Loss: 0.7603863477706909\n",
      "Validation: Epoch [10], Batch [271/938], Loss: 0.9539116024971008\n",
      "Validation: Epoch [10], Batch [272/938], Loss: 0.7029498815536499\n",
      "Validation: Epoch [10], Batch [273/938], Loss: 1.005516529083252\n",
      "Validation: Epoch [10], Batch [274/938], Loss: 0.890653133392334\n",
      "Validation: Epoch [10], Batch [275/938], Loss: 0.6302154660224915\n",
      "Validation: Epoch [10], Batch [276/938], Loss: 0.7105759382247925\n",
      "Validation: Epoch [10], Batch [277/938], Loss: 0.5690962076187134\n",
      "Validation: Epoch [10], Batch [278/938], Loss: 0.711779773235321\n",
      "Validation: Epoch [10], Batch [279/938], Loss: 0.7150723934173584\n",
      "Validation: Epoch [10], Batch [280/938], Loss: 0.7597793936729431\n",
      "Validation: Epoch [10], Batch [281/938], Loss: 0.9450521469116211\n",
      "Validation: Epoch [10], Batch [282/938], Loss: 0.7441282868385315\n",
      "Validation: Epoch [10], Batch [283/938], Loss: 0.7965080738067627\n",
      "Validation: Epoch [10], Batch [284/938], Loss: 0.8914369344711304\n",
      "Validation: Epoch [10], Batch [285/938], Loss: 0.8340380787849426\n",
      "Validation: Epoch [10], Batch [286/938], Loss: 0.7956370115280151\n",
      "Validation: Epoch [10], Batch [287/938], Loss: 0.6829625368118286\n",
      "Validation: Epoch [10], Batch [288/938], Loss: 0.8288002014160156\n",
      "Validation: Epoch [10], Batch [289/938], Loss: 0.7672462463378906\n",
      "Validation: Epoch [10], Batch [290/938], Loss: 0.7504514455795288\n",
      "Validation: Epoch [10], Batch [291/938], Loss: 0.8961777687072754\n",
      "Validation: Epoch [10], Batch [292/938], Loss: 0.7762443423271179\n",
      "Validation: Epoch [10], Batch [293/938], Loss: 0.84553062915802\n",
      "Validation: Epoch [10], Batch [294/938], Loss: 0.9407133460044861\n",
      "Validation: Epoch [10], Batch [295/938], Loss: 0.7994725704193115\n",
      "Validation: Epoch [10], Batch [296/938], Loss: 0.6330173015594482\n",
      "Validation: Epoch [10], Batch [297/938], Loss: 0.8964325189590454\n",
      "Validation: Epoch [10], Batch [298/938], Loss: 0.6975059509277344\n",
      "Validation: Epoch [10], Batch [299/938], Loss: 0.6780600547790527\n",
      "Validation: Epoch [10], Batch [300/938], Loss: 0.7998860478401184\n",
      "Validation: Epoch [10], Batch [301/938], Loss: 0.7551637291908264\n",
      "Validation: Epoch [10], Batch [302/938], Loss: 0.899358868598938\n",
      "Validation: Epoch [10], Batch [303/938], Loss: 0.7537513971328735\n",
      "Validation: Epoch [10], Batch [304/938], Loss: 0.8236904740333557\n",
      "Validation: Epoch [10], Batch [305/938], Loss: 0.7877441048622131\n",
      "Validation: Epoch [10], Batch [306/938], Loss: 0.7522334456443787\n",
      "Validation: Epoch [10], Batch [307/938], Loss: 0.8147525787353516\n",
      "Validation: Epoch [10], Batch [308/938], Loss: 1.0049244165420532\n",
      "Validation: Epoch [10], Batch [309/938], Loss: 0.7673890590667725\n",
      "Validation: Epoch [10], Batch [310/938], Loss: 0.5901627540588379\n",
      "Validation: Epoch [10], Batch [311/938], Loss: 0.6728594899177551\n",
      "Validation: Epoch [10], Batch [312/938], Loss: 0.6219095587730408\n",
      "Validation: Epoch [10], Batch [313/938], Loss: 0.7132957577705383\n",
      "Validation: Epoch [10], Batch [314/938], Loss: 0.7776278853416443\n",
      "Validation: Epoch [10], Batch [315/938], Loss: 0.948862612247467\n",
      "Validation: Epoch [10], Batch [316/938], Loss: 0.7309405207633972\n",
      "Validation: Epoch [10], Batch [317/938], Loss: 0.679793655872345\n",
      "Validation: Epoch [10], Batch [318/938], Loss: 0.7044316530227661\n",
      "Validation: Epoch [10], Batch [319/938], Loss: 0.7326316237449646\n",
      "Validation: Epoch [10], Batch [320/938], Loss: 0.7655681371688843\n",
      "Validation: Epoch [10], Batch [321/938], Loss: 0.6858805418014526\n",
      "Validation: Epoch [10], Batch [322/938], Loss: 0.6606557369232178\n",
      "Validation: Epoch [10], Batch [323/938], Loss: 0.8973234295845032\n",
      "Validation: Epoch [10], Batch [324/938], Loss: 0.7053076028823853\n",
      "Validation: Epoch [10], Batch [325/938], Loss: 0.6062904000282288\n",
      "Validation: Epoch [10], Batch [326/938], Loss: 0.7741196751594543\n",
      "Validation: Epoch [10], Batch [327/938], Loss: 0.8636469841003418\n",
      "Validation: Epoch [10], Batch [328/938], Loss: 0.5560222268104553\n",
      "Validation: Epoch [10], Batch [329/938], Loss: 0.8058778047561646\n",
      "Validation: Epoch [10], Batch [330/938], Loss: 0.9737565517425537\n",
      "Validation: Epoch [10], Batch [331/938], Loss: 0.8453202247619629\n",
      "Validation: Epoch [10], Batch [332/938], Loss: 0.6830885410308838\n",
      "Validation: Epoch [10], Batch [333/938], Loss: 0.9713637828826904\n",
      "Validation: Epoch [10], Batch [334/938], Loss: 0.7103219032287598\n",
      "Validation: Epoch [10], Batch [335/938], Loss: 0.8297502994537354\n",
      "Validation: Epoch [10], Batch [336/938], Loss: 0.7613992691040039\n",
      "Validation: Epoch [10], Batch [337/938], Loss: 0.6370853781700134\n",
      "Validation: Epoch [10], Batch [338/938], Loss: 1.025564193725586\n",
      "Validation: Epoch [10], Batch [339/938], Loss: 0.7833143472671509\n",
      "Validation: Epoch [10], Batch [340/938], Loss: 0.8640485405921936\n",
      "Validation: Epoch [10], Batch [341/938], Loss: 0.9177943468093872\n",
      "Validation: Epoch [10], Batch [342/938], Loss: 0.7913796305656433\n",
      "Validation: Epoch [10], Batch [343/938], Loss: 0.8641343712806702\n",
      "Validation: Epoch [10], Batch [344/938], Loss: 0.8478230237960815\n",
      "Validation: Epoch [10], Batch [345/938], Loss: 0.9024984836578369\n",
      "Validation: Epoch [10], Batch [346/938], Loss: 0.7150912880897522\n",
      "Validation: Epoch [10], Batch [347/938], Loss: 0.7234453558921814\n",
      "Validation: Epoch [10], Batch [348/938], Loss: 0.7673969268798828\n",
      "Validation: Epoch [10], Batch [349/938], Loss: 1.0095903873443604\n",
      "Validation: Epoch [10], Batch [350/938], Loss: 0.9271430373191833\n",
      "Validation: Epoch [10], Batch [351/938], Loss: 0.6026628017425537\n",
      "Validation: Epoch [10], Batch [352/938], Loss: 0.738034188747406\n",
      "Validation: Epoch [10], Batch [353/938], Loss: 1.0314136743545532\n",
      "Validation: Epoch [10], Batch [354/938], Loss: 0.8506425619125366\n",
      "Validation: Epoch [10], Batch [355/938], Loss: 0.8118285536766052\n",
      "Validation: Epoch [10], Batch [356/938], Loss: 0.6789673566818237\n",
      "Validation: Epoch [10], Batch [357/938], Loss: 0.7529422044754028\n",
      "Validation: Epoch [10], Batch [358/938], Loss: 0.9701182842254639\n",
      "Validation: Epoch [10], Batch [359/938], Loss: 0.7046204805374146\n",
      "Validation: Epoch [10], Batch [360/938], Loss: 0.8586759567260742\n",
      "Validation: Epoch [10], Batch [361/938], Loss: 0.9554451704025269\n",
      "Validation: Epoch [10], Batch [362/938], Loss: 0.8000454306602478\n",
      "Validation: Epoch [10], Batch [363/938], Loss: 0.7566340565681458\n",
      "Validation: Epoch [10], Batch [364/938], Loss: 0.7111124992370605\n",
      "Validation: Epoch [10], Batch [365/938], Loss: 1.1324928998947144\n",
      "Validation: Epoch [10], Batch [366/938], Loss: 0.7526446580886841\n",
      "Validation: Epoch [10], Batch [367/938], Loss: 0.909904420375824\n",
      "Validation: Epoch [10], Batch [368/938], Loss: 0.9535759687423706\n",
      "Validation: Epoch [10], Batch [369/938], Loss: 0.7548648715019226\n",
      "Validation: Epoch [10], Batch [370/938], Loss: 0.7408309578895569\n",
      "Validation: Epoch [10], Batch [371/938], Loss: 0.5515191555023193\n",
      "Validation: Epoch [10], Batch [372/938], Loss: 0.6659449338912964\n",
      "Validation: Epoch [10], Batch [373/938], Loss: 0.7813366055488586\n",
      "Validation: Epoch [10], Batch [374/938], Loss: 0.8035322427749634\n",
      "Validation: Epoch [10], Batch [375/938], Loss: 0.6584965586662292\n",
      "Validation: Epoch [10], Batch [376/938], Loss: 0.8501456379890442\n",
      "Validation: Epoch [10], Batch [377/938], Loss: 0.660038411617279\n",
      "Validation: Epoch [10], Batch [378/938], Loss: 0.789166271686554\n",
      "Validation: Epoch [10], Batch [379/938], Loss: 0.6527571678161621\n",
      "Validation: Epoch [10], Batch [380/938], Loss: 0.7876653671264648\n",
      "Validation: Epoch [10], Batch [381/938], Loss: 0.7681779861450195\n",
      "Validation: Epoch [10], Batch [382/938], Loss: 0.8816039562225342\n",
      "Validation: Epoch [10], Batch [383/938], Loss: 0.9693137407302856\n",
      "Validation: Epoch [10], Batch [384/938], Loss: 0.6573764085769653\n",
      "Validation: Epoch [10], Batch [385/938], Loss: 0.8275407552719116\n",
      "Validation: Epoch [10], Batch [386/938], Loss: 0.4539131224155426\n",
      "Validation: Epoch [10], Batch [387/938], Loss: 0.7316075563430786\n",
      "Validation: Epoch [10], Batch [388/938], Loss: 0.9321092963218689\n",
      "Validation: Epoch [10], Batch [389/938], Loss: 0.8089345693588257\n",
      "Validation: Epoch [10], Batch [390/938], Loss: 0.9554215669631958\n",
      "Validation: Epoch [10], Batch [391/938], Loss: 0.827775239944458\n",
      "Validation: Epoch [10], Batch [392/938], Loss: 0.7254608273506165\n",
      "Validation: Epoch [10], Batch [393/938], Loss: 0.6402567625045776\n",
      "Validation: Epoch [10], Batch [394/938], Loss: 0.7844928503036499\n",
      "Validation: Epoch [10], Batch [395/938], Loss: 1.0205285549163818\n",
      "Validation: Epoch [10], Batch [396/938], Loss: 0.8656522035598755\n",
      "Validation: Epoch [10], Batch [397/938], Loss: 0.6880108714103699\n",
      "Validation: Epoch [10], Batch [398/938], Loss: 0.796438455581665\n",
      "Validation: Epoch [10], Batch [399/938], Loss: 0.8856384754180908\n",
      "Validation: Epoch [10], Batch [400/938], Loss: 1.059502124786377\n",
      "Validation: Epoch [10], Batch [401/938], Loss: 0.9324754476547241\n",
      "Validation: Epoch [10], Batch [402/938], Loss: 0.9376770257949829\n",
      "Validation: Epoch [10], Batch [403/938], Loss: 0.8131439685821533\n",
      "Validation: Epoch [10], Batch [404/938], Loss: 0.8930243253707886\n",
      "Validation: Epoch [10], Batch [405/938], Loss: 0.7979349493980408\n",
      "Validation: Epoch [10], Batch [406/938], Loss: 1.0361449718475342\n",
      "Validation: Epoch [10], Batch [407/938], Loss: 0.7531677484512329\n",
      "Validation: Epoch [10], Batch [408/938], Loss: 0.8379241824150085\n",
      "Validation: Epoch [10], Batch [409/938], Loss: 0.5851272344589233\n",
      "Validation: Epoch [10], Batch [410/938], Loss: 0.7881479859352112\n",
      "Validation: Epoch [10], Batch [411/938], Loss: 0.4248666763305664\n",
      "Validation: Epoch [10], Batch [412/938], Loss: 0.764180600643158\n",
      "Validation: Epoch [10], Batch [413/938], Loss: 0.6214715838432312\n",
      "Validation: Epoch [10], Batch [414/938], Loss: 0.9325599074363708\n",
      "Validation: Epoch [10], Batch [415/938], Loss: 0.8879129886627197\n",
      "Validation: Epoch [10], Batch [416/938], Loss: 0.7174931168556213\n",
      "Validation: Epoch [10], Batch [417/938], Loss: 0.8054471015930176\n",
      "Validation: Epoch [10], Batch [418/938], Loss: 0.836069643497467\n",
      "Validation: Epoch [10], Batch [419/938], Loss: 0.9172717928886414\n",
      "Validation: Epoch [10], Batch [420/938], Loss: 0.7127408981323242\n",
      "Validation: Epoch [10], Batch [421/938], Loss: 0.728551983833313\n",
      "Validation: Epoch [10], Batch [422/938], Loss: 0.8896055817604065\n",
      "Validation: Epoch [10], Batch [423/938], Loss: 0.7572543025016785\n",
      "Validation: Epoch [10], Batch [424/938], Loss: 0.8462933897972107\n",
      "Validation: Epoch [10], Batch [425/938], Loss: 0.6589987874031067\n",
      "Validation: Epoch [10], Batch [426/938], Loss: 0.6897424459457397\n",
      "Validation: Epoch [10], Batch [427/938], Loss: 0.8442095518112183\n",
      "Validation: Epoch [10], Batch [428/938], Loss: 0.8411494493484497\n",
      "Validation: Epoch [10], Batch [429/938], Loss: 0.7278186082839966\n",
      "Validation: Epoch [10], Batch [430/938], Loss: 0.8770575523376465\n",
      "Validation: Epoch [10], Batch [431/938], Loss: 0.6619227528572083\n",
      "Validation: Epoch [10], Batch [432/938], Loss: 0.8670272827148438\n",
      "Validation: Epoch [10], Batch [433/938], Loss: 0.6258980631828308\n",
      "Validation: Epoch [10], Batch [434/938], Loss: 0.7033772468566895\n",
      "Validation: Epoch [10], Batch [435/938], Loss: 0.6892701983451843\n",
      "Validation: Epoch [10], Batch [436/938], Loss: 0.8899357318878174\n",
      "Validation: Epoch [10], Batch [437/938], Loss: 0.6862558126449585\n",
      "Validation: Epoch [10], Batch [438/938], Loss: 0.7770959138870239\n",
      "Validation: Epoch [10], Batch [439/938], Loss: 0.7049474716186523\n",
      "Validation: Epoch [10], Batch [440/938], Loss: 0.65132737159729\n",
      "Validation: Epoch [10], Batch [441/938], Loss: 0.6846556663513184\n",
      "Validation: Epoch [10], Batch [442/938], Loss: 0.6465119123458862\n",
      "Validation: Epoch [10], Batch [443/938], Loss: 0.8358803391456604\n",
      "Validation: Epoch [10], Batch [444/938], Loss: 0.7370812892913818\n",
      "Validation: Epoch [10], Batch [445/938], Loss: 0.7072410583496094\n",
      "Validation: Epoch [10], Batch [446/938], Loss: 0.7622052431106567\n",
      "Validation: Epoch [10], Batch [447/938], Loss: 0.94807368516922\n",
      "Validation: Epoch [10], Batch [448/938], Loss: 0.7144167423248291\n",
      "Validation: Epoch [10], Batch [449/938], Loss: 0.7894084453582764\n",
      "Validation: Epoch [10], Batch [450/938], Loss: 0.8233779668807983\n",
      "Validation: Epoch [10], Batch [451/938], Loss: 0.9214679598808289\n",
      "Validation: Epoch [10], Batch [452/938], Loss: 0.7095207571983337\n",
      "Validation: Epoch [10], Batch [453/938], Loss: 0.6648250818252563\n",
      "Validation: Epoch [10], Batch [454/938], Loss: 0.8321729302406311\n",
      "Validation: Epoch [10], Batch [455/938], Loss: 0.6336036920547485\n",
      "Validation: Epoch [10], Batch [456/938], Loss: 0.8884320855140686\n",
      "Validation: Epoch [10], Batch [457/938], Loss: 0.9115335941314697\n",
      "Validation: Epoch [10], Batch [458/938], Loss: 0.5815397500991821\n",
      "Validation: Epoch [10], Batch [459/938], Loss: 1.1549255847930908\n",
      "Validation: Epoch [10], Batch [460/938], Loss: 0.9141451120376587\n",
      "Validation: Epoch [10], Batch [461/938], Loss: 0.7812158465385437\n",
      "Validation: Epoch [10], Batch [462/938], Loss: 0.770211935043335\n",
      "Validation: Epoch [10], Batch [463/938], Loss: 0.8059266209602356\n",
      "Validation: Epoch [10], Batch [464/938], Loss: 0.8721017241477966\n",
      "Validation: Epoch [10], Batch [465/938], Loss: 0.7893722057342529\n",
      "Validation: Epoch [10], Batch [466/938], Loss: 0.7378010153770447\n",
      "Validation: Epoch [10], Batch [467/938], Loss: 0.6361478567123413\n",
      "Validation: Epoch [10], Batch [468/938], Loss: 0.7121714353561401\n",
      "Validation: Epoch [10], Batch [469/938], Loss: 0.7027015686035156\n",
      "Validation: Epoch [10], Batch [470/938], Loss: 1.0235203504562378\n",
      "Validation: Epoch [10], Batch [471/938], Loss: 0.9475290775299072\n",
      "Validation: Epoch [10], Batch [472/938], Loss: 0.567522406578064\n",
      "Validation: Epoch [10], Batch [473/938], Loss: 0.6586055755615234\n",
      "Validation: Epoch [10], Batch [474/938], Loss: 0.7832182049751282\n",
      "Validation: Epoch [10], Batch [475/938], Loss: 0.7952272295951843\n",
      "Validation: Epoch [10], Batch [476/938], Loss: 0.5866405367851257\n",
      "Validation: Epoch [10], Batch [477/938], Loss: 0.917625904083252\n",
      "Validation: Epoch [10], Batch [478/938], Loss: 0.8420020937919617\n",
      "Validation: Epoch [10], Batch [479/938], Loss: 0.6487534046173096\n",
      "Validation: Epoch [10], Batch [480/938], Loss: 0.7789782881736755\n",
      "Validation: Epoch [10], Batch [481/938], Loss: 0.8467714190483093\n",
      "Validation: Epoch [10], Batch [482/938], Loss: 0.6730947494506836\n",
      "Validation: Epoch [10], Batch [483/938], Loss: 1.0760502815246582\n",
      "Validation: Epoch [10], Batch [484/938], Loss: 0.8329237103462219\n",
      "Validation: Epoch [10], Batch [485/938], Loss: 0.7355624437332153\n",
      "Validation: Epoch [10], Batch [486/938], Loss: 0.6788973808288574\n",
      "Validation: Epoch [10], Batch [487/938], Loss: 0.9319872856140137\n",
      "Validation: Epoch [10], Batch [488/938], Loss: 0.6805158853530884\n",
      "Validation: Epoch [10], Batch [489/938], Loss: 0.5836647152900696\n",
      "Validation: Epoch [10], Batch [490/938], Loss: 0.9198096394538879\n",
      "Validation: Epoch [10], Batch [491/938], Loss: 0.7984694242477417\n",
      "Validation: Epoch [10], Batch [492/938], Loss: 0.6361041069030762\n",
      "Validation: Epoch [10], Batch [493/938], Loss: 0.8633785843849182\n",
      "Validation: Epoch [10], Batch [494/938], Loss: 0.853131115436554\n",
      "Validation: Epoch [10], Batch [495/938], Loss: 0.8209626078605652\n",
      "Validation: Epoch [10], Batch [496/938], Loss: 0.8422759771347046\n",
      "Validation: Epoch [10], Batch [497/938], Loss: 0.9333846569061279\n",
      "Validation: Epoch [10], Batch [498/938], Loss: 0.789312481880188\n",
      "Validation: Epoch [10], Batch [499/938], Loss: 0.8356845378875732\n",
      "Validation: Epoch [10], Batch [500/938], Loss: 1.1539182662963867\n",
      "Validation: Epoch [10], Batch [501/938], Loss: 0.959009051322937\n",
      "Validation: Epoch [10], Batch [502/938], Loss: 0.7360873818397522\n",
      "Validation: Epoch [10], Batch [503/938], Loss: 1.069359540939331\n",
      "Validation: Epoch [10], Batch [504/938], Loss: 0.7372830510139465\n",
      "Validation: Epoch [10], Batch [505/938], Loss: 0.8541870713233948\n",
      "Validation: Epoch [10], Batch [506/938], Loss: 0.7731903791427612\n",
      "Validation: Epoch [10], Batch [507/938], Loss: 0.5715574026107788\n",
      "Validation: Epoch [10], Batch [508/938], Loss: 0.8397903442382812\n",
      "Validation: Epoch [10], Batch [509/938], Loss: 0.9775792360305786\n",
      "Validation: Epoch [10], Batch [510/938], Loss: 0.722122073173523\n",
      "Validation: Epoch [10], Batch [511/938], Loss: 0.6198366284370422\n",
      "Validation: Epoch [10], Batch [512/938], Loss: 0.8161658644676208\n",
      "Validation: Epoch [10], Batch [513/938], Loss: 0.6839457750320435\n",
      "Validation: Epoch [10], Batch [514/938], Loss: 0.6921757459640503\n",
      "Validation: Epoch [10], Batch [515/938], Loss: 0.7424343824386597\n",
      "Validation: Epoch [10], Batch [516/938], Loss: 0.7197190523147583\n",
      "Validation: Epoch [10], Batch [517/938], Loss: 0.7470271587371826\n",
      "Validation: Epoch [10], Batch [518/938], Loss: 0.8261829018592834\n",
      "Validation: Epoch [10], Batch [519/938], Loss: 0.6712900400161743\n",
      "Validation: Epoch [10], Batch [520/938], Loss: 0.8258628249168396\n",
      "Validation: Epoch [10], Batch [521/938], Loss: 0.7378039360046387\n",
      "Validation: Epoch [10], Batch [522/938], Loss: 0.7971863746643066\n",
      "Validation: Epoch [10], Batch [523/938], Loss: 0.6734898090362549\n",
      "Validation: Epoch [10], Batch [524/938], Loss: 0.6381353735923767\n",
      "Validation: Epoch [10], Batch [525/938], Loss: 0.7720036506652832\n",
      "Validation: Epoch [10], Batch [526/938], Loss: 0.8074905276298523\n",
      "Validation: Epoch [10], Batch [527/938], Loss: 0.6556838154792786\n",
      "Validation: Epoch [10], Batch [528/938], Loss: 0.5143768787384033\n",
      "Validation: Epoch [10], Batch [529/938], Loss: 0.612287163734436\n",
      "Validation: Epoch [10], Batch [530/938], Loss: 0.8526123762130737\n",
      "Validation: Epoch [10], Batch [531/938], Loss: 0.8471527099609375\n",
      "Validation: Epoch [10], Batch [532/938], Loss: 0.694972038269043\n",
      "Validation: Epoch [10], Batch [533/938], Loss: 0.6767640113830566\n",
      "Validation: Epoch [10], Batch [534/938], Loss: 1.008134126663208\n",
      "Validation: Epoch [10], Batch [535/938], Loss: 0.7404994964599609\n",
      "Validation: Epoch [10], Batch [536/938], Loss: 0.6700705289840698\n",
      "Validation: Epoch [10], Batch [537/938], Loss: 0.7623993158340454\n",
      "Validation: Epoch [10], Batch [538/938], Loss: 0.7328333854675293\n",
      "Validation: Epoch [10], Batch [539/938], Loss: 0.8565487861633301\n",
      "Validation: Epoch [10], Batch [540/938], Loss: 0.5193234086036682\n",
      "Validation: Epoch [10], Batch [541/938], Loss: 0.798318088054657\n",
      "Validation: Epoch [10], Batch [542/938], Loss: 0.8430811166763306\n",
      "Validation: Epoch [10], Batch [543/938], Loss: 1.0913832187652588\n",
      "Validation: Epoch [10], Batch [544/938], Loss: 0.6062569618225098\n",
      "Validation: Epoch [10], Batch [545/938], Loss: 0.876218318939209\n",
      "Validation: Epoch [10], Batch [546/938], Loss: 0.7160072326660156\n",
      "Validation: Epoch [10], Batch [547/938], Loss: 0.820207953453064\n",
      "Validation: Epoch [10], Batch [548/938], Loss: 0.693225085735321\n",
      "Validation: Epoch [10], Batch [549/938], Loss: 0.8289635181427002\n",
      "Validation: Epoch [10], Batch [550/938], Loss: 0.9298176765441895\n",
      "Validation: Epoch [10], Batch [551/938], Loss: 0.7313929796218872\n",
      "Validation: Epoch [10], Batch [552/938], Loss: 0.6688663959503174\n",
      "Validation: Epoch [10], Batch [553/938], Loss: 0.8233380913734436\n",
      "Validation: Epoch [10], Batch [554/938], Loss: 0.8687384128570557\n",
      "Validation: Epoch [10], Batch [555/938], Loss: 0.9642947316169739\n",
      "Validation: Epoch [10], Batch [556/938], Loss: 0.9280672669410706\n",
      "Validation: Epoch [10], Batch [557/938], Loss: 0.9283193349838257\n",
      "Validation: Epoch [10], Batch [558/938], Loss: 0.7662176489830017\n",
      "Validation: Epoch [10], Batch [559/938], Loss: 0.6792887449264526\n",
      "Validation: Epoch [10], Batch [560/938], Loss: 0.904155969619751\n",
      "Validation: Epoch [10], Batch [561/938], Loss: 0.7597814202308655\n",
      "Validation: Epoch [10], Batch [562/938], Loss: 0.8872218132019043\n",
      "Validation: Epoch [10], Batch [563/938], Loss: 0.8730758428573608\n",
      "Validation: Epoch [10], Batch [564/938], Loss: 0.7719571590423584\n",
      "Validation: Epoch [10], Batch [565/938], Loss: 1.1311959028244019\n",
      "Validation: Epoch [10], Batch [566/938], Loss: 0.5847132205963135\n",
      "Validation: Epoch [10], Batch [567/938], Loss: 0.6250159740447998\n",
      "Validation: Epoch [10], Batch [568/938], Loss: 0.7958401441574097\n",
      "Validation: Epoch [10], Batch [569/938], Loss: 1.1557226181030273\n",
      "Validation: Epoch [10], Batch [570/938], Loss: 1.0085556507110596\n",
      "Validation: Epoch [10], Batch [571/938], Loss: 0.5866856575012207\n",
      "Validation: Epoch [10], Batch [572/938], Loss: 0.8145929574966431\n",
      "Validation: Epoch [10], Batch [573/938], Loss: 0.897620439529419\n",
      "Validation: Epoch [10], Batch [574/938], Loss: 0.6810784339904785\n",
      "Validation: Epoch [10], Batch [575/938], Loss: 0.8156646490097046\n",
      "Validation: Epoch [10], Batch [576/938], Loss: 0.7573518753051758\n",
      "Validation: Epoch [10], Batch [577/938], Loss: 0.7618919610977173\n",
      "Validation: Epoch [10], Batch [578/938], Loss: 0.6705422401428223\n",
      "Validation: Epoch [10], Batch [579/938], Loss: 0.8900833129882812\n",
      "Validation: Epoch [10], Batch [580/938], Loss: 0.977814793586731\n",
      "Validation: Epoch [10], Batch [581/938], Loss: 0.9859243035316467\n",
      "Validation: Epoch [10], Batch [582/938], Loss: 0.6469221115112305\n",
      "Validation: Epoch [10], Batch [583/938], Loss: 0.715904712677002\n",
      "Validation: Epoch [10], Batch [584/938], Loss: 0.829240620136261\n",
      "Validation: Epoch [10], Batch [585/938], Loss: 0.7247557044029236\n",
      "Validation: Epoch [10], Batch [586/938], Loss: 0.9302042722702026\n",
      "Validation: Epoch [10], Batch [587/938], Loss: 0.8589019775390625\n",
      "Validation: Epoch [10], Batch [588/938], Loss: 0.5763038396835327\n",
      "Validation: Epoch [10], Batch [589/938], Loss: 0.66628098487854\n",
      "Validation: Epoch [10], Batch [590/938], Loss: 0.7356749773025513\n",
      "Validation: Epoch [10], Batch [591/938], Loss: 0.823581337928772\n",
      "Validation: Epoch [10], Batch [592/938], Loss: 0.8487357497215271\n",
      "Validation: Epoch [10], Batch [593/938], Loss: 0.8193992376327515\n",
      "Validation: Epoch [10], Batch [594/938], Loss: 0.8897647261619568\n",
      "Validation: Epoch [10], Batch [595/938], Loss: 0.796944260597229\n",
      "Validation: Epoch [10], Batch [596/938], Loss: 0.9586870670318604\n",
      "Validation: Epoch [10], Batch [597/938], Loss: 1.0755590200424194\n",
      "Validation: Epoch [10], Batch [598/938], Loss: 0.7985775470733643\n",
      "Validation: Epoch [10], Batch [599/938], Loss: 0.7327439188957214\n",
      "Validation: Epoch [10], Batch [600/938], Loss: 0.7172737121582031\n",
      "Validation: Epoch [10], Batch [601/938], Loss: 0.8909472823143005\n",
      "Validation: Epoch [10], Batch [602/938], Loss: 0.7239174842834473\n",
      "Validation: Epoch [10], Batch [603/938], Loss: 0.5922864079475403\n",
      "Validation: Epoch [10], Batch [604/938], Loss: 0.7923762798309326\n",
      "Validation: Epoch [10], Batch [605/938], Loss: 0.9308855533599854\n",
      "Validation: Epoch [10], Batch [606/938], Loss: 0.8043728470802307\n",
      "Validation: Epoch [10], Batch [607/938], Loss: 0.7227197885513306\n",
      "Validation: Epoch [10], Batch [608/938], Loss: 0.7728886604309082\n",
      "Validation: Epoch [10], Batch [609/938], Loss: 0.7652016878128052\n",
      "Validation: Epoch [10], Batch [610/938], Loss: 0.6800153255462646\n",
      "Validation: Epoch [10], Batch [611/938], Loss: 0.7303873300552368\n",
      "Validation: Epoch [10], Batch [612/938], Loss: 0.6510703563690186\n",
      "Validation: Epoch [10], Batch [613/938], Loss: 0.7110509276390076\n",
      "Validation: Epoch [10], Batch [614/938], Loss: 0.8845656514167786\n",
      "Validation: Epoch [10], Batch [615/938], Loss: 1.0211141109466553\n",
      "Validation: Epoch [10], Batch [616/938], Loss: 0.9286115765571594\n",
      "Validation: Epoch [10], Batch [617/938], Loss: 0.7930407524108887\n",
      "Validation: Epoch [10], Batch [618/938], Loss: 0.7351466417312622\n",
      "Validation: Epoch [10], Batch [619/938], Loss: 0.7865968942642212\n",
      "Validation: Epoch [10], Batch [620/938], Loss: 0.6303495168685913\n",
      "Validation: Epoch [10], Batch [621/938], Loss: 0.7103071808815002\n",
      "Validation: Epoch [10], Batch [622/938], Loss: 0.7847657203674316\n",
      "Validation: Epoch [10], Batch [623/938], Loss: 0.8128278851509094\n",
      "Validation: Epoch [10], Batch [624/938], Loss: 0.656383752822876\n",
      "Validation: Epoch [10], Batch [625/938], Loss: 0.6918373107910156\n",
      "Validation: Epoch [10], Batch [626/938], Loss: 0.8759812712669373\n",
      "Validation: Epoch [10], Batch [627/938], Loss: 0.6727107763290405\n",
      "Validation: Epoch [10], Batch [628/938], Loss: 0.7675571441650391\n",
      "Validation: Epoch [10], Batch [629/938], Loss: 1.0190362930297852\n",
      "Validation: Epoch [10], Batch [630/938], Loss: 0.7383114099502563\n",
      "Validation: Epoch [10], Batch [631/938], Loss: 0.7684487700462341\n",
      "Validation: Epoch [10], Batch [632/938], Loss: 0.6544936299324036\n",
      "Validation: Epoch [10], Batch [633/938], Loss: 0.6429030299186707\n",
      "Validation: Epoch [10], Batch [634/938], Loss: 0.8913325667381287\n",
      "Validation: Epoch [10], Batch [635/938], Loss: 0.6961032748222351\n",
      "Validation: Epoch [10], Batch [636/938], Loss: 0.7017741799354553\n",
      "Validation: Epoch [10], Batch [637/938], Loss: 0.6455047130584717\n",
      "Validation: Epoch [10], Batch [638/938], Loss: 0.7957264184951782\n",
      "Validation: Epoch [10], Batch [639/938], Loss: 0.7484062314033508\n",
      "Validation: Epoch [10], Batch [640/938], Loss: 0.7497709393501282\n",
      "Validation: Epoch [10], Batch [641/938], Loss: 0.7114937901496887\n",
      "Validation: Epoch [10], Batch [642/938], Loss: 0.8236340880393982\n",
      "Validation: Epoch [10], Batch [643/938], Loss: 0.8015430569648743\n",
      "Validation: Epoch [10], Batch [644/938], Loss: 0.6933794021606445\n",
      "Validation: Epoch [10], Batch [645/938], Loss: 0.5680598616600037\n",
      "Validation: Epoch [10], Batch [646/938], Loss: 0.7677260041236877\n",
      "Validation: Epoch [10], Batch [647/938], Loss: 0.742311418056488\n",
      "Validation: Epoch [10], Batch [648/938], Loss: 0.6268954277038574\n",
      "Validation: Epoch [10], Batch [649/938], Loss: 0.5748757719993591\n",
      "Validation: Epoch [10], Batch [650/938], Loss: 0.6099239587783813\n",
      "Validation: Epoch [10], Batch [651/938], Loss: 0.967452883720398\n",
      "Validation: Epoch [10], Batch [652/938], Loss: 0.6301198601722717\n",
      "Validation: Epoch [10], Batch [653/938], Loss: 0.5769663453102112\n",
      "Validation: Epoch [10], Batch [654/938], Loss: 0.6740650534629822\n",
      "Validation: Epoch [10], Batch [655/938], Loss: 0.9567063450813293\n",
      "Validation: Epoch [10], Batch [656/938], Loss: 0.7512715458869934\n",
      "Validation: Epoch [10], Batch [657/938], Loss: 1.059754490852356\n",
      "Validation: Epoch [10], Batch [658/938], Loss: 0.7600126266479492\n",
      "Validation: Epoch [10], Batch [659/938], Loss: 0.8311781287193298\n",
      "Validation: Epoch [10], Batch [660/938], Loss: 0.7339768409729004\n",
      "Validation: Epoch [10], Batch [661/938], Loss: 0.7224709987640381\n",
      "Validation: Epoch [10], Batch [662/938], Loss: 0.7816693782806396\n",
      "Validation: Epoch [10], Batch [663/938], Loss: 0.7208871841430664\n",
      "Validation: Epoch [10], Batch [664/938], Loss: 0.6520418524742126\n",
      "Validation: Epoch [10], Batch [665/938], Loss: 0.691922664642334\n",
      "Validation: Epoch [10], Batch [666/938], Loss: 0.748523473739624\n",
      "Validation: Epoch [10], Batch [667/938], Loss: 0.7857956886291504\n",
      "Validation: Epoch [10], Batch [668/938], Loss: 0.8581183552742004\n",
      "Validation: Epoch [10], Batch [669/938], Loss: 0.632510244846344\n",
      "Validation: Epoch [10], Batch [670/938], Loss: 0.7850836515426636\n",
      "Validation: Epoch [10], Batch [671/938], Loss: 0.7169902920722961\n",
      "Validation: Epoch [10], Batch [672/938], Loss: 0.7302833795547485\n",
      "Validation: Epoch [10], Batch [673/938], Loss: 0.7300291061401367\n",
      "Validation: Epoch [10], Batch [674/938], Loss: 0.8876912593841553\n",
      "Validation: Epoch [10], Batch [675/938], Loss: 0.808444619178772\n",
      "Validation: Epoch [10], Batch [676/938], Loss: 0.8405914306640625\n",
      "Validation: Epoch [10], Batch [677/938], Loss: 0.6165767908096313\n",
      "Validation: Epoch [10], Batch [678/938], Loss: 0.9354017376899719\n",
      "Validation: Epoch [10], Batch [679/938], Loss: 0.7437402606010437\n",
      "Validation: Epoch [10], Batch [680/938], Loss: 0.6715812683105469\n",
      "Validation: Epoch [10], Batch [681/938], Loss: 0.8042407035827637\n",
      "Validation: Epoch [10], Batch [682/938], Loss: 0.8111634254455566\n",
      "Validation: Epoch [10], Batch [683/938], Loss: 0.9644864201545715\n",
      "Validation: Epoch [10], Batch [684/938], Loss: 0.9562907218933105\n",
      "Validation: Epoch [10], Batch [685/938], Loss: 0.8285146951675415\n",
      "Validation: Epoch [10], Batch [686/938], Loss: 0.7099682092666626\n",
      "Validation: Epoch [10], Batch [687/938], Loss: 1.059322476387024\n",
      "Validation: Epoch [10], Batch [688/938], Loss: 0.8350483179092407\n",
      "Validation: Epoch [10], Batch [689/938], Loss: 0.861380398273468\n",
      "Validation: Epoch [10], Batch [690/938], Loss: 0.6104985475540161\n",
      "Validation: Epoch [10], Batch [691/938], Loss: 0.8585853576660156\n",
      "Validation: Epoch [10], Batch [692/938], Loss: 0.9450467824935913\n",
      "Validation: Epoch [10], Batch [693/938], Loss: 0.6555280089378357\n",
      "Validation: Epoch [10], Batch [694/938], Loss: 0.6788669228553772\n",
      "Validation: Epoch [10], Batch [695/938], Loss: 0.626380443572998\n",
      "Validation: Epoch [10], Batch [696/938], Loss: 0.655375063419342\n",
      "Validation: Epoch [10], Batch [697/938], Loss: 0.8884886503219604\n",
      "Validation: Epoch [10], Batch [698/938], Loss: 0.6948456168174744\n",
      "Validation: Epoch [10], Batch [699/938], Loss: 0.7859885692596436\n",
      "Validation: Epoch [10], Batch [700/938], Loss: 0.7448064088821411\n",
      "Validation: Epoch [10], Batch [701/938], Loss: 0.7265385985374451\n",
      "Validation: Epoch [10], Batch [702/938], Loss: 0.9561281204223633\n",
      "Validation: Epoch [10], Batch [703/938], Loss: 0.6185104846954346\n",
      "Validation: Epoch [10], Batch [704/938], Loss: 0.7258122563362122\n",
      "Validation: Epoch [10], Batch [705/938], Loss: 0.8855116963386536\n",
      "Validation: Epoch [10], Batch [706/938], Loss: 0.8122833371162415\n",
      "Validation: Epoch [10], Batch [707/938], Loss: 0.9304290413856506\n",
      "Validation: Epoch [10], Batch [708/938], Loss: 0.7653640508651733\n",
      "Validation: Epoch [10], Batch [709/938], Loss: 0.7283810377120972\n",
      "Validation: Epoch [10], Batch [710/938], Loss: 0.6446418166160583\n",
      "Validation: Epoch [10], Batch [711/938], Loss: 0.7671095132827759\n",
      "Validation: Epoch [10], Batch [712/938], Loss: 0.6469948887825012\n",
      "Validation: Epoch [10], Batch [713/938], Loss: 1.0469789505004883\n",
      "Validation: Epoch [10], Batch [714/938], Loss: 0.5544150471687317\n",
      "Validation: Epoch [10], Batch [715/938], Loss: 0.8963974714279175\n",
      "Validation: Epoch [10], Batch [716/938], Loss: 0.8087924122810364\n",
      "Validation: Epoch [10], Batch [717/938], Loss: 0.5341565012931824\n",
      "Validation: Epoch [10], Batch [718/938], Loss: 0.5525442957878113\n",
      "Validation: Epoch [10], Batch [719/938], Loss: 0.7144573926925659\n",
      "Validation: Epoch [10], Batch [720/938], Loss: 0.724082887172699\n",
      "Validation: Epoch [10], Batch [721/938], Loss: 0.8794390559196472\n",
      "Validation: Epoch [10], Batch [722/938], Loss: 0.7031451463699341\n",
      "Validation: Epoch [10], Batch [723/938], Loss: 0.8952116966247559\n",
      "Validation: Epoch [10], Batch [724/938], Loss: 0.7261835336685181\n",
      "Validation: Epoch [10], Batch [725/938], Loss: 0.9518691301345825\n",
      "Validation: Epoch [10], Batch [726/938], Loss: 0.7428547143936157\n",
      "Validation: Epoch [10], Batch [727/938], Loss: 0.584850549697876\n",
      "Validation: Epoch [10], Batch [728/938], Loss: 0.6685644388198853\n",
      "Validation: Epoch [10], Batch [729/938], Loss: 0.7184953093528748\n",
      "Validation: Epoch [10], Batch [730/938], Loss: 0.7886800169944763\n",
      "Validation: Epoch [10], Batch [731/938], Loss: 0.8011242151260376\n",
      "Validation: Epoch [10], Batch [732/938], Loss: 0.6839780807495117\n",
      "Validation: Epoch [10], Batch [733/938], Loss: 1.0513633489608765\n",
      "Validation: Epoch [10], Batch [734/938], Loss: 0.6915990114212036\n",
      "Validation: Epoch [10], Batch [735/938], Loss: 0.7214608788490295\n",
      "Validation: Epoch [10], Batch [736/938], Loss: 0.7361177206039429\n",
      "Validation: Epoch [10], Batch [737/938], Loss: 0.8749116659164429\n",
      "Validation: Epoch [10], Batch [738/938], Loss: 0.6677118539810181\n",
      "Validation: Epoch [10], Batch [739/938], Loss: 0.7325218915939331\n",
      "Validation: Epoch [10], Batch [740/938], Loss: 1.0134081840515137\n",
      "Validation: Epoch [10], Batch [741/938], Loss: 0.7312668561935425\n",
      "Validation: Epoch [10], Batch [742/938], Loss: 0.7273118495941162\n",
      "Validation: Epoch [10], Batch [743/938], Loss: 0.7262434959411621\n",
      "Validation: Epoch [10], Batch [744/938], Loss: 0.8083464503288269\n",
      "Validation: Epoch [10], Batch [745/938], Loss: 0.7131589651107788\n",
      "Validation: Epoch [10], Batch [746/938], Loss: 0.6171610951423645\n",
      "Validation: Epoch [10], Batch [747/938], Loss: 0.6775051355361938\n",
      "Validation: Epoch [10], Batch [748/938], Loss: 0.709822416305542\n",
      "Validation: Epoch [10], Batch [749/938], Loss: 0.7537816762924194\n",
      "Validation: Epoch [10], Batch [750/938], Loss: 0.8839250802993774\n",
      "Validation: Epoch [10], Batch [751/938], Loss: 0.7868732213973999\n",
      "Validation: Epoch [10], Batch [752/938], Loss: 0.6673249006271362\n",
      "Validation: Epoch [10], Batch [753/938], Loss: 0.5647940635681152\n",
      "Validation: Epoch [10], Batch [754/938], Loss: 0.5992482304573059\n",
      "Validation: Epoch [10], Batch [755/938], Loss: 0.7185165286064148\n",
      "Validation: Epoch [10], Batch [756/938], Loss: 0.9134833812713623\n",
      "Validation: Epoch [10], Batch [757/938], Loss: 0.7593398094177246\n",
      "Validation: Epoch [10], Batch [758/938], Loss: 0.9368008375167847\n",
      "Validation: Epoch [10], Batch [759/938], Loss: 0.7803785800933838\n",
      "Validation: Epoch [10], Batch [760/938], Loss: 0.6334807872772217\n",
      "Validation: Epoch [10], Batch [761/938], Loss: 0.7926846742630005\n",
      "Validation: Epoch [10], Batch [762/938], Loss: 0.8395028710365295\n",
      "Validation: Epoch [10], Batch [763/938], Loss: 0.8180680274963379\n",
      "Validation: Epoch [10], Batch [764/938], Loss: 0.571006178855896\n",
      "Validation: Epoch [10], Batch [765/938], Loss: 0.9420135021209717\n",
      "Validation: Epoch [10], Batch [766/938], Loss: 0.9244722723960876\n",
      "Validation: Epoch [10], Batch [767/938], Loss: 0.8509793281555176\n",
      "Validation: Epoch [10], Batch [768/938], Loss: 0.8654057383537292\n",
      "Validation: Epoch [10], Batch [769/938], Loss: 0.6801115274429321\n",
      "Validation: Epoch [10], Batch [770/938], Loss: 0.7392524480819702\n",
      "Validation: Epoch [10], Batch [771/938], Loss: 0.8623108863830566\n",
      "Validation: Epoch [10], Batch [772/938], Loss: 0.682732880115509\n",
      "Validation: Epoch [10], Batch [773/938], Loss: 0.7016657590866089\n",
      "Validation: Epoch [10], Batch [774/938], Loss: 0.7605446577072144\n",
      "Validation: Epoch [10], Batch [775/938], Loss: 0.7455742359161377\n",
      "Validation: Epoch [10], Batch [776/938], Loss: 0.7624648213386536\n",
      "Validation: Epoch [10], Batch [777/938], Loss: 0.6254572868347168\n",
      "Validation: Epoch [10], Batch [778/938], Loss: 0.7826787233352661\n",
      "Validation: Epoch [10], Batch [779/938], Loss: 0.752536416053772\n",
      "Validation: Epoch [10], Batch [780/938], Loss: 0.593886137008667\n",
      "Validation: Epoch [10], Batch [781/938], Loss: 1.0321375131607056\n",
      "Validation: Epoch [10], Batch [782/938], Loss: 0.7970102429389954\n",
      "Validation: Epoch [10], Batch [783/938], Loss: 0.8210418820381165\n",
      "Validation: Epoch [10], Batch [784/938], Loss: 0.8181524276733398\n",
      "Validation: Epoch [10], Batch [785/938], Loss: 0.6243517994880676\n",
      "Validation: Epoch [10], Batch [786/938], Loss: 0.7000916004180908\n",
      "Validation: Epoch [10], Batch [787/938], Loss: 0.8817785978317261\n",
      "Validation: Epoch [10], Batch [788/938], Loss: 0.7100570797920227\n",
      "Validation: Epoch [10], Batch [789/938], Loss: 0.7651692032814026\n",
      "Validation: Epoch [10], Batch [790/938], Loss: 0.8251595497131348\n",
      "Validation: Epoch [10], Batch [791/938], Loss: 0.5053992867469788\n",
      "Validation: Epoch [10], Batch [792/938], Loss: 0.6470236778259277\n",
      "Validation: Epoch [10], Batch [793/938], Loss: 0.6344749927520752\n",
      "Validation: Epoch [10], Batch [794/938], Loss: 0.6947319507598877\n",
      "Validation: Epoch [10], Batch [795/938], Loss: 0.9006197452545166\n",
      "Validation: Epoch [10], Batch [796/938], Loss: 0.869888424873352\n",
      "Validation: Epoch [10], Batch [797/938], Loss: 0.9817810654640198\n",
      "Validation: Epoch [10], Batch [798/938], Loss: 0.600670576095581\n",
      "Validation: Epoch [10], Batch [799/938], Loss: 0.9116465449333191\n",
      "Validation: Epoch [10], Batch [800/938], Loss: 0.7525258660316467\n",
      "Validation: Epoch [10], Batch [801/938], Loss: 0.6481901407241821\n",
      "Validation: Epoch [10], Batch [802/938], Loss: 0.8396488428115845\n",
      "Validation: Epoch [10], Batch [803/938], Loss: 0.7371018528938293\n",
      "Validation: Epoch [10], Batch [804/938], Loss: 0.7790275812149048\n",
      "Validation: Epoch [10], Batch [805/938], Loss: 0.7452452182769775\n",
      "Validation: Epoch [10], Batch [806/938], Loss: 0.7730089426040649\n",
      "Validation: Epoch [10], Batch [807/938], Loss: 0.8902846574783325\n",
      "Validation: Epoch [10], Batch [808/938], Loss: 0.6452946662902832\n",
      "Validation: Epoch [10], Batch [809/938], Loss: 0.7368199825286865\n",
      "Validation: Epoch [10], Batch [810/938], Loss: 0.8574535250663757\n",
      "Validation: Epoch [10], Batch [811/938], Loss: 0.7295889854431152\n",
      "Validation: Epoch [10], Batch [812/938], Loss: 0.768894612789154\n",
      "Validation: Epoch [10], Batch [813/938], Loss: 0.9831751585006714\n",
      "Validation: Epoch [10], Batch [814/938], Loss: 0.9990108609199524\n",
      "Validation: Epoch [10], Batch [815/938], Loss: 0.7538010478019714\n",
      "Validation: Epoch [10], Batch [816/938], Loss: 0.9321197271347046\n",
      "Validation: Epoch [10], Batch [817/938], Loss: 0.6155124306678772\n",
      "Validation: Epoch [10], Batch [818/938], Loss: 0.5594737529754639\n",
      "Validation: Epoch [10], Batch [819/938], Loss: 1.0418764352798462\n",
      "Validation: Epoch [10], Batch [820/938], Loss: 1.0176682472229004\n",
      "Validation: Epoch [10], Batch [821/938], Loss: 0.5291088819503784\n",
      "Validation: Epoch [10], Batch [822/938], Loss: 0.705917239189148\n",
      "Validation: Epoch [10], Batch [823/938], Loss: 0.8965569734573364\n",
      "Validation: Epoch [10], Batch [824/938], Loss: 0.9670531749725342\n",
      "Validation: Epoch [10], Batch [825/938], Loss: 0.6601026058197021\n",
      "Validation: Epoch [10], Batch [826/938], Loss: 0.772054135799408\n",
      "Validation: Epoch [10], Batch [827/938], Loss: 0.7890430688858032\n",
      "Validation: Epoch [10], Batch [828/938], Loss: 0.8545413017272949\n",
      "Validation: Epoch [10], Batch [829/938], Loss: 0.7533828616142273\n",
      "Validation: Epoch [10], Batch [830/938], Loss: 0.46406111121177673\n",
      "Validation: Epoch [10], Batch [831/938], Loss: 0.8145819902420044\n",
      "Validation: Epoch [10], Batch [832/938], Loss: 0.7689923644065857\n",
      "Validation: Epoch [10], Batch [833/938], Loss: 0.674713671207428\n",
      "Validation: Epoch [10], Batch [834/938], Loss: 1.0174256563186646\n",
      "Validation: Epoch [10], Batch [835/938], Loss: 0.6507699489593506\n",
      "Validation: Epoch [10], Batch [836/938], Loss: 0.8864821195602417\n",
      "Validation: Epoch [10], Batch [837/938], Loss: 0.5330609083175659\n",
      "Validation: Epoch [10], Batch [838/938], Loss: 1.0689845085144043\n",
      "Validation: Epoch [10], Batch [839/938], Loss: 0.7929534912109375\n",
      "Validation: Epoch [10], Batch [840/938], Loss: 1.159488320350647\n",
      "Validation: Epoch [10], Batch [841/938], Loss: 0.863652229309082\n",
      "Validation: Epoch [10], Batch [842/938], Loss: 0.5644345283508301\n",
      "Validation: Epoch [10], Batch [843/938], Loss: 0.9147661924362183\n",
      "Validation: Epoch [10], Batch [844/938], Loss: 1.1672075986862183\n",
      "Validation: Epoch [10], Batch [845/938], Loss: 0.8309013843536377\n",
      "Validation: Epoch [10], Batch [846/938], Loss: 0.7481648325920105\n",
      "Validation: Epoch [10], Batch [847/938], Loss: 0.741889238357544\n",
      "Validation: Epoch [10], Batch [848/938], Loss: 0.8014428019523621\n",
      "Validation: Epoch [10], Batch [849/938], Loss: 0.6951025724411011\n",
      "Validation: Epoch [10], Batch [850/938], Loss: 0.7731183767318726\n",
      "Validation: Epoch [10], Batch [851/938], Loss: 0.8446455001831055\n",
      "Validation: Epoch [10], Batch [852/938], Loss: 0.7052673697471619\n",
      "Validation: Epoch [10], Batch [853/938], Loss: 0.645038366317749\n",
      "Validation: Epoch [10], Batch [854/938], Loss: 0.7345343828201294\n",
      "Validation: Epoch [10], Batch [855/938], Loss: 0.7433573007583618\n",
      "Validation: Epoch [10], Batch [856/938], Loss: 0.8691773414611816\n",
      "Validation: Epoch [10], Batch [857/938], Loss: 0.8711840510368347\n",
      "Validation: Epoch [10], Batch [858/938], Loss: 0.7803722620010376\n",
      "Validation: Epoch [10], Batch [859/938], Loss: 0.6895278096199036\n",
      "Validation: Epoch [10], Batch [860/938], Loss: 0.9324098825454712\n",
      "Validation: Epoch [10], Batch [861/938], Loss: 1.010988473892212\n",
      "Validation: Epoch [10], Batch [862/938], Loss: 0.6282822489738464\n",
      "Validation: Epoch [10], Batch [863/938], Loss: 0.6951295733451843\n",
      "Validation: Epoch [10], Batch [864/938], Loss: 0.819278359413147\n",
      "Validation: Epoch [10], Batch [865/938], Loss: 0.63929682970047\n",
      "Validation: Epoch [10], Batch [866/938], Loss: 0.7277275919914246\n",
      "Validation: Epoch [10], Batch [867/938], Loss: 0.528494119644165\n",
      "Validation: Epoch [10], Batch [868/938], Loss: 0.7021985054016113\n",
      "Validation: Epoch [10], Batch [869/938], Loss: 0.7677149772644043\n",
      "Validation: Epoch [10], Batch [870/938], Loss: 1.061706304550171\n",
      "Validation: Epoch [10], Batch [871/938], Loss: 0.9514120221138\n",
      "Validation: Epoch [10], Batch [872/938], Loss: 0.7313404679298401\n",
      "Validation: Epoch [10], Batch [873/938], Loss: 0.6053773760795593\n",
      "Validation: Epoch [10], Batch [874/938], Loss: 0.556389570236206\n",
      "Validation: Epoch [10], Batch [875/938], Loss: 0.9386284351348877\n",
      "Validation: Epoch [10], Batch [876/938], Loss: 0.7321889996528625\n",
      "Validation: Epoch [10], Batch [877/938], Loss: 0.8171578049659729\n",
      "Validation: Epoch [10], Batch [878/938], Loss: 0.8457450866699219\n",
      "Validation: Epoch [10], Batch [879/938], Loss: 0.7912098169326782\n",
      "Validation: Epoch [10], Batch [880/938], Loss: 0.8422229886054993\n",
      "Validation: Epoch [10], Batch [881/938], Loss: 1.0776169300079346\n",
      "Validation: Epoch [10], Batch [882/938], Loss: 0.6409728527069092\n",
      "Validation: Epoch [10], Batch [883/938], Loss: 0.8674144744873047\n",
      "Validation: Epoch [10], Batch [884/938], Loss: 0.8036941885948181\n",
      "Validation: Epoch [10], Batch [885/938], Loss: 1.055893898010254\n",
      "Validation: Epoch [10], Batch [886/938], Loss: 0.7470911741256714\n",
      "Validation: Epoch [10], Batch [887/938], Loss: 0.7040592432022095\n",
      "Validation: Epoch [10], Batch [888/938], Loss: 0.8848588466644287\n",
      "Validation: Epoch [10], Batch [889/938], Loss: 0.7074223160743713\n",
      "Validation: Epoch [10], Batch [890/938], Loss: 0.6926286220550537\n",
      "Validation: Epoch [10], Batch [891/938], Loss: 0.8069421052932739\n",
      "Validation: Epoch [10], Batch [892/938], Loss: 0.7722570300102234\n",
      "Validation: Epoch [10], Batch [893/938], Loss: 0.9920139908790588\n",
      "Validation: Epoch [10], Batch [894/938], Loss: 0.8481057286262512\n",
      "Validation: Epoch [10], Batch [895/938], Loss: 0.8751856684684753\n",
      "Validation: Epoch [10], Batch [896/938], Loss: 0.7979953289031982\n",
      "Validation: Epoch [10], Batch [897/938], Loss: 0.7341250777244568\n",
      "Validation: Epoch [10], Batch [898/938], Loss: 0.7606212496757507\n",
      "Validation: Epoch [10], Batch [899/938], Loss: 0.9346212148666382\n",
      "Validation: Epoch [10], Batch [900/938], Loss: 0.7221208810806274\n",
      "Validation: Epoch [10], Batch [901/938], Loss: 0.782437801361084\n",
      "Validation: Epoch [10], Batch [902/938], Loss: 0.7846603393554688\n",
      "Validation: Epoch [10], Batch [903/938], Loss: 0.556917130947113\n",
      "Validation: Epoch [10], Batch [904/938], Loss: 0.8863523602485657\n",
      "Validation: Epoch [10], Batch [905/938], Loss: 0.7226969599723816\n",
      "Validation: Epoch [10], Batch [906/938], Loss: 0.5625516176223755\n",
      "Validation: Epoch [10], Batch [907/938], Loss: 0.7818360328674316\n",
      "Validation: Epoch [10], Batch [908/938], Loss: 1.175970196723938\n",
      "Validation: Epoch [10], Batch [909/938], Loss: 0.8928760886192322\n",
      "Validation: Epoch [10], Batch [910/938], Loss: 0.6467598080635071\n",
      "Validation: Epoch [10], Batch [911/938], Loss: 0.6123140454292297\n",
      "Validation: Epoch [10], Batch [912/938], Loss: 0.9809643030166626\n",
      "Validation: Epoch [10], Batch [913/938], Loss: 0.9269152283668518\n",
      "Validation: Epoch [10], Batch [914/938], Loss: 0.9298675060272217\n",
      "Validation: Epoch [10], Batch [915/938], Loss: 0.8080166578292847\n",
      "Validation: Epoch [10], Batch [916/938], Loss: 0.9471663236618042\n",
      "Validation: Epoch [10], Batch [917/938], Loss: 0.788755476474762\n",
      "Validation: Epoch [10], Batch [918/938], Loss: 0.7589113712310791\n",
      "Validation: Epoch [10], Batch [919/938], Loss: 0.8304915428161621\n",
      "Validation: Epoch [10], Batch [920/938], Loss: 0.5858121514320374\n",
      "Validation: Epoch [10], Batch [921/938], Loss: 0.5751634836196899\n",
      "Validation: Epoch [10], Batch [922/938], Loss: 0.8943240642547607\n",
      "Validation: Epoch [10], Batch [923/938], Loss: 0.8069519996643066\n",
      "Validation: Epoch [10], Batch [924/938], Loss: 0.8724266290664673\n",
      "Validation: Epoch [10], Batch [925/938], Loss: 0.556748628616333\n",
      "Validation: Epoch [10], Batch [926/938], Loss: 0.9123801589012146\n",
      "Validation: Epoch [10], Batch [927/938], Loss: 0.7847262024879456\n",
      "Validation: Epoch [10], Batch [928/938], Loss: 0.937096357345581\n",
      "Validation: Epoch [10], Batch [929/938], Loss: 0.7001404166221619\n",
      "Validation: Epoch [10], Batch [930/938], Loss: 0.6002530455589294\n",
      "Validation: Epoch [10], Batch [931/938], Loss: 0.7330220937728882\n",
      "Validation: Epoch [10], Batch [932/938], Loss: 0.818075954914093\n",
      "Validation: Epoch [10], Batch [933/938], Loss: 0.6205776929855347\n",
      "Validation: Epoch [10], Batch [934/938], Loss: 0.7083257436752319\n",
      "Validation: Epoch [10], Batch [935/938], Loss: 0.8500197529792786\n",
      "Validation: Epoch [10], Batch [936/938], Loss: 0.9474322199821472\n",
      "Validation: Epoch [10], Batch [937/938], Loss: 0.7925224304199219\n",
      "Validation: Epoch [10], Batch [938/938], Loss: 0.5789958238601685\n",
      "Accuracy of test set: 0.74555\n",
      "Train: Epoch [11], Batch [1/938], Loss: 0.7103471755981445\n",
      "Train: Epoch [11], Batch [2/938], Loss: 0.8408180475234985\n",
      "Train: Epoch [11], Batch [3/938], Loss: 0.8788774013519287\n",
      "Train: Epoch [11], Batch [4/938], Loss: 0.801176130771637\n",
      "Train: Epoch [11], Batch [5/938], Loss: 0.5978437066078186\n",
      "Train: Epoch [11], Batch [6/938], Loss: 1.1104623079299927\n",
      "Train: Epoch [11], Batch [7/938], Loss: 0.8803166747093201\n",
      "Train: Epoch [11], Batch [8/938], Loss: 0.829670250415802\n",
      "Train: Epoch [11], Batch [9/938], Loss: 0.6573363542556763\n",
      "Train: Epoch [11], Batch [10/938], Loss: 0.6690006256103516\n",
      "Train: Epoch [11], Batch [11/938], Loss: 0.6080381274223328\n",
      "Train: Epoch [11], Batch [12/938], Loss: 0.7548826932907104\n",
      "Train: Epoch [11], Batch [13/938], Loss: 0.7682724595069885\n",
      "Train: Epoch [11], Batch [14/938], Loss: 0.9701766967773438\n",
      "Train: Epoch [11], Batch [15/938], Loss: 0.681351900100708\n",
      "Train: Epoch [11], Batch [16/938], Loss: 0.7186232209205627\n",
      "Train: Epoch [11], Batch [17/938], Loss: 0.8803476095199585\n",
      "Train: Epoch [11], Batch [18/938], Loss: 0.8207336664199829\n",
      "Train: Epoch [11], Batch [19/938], Loss: 0.7867672443389893\n",
      "Train: Epoch [11], Batch [20/938], Loss: 0.6288728713989258\n",
      "Train: Epoch [11], Batch [21/938], Loss: 0.7103546261787415\n",
      "Train: Epoch [11], Batch [22/938], Loss: 0.7401328086853027\n",
      "Train: Epoch [11], Batch [23/938], Loss: 0.7718192934989929\n",
      "Train: Epoch [11], Batch [24/938], Loss: 0.6639702320098877\n",
      "Train: Epoch [11], Batch [25/938], Loss: 0.6379637122154236\n",
      "Train: Epoch [11], Batch [26/938], Loss: 0.8366637229919434\n",
      "Train: Epoch [11], Batch [27/938], Loss: 0.6442314386367798\n",
      "Train: Epoch [11], Batch [28/938], Loss: 0.6211665868759155\n",
      "Train: Epoch [11], Batch [29/938], Loss: 0.8416118025779724\n",
      "Train: Epoch [11], Batch [30/938], Loss: 0.9794772863388062\n",
      "Train: Epoch [11], Batch [31/938], Loss: 0.8343228101730347\n",
      "Train: Epoch [11], Batch [32/938], Loss: 0.5778505802154541\n",
      "Train: Epoch [11], Batch [33/938], Loss: 0.6813594102859497\n",
      "Train: Epoch [11], Batch [34/938], Loss: 0.9088326096534729\n",
      "Train: Epoch [11], Batch [35/938], Loss: 0.8692696690559387\n",
      "Train: Epoch [11], Batch [36/938], Loss: 0.7693009972572327\n",
      "Train: Epoch [11], Batch [37/938], Loss: 0.7315031886100769\n",
      "Train: Epoch [11], Batch [38/938], Loss: 0.6674247980117798\n",
      "Train: Epoch [11], Batch [39/938], Loss: 0.6888535022735596\n",
      "Train: Epoch [11], Batch [40/938], Loss: 0.8258856534957886\n",
      "Train: Epoch [11], Batch [41/938], Loss: 0.5991771221160889\n",
      "Train: Epoch [11], Batch [42/938], Loss: 0.7994481325149536\n",
      "Train: Epoch [11], Batch [43/938], Loss: 0.9079785943031311\n",
      "Train: Epoch [11], Batch [44/938], Loss: 0.5058009028434753\n",
      "Train: Epoch [11], Batch [45/938], Loss: 0.5549815893173218\n",
      "Train: Epoch [11], Batch [46/938], Loss: 0.7655935883522034\n",
      "Train: Epoch [11], Batch [47/938], Loss: 0.9020015001296997\n",
      "Train: Epoch [11], Batch [48/938], Loss: 0.716086208820343\n",
      "Train: Epoch [11], Batch [49/938], Loss: 0.5345749258995056\n",
      "Train: Epoch [11], Batch [50/938], Loss: 0.788590669631958\n",
      "Train: Epoch [11], Batch [51/938], Loss: 0.74509596824646\n",
      "Train: Epoch [11], Batch [52/938], Loss: 0.8235336542129517\n",
      "Train: Epoch [11], Batch [53/938], Loss: 0.8147329092025757\n",
      "Train: Epoch [11], Batch [54/938], Loss: 0.7920617461204529\n",
      "Train: Epoch [11], Batch [55/938], Loss: 0.7137489914894104\n",
      "Train: Epoch [11], Batch [56/938], Loss: 0.6884958744049072\n",
      "Train: Epoch [11], Batch [57/938], Loss: 0.8501518964767456\n",
      "Train: Epoch [11], Batch [58/938], Loss: 0.6911343336105347\n",
      "Train: Epoch [11], Batch [59/938], Loss: 0.7679530382156372\n",
      "Train: Epoch [11], Batch [60/938], Loss: 0.6233944296836853\n",
      "Train: Epoch [11], Batch [61/938], Loss: 0.7643470764160156\n",
      "Train: Epoch [11], Batch [62/938], Loss: 0.7239614129066467\n",
      "Train: Epoch [11], Batch [63/938], Loss: 0.5713385343551636\n",
      "Train: Epoch [11], Batch [64/938], Loss: 0.6769759654998779\n",
      "Train: Epoch [11], Batch [65/938], Loss: 0.6228720545768738\n",
      "Train: Epoch [11], Batch [66/938], Loss: 0.7607833743095398\n",
      "Train: Epoch [11], Batch [67/938], Loss: 0.6886507272720337\n",
      "Train: Epoch [11], Batch [68/938], Loss: 0.6795716285705566\n",
      "Train: Epoch [11], Batch [69/938], Loss: 0.7708049416542053\n",
      "Train: Epoch [11], Batch [70/938], Loss: 0.751991868019104\n",
      "Train: Epoch [11], Batch [71/938], Loss: 0.7999618053436279\n",
      "Train: Epoch [11], Batch [72/938], Loss: 0.890080988407135\n",
      "Train: Epoch [11], Batch [73/938], Loss: 0.8950513601303101\n",
      "Train: Epoch [11], Batch [74/938], Loss: 1.0438151359558105\n",
      "Train: Epoch [11], Batch [75/938], Loss: 0.8343332409858704\n",
      "Train: Epoch [11], Batch [76/938], Loss: 0.7023305296897888\n",
      "Train: Epoch [11], Batch [77/938], Loss: 0.7923271059989929\n",
      "Train: Epoch [11], Batch [78/938], Loss: 0.8448975086212158\n",
      "Train: Epoch [11], Batch [79/938], Loss: 0.8608349561691284\n",
      "Train: Epoch [11], Batch [80/938], Loss: 0.5623324513435364\n",
      "Train: Epoch [11], Batch [81/938], Loss: 0.7051298022270203\n",
      "Train: Epoch [11], Batch [82/938], Loss: 0.7217317223548889\n",
      "Train: Epoch [11], Batch [83/938], Loss: 0.6779532432556152\n",
      "Train: Epoch [11], Batch [84/938], Loss: 0.6636788845062256\n",
      "Train: Epoch [11], Batch [85/938], Loss: 0.9132814407348633\n",
      "Train: Epoch [11], Batch [86/938], Loss: 0.8611829876899719\n",
      "Train: Epoch [11], Batch [87/938], Loss: 0.7984676361083984\n",
      "Train: Epoch [11], Batch [88/938], Loss: 0.789191722869873\n",
      "Train: Epoch [11], Batch [89/938], Loss: 0.7696376442909241\n",
      "Train: Epoch [11], Batch [90/938], Loss: 0.5711244344711304\n",
      "Train: Epoch [11], Batch [91/938], Loss: 1.0589590072631836\n",
      "Train: Epoch [11], Batch [92/938], Loss: 0.556023895740509\n",
      "Train: Epoch [11], Batch [93/938], Loss: 0.589451789855957\n",
      "Train: Epoch [11], Batch [94/938], Loss: 0.486677348613739\n",
      "Train: Epoch [11], Batch [95/938], Loss: 0.8223869800567627\n",
      "Train: Epoch [11], Batch [96/938], Loss: 0.8049623370170593\n",
      "Train: Epoch [11], Batch [97/938], Loss: 0.9324010014533997\n",
      "Train: Epoch [11], Batch [98/938], Loss: 0.7240921258926392\n",
      "Train: Epoch [11], Batch [99/938], Loss: 0.49624085426330566\n",
      "Train: Epoch [11], Batch [100/938], Loss: 0.8426055908203125\n",
      "Train: Epoch [11], Batch [101/938], Loss: 0.5233806371688843\n",
      "Train: Epoch [11], Batch [102/938], Loss: 0.6281511187553406\n",
      "Train: Epoch [11], Batch [103/938], Loss: 0.7374587655067444\n",
      "Train: Epoch [11], Batch [104/938], Loss: 0.7310122847557068\n",
      "Train: Epoch [11], Batch [105/938], Loss: 0.9563426375389099\n",
      "Train: Epoch [11], Batch [106/938], Loss: 0.36630457639694214\n",
      "Train: Epoch [11], Batch [107/938], Loss: 0.8808769583702087\n",
      "Train: Epoch [11], Batch [108/938], Loss: 0.8213342428207397\n",
      "Train: Epoch [11], Batch [109/938], Loss: 0.9242241382598877\n",
      "Train: Epoch [11], Batch [110/938], Loss: 0.7569422125816345\n",
      "Train: Epoch [11], Batch [111/938], Loss: 0.8230387568473816\n",
      "Train: Epoch [11], Batch [112/938], Loss: 0.8768382668495178\n",
      "Train: Epoch [11], Batch [113/938], Loss: 0.7488590478897095\n",
      "Train: Epoch [11], Batch [114/938], Loss: 1.051382303237915\n",
      "Train: Epoch [11], Batch [115/938], Loss: 0.766301691532135\n",
      "Train: Epoch [11], Batch [116/938], Loss: 0.5211785435676575\n",
      "Train: Epoch [11], Batch [117/938], Loss: 0.7053428292274475\n",
      "Train: Epoch [11], Batch [118/938], Loss: 0.854210376739502\n",
      "Train: Epoch [11], Batch [119/938], Loss: 0.6646263599395752\n",
      "Train: Epoch [11], Batch [120/938], Loss: 1.013466715812683\n",
      "Train: Epoch [11], Batch [121/938], Loss: 0.7242695093154907\n",
      "Train: Epoch [11], Batch [122/938], Loss: 0.8869642615318298\n",
      "Train: Epoch [11], Batch [123/938], Loss: 0.857814371585846\n",
      "Train: Epoch [11], Batch [124/938], Loss: 0.8462781310081482\n",
      "Train: Epoch [11], Batch [125/938], Loss: 0.7763835191726685\n",
      "Train: Epoch [11], Batch [126/938], Loss: 0.8185189962387085\n",
      "Train: Epoch [11], Batch [127/938], Loss: 0.8284304141998291\n",
      "Train: Epoch [11], Batch [128/938], Loss: 0.8754287362098694\n",
      "Train: Epoch [11], Batch [129/938], Loss: 0.9464177489280701\n",
      "Train: Epoch [11], Batch [130/938], Loss: 0.5844883918762207\n",
      "Train: Epoch [11], Batch [131/938], Loss: 0.7073523998260498\n",
      "Train: Epoch [11], Batch [132/938], Loss: 0.8801281452178955\n",
      "Train: Epoch [11], Batch [133/938], Loss: 0.7933756709098816\n",
      "Train: Epoch [11], Batch [134/938], Loss: 1.03638756275177\n",
      "Train: Epoch [11], Batch [135/938], Loss: 0.7163366079330444\n",
      "Train: Epoch [11], Batch [136/938], Loss: 0.804913341999054\n",
      "Train: Epoch [11], Batch [137/938], Loss: 0.6704087257385254\n",
      "Train: Epoch [11], Batch [138/938], Loss: 0.965679943561554\n",
      "Train: Epoch [11], Batch [139/938], Loss: 0.884307324886322\n",
      "Train: Epoch [11], Batch [140/938], Loss: 0.6094145178794861\n",
      "Train: Epoch [11], Batch [141/938], Loss: 0.8226128220558167\n",
      "Train: Epoch [11], Batch [142/938], Loss: 0.6258917450904846\n",
      "Train: Epoch [11], Batch [143/938], Loss: 0.9231618642807007\n",
      "Train: Epoch [11], Batch [144/938], Loss: 0.7243411540985107\n",
      "Train: Epoch [11], Batch [145/938], Loss: 0.7075192928314209\n",
      "Train: Epoch [11], Batch [146/938], Loss: 0.5654838681221008\n",
      "Train: Epoch [11], Batch [147/938], Loss: 0.8484734892845154\n",
      "Train: Epoch [11], Batch [148/938], Loss: 0.8025985360145569\n",
      "Train: Epoch [11], Batch [149/938], Loss: 0.9687543511390686\n",
      "Train: Epoch [11], Batch [150/938], Loss: 0.9875599145889282\n",
      "Train: Epoch [11], Batch [151/938], Loss: 1.0058950185775757\n",
      "Train: Epoch [11], Batch [152/938], Loss: 0.7449468970298767\n",
      "Train: Epoch [11], Batch [153/938], Loss: 0.8213825225830078\n",
      "Train: Epoch [11], Batch [154/938], Loss: 0.7891103625297546\n",
      "Train: Epoch [11], Batch [155/938], Loss: 0.7596657872200012\n",
      "Train: Epoch [11], Batch [156/938], Loss: 0.9862646460533142\n",
      "Train: Epoch [11], Batch [157/938], Loss: 0.6871505975723267\n",
      "Train: Epoch [11], Batch [158/938], Loss: 0.7839518785476685\n",
      "Train: Epoch [11], Batch [159/938], Loss: 0.9237487316131592\n",
      "Train: Epoch [11], Batch [160/938], Loss: 0.6982240080833435\n",
      "Train: Epoch [11], Batch [161/938], Loss: 0.9271490573883057\n",
      "Train: Epoch [11], Batch [162/938], Loss: 0.7872486710548401\n",
      "Train: Epoch [11], Batch [163/938], Loss: 0.7744821906089783\n",
      "Train: Epoch [11], Batch [164/938], Loss: 0.852131724357605\n",
      "Train: Epoch [11], Batch [165/938], Loss: 0.8134125471115112\n",
      "Train: Epoch [11], Batch [166/938], Loss: 0.8536802530288696\n",
      "Train: Epoch [11], Batch [167/938], Loss: 0.9311973452568054\n",
      "Train: Epoch [11], Batch [168/938], Loss: 0.8793905377388\n",
      "Train: Epoch [11], Batch [169/938], Loss: 0.8181167840957642\n",
      "Train: Epoch [11], Batch [170/938], Loss: 0.771012544631958\n",
      "Train: Epoch [11], Batch [171/938], Loss: 0.6248487234115601\n",
      "Train: Epoch [11], Batch [172/938], Loss: 0.7423290014266968\n",
      "Train: Epoch [11], Batch [173/938], Loss: 0.896666944026947\n",
      "Train: Epoch [11], Batch [174/938], Loss: 0.5296695232391357\n",
      "Train: Epoch [11], Batch [175/938], Loss: 0.6628096699714661\n",
      "Train: Epoch [11], Batch [176/938], Loss: 0.9888244867324829\n",
      "Train: Epoch [11], Batch [177/938], Loss: 0.7235664129257202\n",
      "Train: Epoch [11], Batch [178/938], Loss: 0.7885400056838989\n",
      "Train: Epoch [11], Batch [179/938], Loss: 0.4737294614315033\n",
      "Train: Epoch [11], Batch [180/938], Loss: 0.7523862719535828\n",
      "Train: Epoch [11], Batch [181/938], Loss: 0.954027533531189\n",
      "Train: Epoch [11], Batch [182/938], Loss: 0.5409291386604309\n",
      "Train: Epoch [11], Batch [183/938], Loss: 0.7546305060386658\n",
      "Train: Epoch [11], Batch [184/938], Loss: 0.7719208002090454\n",
      "Train: Epoch [11], Batch [185/938], Loss: 0.9010260105133057\n",
      "Train: Epoch [11], Batch [186/938], Loss: 0.7273735404014587\n",
      "Train: Epoch [11], Batch [187/938], Loss: 0.6171807646751404\n",
      "Train: Epoch [11], Batch [188/938], Loss: 0.7697522044181824\n",
      "Train: Epoch [11], Batch [189/938], Loss: 0.6283630132675171\n",
      "Train: Epoch [11], Batch [190/938], Loss: 1.0207219123840332\n",
      "Train: Epoch [11], Batch [191/938], Loss: 0.7995802760124207\n",
      "Train: Epoch [11], Batch [192/938], Loss: 0.6376436352729797\n",
      "Train: Epoch [11], Batch [193/938], Loss: 0.688838541507721\n",
      "Train: Epoch [11], Batch [194/938], Loss: 0.8201290965080261\n",
      "Train: Epoch [11], Batch [195/938], Loss: 0.678654134273529\n",
      "Train: Epoch [11], Batch [196/938], Loss: 0.549644410610199\n",
      "Train: Epoch [11], Batch [197/938], Loss: 0.6617892384529114\n",
      "Train: Epoch [11], Batch [198/938], Loss: 0.7875082492828369\n",
      "Train: Epoch [11], Batch [199/938], Loss: 0.8430489897727966\n",
      "Train: Epoch [11], Batch [200/938], Loss: 0.7634419202804565\n",
      "Train: Epoch [11], Batch [201/938], Loss: 0.8897477388381958\n",
      "Train: Epoch [11], Batch [202/938], Loss: 0.8191519379615784\n",
      "Train: Epoch [11], Batch [203/938], Loss: 1.0665507316589355\n",
      "Train: Epoch [11], Batch [204/938], Loss: 0.6481761932373047\n",
      "Train: Epoch [11], Batch [205/938], Loss: 0.8219885230064392\n",
      "Train: Epoch [11], Batch [206/938], Loss: 0.7606392502784729\n",
      "Train: Epoch [11], Batch [207/938], Loss: 0.8213568329811096\n",
      "Train: Epoch [11], Batch [208/938], Loss: 0.7303793430328369\n",
      "Train: Epoch [11], Batch [209/938], Loss: 0.729217529296875\n",
      "Train: Epoch [11], Batch [210/938], Loss: 0.6560836434364319\n",
      "Train: Epoch [11], Batch [211/938], Loss: 0.8215397000312805\n",
      "Train: Epoch [11], Batch [212/938], Loss: 0.5776726603507996\n",
      "Train: Epoch [11], Batch [213/938], Loss: 0.5511903762817383\n",
      "Train: Epoch [11], Batch [214/938], Loss: 1.0657691955566406\n",
      "Train: Epoch [11], Batch [215/938], Loss: 0.9978468418121338\n",
      "Train: Epoch [11], Batch [216/938], Loss: 0.6510453224182129\n",
      "Train: Epoch [11], Batch [217/938], Loss: 0.5472466945648193\n",
      "Train: Epoch [11], Batch [218/938], Loss: 0.7550083994865417\n",
      "Train: Epoch [11], Batch [219/938], Loss: 0.9044939875602722\n",
      "Train: Epoch [11], Batch [220/938], Loss: 0.8044794201850891\n",
      "Train: Epoch [11], Batch [221/938], Loss: 0.867986261844635\n",
      "Train: Epoch [11], Batch [222/938], Loss: 0.8925745487213135\n",
      "Train: Epoch [11], Batch [223/938], Loss: 0.8118625283241272\n",
      "Train: Epoch [11], Batch [224/938], Loss: 0.8215821385383606\n",
      "Train: Epoch [11], Batch [225/938], Loss: 0.8073148131370544\n",
      "Train: Epoch [11], Batch [226/938], Loss: 0.880695104598999\n",
      "Train: Epoch [11], Batch [227/938], Loss: 0.6164476871490479\n",
      "Train: Epoch [11], Batch [228/938], Loss: 0.44968026876449585\n",
      "Train: Epoch [11], Batch [229/938], Loss: 0.7863214015960693\n",
      "Train: Epoch [11], Batch [230/938], Loss: 0.6378418207168579\n",
      "Train: Epoch [11], Batch [231/938], Loss: 0.8105703592300415\n",
      "Train: Epoch [11], Batch [232/938], Loss: 0.8741106986999512\n",
      "Train: Epoch [11], Batch [233/938], Loss: 0.6383118629455566\n",
      "Train: Epoch [11], Batch [234/938], Loss: 0.5479024648666382\n",
      "Train: Epoch [11], Batch [235/938], Loss: 0.8226604461669922\n",
      "Train: Epoch [11], Batch [236/938], Loss: 0.720014214515686\n",
      "Train: Epoch [11], Batch [237/938], Loss: 0.7670232057571411\n",
      "Train: Epoch [11], Batch [238/938], Loss: 0.7684534788131714\n",
      "Train: Epoch [11], Batch [239/938], Loss: 0.7865070700645447\n",
      "Train: Epoch [11], Batch [240/938], Loss: 0.8442223072052002\n",
      "Train: Epoch [11], Batch [241/938], Loss: 0.7386572957038879\n",
      "Train: Epoch [11], Batch [242/938], Loss: 0.6776783466339111\n",
      "Train: Epoch [11], Batch [243/938], Loss: 0.597531795501709\n",
      "Train: Epoch [11], Batch [244/938], Loss: 0.9388757944107056\n",
      "Train: Epoch [11], Batch [245/938], Loss: 0.8870968222618103\n",
      "Train: Epoch [11], Batch [246/938], Loss: 0.6323142051696777\n",
      "Train: Epoch [11], Batch [247/938], Loss: 0.6452017426490784\n",
      "Train: Epoch [11], Batch [248/938], Loss: 1.0090110301971436\n",
      "Train: Epoch [11], Batch [249/938], Loss: 0.8078963756561279\n",
      "Train: Epoch [11], Batch [250/938], Loss: 0.8045207858085632\n",
      "Train: Epoch [11], Batch [251/938], Loss: 0.7795329093933105\n",
      "Train: Epoch [11], Batch [252/938], Loss: 0.8378716111183167\n",
      "Train: Epoch [11], Batch [253/938], Loss: 0.7804091572761536\n",
      "Train: Epoch [11], Batch [254/938], Loss: 0.7897552251815796\n",
      "Train: Epoch [11], Batch [255/938], Loss: 0.7053559422492981\n",
      "Train: Epoch [11], Batch [256/938], Loss: 0.6911721229553223\n",
      "Train: Epoch [11], Batch [257/938], Loss: 0.6190151572227478\n",
      "Train: Epoch [11], Batch [258/938], Loss: 0.9470129609107971\n",
      "Train: Epoch [11], Batch [259/938], Loss: 1.082139253616333\n",
      "Train: Epoch [11], Batch [260/938], Loss: 1.0475579500198364\n",
      "Train: Epoch [11], Batch [261/938], Loss: 0.7421033382415771\n",
      "Train: Epoch [11], Batch [262/938], Loss: 0.5718200206756592\n",
      "Train: Epoch [11], Batch [263/938], Loss: 0.7159070372581482\n",
      "Train: Epoch [11], Batch [264/938], Loss: 0.5874412059783936\n",
      "Train: Epoch [11], Batch [265/938], Loss: 1.0048444271087646\n",
      "Train: Epoch [11], Batch [266/938], Loss: 0.8274915814399719\n",
      "Train: Epoch [11], Batch [267/938], Loss: 0.5274206399917603\n",
      "Train: Epoch [11], Batch [268/938], Loss: 0.814240038394928\n",
      "Train: Epoch [11], Batch [269/938], Loss: 0.7991246581077576\n",
      "Train: Epoch [11], Batch [270/938], Loss: 0.5829620361328125\n",
      "Train: Epoch [11], Batch [271/938], Loss: 0.9380013346672058\n",
      "Train: Epoch [11], Batch [272/938], Loss: 0.6388574838638306\n",
      "Train: Epoch [11], Batch [273/938], Loss: 1.0350037813186646\n",
      "Train: Epoch [11], Batch [274/938], Loss: 0.5551211833953857\n",
      "Train: Epoch [11], Batch [275/938], Loss: 0.7302621006965637\n",
      "Train: Epoch [11], Batch [276/938], Loss: 0.8320938348770142\n",
      "Train: Epoch [11], Batch [277/938], Loss: 0.7128356099128723\n",
      "Train: Epoch [11], Batch [278/938], Loss: 0.6845390200614929\n",
      "Train: Epoch [11], Batch [279/938], Loss: 0.6100786924362183\n",
      "Train: Epoch [11], Batch [280/938], Loss: 0.5916535258293152\n",
      "Train: Epoch [11], Batch [281/938], Loss: 0.8387076258659363\n",
      "Train: Epoch [11], Batch [282/938], Loss: 0.650645911693573\n",
      "Train: Epoch [11], Batch [283/938], Loss: 0.8923161029815674\n",
      "Train: Epoch [11], Batch [284/938], Loss: 0.9846627712249756\n",
      "Train: Epoch [11], Batch [285/938], Loss: 0.9054927825927734\n",
      "Train: Epoch [11], Batch [286/938], Loss: 0.7142258882522583\n",
      "Train: Epoch [11], Batch [287/938], Loss: 0.8774194717407227\n",
      "Train: Epoch [11], Batch [288/938], Loss: 0.6799118518829346\n",
      "Train: Epoch [11], Batch [289/938], Loss: 0.8599486351013184\n",
      "Train: Epoch [11], Batch [290/938], Loss: 0.7901121377944946\n",
      "Train: Epoch [11], Batch [291/938], Loss: 0.9541746377944946\n",
      "Train: Epoch [11], Batch [292/938], Loss: 0.7877002954483032\n",
      "Train: Epoch [11], Batch [293/938], Loss: 0.9194110035896301\n",
      "Train: Epoch [11], Batch [294/938], Loss: 0.6420190930366516\n",
      "Train: Epoch [11], Batch [295/938], Loss: 0.8943344950675964\n",
      "Train: Epoch [11], Batch [296/938], Loss: 1.1001278162002563\n",
      "Train: Epoch [11], Batch [297/938], Loss: 0.8241150975227356\n",
      "Train: Epoch [11], Batch [298/938], Loss: 0.745608925819397\n",
      "Train: Epoch [11], Batch [299/938], Loss: 0.9029573202133179\n",
      "Train: Epoch [11], Batch [300/938], Loss: 0.7823399901390076\n",
      "Train: Epoch [11], Batch [301/938], Loss: 0.7032301425933838\n",
      "Train: Epoch [11], Batch [302/938], Loss: 0.9869576692581177\n",
      "Train: Epoch [11], Batch [303/938], Loss: 0.7671231031417847\n",
      "Train: Epoch [11], Batch [304/938], Loss: 0.7299259901046753\n",
      "Train: Epoch [11], Batch [305/938], Loss: 0.7857193946838379\n",
      "Train: Epoch [11], Batch [306/938], Loss: 0.8367630839347839\n",
      "Train: Epoch [11], Batch [307/938], Loss: 0.9439818859100342\n",
      "Train: Epoch [11], Batch [308/938], Loss: 0.7821760177612305\n",
      "Train: Epoch [11], Batch [309/938], Loss: 0.7098538279533386\n",
      "Train: Epoch [11], Batch [310/938], Loss: 0.9365665912628174\n",
      "Train: Epoch [11], Batch [311/938], Loss: 0.7018823027610779\n",
      "Train: Epoch [11], Batch [312/938], Loss: 0.9197804927825928\n",
      "Train: Epoch [11], Batch [313/938], Loss: 0.7713238596916199\n",
      "Train: Epoch [11], Batch [314/938], Loss: 0.9387529492378235\n",
      "Train: Epoch [11], Batch [315/938], Loss: 0.8571376204490662\n",
      "Train: Epoch [11], Batch [316/938], Loss: 0.7057382464408875\n",
      "Train: Epoch [11], Batch [317/938], Loss: 0.5358980298042297\n",
      "Train: Epoch [11], Batch [318/938], Loss: 0.8701642751693726\n",
      "Train: Epoch [11], Batch [319/938], Loss: 0.901544988155365\n",
      "Train: Epoch [11], Batch [320/938], Loss: 0.8731210231781006\n",
      "Train: Epoch [11], Batch [321/938], Loss: 0.6182946562767029\n",
      "Train: Epoch [11], Batch [322/938], Loss: 0.8810678720474243\n",
      "Train: Epoch [11], Batch [323/938], Loss: 0.7503929734230042\n",
      "Train: Epoch [11], Batch [324/938], Loss: 0.8132675886154175\n",
      "Train: Epoch [11], Batch [325/938], Loss: 0.9324414730072021\n",
      "Train: Epoch [11], Batch [326/938], Loss: 0.7080267071723938\n",
      "Train: Epoch [11], Batch [327/938], Loss: 0.8858016729354858\n",
      "Train: Epoch [11], Batch [328/938], Loss: 0.6753716468811035\n",
      "Train: Epoch [11], Batch [329/938], Loss: 0.5720104575157166\n",
      "Train: Epoch [11], Batch [330/938], Loss: 0.8898406624794006\n",
      "Train: Epoch [11], Batch [331/938], Loss: 0.717656135559082\n",
      "Train: Epoch [11], Batch [332/938], Loss: 0.6704862713813782\n",
      "Train: Epoch [11], Batch [333/938], Loss: 0.6415508985519409\n",
      "Train: Epoch [11], Batch [334/938], Loss: 0.6051859259605408\n",
      "Train: Epoch [11], Batch [335/938], Loss: 0.5875132083892822\n",
      "Train: Epoch [11], Batch [336/938], Loss: 0.7964416146278381\n",
      "Train: Epoch [11], Batch [337/938], Loss: 0.6224411129951477\n",
      "Train: Epoch [11], Batch [338/938], Loss: 0.7208864688873291\n",
      "Train: Epoch [11], Batch [339/938], Loss: 0.8261969089508057\n",
      "Train: Epoch [11], Batch [340/938], Loss: 0.7589377164840698\n",
      "Train: Epoch [11], Batch [341/938], Loss: 0.6452308893203735\n",
      "Train: Epoch [11], Batch [342/938], Loss: 0.8753154277801514\n",
      "Train: Epoch [11], Batch [343/938], Loss: 0.6173382997512817\n",
      "Train: Epoch [11], Batch [344/938], Loss: 0.778505265712738\n",
      "Train: Epoch [11], Batch [345/938], Loss: 0.701733410358429\n",
      "Train: Epoch [11], Batch [346/938], Loss: 0.8496054410934448\n",
      "Train: Epoch [11], Batch [347/938], Loss: 0.5149204134941101\n",
      "Train: Epoch [11], Batch [348/938], Loss: 0.902848482131958\n",
      "Train: Epoch [11], Batch [349/938], Loss: 0.8017110824584961\n",
      "Train: Epoch [11], Batch [350/938], Loss: 0.9556260108947754\n",
      "Train: Epoch [11], Batch [351/938], Loss: 0.5786074995994568\n",
      "Train: Epoch [11], Batch [352/938], Loss: 0.9146603941917419\n",
      "Train: Epoch [11], Batch [353/938], Loss: 0.805298924446106\n",
      "Train: Epoch [11], Batch [354/938], Loss: 0.8391714692115784\n",
      "Train: Epoch [11], Batch [355/938], Loss: 0.9033178091049194\n",
      "Train: Epoch [11], Batch [356/938], Loss: 0.7255238890647888\n",
      "Train: Epoch [11], Batch [357/938], Loss: 0.7916527986526489\n",
      "Train: Epoch [11], Batch [358/938], Loss: 0.7520928978919983\n",
      "Train: Epoch [11], Batch [359/938], Loss: 0.8982709050178528\n",
      "Train: Epoch [11], Batch [360/938], Loss: 0.7083503007888794\n",
      "Train: Epoch [11], Batch [361/938], Loss: 0.6410238146781921\n",
      "Train: Epoch [11], Batch [362/938], Loss: 0.8992294669151306\n",
      "Train: Epoch [11], Batch [363/938], Loss: 0.9714434742927551\n",
      "Train: Epoch [11], Batch [364/938], Loss: 0.8078820705413818\n",
      "Train: Epoch [11], Batch [365/938], Loss: 0.645157516002655\n",
      "Train: Epoch [11], Batch [366/938], Loss: 0.8215428590774536\n",
      "Train: Epoch [11], Batch [367/938], Loss: 0.8160997629165649\n",
      "Train: Epoch [11], Batch [368/938], Loss: 0.7081886529922485\n",
      "Train: Epoch [11], Batch [369/938], Loss: 0.8687559366226196\n",
      "Train: Epoch [11], Batch [370/938], Loss: 0.7973150014877319\n",
      "Train: Epoch [11], Batch [371/938], Loss: 0.6452460885047913\n",
      "Train: Epoch [11], Batch [372/938], Loss: 0.7523432970046997\n",
      "Train: Epoch [11], Batch [373/938], Loss: 0.8503885269165039\n",
      "Train: Epoch [11], Batch [374/938], Loss: 0.5838068127632141\n",
      "Train: Epoch [11], Batch [375/938], Loss: 0.8652803897857666\n",
      "Train: Epoch [11], Batch [376/938], Loss: 0.6395506262779236\n",
      "Train: Epoch [11], Batch [377/938], Loss: 0.8048416376113892\n",
      "Train: Epoch [11], Batch [378/938], Loss: 0.7190839052200317\n",
      "Train: Epoch [11], Batch [379/938], Loss: 0.7129172086715698\n",
      "Train: Epoch [11], Batch [380/938], Loss: 1.0175797939300537\n",
      "Train: Epoch [11], Batch [381/938], Loss: 0.7276910543441772\n",
      "Train: Epoch [11], Batch [382/938], Loss: 0.6189120411872864\n",
      "Train: Epoch [11], Batch [383/938], Loss: 0.980898916721344\n",
      "Train: Epoch [11], Batch [384/938], Loss: 0.8151806592941284\n",
      "Train: Epoch [11], Batch [385/938], Loss: 0.607141375541687\n",
      "Train: Epoch [11], Batch [386/938], Loss: 0.7201710939407349\n",
      "Train: Epoch [11], Batch [387/938], Loss: 0.6906701922416687\n",
      "Train: Epoch [11], Batch [388/938], Loss: 0.8164623379707336\n",
      "Train: Epoch [11], Batch [389/938], Loss: 0.8814910650253296\n",
      "Train: Epoch [11], Batch [390/938], Loss: 0.9342652559280396\n",
      "Train: Epoch [11], Batch [391/938], Loss: 0.6965688467025757\n",
      "Train: Epoch [11], Batch [392/938], Loss: 0.644030749797821\n",
      "Train: Epoch [11], Batch [393/938], Loss: 0.6800975799560547\n",
      "Train: Epoch [11], Batch [394/938], Loss: 1.0603581666946411\n",
      "Train: Epoch [11], Batch [395/938], Loss: 1.0390788316726685\n",
      "Train: Epoch [11], Batch [396/938], Loss: 0.9529277086257935\n",
      "Train: Epoch [11], Batch [397/938], Loss: 0.95556640625\n",
      "Train: Epoch [11], Batch [398/938], Loss: 0.8685250878334045\n",
      "Train: Epoch [11], Batch [399/938], Loss: 0.43491625785827637\n",
      "Train: Epoch [11], Batch [400/938], Loss: 0.9232444763183594\n",
      "Train: Epoch [11], Batch [401/938], Loss: 0.7637379169464111\n",
      "Train: Epoch [11], Batch [402/938], Loss: 0.9241072535514832\n",
      "Train: Epoch [11], Batch [403/938], Loss: 0.8221119046211243\n",
      "Train: Epoch [11], Batch [404/938], Loss: 0.861628532409668\n",
      "Train: Epoch [11], Batch [405/938], Loss: 0.6181268692016602\n",
      "Train: Epoch [11], Batch [406/938], Loss: 0.917812168598175\n",
      "Train: Epoch [11], Batch [407/938], Loss: 0.7438471913337708\n",
      "Train: Epoch [11], Batch [408/938], Loss: 0.6814216375350952\n",
      "Train: Epoch [11], Batch [409/938], Loss: 0.5954805016517639\n",
      "Train: Epoch [11], Batch [410/938], Loss: 0.6179900765419006\n",
      "Train: Epoch [11], Batch [411/938], Loss: 0.7017045021057129\n",
      "Train: Epoch [11], Batch [412/938], Loss: 0.980699896812439\n",
      "Train: Epoch [11], Batch [413/938], Loss: 0.8263031840324402\n",
      "Train: Epoch [11], Batch [414/938], Loss: 0.7411543130874634\n",
      "Train: Epoch [11], Batch [415/938], Loss: 0.5099350214004517\n",
      "Train: Epoch [11], Batch [416/938], Loss: 0.7315513491630554\n",
      "Train: Epoch [11], Batch [417/938], Loss: 0.6434280276298523\n",
      "Train: Epoch [11], Batch [418/938], Loss: 0.8981831073760986\n",
      "Train: Epoch [11], Batch [419/938], Loss: 0.7717045545578003\n",
      "Train: Epoch [11], Batch [420/938], Loss: 0.7855798602104187\n",
      "Train: Epoch [11], Batch [421/938], Loss: 0.7863174080848694\n",
      "Train: Epoch [11], Batch [422/938], Loss: 0.8888531923294067\n",
      "Train: Epoch [11], Batch [423/938], Loss: 0.7675619125366211\n",
      "Train: Epoch [11], Batch [424/938], Loss: 0.9123692512512207\n",
      "Train: Epoch [11], Batch [425/938], Loss: 0.8374754786491394\n",
      "Train: Epoch [11], Batch [426/938], Loss: 0.8483285307884216\n",
      "Train: Epoch [11], Batch [427/938], Loss: 0.7464790344238281\n",
      "Train: Epoch [11], Batch [428/938], Loss: 0.9228491187095642\n",
      "Train: Epoch [11], Batch [429/938], Loss: 0.8316686153411865\n",
      "Train: Epoch [11], Batch [430/938], Loss: 0.7507123947143555\n",
      "Train: Epoch [11], Batch [431/938], Loss: 0.8391544222831726\n",
      "Train: Epoch [11], Batch [432/938], Loss: 0.6786002516746521\n",
      "Train: Epoch [11], Batch [433/938], Loss: 0.718567967414856\n",
      "Train: Epoch [11], Batch [434/938], Loss: 0.9155898094177246\n",
      "Train: Epoch [11], Batch [435/938], Loss: 0.6109912395477295\n",
      "Train: Epoch [11], Batch [436/938], Loss: 0.8260395526885986\n",
      "Train: Epoch [11], Batch [437/938], Loss: 0.7290361523628235\n",
      "Train: Epoch [11], Batch [438/938], Loss: 0.6203829050064087\n",
      "Train: Epoch [11], Batch [439/938], Loss: 0.9407910108566284\n",
      "Train: Epoch [11], Batch [440/938], Loss: 0.6851402521133423\n",
      "Train: Epoch [11], Batch [441/938], Loss: 0.6494671106338501\n",
      "Train: Epoch [11], Batch [442/938], Loss: 0.6726957559585571\n",
      "Train: Epoch [11], Batch [443/938], Loss: 0.5753566026687622\n",
      "Train: Epoch [11], Batch [444/938], Loss: 0.6024906039237976\n",
      "Train: Epoch [11], Batch [445/938], Loss: 0.5612772703170776\n",
      "Train: Epoch [11], Batch [446/938], Loss: 0.6767687797546387\n",
      "Train: Epoch [11], Batch [447/938], Loss: 0.9881276488304138\n",
      "Train: Epoch [11], Batch [448/938], Loss: 0.9245467185974121\n",
      "Train: Epoch [11], Batch [449/938], Loss: 0.5974343419075012\n",
      "Train: Epoch [11], Batch [450/938], Loss: 0.7177671194076538\n",
      "Train: Epoch [11], Batch [451/938], Loss: 0.6703690886497498\n",
      "Train: Epoch [11], Batch [452/938], Loss: 0.6089288592338562\n",
      "Train: Epoch [11], Batch [453/938], Loss: 0.701069712638855\n",
      "Train: Epoch [11], Batch [454/938], Loss: 0.6132903695106506\n",
      "Train: Epoch [11], Batch [455/938], Loss: 0.6436663866043091\n",
      "Train: Epoch [11], Batch [456/938], Loss: 0.777384877204895\n",
      "Train: Epoch [11], Batch [457/938], Loss: 0.7556315064430237\n",
      "Train: Epoch [11], Batch [458/938], Loss: 1.0716136693954468\n",
      "Train: Epoch [11], Batch [459/938], Loss: 0.6730036735534668\n",
      "Train: Epoch [11], Batch [460/938], Loss: 0.7036792039871216\n",
      "Train: Epoch [11], Batch [461/938], Loss: 0.9090631008148193\n",
      "Train: Epoch [11], Batch [462/938], Loss: 0.6905852556228638\n",
      "Train: Epoch [11], Batch [463/938], Loss: 0.6213067770004272\n",
      "Train: Epoch [11], Batch [464/938], Loss: 0.6868173480033875\n",
      "Train: Epoch [11], Batch [465/938], Loss: 0.9566921591758728\n",
      "Train: Epoch [11], Batch [466/938], Loss: 0.7845590114593506\n",
      "Train: Epoch [11], Batch [467/938], Loss: 0.7933640480041504\n",
      "Train: Epoch [11], Batch [468/938], Loss: 0.6391894817352295\n",
      "Train: Epoch [11], Batch [469/938], Loss: 0.4968186616897583\n",
      "Train: Epoch [11], Batch [470/938], Loss: 0.8246898651123047\n",
      "Train: Epoch [11], Batch [471/938], Loss: 0.9586641788482666\n",
      "Train: Epoch [11], Batch [472/938], Loss: 0.6641838550567627\n",
      "Train: Epoch [11], Batch [473/938], Loss: 0.6128963232040405\n",
      "Train: Epoch [11], Batch [474/938], Loss: 0.7550923824310303\n",
      "Train: Epoch [11], Batch [475/938], Loss: 0.6692517399787903\n",
      "Train: Epoch [11], Batch [476/938], Loss: 0.5923064947128296\n",
      "Train: Epoch [11], Batch [477/938], Loss: 0.5683621764183044\n",
      "Train: Epoch [11], Batch [478/938], Loss: 0.6289136409759521\n",
      "Train: Epoch [11], Batch [479/938], Loss: 0.7742115259170532\n",
      "Train: Epoch [11], Batch [480/938], Loss: 0.7517107725143433\n",
      "Train: Epoch [11], Batch [481/938], Loss: 0.6071216464042664\n",
      "Train: Epoch [11], Batch [482/938], Loss: 0.7443788051605225\n",
      "Train: Epoch [11], Batch [483/938], Loss: 0.8336889743804932\n",
      "Train: Epoch [11], Batch [484/938], Loss: 0.5546342134475708\n",
      "Train: Epoch [11], Batch [485/938], Loss: 0.49527207016944885\n",
      "Train: Epoch [11], Batch [486/938], Loss: 0.8380377292633057\n",
      "Train: Epoch [11], Batch [487/938], Loss: 0.9827429056167603\n",
      "Train: Epoch [11], Batch [488/938], Loss: 0.6229755878448486\n",
      "Train: Epoch [11], Batch [489/938], Loss: 0.7285210490226746\n",
      "Train: Epoch [11], Batch [490/938], Loss: 0.7649649381637573\n",
      "Train: Epoch [11], Batch [491/938], Loss: 1.0062875747680664\n",
      "Train: Epoch [11], Batch [492/938], Loss: 0.546603798866272\n",
      "Train: Epoch [11], Batch [493/938], Loss: 0.7981187701225281\n",
      "Train: Epoch [11], Batch [494/938], Loss: 0.5247673392295837\n",
      "Train: Epoch [11], Batch [495/938], Loss: 0.7151638865470886\n",
      "Train: Epoch [11], Batch [496/938], Loss: 0.703078031539917\n",
      "Train: Epoch [11], Batch [497/938], Loss: 0.7951270937919617\n",
      "Train: Epoch [11], Batch [498/938], Loss: 0.7034732699394226\n",
      "Train: Epoch [11], Batch [499/938], Loss: 0.8837201595306396\n",
      "Train: Epoch [11], Batch [500/938], Loss: 0.9147385358810425\n",
      "Train: Epoch [11], Batch [501/938], Loss: 0.8218631148338318\n",
      "Train: Epoch [11], Batch [502/938], Loss: 0.7977279424667358\n",
      "Train: Epoch [11], Batch [503/938], Loss: 0.8975996971130371\n",
      "Train: Epoch [11], Batch [504/938], Loss: 0.8760713338851929\n",
      "Train: Epoch [11], Batch [505/938], Loss: 0.8951382637023926\n",
      "Train: Epoch [11], Batch [506/938], Loss: 0.8706247210502625\n",
      "Train: Epoch [11], Batch [507/938], Loss: 0.7186858057975769\n",
      "Train: Epoch [11], Batch [508/938], Loss: 0.7358386516571045\n",
      "Train: Epoch [11], Batch [509/938], Loss: 0.8906505703926086\n",
      "Train: Epoch [11], Batch [510/938], Loss: 0.6981740593910217\n",
      "Train: Epoch [11], Batch [511/938], Loss: 0.5476459860801697\n",
      "Train: Epoch [11], Batch [512/938], Loss: 0.7162601351737976\n",
      "Train: Epoch [11], Batch [513/938], Loss: 0.7185231447219849\n",
      "Train: Epoch [11], Batch [514/938], Loss: 0.953507125377655\n",
      "Train: Epoch [11], Batch [515/938], Loss: 0.8942584991455078\n",
      "Train: Epoch [11], Batch [516/938], Loss: 0.7505161166191101\n",
      "Train: Epoch [11], Batch [517/938], Loss: 0.6174920797348022\n",
      "Train: Epoch [11], Batch [518/938], Loss: 0.6584581732749939\n",
      "Train: Epoch [11], Batch [519/938], Loss: 0.9836241602897644\n",
      "Train: Epoch [11], Batch [520/938], Loss: 0.8262189030647278\n",
      "Train: Epoch [11], Batch [521/938], Loss: 1.1179090738296509\n",
      "Train: Epoch [11], Batch [522/938], Loss: 0.5364786386489868\n",
      "Train: Epoch [11], Batch [523/938], Loss: 0.6501566171646118\n",
      "Train: Epoch [11], Batch [524/938], Loss: 0.8305362462997437\n",
      "Train: Epoch [11], Batch [525/938], Loss: 0.8122639656066895\n",
      "Train: Epoch [11], Batch [526/938], Loss: 0.6930857300758362\n",
      "Train: Epoch [11], Batch [527/938], Loss: 0.7155553102493286\n",
      "Train: Epoch [11], Batch [528/938], Loss: 0.826419472694397\n",
      "Train: Epoch [11], Batch [529/938], Loss: 0.5640256404876709\n",
      "Train: Epoch [11], Batch [530/938], Loss: 0.639798641204834\n",
      "Train: Epoch [11], Batch [531/938], Loss: 0.6371784210205078\n",
      "Train: Epoch [11], Batch [532/938], Loss: 0.8403564095497131\n",
      "Train: Epoch [11], Batch [533/938], Loss: 0.6989025473594666\n",
      "Train: Epoch [11], Batch [534/938], Loss: 1.006484031677246\n",
      "Train: Epoch [11], Batch [535/938], Loss: 0.5564443469047546\n",
      "Train: Epoch [11], Batch [536/938], Loss: 0.8959509134292603\n",
      "Train: Epoch [11], Batch [537/938], Loss: 0.9731448888778687\n",
      "Train: Epoch [11], Batch [538/938], Loss: 0.5124965906143188\n",
      "Train: Epoch [11], Batch [539/938], Loss: 0.7057822346687317\n",
      "Train: Epoch [11], Batch [540/938], Loss: 0.591372549533844\n",
      "Train: Epoch [11], Batch [541/938], Loss: 0.5978940725326538\n",
      "Train: Epoch [11], Batch [542/938], Loss: 0.5170798301696777\n",
      "Train: Epoch [11], Batch [543/938], Loss: 0.990146279335022\n",
      "Train: Epoch [11], Batch [544/938], Loss: 0.5913830995559692\n",
      "Train: Epoch [11], Batch [545/938], Loss: 0.7750458121299744\n",
      "Train: Epoch [11], Batch [546/938], Loss: 0.8325622081756592\n",
      "Train: Epoch [11], Batch [547/938], Loss: 0.5648929476737976\n",
      "Train: Epoch [11], Batch [548/938], Loss: 0.8770506978034973\n",
      "Train: Epoch [11], Batch [549/938], Loss: 0.7547051310539246\n",
      "Train: Epoch [11], Batch [550/938], Loss: 0.5753552913665771\n",
      "Train: Epoch [11], Batch [551/938], Loss: 0.6133311986923218\n",
      "Train: Epoch [11], Batch [552/938], Loss: 0.6835657954216003\n",
      "Train: Epoch [11], Batch [553/938], Loss: 0.9187877774238586\n",
      "Train: Epoch [11], Batch [554/938], Loss: 0.7588759660720825\n",
      "Train: Epoch [11], Batch [555/938], Loss: 0.8526305556297302\n",
      "Train: Epoch [11], Batch [556/938], Loss: 0.6064817309379578\n",
      "Train: Epoch [11], Batch [557/938], Loss: 0.8472177982330322\n",
      "Train: Epoch [11], Batch [558/938], Loss: 0.724290132522583\n",
      "Train: Epoch [11], Batch [559/938], Loss: 0.5326018333435059\n",
      "Train: Epoch [11], Batch [560/938], Loss: 0.8536399006843567\n",
      "Train: Epoch [11], Batch [561/938], Loss: 0.8016965985298157\n",
      "Train: Epoch [11], Batch [562/938], Loss: 0.7979897260665894\n",
      "Train: Epoch [11], Batch [563/938], Loss: 0.5721851587295532\n",
      "Train: Epoch [11], Batch [564/938], Loss: 0.7138822078704834\n",
      "Train: Epoch [11], Batch [565/938], Loss: 0.7363713383674622\n",
      "Train: Epoch [11], Batch [566/938], Loss: 0.6951601505279541\n",
      "Train: Epoch [11], Batch [567/938], Loss: 0.8114745616912842\n",
      "Train: Epoch [11], Batch [568/938], Loss: 0.7065766453742981\n",
      "Train: Epoch [11], Batch [569/938], Loss: 0.5819632411003113\n",
      "Train: Epoch [11], Batch [570/938], Loss: 1.134872555732727\n",
      "Train: Epoch [11], Batch [571/938], Loss: 0.4908093810081482\n",
      "Train: Epoch [11], Batch [572/938], Loss: 0.7654145359992981\n",
      "Train: Epoch [11], Batch [573/938], Loss: 0.8108243346214294\n",
      "Train: Epoch [11], Batch [574/938], Loss: 0.8744716048240662\n",
      "Train: Epoch [11], Batch [575/938], Loss: 0.6219600439071655\n",
      "Train: Epoch [11], Batch [576/938], Loss: 0.7333900332450867\n",
      "Train: Epoch [11], Batch [577/938], Loss: 0.7772089242935181\n",
      "Train: Epoch [11], Batch [578/938], Loss: 0.8835270404815674\n",
      "Train: Epoch [11], Batch [579/938], Loss: 0.9982591271400452\n",
      "Train: Epoch [11], Batch [580/938], Loss: 0.7647721171379089\n",
      "Train: Epoch [11], Batch [581/938], Loss: 0.8092694282531738\n",
      "Train: Epoch [11], Batch [582/938], Loss: 0.7304172515869141\n",
      "Train: Epoch [11], Batch [583/938], Loss: 0.6261826753616333\n",
      "Train: Epoch [11], Batch [584/938], Loss: 0.9764367938041687\n",
      "Train: Epoch [11], Batch [585/938], Loss: 0.7360923290252686\n",
      "Train: Epoch [11], Batch [586/938], Loss: 0.7264155745506287\n",
      "Train: Epoch [11], Batch [587/938], Loss: 0.9001457095146179\n",
      "Train: Epoch [11], Batch [588/938], Loss: 0.5206514596939087\n",
      "Train: Epoch [11], Batch [589/938], Loss: 0.7439689636230469\n",
      "Train: Epoch [11], Batch [590/938], Loss: 0.8548526763916016\n",
      "Train: Epoch [11], Batch [591/938], Loss: 0.4594712257385254\n",
      "Train: Epoch [11], Batch [592/938], Loss: 0.8946788907051086\n",
      "Train: Epoch [11], Batch [593/938], Loss: 0.7545669674873352\n",
      "Train: Epoch [11], Batch [594/938], Loss: 0.49661365151405334\n",
      "Train: Epoch [11], Batch [595/938], Loss: 0.5969492197036743\n",
      "Train: Epoch [11], Batch [596/938], Loss: 0.8779600858688354\n",
      "Train: Epoch [11], Batch [597/938], Loss: 0.9555277824401855\n",
      "Train: Epoch [11], Batch [598/938], Loss: 0.743269145488739\n",
      "Train: Epoch [11], Batch [599/938], Loss: 0.6527099013328552\n",
      "Train: Epoch [11], Batch [600/938], Loss: 0.5284900069236755\n",
      "Train: Epoch [11], Batch [601/938], Loss: 0.756935179233551\n",
      "Train: Epoch [11], Batch [602/938], Loss: 0.7176733016967773\n",
      "Train: Epoch [11], Batch [603/938], Loss: 0.55792236328125\n",
      "Train: Epoch [11], Batch [604/938], Loss: 0.80759197473526\n",
      "Train: Epoch [11], Batch [605/938], Loss: 0.7165142297744751\n",
      "Train: Epoch [11], Batch [606/938], Loss: 0.5108174681663513\n",
      "Train: Epoch [11], Batch [607/938], Loss: 0.8014394640922546\n",
      "Train: Epoch [11], Batch [608/938], Loss: 0.9395918250083923\n",
      "Train: Epoch [11], Batch [609/938], Loss: 0.8820252418518066\n",
      "Train: Epoch [11], Batch [610/938], Loss: 0.7349370718002319\n",
      "Train: Epoch [11], Batch [611/938], Loss: 0.5160155892372131\n",
      "Train: Epoch [11], Batch [612/938], Loss: 0.9514501094818115\n",
      "Train: Epoch [11], Batch [613/938], Loss: 0.6908665299415588\n",
      "Train: Epoch [11], Batch [614/938], Loss: 0.8245209455490112\n",
      "Train: Epoch [11], Batch [615/938], Loss: 0.7463210821151733\n",
      "Train: Epoch [11], Batch [616/938], Loss: 0.8552953004837036\n",
      "Train: Epoch [11], Batch [617/938], Loss: 0.8053207397460938\n",
      "Train: Epoch [11], Batch [618/938], Loss: 0.7598443627357483\n",
      "Train: Epoch [11], Batch [619/938], Loss: 0.7678173780441284\n",
      "Train: Epoch [11], Batch [620/938], Loss: 0.9308168888092041\n",
      "Train: Epoch [11], Batch [621/938], Loss: 0.7781387567520142\n",
      "Train: Epoch [11], Batch [622/938], Loss: 0.5967681407928467\n",
      "Train: Epoch [11], Batch [623/938], Loss: 0.6569939851760864\n",
      "Train: Epoch [11], Batch [624/938], Loss: 1.0722534656524658\n",
      "Train: Epoch [11], Batch [625/938], Loss: 0.6745408773422241\n",
      "Train: Epoch [11], Batch [626/938], Loss: 0.6433019042015076\n",
      "Train: Epoch [11], Batch [627/938], Loss: 0.9472718834877014\n",
      "Train: Epoch [11], Batch [628/938], Loss: 0.8093806505203247\n",
      "Train: Epoch [11], Batch [629/938], Loss: 0.7612134218215942\n",
      "Train: Epoch [11], Batch [630/938], Loss: 0.8831804990768433\n",
      "Train: Epoch [11], Batch [631/938], Loss: 0.7855584025382996\n",
      "Train: Epoch [11], Batch [632/938], Loss: 1.0058722496032715\n",
      "Train: Epoch [11], Batch [633/938], Loss: 0.8591779470443726\n",
      "Train: Epoch [11], Batch [634/938], Loss: 0.6742650866508484\n",
      "Train: Epoch [11], Batch [635/938], Loss: 0.7390298247337341\n",
      "Train: Epoch [11], Batch [636/938], Loss: 0.7702295780181885\n",
      "Train: Epoch [11], Batch [637/938], Loss: 0.8253501057624817\n",
      "Train: Epoch [11], Batch [638/938], Loss: 0.5322698950767517\n",
      "Train: Epoch [11], Batch [639/938], Loss: 0.9296283721923828\n",
      "Train: Epoch [11], Batch [640/938], Loss: 0.8108067512512207\n",
      "Train: Epoch [11], Batch [641/938], Loss: 1.246761441230774\n",
      "Train: Epoch [11], Batch [642/938], Loss: 0.8239539265632629\n",
      "Train: Epoch [11], Batch [643/938], Loss: 0.7738940715789795\n",
      "Train: Epoch [11], Batch [644/938], Loss: 0.7258210778236389\n",
      "Train: Epoch [11], Batch [645/938], Loss: 0.4926031827926636\n",
      "Train: Epoch [11], Batch [646/938], Loss: 0.8029524087905884\n",
      "Train: Epoch [11], Batch [647/938], Loss: 0.6421783566474915\n",
      "Train: Epoch [11], Batch [648/938], Loss: 0.8271932601928711\n",
      "Train: Epoch [11], Batch [649/938], Loss: 0.6892517805099487\n",
      "Train: Epoch [11], Batch [650/938], Loss: 0.8489571809768677\n",
      "Train: Epoch [11], Batch [651/938], Loss: 1.0929089784622192\n",
      "Train: Epoch [11], Batch [652/938], Loss: 0.6631863117218018\n",
      "Train: Epoch [11], Batch [653/938], Loss: 0.622508704662323\n",
      "Train: Epoch [11], Batch [654/938], Loss: 0.6545217633247375\n",
      "Train: Epoch [11], Batch [655/938], Loss: 0.6484115719795227\n",
      "Train: Epoch [11], Batch [656/938], Loss: 0.6592723727226257\n",
      "Train: Epoch [11], Batch [657/938], Loss: 0.7433665990829468\n",
      "Train: Epoch [11], Batch [658/938], Loss: 0.5673943758010864\n",
      "Train: Epoch [11], Batch [659/938], Loss: 0.7230217456817627\n",
      "Train: Epoch [11], Batch [660/938], Loss: 0.6848495006561279\n",
      "Train: Epoch [11], Batch [661/938], Loss: 0.5485330820083618\n",
      "Train: Epoch [11], Batch [662/938], Loss: 0.5874044895172119\n",
      "Train: Epoch [11], Batch [663/938], Loss: 0.5873104929924011\n",
      "Train: Epoch [11], Batch [664/938], Loss: 0.7660036087036133\n",
      "Train: Epoch [11], Batch [665/938], Loss: 0.6193417906761169\n",
      "Train: Epoch [11], Batch [666/938], Loss: 0.7192232012748718\n",
      "Train: Epoch [11], Batch [667/938], Loss: 0.8849244117736816\n",
      "Train: Epoch [11], Batch [668/938], Loss: 0.5473933815956116\n",
      "Train: Epoch [11], Batch [669/938], Loss: 0.6402326822280884\n",
      "Train: Epoch [11], Batch [670/938], Loss: 0.6208088994026184\n",
      "Train: Epoch [11], Batch [671/938], Loss: 0.5962170958518982\n",
      "Train: Epoch [11], Batch [672/938], Loss: 0.7949697375297546\n",
      "Train: Epoch [11], Batch [673/938], Loss: 0.918117880821228\n",
      "Train: Epoch [11], Batch [674/938], Loss: 0.6031494140625\n",
      "Train: Epoch [11], Batch [675/938], Loss: 0.8077961206436157\n",
      "Train: Epoch [11], Batch [676/938], Loss: 0.767427384853363\n",
      "Train: Epoch [11], Batch [677/938], Loss: 0.5904504656791687\n",
      "Train: Epoch [11], Batch [678/938], Loss: 0.6735187768936157\n",
      "Train: Epoch [11], Batch [679/938], Loss: 0.6927586793899536\n",
      "Train: Epoch [11], Batch [680/938], Loss: 0.6651706695556641\n",
      "Train: Epoch [11], Batch [681/938], Loss: 0.6988562345504761\n",
      "Train: Epoch [11], Batch [682/938], Loss: 0.6744917631149292\n",
      "Train: Epoch [11], Batch [683/938], Loss: 0.9274361729621887\n",
      "Train: Epoch [11], Batch [684/938], Loss: 0.9947072863578796\n",
      "Train: Epoch [11], Batch [685/938], Loss: 0.7442256212234497\n",
      "Train: Epoch [11], Batch [686/938], Loss: 0.818799614906311\n",
      "Train: Epoch [11], Batch [687/938], Loss: 0.8708317279815674\n",
      "Train: Epoch [11], Batch [688/938], Loss: 0.857176661491394\n",
      "Train: Epoch [11], Batch [689/938], Loss: 0.7453407049179077\n",
      "Train: Epoch [11], Batch [690/938], Loss: 0.9079522490501404\n",
      "Train: Epoch [11], Batch [691/938], Loss: 0.7909444570541382\n",
      "Train: Epoch [11], Batch [692/938], Loss: 0.6087424159049988\n",
      "Train: Epoch [11], Batch [693/938], Loss: 1.244012713432312\n",
      "Train: Epoch [11], Batch [694/938], Loss: 0.609244704246521\n",
      "Train: Epoch [11], Batch [695/938], Loss: 0.8445020914077759\n",
      "Train: Epoch [11], Batch [696/938], Loss: 0.6265861392021179\n",
      "Train: Epoch [11], Batch [697/938], Loss: 0.8632960319519043\n",
      "Train: Epoch [11], Batch [698/938], Loss: 0.7341670393943787\n",
      "Train: Epoch [11], Batch [699/938], Loss: 0.8953552842140198\n",
      "Train: Epoch [11], Batch [700/938], Loss: 0.49587690830230713\n",
      "Train: Epoch [11], Batch [701/938], Loss: 0.9106362462043762\n",
      "Train: Epoch [11], Batch [702/938], Loss: 0.9005728960037231\n",
      "Train: Epoch [11], Batch [703/938], Loss: 0.7246668338775635\n",
      "Train: Epoch [11], Batch [704/938], Loss: 0.7106603384017944\n",
      "Train: Epoch [11], Batch [705/938], Loss: 0.7741409540176392\n",
      "Train: Epoch [11], Batch [706/938], Loss: 0.562946617603302\n",
      "Train: Epoch [11], Batch [707/938], Loss: 0.862605631351471\n",
      "Train: Epoch [11], Batch [708/938], Loss: 0.527510404586792\n",
      "Train: Epoch [11], Batch [709/938], Loss: 0.855690062046051\n",
      "Train: Epoch [11], Batch [710/938], Loss: 0.5259444117546082\n",
      "Train: Epoch [11], Batch [711/938], Loss: 0.5741369724273682\n",
      "Train: Epoch [11], Batch [712/938], Loss: 0.7435398101806641\n",
      "Train: Epoch [11], Batch [713/938], Loss: 0.8849588632583618\n",
      "Train: Epoch [11], Batch [714/938], Loss: 0.42038488388061523\n",
      "Train: Epoch [11], Batch [715/938], Loss: 0.7016346454620361\n",
      "Train: Epoch [11], Batch [716/938], Loss: 0.6289741396903992\n",
      "Train: Epoch [11], Batch [717/938], Loss: 0.6166158318519592\n",
      "Train: Epoch [11], Batch [718/938], Loss: 0.9405637979507446\n",
      "Train: Epoch [11], Batch [719/938], Loss: 0.802403450012207\n",
      "Train: Epoch [11], Batch [720/938], Loss: 0.8619861602783203\n",
      "Train: Epoch [11], Batch [721/938], Loss: 0.685843288898468\n",
      "Train: Epoch [11], Batch [722/938], Loss: 0.8489406704902649\n",
      "Train: Epoch [11], Batch [723/938], Loss: 0.96767258644104\n",
      "Train: Epoch [11], Batch [724/938], Loss: 0.6664009094238281\n",
      "Train: Epoch [11], Batch [725/938], Loss: 1.0073293447494507\n",
      "Train: Epoch [11], Batch [726/938], Loss: 0.85530686378479\n",
      "Train: Epoch [11], Batch [727/938], Loss: 0.7464996576309204\n",
      "Train: Epoch [11], Batch [728/938], Loss: 0.646182656288147\n",
      "Train: Epoch [11], Batch [729/938], Loss: 0.671233057975769\n",
      "Train: Epoch [11], Batch [730/938], Loss: 0.6140651702880859\n",
      "Train: Epoch [11], Batch [731/938], Loss: 0.6535285711288452\n",
      "Train: Epoch [11], Batch [732/938], Loss: 0.6713926196098328\n",
      "Train: Epoch [11], Batch [733/938], Loss: 0.4333766996860504\n",
      "Train: Epoch [11], Batch [734/938], Loss: 0.7265164256095886\n",
      "Train: Epoch [11], Batch [735/938], Loss: 0.8610089421272278\n",
      "Train: Epoch [11], Batch [736/938], Loss: 0.7694981694221497\n",
      "Train: Epoch [11], Batch [737/938], Loss: 0.6258635520935059\n",
      "Train: Epoch [11], Batch [738/938], Loss: 0.9026358723640442\n",
      "Train: Epoch [11], Batch [739/938], Loss: 0.6319600343704224\n",
      "Train: Epoch [11], Batch [740/938], Loss: 0.797561526298523\n",
      "Train: Epoch [11], Batch [741/938], Loss: 0.7485800981521606\n",
      "Train: Epoch [11], Batch [742/938], Loss: 0.7455033659934998\n",
      "Train: Epoch [11], Batch [743/938], Loss: 0.8704748749732971\n",
      "Train: Epoch [11], Batch [744/938], Loss: 0.4767780303955078\n",
      "Train: Epoch [11], Batch [745/938], Loss: 0.9003922939300537\n",
      "Train: Epoch [11], Batch [746/938], Loss: 0.7272313237190247\n",
      "Train: Epoch [11], Batch [747/938], Loss: 0.899594783782959\n",
      "Train: Epoch [11], Batch [748/938], Loss: 0.8505228161811829\n",
      "Train: Epoch [11], Batch [749/938], Loss: 0.7922295331954956\n",
      "Train: Epoch [11], Batch [750/938], Loss: 0.803473949432373\n",
      "Train: Epoch [11], Batch [751/938], Loss: 0.6440635919570923\n",
      "Train: Epoch [11], Batch [752/938], Loss: 0.937869668006897\n",
      "Train: Epoch [11], Batch [753/938], Loss: 0.7443059682846069\n",
      "Train: Epoch [11], Batch [754/938], Loss: 0.7491975426673889\n",
      "Train: Epoch [11], Batch [755/938], Loss: 0.8801359534263611\n",
      "Train: Epoch [11], Batch [756/938], Loss: 0.8877485990524292\n",
      "Train: Epoch [11], Batch [757/938], Loss: 0.7307041883468628\n",
      "Train: Epoch [11], Batch [758/938], Loss: 0.5970003008842468\n",
      "Train: Epoch [11], Batch [759/938], Loss: 1.023025631904602\n",
      "Train: Epoch [11], Batch [760/938], Loss: 0.875167965888977\n",
      "Train: Epoch [11], Batch [761/938], Loss: 0.7208960056304932\n",
      "Train: Epoch [11], Batch [762/938], Loss: 0.7294245362281799\n",
      "Train: Epoch [11], Batch [763/938], Loss: 0.7158740758895874\n",
      "Train: Epoch [11], Batch [764/938], Loss: 0.8021933436393738\n",
      "Train: Epoch [11], Batch [765/938], Loss: 0.7041527032852173\n",
      "Train: Epoch [11], Batch [766/938], Loss: 0.9759840965270996\n",
      "Train: Epoch [11], Batch [767/938], Loss: 0.8185821771621704\n",
      "Train: Epoch [11], Batch [768/938], Loss: 0.8395047187805176\n",
      "Train: Epoch [11], Batch [769/938], Loss: 0.6831064820289612\n",
      "Train: Epoch [11], Batch [770/938], Loss: 0.7461413145065308\n",
      "Train: Epoch [11], Batch [771/938], Loss: 0.6803920269012451\n",
      "Train: Epoch [11], Batch [772/938], Loss: 0.7963130474090576\n",
      "Train: Epoch [11], Batch [773/938], Loss: 0.6979251503944397\n",
      "Train: Epoch [11], Batch [774/938], Loss: 0.6658485531806946\n",
      "Train: Epoch [11], Batch [775/938], Loss: 0.7132167220115662\n",
      "Train: Epoch [11], Batch [776/938], Loss: 0.9566361904144287\n",
      "Train: Epoch [11], Batch [777/938], Loss: 0.41068392992019653\n",
      "Train: Epoch [11], Batch [778/938], Loss: 0.8418749570846558\n",
      "Train: Epoch [11], Batch [779/938], Loss: 0.8667053580284119\n",
      "Train: Epoch [11], Batch [780/938], Loss: 0.7509146332740784\n",
      "Train: Epoch [11], Batch [781/938], Loss: 1.0303676128387451\n",
      "Train: Epoch [11], Batch [782/938], Loss: 0.7425293326377869\n",
      "Train: Epoch [11], Batch [783/938], Loss: 0.7287654876708984\n",
      "Train: Epoch [11], Batch [784/938], Loss: 0.6150940656661987\n",
      "Train: Epoch [11], Batch [785/938], Loss: 0.5621495246887207\n",
      "Train: Epoch [11], Batch [786/938], Loss: 0.7308926582336426\n",
      "Train: Epoch [11], Batch [787/938], Loss: 0.8530389666557312\n",
      "Train: Epoch [11], Batch [788/938], Loss: 0.5224081873893738\n",
      "Train: Epoch [11], Batch [789/938], Loss: 0.9436322450637817\n",
      "Train: Epoch [11], Batch [790/938], Loss: 0.701097846031189\n",
      "Train: Epoch [11], Batch [791/938], Loss: 0.802398145198822\n",
      "Train: Epoch [11], Batch [792/938], Loss: 0.6678211092948914\n",
      "Train: Epoch [11], Batch [793/938], Loss: 0.6844053864479065\n",
      "Train: Epoch [11], Batch [794/938], Loss: 0.6827518939971924\n",
      "Train: Epoch [11], Batch [795/938], Loss: 0.6214680075645447\n",
      "Train: Epoch [11], Batch [796/938], Loss: 0.6989023685455322\n",
      "Train: Epoch [11], Batch [797/938], Loss: 0.9848201870918274\n",
      "Train: Epoch [11], Batch [798/938], Loss: 0.8315643668174744\n",
      "Train: Epoch [11], Batch [799/938], Loss: 0.9512866735458374\n",
      "Train: Epoch [11], Batch [800/938], Loss: 1.0762115716934204\n",
      "Train: Epoch [11], Batch [801/938], Loss: 0.5649533271789551\n",
      "Train: Epoch [11], Batch [802/938], Loss: 0.7733831405639648\n",
      "Train: Epoch [11], Batch [803/938], Loss: 0.6766763925552368\n",
      "Train: Epoch [11], Batch [804/938], Loss: 0.6915299892425537\n",
      "Train: Epoch [11], Batch [805/938], Loss: 0.5460628271102905\n",
      "Train: Epoch [11], Batch [806/938], Loss: 0.7932149171829224\n",
      "Train: Epoch [11], Batch [807/938], Loss: 0.6198988556861877\n",
      "Train: Epoch [11], Batch [808/938], Loss: 0.6694498658180237\n",
      "Train: Epoch [11], Batch [809/938], Loss: 0.7038339972496033\n",
      "Train: Epoch [11], Batch [810/938], Loss: 0.6765977740287781\n",
      "Train: Epoch [11], Batch [811/938], Loss: 0.7386398315429688\n",
      "Train: Epoch [11], Batch [812/938], Loss: 0.6941037178039551\n",
      "Train: Epoch [11], Batch [813/938], Loss: 0.5850415229797363\n",
      "Train: Epoch [11], Batch [814/938], Loss: 0.9376945495605469\n",
      "Train: Epoch [11], Batch [815/938], Loss: 0.7667390704154968\n",
      "Train: Epoch [11], Batch [816/938], Loss: 0.7785974740982056\n",
      "Train: Epoch [11], Batch [817/938], Loss: 0.8274535536766052\n",
      "Train: Epoch [11], Batch [818/938], Loss: 0.7598127126693726\n",
      "Train: Epoch [11], Batch [819/938], Loss: 0.8197196125984192\n",
      "Train: Epoch [11], Batch [820/938], Loss: 0.5200109481811523\n",
      "Train: Epoch [11], Batch [821/938], Loss: 0.9357410073280334\n",
      "Train: Epoch [11], Batch [822/938], Loss: 0.6735294461250305\n",
      "Train: Epoch [11], Batch [823/938], Loss: 0.8237937688827515\n",
      "Train: Epoch [11], Batch [824/938], Loss: 0.6905878782272339\n",
      "Train: Epoch [11], Batch [825/938], Loss: 0.7962015867233276\n",
      "Train: Epoch [11], Batch [826/938], Loss: 0.8633991479873657\n",
      "Train: Epoch [11], Batch [827/938], Loss: 0.7417261600494385\n",
      "Train: Epoch [11], Batch [828/938], Loss: 1.0264685153961182\n",
      "Train: Epoch [11], Batch [829/938], Loss: 0.7443471550941467\n",
      "Train: Epoch [11], Batch [830/938], Loss: 0.726464569568634\n",
      "Train: Epoch [11], Batch [831/938], Loss: 0.5966507196426392\n",
      "Train: Epoch [11], Batch [832/938], Loss: 0.7288017868995667\n",
      "Train: Epoch [11], Batch [833/938], Loss: 0.8394390344619751\n",
      "Train: Epoch [11], Batch [834/938], Loss: 0.7567363977432251\n",
      "Train: Epoch [11], Batch [835/938], Loss: 0.5944148302078247\n",
      "Train: Epoch [11], Batch [836/938], Loss: 0.7592294812202454\n",
      "Train: Epoch [11], Batch [837/938], Loss: 0.711067795753479\n",
      "Train: Epoch [11], Batch [838/938], Loss: 0.7578238844871521\n",
      "Train: Epoch [11], Batch [839/938], Loss: 0.7608618140220642\n",
      "Train: Epoch [11], Batch [840/938], Loss: 0.8421643972396851\n",
      "Train: Epoch [11], Batch [841/938], Loss: 0.783277690410614\n",
      "Train: Epoch [11], Batch [842/938], Loss: 0.8985229134559631\n",
      "Train: Epoch [11], Batch [843/938], Loss: 0.9015684127807617\n",
      "Train: Epoch [11], Batch [844/938], Loss: 0.6963240504264832\n",
      "Train: Epoch [11], Batch [845/938], Loss: 0.6196302175521851\n",
      "Train: Epoch [11], Batch [846/938], Loss: 0.6279288530349731\n",
      "Train: Epoch [11], Batch [847/938], Loss: 0.8815827965736389\n",
      "Train: Epoch [11], Batch [848/938], Loss: 0.692361056804657\n",
      "Train: Epoch [11], Batch [849/938], Loss: 0.6946876645088196\n",
      "Train: Epoch [11], Batch [850/938], Loss: 0.6104695200920105\n",
      "Train: Epoch [11], Batch [851/938], Loss: 0.7447667121887207\n",
      "Train: Epoch [11], Batch [852/938], Loss: 0.8458213806152344\n",
      "Train: Epoch [11], Batch [853/938], Loss: 0.7693254351615906\n",
      "Train: Epoch [11], Batch [854/938], Loss: 0.6558864712715149\n",
      "Train: Epoch [11], Batch [855/938], Loss: 0.5341007113456726\n",
      "Train: Epoch [11], Batch [856/938], Loss: 0.9032886028289795\n",
      "Train: Epoch [11], Batch [857/938], Loss: 1.1133196353912354\n",
      "Train: Epoch [11], Batch [858/938], Loss: 0.6398864984512329\n",
      "Train: Epoch [11], Batch [859/938], Loss: 0.737775444984436\n",
      "Train: Epoch [11], Batch [860/938], Loss: 0.7729437947273254\n",
      "Train: Epoch [11], Batch [861/938], Loss: 0.6829509735107422\n",
      "Train: Epoch [11], Batch [862/938], Loss: 1.00962495803833\n",
      "Train: Epoch [11], Batch [863/938], Loss: 0.7721769213676453\n",
      "Train: Epoch [11], Batch [864/938], Loss: 0.9044138193130493\n",
      "Train: Epoch [11], Batch [865/938], Loss: 0.8391336798667908\n",
      "Train: Epoch [11], Batch [866/938], Loss: 0.788140058517456\n",
      "Train: Epoch [11], Batch [867/938], Loss: 0.7139400839805603\n",
      "Train: Epoch [11], Batch [868/938], Loss: 0.6020691990852356\n",
      "Train: Epoch [11], Batch [869/938], Loss: 0.7039666771888733\n",
      "Train: Epoch [11], Batch [870/938], Loss: 1.071289300918579\n",
      "Train: Epoch [11], Batch [871/938], Loss: 0.5942345261573792\n",
      "Train: Epoch [11], Batch [872/938], Loss: 0.7452487945556641\n",
      "Train: Epoch [11], Batch [873/938], Loss: 0.6175490617752075\n",
      "Train: Epoch [11], Batch [874/938], Loss: 0.8805888891220093\n",
      "Train: Epoch [11], Batch [875/938], Loss: 0.8323351144790649\n",
      "Train: Epoch [11], Batch [876/938], Loss: 0.8157336711883545\n",
      "Train: Epoch [11], Batch [877/938], Loss: 0.8358474969863892\n",
      "Train: Epoch [11], Batch [878/938], Loss: 0.9127805233001709\n",
      "Train: Epoch [11], Batch [879/938], Loss: 0.4695228338241577\n",
      "Train: Epoch [11], Batch [880/938], Loss: 0.7311238646507263\n",
      "Train: Epoch [11], Batch [881/938], Loss: 0.7907691597938538\n",
      "Train: Epoch [11], Batch [882/938], Loss: 0.5932739973068237\n",
      "Train: Epoch [11], Batch [883/938], Loss: 0.9625334739685059\n",
      "Train: Epoch [11], Batch [884/938], Loss: 0.6560389399528503\n",
      "Train: Epoch [11], Batch [885/938], Loss: 0.8613330721855164\n",
      "Train: Epoch [11], Batch [886/938], Loss: 0.7430307865142822\n",
      "Train: Epoch [11], Batch [887/938], Loss: 0.7302956581115723\n",
      "Train: Epoch [11], Batch [888/938], Loss: 0.4957033097743988\n",
      "Train: Epoch [11], Batch [889/938], Loss: 0.9011211395263672\n",
      "Train: Epoch [11], Batch [890/938], Loss: 0.9759224653244019\n",
      "Train: Epoch [11], Batch [891/938], Loss: 0.8616957664489746\n",
      "Train: Epoch [11], Batch [892/938], Loss: 0.909608006477356\n",
      "Train: Epoch [11], Batch [893/938], Loss: 0.8652310371398926\n",
      "Train: Epoch [11], Batch [894/938], Loss: 0.7170683145523071\n",
      "Train: Epoch [11], Batch [895/938], Loss: 0.7784495949745178\n",
      "Train: Epoch [11], Batch [896/938], Loss: 0.6556669473648071\n",
      "Train: Epoch [11], Batch [897/938], Loss: 0.6566387414932251\n",
      "Train: Epoch [11], Batch [898/938], Loss: 0.7506220936775208\n",
      "Train: Epoch [11], Batch [899/938], Loss: 0.6066071391105652\n",
      "Train: Epoch [11], Batch [900/938], Loss: 0.6883223652839661\n",
      "Train: Epoch [11], Batch [901/938], Loss: 0.6737749576568604\n",
      "Train: Epoch [11], Batch [902/938], Loss: 0.7206810712814331\n",
      "Train: Epoch [11], Batch [903/938], Loss: 0.9674473404884338\n",
      "Train: Epoch [11], Batch [904/938], Loss: 0.6856884956359863\n",
      "Train: Epoch [11], Batch [905/938], Loss: 0.8513481020927429\n",
      "Train: Epoch [11], Batch [906/938], Loss: 0.6994560956954956\n",
      "Train: Epoch [11], Batch [907/938], Loss: 0.8127126097679138\n",
      "Train: Epoch [11], Batch [908/938], Loss: 0.6981290578842163\n",
      "Train: Epoch [11], Batch [909/938], Loss: 0.7483503818511963\n",
      "Train: Epoch [11], Batch [910/938], Loss: 0.5925989151000977\n",
      "Train: Epoch [11], Batch [911/938], Loss: 0.694298267364502\n",
      "Train: Epoch [11], Batch [912/938], Loss: 0.6919248104095459\n",
      "Train: Epoch [11], Batch [913/938], Loss: 0.9173503518104553\n",
      "Train: Epoch [11], Batch [914/938], Loss: 0.6985718011856079\n",
      "Train: Epoch [11], Batch [915/938], Loss: 0.7677572965621948\n",
      "Train: Epoch [11], Batch [916/938], Loss: 1.050119161605835\n",
      "Train: Epoch [11], Batch [917/938], Loss: 0.5927342176437378\n",
      "Train: Epoch [11], Batch [918/938], Loss: 0.9060000777244568\n",
      "Train: Epoch [11], Batch [919/938], Loss: 0.939194917678833\n",
      "Train: Epoch [11], Batch [920/938], Loss: 0.5773391127586365\n",
      "Train: Epoch [11], Batch [921/938], Loss: 0.6927428245544434\n",
      "Train: Epoch [11], Batch [922/938], Loss: 0.6538745164871216\n",
      "Train: Epoch [11], Batch [923/938], Loss: 0.8066227436065674\n",
      "Train: Epoch [11], Batch [924/938], Loss: 0.568975567817688\n",
      "Train: Epoch [11], Batch [925/938], Loss: 0.5664203763008118\n",
      "Train: Epoch [11], Batch [926/938], Loss: 0.5761436223983765\n",
      "Train: Epoch [11], Batch [927/938], Loss: 0.7594307661056519\n",
      "Train: Epoch [11], Batch [928/938], Loss: 0.8331332802772522\n",
      "Train: Epoch [11], Batch [929/938], Loss: 0.7835724949836731\n",
      "Train: Epoch [11], Batch [930/938], Loss: 0.4483189880847931\n",
      "Train: Epoch [11], Batch [931/938], Loss: 0.6725081205368042\n",
      "Train: Epoch [11], Batch [932/938], Loss: 0.6397736668586731\n",
      "Train: Epoch [11], Batch [933/938], Loss: 0.5672023296356201\n",
      "Train: Epoch [11], Batch [934/938], Loss: 0.6835088729858398\n",
      "Train: Epoch [11], Batch [935/938], Loss: 0.8262155652046204\n",
      "Train: Epoch [11], Batch [936/938], Loss: 0.8920527100563049\n",
      "Train: Epoch [11], Batch [937/938], Loss: 0.6249427199363708\n",
      "Train: Epoch [11], Batch [938/938], Loss: 0.9502089619636536\n",
      "Accuracy of train set: 0.7613166666666666\n",
      "Validation: Epoch [11], Batch [1/938], Loss: 0.7411060929298401\n",
      "Validation: Epoch [11], Batch [2/938], Loss: 0.714749813079834\n",
      "Validation: Epoch [11], Batch [3/938], Loss: 0.9376991391181946\n",
      "Validation: Epoch [11], Batch [4/938], Loss: 0.9846900701522827\n",
      "Validation: Epoch [11], Batch [5/938], Loss: 1.0802284479141235\n",
      "Validation: Epoch [11], Batch [6/938], Loss: 0.9725731611251831\n",
      "Validation: Epoch [11], Batch [7/938], Loss: 0.6259284615516663\n",
      "Validation: Epoch [11], Batch [8/938], Loss: 0.6917929649353027\n",
      "Validation: Epoch [11], Batch [9/938], Loss: 0.8552204966545105\n",
      "Validation: Epoch [11], Batch [10/938], Loss: 0.6596954464912415\n",
      "Validation: Epoch [11], Batch [11/938], Loss: 0.6810198426246643\n",
      "Validation: Epoch [11], Batch [12/938], Loss: 0.6576033234596252\n",
      "Validation: Epoch [11], Batch [13/938], Loss: 0.6172006726264954\n",
      "Validation: Epoch [11], Batch [14/938], Loss: 0.6431103944778442\n",
      "Validation: Epoch [11], Batch [15/938], Loss: 0.5591498613357544\n",
      "Validation: Epoch [11], Batch [16/938], Loss: 0.6975302696228027\n",
      "Validation: Epoch [11], Batch [17/938], Loss: 0.7248398065567017\n",
      "Validation: Epoch [11], Batch [18/938], Loss: 0.9092235565185547\n",
      "Validation: Epoch [11], Batch [19/938], Loss: 0.6259169578552246\n",
      "Validation: Epoch [11], Batch [20/938], Loss: 0.8353468775749207\n",
      "Validation: Epoch [11], Batch [21/938], Loss: 0.48601701855659485\n",
      "Validation: Epoch [11], Batch [22/938], Loss: 0.7427812218666077\n",
      "Validation: Epoch [11], Batch [23/938], Loss: 0.8187416195869446\n",
      "Validation: Epoch [11], Batch [24/938], Loss: 0.9040126204490662\n",
      "Validation: Epoch [11], Batch [25/938], Loss: 0.6814225912094116\n",
      "Validation: Epoch [11], Batch [26/938], Loss: 0.6040704846382141\n",
      "Validation: Epoch [11], Batch [27/938], Loss: 0.7011510729789734\n",
      "Validation: Epoch [11], Batch [28/938], Loss: 0.8136663436889648\n",
      "Validation: Epoch [11], Batch [29/938], Loss: 0.8913048505783081\n",
      "Validation: Epoch [11], Batch [30/938], Loss: 0.6816651225090027\n",
      "Validation: Epoch [11], Batch [31/938], Loss: 0.868035614490509\n",
      "Validation: Epoch [11], Batch [32/938], Loss: 0.8978487849235535\n",
      "Validation: Epoch [11], Batch [33/938], Loss: 1.0421391725540161\n",
      "Validation: Epoch [11], Batch [34/938], Loss: 0.6174052357673645\n",
      "Validation: Epoch [11], Batch [35/938], Loss: 0.7174779772758484\n",
      "Validation: Epoch [11], Batch [36/938], Loss: 0.6709403991699219\n",
      "Validation: Epoch [11], Batch [37/938], Loss: 0.8703948259353638\n",
      "Validation: Epoch [11], Batch [38/938], Loss: 0.6511309146881104\n",
      "Validation: Epoch [11], Batch [39/938], Loss: 0.44308391213417053\n",
      "Validation: Epoch [11], Batch [40/938], Loss: 0.7325155735015869\n",
      "Validation: Epoch [11], Batch [41/938], Loss: 0.5781309604644775\n",
      "Validation: Epoch [11], Batch [42/938], Loss: 0.6303820013999939\n",
      "Validation: Epoch [11], Batch [43/938], Loss: 0.5350252389907837\n",
      "Validation: Epoch [11], Batch [44/938], Loss: 0.6711010336875916\n",
      "Validation: Epoch [11], Batch [45/938], Loss: 0.8203141689300537\n",
      "Validation: Epoch [11], Batch [46/938], Loss: 0.6800856590270996\n",
      "Validation: Epoch [11], Batch [47/938], Loss: 0.7763006091117859\n",
      "Validation: Epoch [11], Batch [48/938], Loss: 0.7184061408042908\n",
      "Validation: Epoch [11], Batch [49/938], Loss: 0.6729357242584229\n",
      "Validation: Epoch [11], Batch [50/938], Loss: 0.8627818822860718\n",
      "Validation: Epoch [11], Batch [51/938], Loss: 0.6233099102973938\n",
      "Validation: Epoch [11], Batch [52/938], Loss: 0.8304489850997925\n",
      "Validation: Epoch [11], Batch [53/938], Loss: 0.6270372867584229\n",
      "Validation: Epoch [11], Batch [54/938], Loss: 0.7326188087463379\n",
      "Validation: Epoch [11], Batch [55/938], Loss: 1.198750615119934\n",
      "Validation: Epoch [11], Batch [56/938], Loss: 0.6901384592056274\n",
      "Validation: Epoch [11], Batch [57/938], Loss: 0.5715227127075195\n",
      "Validation: Epoch [11], Batch [58/938], Loss: 0.8694213032722473\n",
      "Validation: Epoch [11], Batch [59/938], Loss: 0.7451141476631165\n",
      "Validation: Epoch [11], Batch [60/938], Loss: 0.5962507128715515\n",
      "Validation: Epoch [11], Batch [61/938], Loss: 1.0424896478652954\n",
      "Validation: Epoch [11], Batch [62/938], Loss: 0.7162905931472778\n",
      "Validation: Epoch [11], Batch [63/938], Loss: 0.7375870943069458\n",
      "Validation: Epoch [11], Batch [64/938], Loss: 0.6620697379112244\n",
      "Validation: Epoch [11], Batch [65/938], Loss: 0.7446837425231934\n",
      "Validation: Epoch [11], Batch [66/938], Loss: 0.6699891686439514\n",
      "Validation: Epoch [11], Batch [67/938], Loss: 0.8859872817993164\n",
      "Validation: Epoch [11], Batch [68/938], Loss: 0.645589292049408\n",
      "Validation: Epoch [11], Batch [69/938], Loss: 0.7252709865570068\n",
      "Validation: Epoch [11], Batch [70/938], Loss: 0.7399603128433228\n",
      "Validation: Epoch [11], Batch [71/938], Loss: 0.8028007745742798\n",
      "Validation: Epoch [11], Batch [72/938], Loss: 1.0127677917480469\n",
      "Validation: Epoch [11], Batch [73/938], Loss: 0.6058209538459778\n",
      "Validation: Epoch [11], Batch [74/938], Loss: 0.6601250171661377\n",
      "Validation: Epoch [11], Batch [75/938], Loss: 0.710906982421875\n",
      "Validation: Epoch [11], Batch [76/938], Loss: 0.7846039533615112\n",
      "Validation: Epoch [11], Batch [77/938], Loss: 1.0391252040863037\n",
      "Validation: Epoch [11], Batch [78/938], Loss: 0.7909961938858032\n",
      "Validation: Epoch [11], Batch [79/938], Loss: 0.9781646132469177\n",
      "Validation: Epoch [11], Batch [80/938], Loss: 0.5499598979949951\n",
      "Validation: Epoch [11], Batch [81/938], Loss: 0.5901602506637573\n",
      "Validation: Epoch [11], Batch [82/938], Loss: 0.8108500838279724\n",
      "Validation: Epoch [11], Batch [83/938], Loss: 0.7828226685523987\n",
      "Validation: Epoch [11], Batch [84/938], Loss: 0.7857496738433838\n",
      "Validation: Epoch [11], Batch [85/938], Loss: 0.6817522644996643\n",
      "Validation: Epoch [11], Batch [86/938], Loss: 0.6936779618263245\n",
      "Validation: Epoch [11], Batch [87/938], Loss: 0.7380965352058411\n",
      "Validation: Epoch [11], Batch [88/938], Loss: 0.8971920609474182\n",
      "Validation: Epoch [11], Batch [89/938], Loss: 0.5525328516960144\n",
      "Validation: Epoch [11], Batch [90/938], Loss: 0.45964616537094116\n",
      "Validation: Epoch [11], Batch [91/938], Loss: 1.0955712795257568\n",
      "Validation: Epoch [11], Batch [92/938], Loss: 1.0205904245376587\n",
      "Validation: Epoch [11], Batch [93/938], Loss: 0.7418370842933655\n",
      "Validation: Epoch [11], Batch [94/938], Loss: 0.6218724250793457\n",
      "Validation: Epoch [11], Batch [95/938], Loss: 0.6267651319503784\n",
      "Validation: Epoch [11], Batch [96/938], Loss: 0.8527312278747559\n",
      "Validation: Epoch [11], Batch [97/938], Loss: 0.6748570799827576\n",
      "Validation: Epoch [11], Batch [98/938], Loss: 0.862653911113739\n",
      "Validation: Epoch [11], Batch [99/938], Loss: 0.7229938507080078\n",
      "Validation: Epoch [11], Batch [100/938], Loss: 0.6486449241638184\n",
      "Validation: Epoch [11], Batch [101/938], Loss: 0.9044399261474609\n",
      "Validation: Epoch [11], Batch [102/938], Loss: 0.6265792846679688\n",
      "Validation: Epoch [11], Batch [103/938], Loss: 0.8066744804382324\n",
      "Validation: Epoch [11], Batch [104/938], Loss: 0.7822646498680115\n",
      "Validation: Epoch [11], Batch [105/938], Loss: 0.8062307238578796\n",
      "Validation: Epoch [11], Batch [106/938], Loss: 0.5923610925674438\n",
      "Validation: Epoch [11], Batch [107/938], Loss: 0.7069990634918213\n",
      "Validation: Epoch [11], Batch [108/938], Loss: 0.6173259019851685\n",
      "Validation: Epoch [11], Batch [109/938], Loss: 0.7761761546134949\n",
      "Validation: Epoch [11], Batch [110/938], Loss: 0.6754347085952759\n",
      "Validation: Epoch [11], Batch [111/938], Loss: 0.8769553899765015\n",
      "Validation: Epoch [11], Batch [112/938], Loss: 0.928132176399231\n",
      "Validation: Epoch [11], Batch [113/938], Loss: 0.6920276880264282\n",
      "Validation: Epoch [11], Batch [114/938], Loss: 0.7343646287918091\n",
      "Validation: Epoch [11], Batch [115/938], Loss: 0.704696774482727\n",
      "Validation: Epoch [11], Batch [116/938], Loss: 0.7809100151062012\n",
      "Validation: Epoch [11], Batch [117/938], Loss: 0.6597804427146912\n",
      "Validation: Epoch [11], Batch [118/938], Loss: 0.7305725812911987\n",
      "Validation: Epoch [11], Batch [119/938], Loss: 0.7411986589431763\n",
      "Validation: Epoch [11], Batch [120/938], Loss: 0.7181110382080078\n",
      "Validation: Epoch [11], Batch [121/938], Loss: 0.736992359161377\n",
      "Validation: Epoch [11], Batch [122/938], Loss: 0.7941946387290955\n",
      "Validation: Epoch [11], Batch [123/938], Loss: 0.8295747637748718\n",
      "Validation: Epoch [11], Batch [124/938], Loss: 0.6721619963645935\n",
      "Validation: Epoch [11], Batch [125/938], Loss: 0.8163526058197021\n",
      "Validation: Epoch [11], Batch [126/938], Loss: 0.660982608795166\n",
      "Validation: Epoch [11], Batch [127/938], Loss: 0.742733359336853\n",
      "Validation: Epoch [11], Batch [128/938], Loss: 0.893359363079071\n",
      "Validation: Epoch [11], Batch [129/938], Loss: 0.778568685054779\n",
      "Validation: Epoch [11], Batch [130/938], Loss: 0.7271828055381775\n",
      "Validation: Epoch [11], Batch [131/938], Loss: 0.8132642507553101\n",
      "Validation: Epoch [11], Batch [132/938], Loss: 0.8456928730010986\n",
      "Validation: Epoch [11], Batch [133/938], Loss: 0.7363535165786743\n",
      "Validation: Epoch [11], Batch [134/938], Loss: 0.5832628011703491\n",
      "Validation: Epoch [11], Batch [135/938], Loss: 0.8971400260925293\n",
      "Validation: Epoch [11], Batch [136/938], Loss: 0.5989091992378235\n",
      "Validation: Epoch [11], Batch [137/938], Loss: 0.5485461950302124\n",
      "Validation: Epoch [11], Batch [138/938], Loss: 0.8013946413993835\n",
      "Validation: Epoch [11], Batch [139/938], Loss: 0.7326147556304932\n",
      "Validation: Epoch [11], Batch [140/938], Loss: 0.6228833794593811\n",
      "Validation: Epoch [11], Batch [141/938], Loss: 0.6362420320510864\n",
      "Validation: Epoch [11], Batch [142/938], Loss: 0.8122506141662598\n",
      "Validation: Epoch [11], Batch [143/938], Loss: 0.813108503818512\n",
      "Validation: Epoch [11], Batch [144/938], Loss: 0.7383044362068176\n",
      "Validation: Epoch [11], Batch [145/938], Loss: 0.7005888223648071\n",
      "Validation: Epoch [11], Batch [146/938], Loss: 0.6494114398956299\n",
      "Validation: Epoch [11], Batch [147/938], Loss: 0.9875883460044861\n",
      "Validation: Epoch [11], Batch [148/938], Loss: 1.0930055379867554\n",
      "Validation: Epoch [11], Batch [149/938], Loss: 0.7847416400909424\n",
      "Validation: Epoch [11], Batch [150/938], Loss: 0.8439689874649048\n",
      "Validation: Epoch [11], Batch [151/938], Loss: 0.7523910999298096\n",
      "Validation: Epoch [11], Batch [152/938], Loss: 0.734946608543396\n",
      "Validation: Epoch [11], Batch [153/938], Loss: 0.7646070718765259\n",
      "Validation: Epoch [11], Batch [154/938], Loss: 0.5418619513511658\n",
      "Validation: Epoch [11], Batch [155/938], Loss: 0.7280007600784302\n",
      "Validation: Epoch [11], Batch [156/938], Loss: 0.723532497882843\n",
      "Validation: Epoch [11], Batch [157/938], Loss: 0.6173466444015503\n",
      "Validation: Epoch [11], Batch [158/938], Loss: 0.7868579030036926\n",
      "Validation: Epoch [11], Batch [159/938], Loss: 0.7015993595123291\n",
      "Validation: Epoch [11], Batch [160/938], Loss: 0.807498037815094\n",
      "Validation: Epoch [11], Batch [161/938], Loss: 0.8739584684371948\n",
      "Validation: Epoch [11], Batch [162/938], Loss: 0.6405470371246338\n",
      "Validation: Epoch [11], Batch [163/938], Loss: 0.6318829655647278\n",
      "Validation: Epoch [11], Batch [164/938], Loss: 0.857135534286499\n",
      "Validation: Epoch [11], Batch [165/938], Loss: 0.6267605423927307\n",
      "Validation: Epoch [11], Batch [166/938], Loss: 0.7802643775939941\n",
      "Validation: Epoch [11], Batch [167/938], Loss: 0.9563258290290833\n",
      "Validation: Epoch [11], Batch [168/938], Loss: 0.9956563711166382\n",
      "Validation: Epoch [11], Batch [169/938], Loss: 0.6569566130638123\n",
      "Validation: Epoch [11], Batch [170/938], Loss: 0.997181236743927\n",
      "Validation: Epoch [11], Batch [171/938], Loss: 0.8547019362449646\n",
      "Validation: Epoch [11], Batch [172/938], Loss: 0.5800871849060059\n",
      "Validation: Epoch [11], Batch [173/938], Loss: 0.824709415435791\n",
      "Validation: Epoch [11], Batch [174/938], Loss: 0.8423128724098206\n",
      "Validation: Epoch [11], Batch [175/938], Loss: 0.8964800834655762\n",
      "Validation: Epoch [11], Batch [176/938], Loss: 0.700899064540863\n",
      "Validation: Epoch [11], Batch [177/938], Loss: 0.7622343897819519\n",
      "Validation: Epoch [11], Batch [178/938], Loss: 0.850990355014801\n",
      "Validation: Epoch [11], Batch [179/938], Loss: 0.9045518636703491\n",
      "Validation: Epoch [11], Batch [180/938], Loss: 0.5088721513748169\n",
      "Validation: Epoch [11], Batch [181/938], Loss: 0.7954345941543579\n",
      "Validation: Epoch [11], Batch [182/938], Loss: 0.6637977361679077\n",
      "Validation: Epoch [11], Batch [183/938], Loss: 0.7959426641464233\n",
      "Validation: Epoch [11], Batch [184/938], Loss: 0.8667096495628357\n",
      "Validation: Epoch [11], Batch [185/938], Loss: 0.7437108755111694\n",
      "Validation: Epoch [11], Batch [186/938], Loss: 0.8524933457374573\n",
      "Validation: Epoch [11], Batch [187/938], Loss: 0.704522967338562\n",
      "Validation: Epoch [11], Batch [188/938], Loss: 0.6283533573150635\n",
      "Validation: Epoch [11], Batch [189/938], Loss: 0.7863783240318298\n",
      "Validation: Epoch [11], Batch [190/938], Loss: 0.7371371984481812\n",
      "Validation: Epoch [11], Batch [191/938], Loss: 0.9680537581443787\n",
      "Validation: Epoch [11], Batch [192/938], Loss: 0.8987570405006409\n",
      "Validation: Epoch [11], Batch [193/938], Loss: 0.8903205990791321\n",
      "Validation: Epoch [11], Batch [194/938], Loss: 0.623042106628418\n",
      "Validation: Epoch [11], Batch [195/938], Loss: 0.9682539701461792\n",
      "Validation: Epoch [11], Batch [196/938], Loss: 0.5199872255325317\n",
      "Validation: Epoch [11], Batch [197/938], Loss: 1.030917763710022\n",
      "Validation: Epoch [11], Batch [198/938], Loss: 0.6263987421989441\n",
      "Validation: Epoch [11], Batch [199/938], Loss: 0.9120081663131714\n",
      "Validation: Epoch [11], Batch [200/938], Loss: 0.9567853212356567\n",
      "Validation: Epoch [11], Batch [201/938], Loss: 1.0151220560073853\n",
      "Validation: Epoch [11], Batch [202/938], Loss: 0.719581663608551\n",
      "Validation: Epoch [11], Batch [203/938], Loss: 0.7936868071556091\n",
      "Validation: Epoch [11], Batch [204/938], Loss: 0.8958561420440674\n",
      "Validation: Epoch [11], Batch [205/938], Loss: 0.6237481236457825\n",
      "Validation: Epoch [11], Batch [206/938], Loss: 0.6992292404174805\n",
      "Validation: Epoch [11], Batch [207/938], Loss: 0.81565260887146\n",
      "Validation: Epoch [11], Batch [208/938], Loss: 0.7175761461257935\n",
      "Validation: Epoch [11], Batch [209/938], Loss: 0.8789291381835938\n",
      "Validation: Epoch [11], Batch [210/938], Loss: 0.8049509525299072\n",
      "Validation: Epoch [11], Batch [211/938], Loss: 0.801306962966919\n",
      "Validation: Epoch [11], Batch [212/938], Loss: 0.7571418881416321\n",
      "Validation: Epoch [11], Batch [213/938], Loss: 0.9953142404556274\n",
      "Validation: Epoch [11], Batch [214/938], Loss: 0.7066730856895447\n",
      "Validation: Epoch [11], Batch [215/938], Loss: 1.0435329675674438\n",
      "Validation: Epoch [11], Batch [216/938], Loss: 1.0075072050094604\n",
      "Validation: Epoch [11], Batch [217/938], Loss: 0.8725259304046631\n",
      "Validation: Epoch [11], Batch [218/938], Loss: 0.7939698696136475\n",
      "Validation: Epoch [11], Batch [219/938], Loss: 0.9827376008033752\n",
      "Validation: Epoch [11], Batch [220/938], Loss: 0.731746256351471\n",
      "Validation: Epoch [11], Batch [221/938], Loss: 0.6318188905715942\n",
      "Validation: Epoch [11], Batch [222/938], Loss: 0.551393985748291\n",
      "Validation: Epoch [11], Batch [223/938], Loss: 0.7912372350692749\n",
      "Validation: Epoch [11], Batch [224/938], Loss: 0.7447934150695801\n",
      "Validation: Epoch [11], Batch [225/938], Loss: 0.7913340926170349\n",
      "Validation: Epoch [11], Batch [226/938], Loss: 0.9166520237922668\n",
      "Validation: Epoch [11], Batch [227/938], Loss: 0.6867952346801758\n",
      "Validation: Epoch [11], Batch [228/938], Loss: 0.5177971720695496\n",
      "Validation: Epoch [11], Batch [229/938], Loss: 0.41841575503349304\n",
      "Validation: Epoch [11], Batch [230/938], Loss: 0.5799096822738647\n",
      "Validation: Epoch [11], Batch [231/938], Loss: 0.7768620252609253\n",
      "Validation: Epoch [11], Batch [232/938], Loss: 0.816206693649292\n",
      "Validation: Epoch [11], Batch [233/938], Loss: 0.8285819292068481\n",
      "Validation: Epoch [11], Batch [234/938], Loss: 0.9170450568199158\n",
      "Validation: Epoch [11], Batch [235/938], Loss: 0.6599294543266296\n",
      "Validation: Epoch [11], Batch [236/938], Loss: 0.8557013273239136\n",
      "Validation: Epoch [11], Batch [237/938], Loss: 0.7373681664466858\n",
      "Validation: Epoch [11], Batch [238/938], Loss: 0.535466194152832\n",
      "Validation: Epoch [11], Batch [239/938], Loss: 1.1427881717681885\n",
      "Validation: Epoch [11], Batch [240/938], Loss: 0.7462223172187805\n",
      "Validation: Epoch [11], Batch [241/938], Loss: 0.969778299331665\n",
      "Validation: Epoch [11], Batch [242/938], Loss: 0.8218303918838501\n",
      "Validation: Epoch [11], Batch [243/938], Loss: 0.8140268921852112\n",
      "Validation: Epoch [11], Batch [244/938], Loss: 1.1239287853240967\n",
      "Validation: Epoch [11], Batch [245/938], Loss: 0.6682441234588623\n",
      "Validation: Epoch [11], Batch [246/938], Loss: 0.6217754483222961\n",
      "Validation: Epoch [11], Batch [247/938], Loss: 0.46136417984962463\n",
      "Validation: Epoch [11], Batch [248/938], Loss: 0.9842053055763245\n",
      "Validation: Epoch [11], Batch [249/938], Loss: 0.7703543305397034\n",
      "Validation: Epoch [11], Batch [250/938], Loss: 0.9429216384887695\n",
      "Validation: Epoch [11], Batch [251/938], Loss: 1.126124620437622\n",
      "Validation: Epoch [11], Batch [252/938], Loss: 0.6661256551742554\n",
      "Validation: Epoch [11], Batch [253/938], Loss: 0.7022806406021118\n",
      "Validation: Epoch [11], Batch [254/938], Loss: 0.750546395778656\n",
      "Validation: Epoch [11], Batch [255/938], Loss: 0.8439173698425293\n",
      "Validation: Epoch [11], Batch [256/938], Loss: 0.6329182386398315\n",
      "Validation: Epoch [11], Batch [257/938], Loss: 0.6462413668632507\n",
      "Validation: Epoch [11], Batch [258/938], Loss: 0.6677070260047913\n",
      "Validation: Epoch [11], Batch [259/938], Loss: 0.8161150217056274\n",
      "Validation: Epoch [11], Batch [260/938], Loss: 0.5836321115493774\n",
      "Validation: Epoch [11], Batch [261/938], Loss: 0.7454469799995422\n",
      "Validation: Epoch [11], Batch [262/938], Loss: 0.9654380083084106\n",
      "Validation: Epoch [11], Batch [263/938], Loss: 0.6855036020278931\n",
      "Validation: Epoch [11], Batch [264/938], Loss: 0.6930362582206726\n",
      "Validation: Epoch [11], Batch [265/938], Loss: 0.7992058396339417\n",
      "Validation: Epoch [11], Batch [266/938], Loss: 0.9000034332275391\n",
      "Validation: Epoch [11], Batch [267/938], Loss: 0.8521766662597656\n",
      "Validation: Epoch [11], Batch [268/938], Loss: 1.0740591287612915\n",
      "Validation: Epoch [11], Batch [269/938], Loss: 0.7108055353164673\n",
      "Validation: Epoch [11], Batch [270/938], Loss: 0.7022901177406311\n",
      "Validation: Epoch [11], Batch [271/938], Loss: 0.6103681921958923\n",
      "Validation: Epoch [11], Batch [272/938], Loss: 0.8126770257949829\n",
      "Validation: Epoch [11], Batch [273/938], Loss: 0.7544663548469543\n",
      "Validation: Epoch [11], Batch [274/938], Loss: 0.8643602728843689\n",
      "Validation: Epoch [11], Batch [275/938], Loss: 0.6885844469070435\n",
      "Validation: Epoch [11], Batch [276/938], Loss: 0.7968968152999878\n",
      "Validation: Epoch [11], Batch [277/938], Loss: 0.9180481433868408\n",
      "Validation: Epoch [11], Batch [278/938], Loss: 0.8532962799072266\n",
      "Validation: Epoch [11], Batch [279/938], Loss: 1.0157756805419922\n",
      "Validation: Epoch [11], Batch [280/938], Loss: 0.5778663754463196\n",
      "Validation: Epoch [11], Batch [281/938], Loss: 0.6671310067176819\n",
      "Validation: Epoch [11], Batch [282/938], Loss: 0.7059677243232727\n",
      "Validation: Epoch [11], Batch [283/938], Loss: 0.6357266902923584\n",
      "Validation: Epoch [11], Batch [284/938], Loss: 0.8636236786842346\n",
      "Validation: Epoch [11], Batch [285/938], Loss: 0.6309233903884888\n",
      "Validation: Epoch [11], Batch [286/938], Loss: 0.8362122178077698\n",
      "Validation: Epoch [11], Batch [287/938], Loss: 0.986907422542572\n",
      "Validation: Epoch [11], Batch [288/938], Loss: 0.7259145379066467\n",
      "Validation: Epoch [11], Batch [289/938], Loss: 0.7183157205581665\n",
      "Validation: Epoch [11], Batch [290/938], Loss: 0.6451488733291626\n",
      "Validation: Epoch [11], Batch [291/938], Loss: 0.7330382466316223\n",
      "Validation: Epoch [11], Batch [292/938], Loss: 0.7849735617637634\n",
      "Validation: Epoch [11], Batch [293/938], Loss: 0.7849915027618408\n",
      "Validation: Epoch [11], Batch [294/938], Loss: 0.7179479598999023\n",
      "Validation: Epoch [11], Batch [295/938], Loss: 0.8598261475563049\n",
      "Validation: Epoch [11], Batch [296/938], Loss: 0.5714139342308044\n",
      "Validation: Epoch [11], Batch [297/938], Loss: 0.818135142326355\n",
      "Validation: Epoch [11], Batch [298/938], Loss: 0.7118405699729919\n",
      "Validation: Epoch [11], Batch [299/938], Loss: 0.6298600435256958\n",
      "Validation: Epoch [11], Batch [300/938], Loss: 0.8930805325508118\n",
      "Validation: Epoch [11], Batch [301/938], Loss: 0.6506786942481995\n",
      "Validation: Epoch [11], Batch [302/938], Loss: 0.8262166380882263\n",
      "Validation: Epoch [11], Batch [303/938], Loss: 0.844532310962677\n",
      "Validation: Epoch [11], Batch [304/938], Loss: 0.868975818157196\n",
      "Validation: Epoch [11], Batch [305/938], Loss: 0.930423378944397\n",
      "Validation: Epoch [11], Batch [306/938], Loss: 1.0191353559494019\n",
      "Validation: Epoch [11], Batch [307/938], Loss: 0.6898199319839478\n",
      "Validation: Epoch [11], Batch [308/938], Loss: 0.6128090023994446\n",
      "Validation: Epoch [11], Batch [309/938], Loss: 0.695136308670044\n",
      "Validation: Epoch [11], Batch [310/938], Loss: 0.895920991897583\n",
      "Validation: Epoch [11], Batch [311/938], Loss: 0.7623034119606018\n",
      "Validation: Epoch [11], Batch [312/938], Loss: 0.8439141511917114\n",
      "Validation: Epoch [11], Batch [313/938], Loss: 0.8043676018714905\n",
      "Validation: Epoch [11], Batch [314/938], Loss: 0.666488528251648\n",
      "Validation: Epoch [11], Batch [315/938], Loss: 0.9378669857978821\n",
      "Validation: Epoch [11], Batch [316/938], Loss: 0.8122251033782959\n",
      "Validation: Epoch [11], Batch [317/938], Loss: 0.8415101170539856\n",
      "Validation: Epoch [11], Batch [318/938], Loss: 0.7653710842132568\n",
      "Validation: Epoch [11], Batch [319/938], Loss: 0.6903293132781982\n",
      "Validation: Epoch [11], Batch [320/938], Loss: 0.8073024749755859\n",
      "Validation: Epoch [11], Batch [321/938], Loss: 0.731078028678894\n",
      "Validation: Epoch [11], Batch [322/938], Loss: 0.8664836287498474\n",
      "Validation: Epoch [11], Batch [323/938], Loss: 0.7655667066574097\n",
      "Validation: Epoch [11], Batch [324/938], Loss: 0.813180148601532\n",
      "Validation: Epoch [11], Batch [325/938], Loss: 0.98393714427948\n",
      "Validation: Epoch [11], Batch [326/938], Loss: 1.1648954153060913\n",
      "Validation: Epoch [11], Batch [327/938], Loss: 0.6054741740226746\n",
      "Validation: Epoch [11], Batch [328/938], Loss: 1.2723926305770874\n",
      "Validation: Epoch [11], Batch [329/938], Loss: 0.7335520386695862\n",
      "Validation: Epoch [11], Batch [330/938], Loss: 0.8360893726348877\n",
      "Validation: Epoch [11], Batch [331/938], Loss: 0.7180495858192444\n",
      "Validation: Epoch [11], Batch [332/938], Loss: 0.7062488198280334\n",
      "Validation: Epoch [11], Batch [333/938], Loss: 0.765744149684906\n",
      "Validation: Epoch [11], Batch [334/938], Loss: 0.7262629270553589\n",
      "Validation: Epoch [11], Batch [335/938], Loss: 1.0908939838409424\n",
      "Validation: Epoch [11], Batch [336/938], Loss: 0.7120869159698486\n",
      "Validation: Epoch [11], Batch [337/938], Loss: 0.8486779928207397\n",
      "Validation: Epoch [11], Batch [338/938], Loss: 0.7475784420967102\n",
      "Validation: Epoch [11], Batch [339/938], Loss: 0.7137320041656494\n",
      "Validation: Epoch [11], Batch [340/938], Loss: 0.840815544128418\n",
      "Validation: Epoch [11], Batch [341/938], Loss: 0.840736448764801\n",
      "Validation: Epoch [11], Batch [342/938], Loss: 0.9457719922065735\n",
      "Validation: Epoch [11], Batch [343/938], Loss: 0.6335135698318481\n",
      "Validation: Epoch [11], Batch [344/938], Loss: 0.7523787021636963\n",
      "Validation: Epoch [11], Batch [345/938], Loss: 0.5689190626144409\n",
      "Validation: Epoch [11], Batch [346/938], Loss: 0.5796425938606262\n",
      "Validation: Epoch [11], Batch [347/938], Loss: 0.8595965504646301\n",
      "Validation: Epoch [11], Batch [348/938], Loss: 1.0823894739151\n",
      "Validation: Epoch [11], Batch [349/938], Loss: 0.8399227857589722\n",
      "Validation: Epoch [11], Batch [350/938], Loss: 0.8165431618690491\n",
      "Validation: Epoch [11], Batch [351/938], Loss: 0.6294510364532471\n",
      "Validation: Epoch [11], Batch [352/938], Loss: 1.0221840143203735\n",
      "Validation: Epoch [11], Batch [353/938], Loss: 0.8530229330062866\n",
      "Validation: Epoch [11], Batch [354/938], Loss: 0.7085186243057251\n",
      "Validation: Epoch [11], Batch [355/938], Loss: 0.4832206964492798\n",
      "Validation: Epoch [11], Batch [356/938], Loss: 0.8572435975074768\n",
      "Validation: Epoch [11], Batch [357/938], Loss: 1.0109663009643555\n",
      "Validation: Epoch [11], Batch [358/938], Loss: 1.0943483114242554\n",
      "Validation: Epoch [11], Batch [359/938], Loss: 0.584750771522522\n",
      "Validation: Epoch [11], Batch [360/938], Loss: 0.606871485710144\n",
      "Validation: Epoch [11], Batch [361/938], Loss: 1.0105012655258179\n",
      "Validation: Epoch [11], Batch [362/938], Loss: 0.679899275302887\n",
      "Validation: Epoch [11], Batch [363/938], Loss: 0.9269931316375732\n",
      "Validation: Epoch [11], Batch [364/938], Loss: 0.46681952476501465\n",
      "Validation: Epoch [11], Batch [365/938], Loss: 0.9290440082550049\n",
      "Validation: Epoch [11], Batch [366/938], Loss: 0.7196997404098511\n",
      "Validation: Epoch [11], Batch [367/938], Loss: 0.5953905582427979\n",
      "Validation: Epoch [11], Batch [368/938], Loss: 0.4730362296104431\n",
      "Validation: Epoch [11], Batch [369/938], Loss: 0.5425956845283508\n",
      "Validation: Epoch [11], Batch [370/938], Loss: 0.6678856611251831\n",
      "Validation: Epoch [11], Batch [371/938], Loss: 0.9183276891708374\n",
      "Validation: Epoch [11], Batch [372/938], Loss: 0.675156831741333\n",
      "Validation: Epoch [11], Batch [373/938], Loss: 0.6637650728225708\n",
      "Validation: Epoch [11], Batch [374/938], Loss: 0.5817598700523376\n",
      "Validation: Epoch [11], Batch [375/938], Loss: 0.7553213238716125\n",
      "Validation: Epoch [11], Batch [376/938], Loss: 0.6635473966598511\n",
      "Validation: Epoch [11], Batch [377/938], Loss: 0.7267321944236755\n",
      "Validation: Epoch [11], Batch [378/938], Loss: 0.8426660299301147\n",
      "Validation: Epoch [11], Batch [379/938], Loss: 0.7822015881538391\n",
      "Validation: Epoch [11], Batch [380/938], Loss: 0.7733561396598816\n",
      "Validation: Epoch [11], Batch [381/938], Loss: 0.5267266035079956\n",
      "Validation: Epoch [11], Batch [382/938], Loss: 0.825718879699707\n",
      "Validation: Epoch [11], Batch [383/938], Loss: 0.6131184101104736\n",
      "Validation: Epoch [11], Batch [384/938], Loss: 0.7016266584396362\n",
      "Validation: Epoch [11], Batch [385/938], Loss: 0.5560874938964844\n",
      "Validation: Epoch [11], Batch [386/938], Loss: 0.7019571661949158\n",
      "Validation: Epoch [11], Batch [387/938], Loss: 0.9433209300041199\n",
      "Validation: Epoch [11], Batch [388/938], Loss: 0.9756706357002258\n",
      "Validation: Epoch [11], Batch [389/938], Loss: 0.8840749263763428\n",
      "Validation: Epoch [11], Batch [390/938], Loss: 0.8475067019462585\n",
      "Validation: Epoch [11], Batch [391/938], Loss: 0.7185297608375549\n",
      "Validation: Epoch [11], Batch [392/938], Loss: 0.8878061771392822\n",
      "Validation: Epoch [11], Batch [393/938], Loss: 0.8364685773849487\n",
      "Validation: Epoch [11], Batch [394/938], Loss: 0.6532874703407288\n",
      "Validation: Epoch [11], Batch [395/938], Loss: 0.5963488817214966\n",
      "Validation: Epoch [11], Batch [396/938], Loss: 0.9126878380775452\n",
      "Validation: Epoch [11], Batch [397/938], Loss: 0.7504905462265015\n",
      "Validation: Epoch [11], Batch [398/938], Loss: 0.7599037289619446\n",
      "Validation: Epoch [11], Batch [399/938], Loss: 0.780174195766449\n",
      "Validation: Epoch [11], Batch [400/938], Loss: 0.9217988848686218\n",
      "Validation: Epoch [11], Batch [401/938], Loss: 0.6080033779144287\n",
      "Validation: Epoch [11], Batch [402/938], Loss: 0.9458229541778564\n",
      "Validation: Epoch [11], Batch [403/938], Loss: 0.7799726128578186\n",
      "Validation: Epoch [11], Batch [404/938], Loss: 0.7082515358924866\n",
      "Validation: Epoch [11], Batch [405/938], Loss: 0.6834985613822937\n",
      "Validation: Epoch [11], Batch [406/938], Loss: 0.6317850947380066\n",
      "Validation: Epoch [11], Batch [407/938], Loss: 0.6942299604415894\n",
      "Validation: Epoch [11], Batch [408/938], Loss: 0.5784084796905518\n",
      "Validation: Epoch [11], Batch [409/938], Loss: 0.6361607909202576\n",
      "Validation: Epoch [11], Batch [410/938], Loss: 1.203100562095642\n",
      "Validation: Epoch [11], Batch [411/938], Loss: 0.6662492156028748\n",
      "Validation: Epoch [11], Batch [412/938], Loss: 0.9237951040267944\n",
      "Validation: Epoch [11], Batch [413/938], Loss: 0.7821735143661499\n",
      "Validation: Epoch [11], Batch [414/938], Loss: 0.7037960290908813\n",
      "Validation: Epoch [11], Batch [415/938], Loss: 0.7476261258125305\n",
      "Validation: Epoch [11], Batch [416/938], Loss: 0.8424988985061646\n",
      "Validation: Epoch [11], Batch [417/938], Loss: 0.8613601922988892\n",
      "Validation: Epoch [11], Batch [418/938], Loss: 0.9445252418518066\n",
      "Validation: Epoch [11], Batch [419/938], Loss: 0.8105515837669373\n",
      "Validation: Epoch [11], Batch [420/938], Loss: 0.757429301738739\n",
      "Validation: Epoch [11], Batch [421/938], Loss: 0.6904405355453491\n",
      "Validation: Epoch [11], Batch [422/938], Loss: 0.7198370695114136\n",
      "Validation: Epoch [11], Batch [423/938], Loss: 0.7694816589355469\n",
      "Validation: Epoch [11], Batch [424/938], Loss: 0.5527607202529907\n",
      "Validation: Epoch [11], Batch [425/938], Loss: 0.8062231540679932\n",
      "Validation: Epoch [11], Batch [426/938], Loss: 0.7159124612808228\n",
      "Validation: Epoch [11], Batch [427/938], Loss: 0.6104984879493713\n",
      "Validation: Epoch [11], Batch [428/938], Loss: 0.647706925868988\n",
      "Validation: Epoch [11], Batch [429/938], Loss: 0.744049608707428\n",
      "Validation: Epoch [11], Batch [430/938], Loss: 0.9046549797058105\n",
      "Validation: Epoch [11], Batch [431/938], Loss: 0.6298050284385681\n",
      "Validation: Epoch [11], Batch [432/938], Loss: 0.7337799668312073\n",
      "Validation: Epoch [11], Batch [433/938], Loss: 1.1804741621017456\n",
      "Validation: Epoch [11], Batch [434/938], Loss: 0.7981194257736206\n",
      "Validation: Epoch [11], Batch [435/938], Loss: 0.8149026036262512\n",
      "Validation: Epoch [11], Batch [436/938], Loss: 0.7237272262573242\n",
      "Validation: Epoch [11], Batch [437/938], Loss: 0.7185555100440979\n",
      "Validation: Epoch [11], Batch [438/938], Loss: 0.9006900787353516\n",
      "Validation: Epoch [11], Batch [439/938], Loss: 0.8148671388626099\n",
      "Validation: Epoch [11], Batch [440/938], Loss: 0.7407042384147644\n",
      "Validation: Epoch [11], Batch [441/938], Loss: 0.685392439365387\n",
      "Validation: Epoch [11], Batch [442/938], Loss: 0.7742816209793091\n",
      "Validation: Epoch [11], Batch [443/938], Loss: 0.8172146081924438\n",
      "Validation: Epoch [11], Batch [444/938], Loss: 0.9098242521286011\n",
      "Validation: Epoch [11], Batch [445/938], Loss: 0.8987560868263245\n",
      "Validation: Epoch [11], Batch [446/938], Loss: 0.7424852252006531\n",
      "Validation: Epoch [11], Batch [447/938], Loss: 0.649164617061615\n",
      "Validation: Epoch [11], Batch [448/938], Loss: 0.9697111248970032\n",
      "Validation: Epoch [11], Batch [449/938], Loss: 0.7538559436798096\n",
      "Validation: Epoch [11], Batch [450/938], Loss: 0.824323296546936\n",
      "Validation: Epoch [11], Batch [451/938], Loss: 0.9002580642700195\n",
      "Validation: Epoch [11], Batch [452/938], Loss: 0.5458520650863647\n",
      "Validation: Epoch [11], Batch [453/938], Loss: 1.0225378274917603\n",
      "Validation: Epoch [11], Batch [454/938], Loss: 1.0132811069488525\n",
      "Validation: Epoch [11], Batch [455/938], Loss: 0.7861455678939819\n",
      "Validation: Epoch [11], Batch [456/938], Loss: 0.7080424427986145\n",
      "Validation: Epoch [11], Batch [457/938], Loss: 0.6981704235076904\n",
      "Validation: Epoch [11], Batch [458/938], Loss: 0.8862623572349548\n",
      "Validation: Epoch [11], Batch [459/938], Loss: 0.5197140574455261\n",
      "Validation: Epoch [11], Batch [460/938], Loss: 1.049838900566101\n",
      "Validation: Epoch [11], Batch [461/938], Loss: 0.8327745795249939\n",
      "Validation: Epoch [11], Batch [462/938], Loss: 0.7624776363372803\n",
      "Validation: Epoch [11], Batch [463/938], Loss: 0.5304415822029114\n",
      "Validation: Epoch [11], Batch [464/938], Loss: 0.8471132516860962\n",
      "Validation: Epoch [11], Batch [465/938], Loss: 0.9222579598426819\n",
      "Validation: Epoch [11], Batch [466/938], Loss: 0.8638537526130676\n",
      "Validation: Epoch [11], Batch [467/938], Loss: 0.5958382487297058\n",
      "Validation: Epoch [11], Batch [468/938], Loss: 0.8694244027137756\n",
      "Validation: Epoch [11], Batch [469/938], Loss: 0.9354067444801331\n",
      "Validation: Epoch [11], Batch [470/938], Loss: 1.016574740409851\n",
      "Validation: Epoch [11], Batch [471/938], Loss: 0.6941410303115845\n",
      "Validation: Epoch [11], Batch [472/938], Loss: 0.85984206199646\n",
      "Validation: Epoch [11], Batch [473/938], Loss: 0.6387308239936829\n",
      "Validation: Epoch [11], Batch [474/938], Loss: 0.6009290218353271\n",
      "Validation: Epoch [11], Batch [475/938], Loss: 0.6651337742805481\n",
      "Validation: Epoch [11], Batch [476/938], Loss: 0.7929691672325134\n",
      "Validation: Epoch [11], Batch [477/938], Loss: 1.02601957321167\n",
      "Validation: Epoch [11], Batch [478/938], Loss: 0.9949773550033569\n",
      "Validation: Epoch [11], Batch [479/938], Loss: 0.8596470952033997\n",
      "Validation: Epoch [11], Batch [480/938], Loss: 1.2552872896194458\n",
      "Validation: Epoch [11], Batch [481/938], Loss: 0.6413761377334595\n",
      "Validation: Epoch [11], Batch [482/938], Loss: 0.9772391319274902\n",
      "Validation: Epoch [11], Batch [483/938], Loss: 0.7881643176078796\n",
      "Validation: Epoch [11], Batch [484/938], Loss: 1.0201207399368286\n",
      "Validation: Epoch [11], Batch [485/938], Loss: 0.702908992767334\n",
      "Validation: Epoch [11], Batch [486/938], Loss: 0.8338217735290527\n",
      "Validation: Epoch [11], Batch [487/938], Loss: 0.9383677840232849\n",
      "Validation: Epoch [11], Batch [488/938], Loss: 0.41848424077033997\n",
      "Validation: Epoch [11], Batch [489/938], Loss: 0.7593443393707275\n",
      "Validation: Epoch [11], Batch [490/938], Loss: 0.7711576223373413\n",
      "Validation: Epoch [11], Batch [491/938], Loss: 0.6509349346160889\n",
      "Validation: Epoch [11], Batch [492/938], Loss: 0.7965042591094971\n",
      "Validation: Epoch [11], Batch [493/938], Loss: 0.7925682663917542\n",
      "Validation: Epoch [11], Batch [494/938], Loss: 0.6723390817642212\n",
      "Validation: Epoch [11], Batch [495/938], Loss: 0.6817806959152222\n",
      "Validation: Epoch [11], Batch [496/938], Loss: 0.8389475345611572\n",
      "Validation: Epoch [11], Batch [497/938], Loss: 0.7277013063430786\n",
      "Validation: Epoch [11], Batch [498/938], Loss: 0.8405176401138306\n",
      "Validation: Epoch [11], Batch [499/938], Loss: 0.7200208902359009\n",
      "Validation: Epoch [11], Batch [500/938], Loss: 0.9032942652702332\n",
      "Validation: Epoch [11], Batch [501/938], Loss: 0.8192542791366577\n",
      "Validation: Epoch [11], Batch [502/938], Loss: 0.6760390996932983\n",
      "Validation: Epoch [11], Batch [503/938], Loss: 0.7376910448074341\n",
      "Validation: Epoch [11], Batch [504/938], Loss: 1.047950267791748\n",
      "Validation: Epoch [11], Batch [505/938], Loss: 0.9048862457275391\n",
      "Validation: Epoch [11], Batch [506/938], Loss: 0.7430362701416016\n",
      "Validation: Epoch [11], Batch [507/938], Loss: 0.6123397946357727\n",
      "Validation: Epoch [11], Batch [508/938], Loss: 0.7841797471046448\n",
      "Validation: Epoch [11], Batch [509/938], Loss: 0.6918292045593262\n",
      "Validation: Epoch [11], Batch [510/938], Loss: 0.8140348196029663\n",
      "Validation: Epoch [11], Batch [511/938], Loss: 0.9139305353164673\n",
      "Validation: Epoch [11], Batch [512/938], Loss: 0.6233564615249634\n",
      "Validation: Epoch [11], Batch [513/938], Loss: 0.5254834890365601\n",
      "Validation: Epoch [11], Batch [514/938], Loss: 0.7917723059654236\n",
      "Validation: Epoch [11], Batch [515/938], Loss: 0.5740195512771606\n",
      "Validation: Epoch [11], Batch [516/938], Loss: 0.6798925399780273\n",
      "Validation: Epoch [11], Batch [517/938], Loss: 0.7582001090049744\n",
      "Validation: Epoch [11], Batch [518/938], Loss: 0.74665367603302\n",
      "Validation: Epoch [11], Batch [519/938], Loss: 0.6087063550949097\n",
      "Validation: Epoch [11], Batch [520/938], Loss: 0.7606306672096252\n",
      "Validation: Epoch [11], Batch [521/938], Loss: 0.923102617263794\n",
      "Validation: Epoch [11], Batch [522/938], Loss: 0.8535409569740295\n",
      "Validation: Epoch [11], Batch [523/938], Loss: 0.7464776039123535\n",
      "Validation: Epoch [11], Batch [524/938], Loss: 0.8481890559196472\n",
      "Validation: Epoch [11], Batch [525/938], Loss: 0.7698816657066345\n",
      "Validation: Epoch [11], Batch [526/938], Loss: 0.8334442973136902\n",
      "Validation: Epoch [11], Batch [527/938], Loss: 0.6694474220275879\n",
      "Validation: Epoch [11], Batch [528/938], Loss: 0.8791785836219788\n",
      "Validation: Epoch [11], Batch [529/938], Loss: 0.8193574547767639\n",
      "Validation: Epoch [11], Batch [530/938], Loss: 0.8473483324050903\n",
      "Validation: Epoch [11], Batch [531/938], Loss: 0.862273633480072\n",
      "Validation: Epoch [11], Batch [532/938], Loss: 1.0309163331985474\n",
      "Validation: Epoch [11], Batch [533/938], Loss: 0.6238399744033813\n",
      "Validation: Epoch [11], Batch [534/938], Loss: 0.6294329762458801\n",
      "Validation: Epoch [11], Batch [535/938], Loss: 0.9522353410720825\n",
      "Validation: Epoch [11], Batch [536/938], Loss: 0.8749193549156189\n",
      "Validation: Epoch [11], Batch [537/938], Loss: 0.8725376129150391\n",
      "Validation: Epoch [11], Batch [538/938], Loss: 0.9306744337081909\n",
      "Validation: Epoch [11], Batch [539/938], Loss: 0.6475046873092651\n",
      "Validation: Epoch [11], Batch [540/938], Loss: 0.5215228199958801\n",
      "Validation: Epoch [11], Batch [541/938], Loss: 0.802532434463501\n",
      "Validation: Epoch [11], Batch [542/938], Loss: 0.6046669483184814\n",
      "Validation: Epoch [11], Batch [543/938], Loss: 0.6236567497253418\n",
      "Validation: Epoch [11], Batch [544/938], Loss: 0.8520753979682922\n",
      "Validation: Epoch [11], Batch [545/938], Loss: 0.4108450710773468\n",
      "Validation: Epoch [11], Batch [546/938], Loss: 0.6649391055107117\n",
      "Validation: Epoch [11], Batch [547/938], Loss: 0.6837834715843201\n",
      "Validation: Epoch [11], Batch [548/938], Loss: 0.6977354288101196\n",
      "Validation: Epoch [11], Batch [549/938], Loss: 0.5303395986557007\n",
      "Validation: Epoch [11], Batch [550/938], Loss: 0.7734687328338623\n",
      "Validation: Epoch [11], Batch [551/938], Loss: 0.6888473629951477\n",
      "Validation: Epoch [11], Batch [552/938], Loss: 0.7346980571746826\n",
      "Validation: Epoch [11], Batch [553/938], Loss: 0.6384074091911316\n",
      "Validation: Epoch [11], Batch [554/938], Loss: 0.5476791858673096\n",
      "Validation: Epoch [11], Batch [555/938], Loss: 0.9116103649139404\n",
      "Validation: Epoch [11], Batch [556/938], Loss: 0.8270958662033081\n",
      "Validation: Epoch [11], Batch [557/938], Loss: 0.8527595400810242\n",
      "Validation: Epoch [11], Batch [558/938], Loss: 0.7306368947029114\n",
      "Validation: Epoch [11], Batch [559/938], Loss: 0.7276843786239624\n",
      "Validation: Epoch [11], Batch [560/938], Loss: 0.6329981684684753\n",
      "Validation: Epoch [11], Batch [561/938], Loss: 0.9450344443321228\n",
      "Validation: Epoch [11], Batch [562/938], Loss: 0.6444408893585205\n",
      "Validation: Epoch [11], Batch [563/938], Loss: 0.8646621704101562\n",
      "Validation: Epoch [11], Batch [564/938], Loss: 1.025326132774353\n",
      "Validation: Epoch [11], Batch [565/938], Loss: 0.5895765423774719\n",
      "Validation: Epoch [11], Batch [566/938], Loss: 0.7796018123626709\n",
      "Validation: Epoch [11], Batch [567/938], Loss: 0.9968675971031189\n",
      "Validation: Epoch [11], Batch [568/938], Loss: 0.59608393907547\n",
      "Validation: Epoch [11], Batch [569/938], Loss: 0.7890027761459351\n",
      "Validation: Epoch [11], Batch [570/938], Loss: 0.8231770992279053\n",
      "Validation: Epoch [11], Batch [571/938], Loss: 0.9024093151092529\n",
      "Validation: Epoch [11], Batch [572/938], Loss: 0.5437806844711304\n",
      "Validation: Epoch [11], Batch [573/938], Loss: 0.7085461616516113\n",
      "Validation: Epoch [11], Batch [574/938], Loss: 0.8725070357322693\n",
      "Validation: Epoch [11], Batch [575/938], Loss: 1.0499768257141113\n",
      "Validation: Epoch [11], Batch [576/938], Loss: 0.860100269317627\n",
      "Validation: Epoch [11], Batch [577/938], Loss: 0.7706631422042847\n",
      "Validation: Epoch [11], Batch [578/938], Loss: 0.7312594056129456\n",
      "Validation: Epoch [11], Batch [579/938], Loss: 0.7018311023712158\n",
      "Validation: Epoch [11], Batch [580/938], Loss: 0.8283625841140747\n",
      "Validation: Epoch [11], Batch [581/938], Loss: 0.6941494941711426\n",
      "Validation: Epoch [11], Batch [582/938], Loss: 0.843820333480835\n",
      "Validation: Epoch [11], Batch [583/938], Loss: 0.7023388147354126\n",
      "Validation: Epoch [11], Batch [584/938], Loss: 0.6453448534011841\n",
      "Validation: Epoch [11], Batch [585/938], Loss: 0.9137830138206482\n",
      "Validation: Epoch [11], Batch [586/938], Loss: 1.1415817737579346\n",
      "Validation: Epoch [11], Batch [587/938], Loss: 0.5827618837356567\n",
      "Validation: Epoch [11], Batch [588/938], Loss: 0.8959377408027649\n",
      "Validation: Epoch [11], Batch [589/938], Loss: 0.8634193539619446\n",
      "Validation: Epoch [11], Batch [590/938], Loss: 0.7444844245910645\n",
      "Validation: Epoch [11], Batch [591/938], Loss: 0.8187450766563416\n",
      "Validation: Epoch [11], Batch [592/938], Loss: 0.5554296374320984\n",
      "Validation: Epoch [11], Batch [593/938], Loss: 0.8444048762321472\n",
      "Validation: Epoch [11], Batch [594/938], Loss: 0.7608906626701355\n",
      "Validation: Epoch [11], Batch [595/938], Loss: 0.7245575189590454\n",
      "Validation: Epoch [11], Batch [596/938], Loss: 0.6474172472953796\n",
      "Validation: Epoch [11], Batch [597/938], Loss: 0.6884645223617554\n",
      "Validation: Epoch [11], Batch [598/938], Loss: 0.7721784114837646\n",
      "Validation: Epoch [11], Batch [599/938], Loss: 0.6686270236968994\n",
      "Validation: Epoch [11], Batch [600/938], Loss: 0.738044261932373\n",
      "Validation: Epoch [11], Batch [601/938], Loss: 0.7047203779220581\n",
      "Validation: Epoch [11], Batch [602/938], Loss: 0.9607237577438354\n",
      "Validation: Epoch [11], Batch [603/938], Loss: 0.8659467697143555\n",
      "Validation: Epoch [11], Batch [604/938], Loss: 0.4062454402446747\n",
      "Validation: Epoch [11], Batch [605/938], Loss: 0.7961433529853821\n",
      "Validation: Epoch [11], Batch [606/938], Loss: 0.6283297538757324\n",
      "Validation: Epoch [11], Batch [607/938], Loss: 0.5853465795516968\n",
      "Validation: Epoch [11], Batch [608/938], Loss: 0.662617564201355\n",
      "Validation: Epoch [11], Batch [609/938], Loss: 0.6565612554550171\n",
      "Validation: Epoch [11], Batch [610/938], Loss: 0.5776000618934631\n",
      "Validation: Epoch [11], Batch [611/938], Loss: 0.8054364323616028\n",
      "Validation: Epoch [11], Batch [612/938], Loss: 0.6883576512336731\n",
      "Validation: Epoch [11], Batch [613/938], Loss: 0.6207635998725891\n",
      "Validation: Epoch [11], Batch [614/938], Loss: 0.7495749592781067\n",
      "Validation: Epoch [11], Batch [615/938], Loss: 0.8379126787185669\n",
      "Validation: Epoch [11], Batch [616/938], Loss: 0.683839738368988\n",
      "Validation: Epoch [11], Batch [617/938], Loss: 0.5433162450790405\n",
      "Validation: Epoch [11], Batch [618/938], Loss: 0.6243802905082703\n",
      "Validation: Epoch [11], Batch [619/938], Loss: 0.7934045791625977\n",
      "Validation: Epoch [11], Batch [620/938], Loss: 0.8037636876106262\n",
      "Validation: Epoch [11], Batch [621/938], Loss: 1.0919743776321411\n",
      "Validation: Epoch [11], Batch [622/938], Loss: 0.9097424745559692\n",
      "Validation: Epoch [11], Batch [623/938], Loss: 0.8529436588287354\n",
      "Validation: Epoch [11], Batch [624/938], Loss: 0.7176288366317749\n",
      "Validation: Epoch [11], Batch [625/938], Loss: 0.8295220732688904\n",
      "Validation: Epoch [11], Batch [626/938], Loss: 0.9146988987922668\n",
      "Validation: Epoch [11], Batch [627/938], Loss: 0.7404666543006897\n",
      "Validation: Epoch [11], Batch [628/938], Loss: 0.5398275852203369\n",
      "Validation: Epoch [11], Batch [629/938], Loss: 0.6309071779251099\n",
      "Validation: Epoch [11], Batch [630/938], Loss: 0.7853286266326904\n",
      "Validation: Epoch [11], Batch [631/938], Loss: 0.77093106508255\n",
      "Validation: Epoch [11], Batch [632/938], Loss: 0.6247705817222595\n",
      "Validation: Epoch [11], Batch [633/938], Loss: 0.7201093435287476\n",
      "Validation: Epoch [11], Batch [634/938], Loss: 0.5060925483703613\n",
      "Validation: Epoch [11], Batch [635/938], Loss: 0.4871309995651245\n",
      "Validation: Epoch [11], Batch [636/938], Loss: 0.6125737428665161\n",
      "Validation: Epoch [11], Batch [637/938], Loss: 0.7037670612335205\n",
      "Validation: Epoch [11], Batch [638/938], Loss: 0.7751366496086121\n",
      "Validation: Epoch [11], Batch [639/938], Loss: 0.7464258074760437\n",
      "Validation: Epoch [11], Batch [640/938], Loss: 0.7920326590538025\n",
      "Validation: Epoch [11], Batch [641/938], Loss: 0.8923571109771729\n",
      "Validation: Epoch [11], Batch [642/938], Loss: 0.7083464860916138\n",
      "Validation: Epoch [11], Batch [643/938], Loss: 0.6218542456626892\n",
      "Validation: Epoch [11], Batch [644/938], Loss: 0.6565141677856445\n",
      "Validation: Epoch [11], Batch [645/938], Loss: 0.8344032764434814\n",
      "Validation: Epoch [11], Batch [646/938], Loss: 0.7043304443359375\n",
      "Validation: Epoch [11], Batch [647/938], Loss: 0.9761955142021179\n",
      "Validation: Epoch [11], Batch [648/938], Loss: 0.6156482696533203\n",
      "Validation: Epoch [11], Batch [649/938], Loss: 0.7129557728767395\n",
      "Validation: Epoch [11], Batch [650/938], Loss: 1.0294420719146729\n",
      "Validation: Epoch [11], Batch [651/938], Loss: 0.5211069583892822\n",
      "Validation: Epoch [11], Batch [652/938], Loss: 0.7898657321929932\n",
      "Validation: Epoch [11], Batch [653/938], Loss: 0.5591356158256531\n",
      "Validation: Epoch [11], Batch [654/938], Loss: 0.7239862084388733\n",
      "Validation: Epoch [11], Batch [655/938], Loss: 0.7814022302627563\n",
      "Validation: Epoch [11], Batch [656/938], Loss: 0.8783600330352783\n",
      "Validation: Epoch [11], Batch [657/938], Loss: 0.576691746711731\n",
      "Validation: Epoch [11], Batch [658/938], Loss: 0.7959665060043335\n",
      "Validation: Epoch [11], Batch [659/938], Loss: 0.8601096272468567\n",
      "Validation: Epoch [11], Batch [660/938], Loss: 0.6773915886878967\n",
      "Validation: Epoch [11], Batch [661/938], Loss: 0.7866287231445312\n",
      "Validation: Epoch [11], Batch [662/938], Loss: 0.7960721850395203\n",
      "Validation: Epoch [11], Batch [663/938], Loss: 0.7602266669273376\n",
      "Validation: Epoch [11], Batch [664/938], Loss: 1.092903733253479\n",
      "Validation: Epoch [11], Batch [665/938], Loss: 0.7902526259422302\n",
      "Validation: Epoch [11], Batch [666/938], Loss: 0.7768281698226929\n",
      "Validation: Epoch [11], Batch [667/938], Loss: 0.8020906448364258\n",
      "Validation: Epoch [11], Batch [668/938], Loss: 0.6882067322731018\n",
      "Validation: Epoch [11], Batch [669/938], Loss: 0.7822954058647156\n",
      "Validation: Epoch [11], Batch [670/938], Loss: 0.7802308797836304\n",
      "Validation: Epoch [11], Batch [671/938], Loss: 0.6306532621383667\n",
      "Validation: Epoch [11], Batch [672/938], Loss: 0.8871515393257141\n",
      "Validation: Epoch [11], Batch [673/938], Loss: 0.8557392954826355\n",
      "Validation: Epoch [11], Batch [674/938], Loss: 1.0074071884155273\n",
      "Validation: Epoch [11], Batch [675/938], Loss: 0.6836109161376953\n",
      "Validation: Epoch [11], Batch [676/938], Loss: 0.8319826126098633\n",
      "Validation: Epoch [11], Batch [677/938], Loss: 0.7607480883598328\n",
      "Validation: Epoch [11], Batch [678/938], Loss: 0.6994951367378235\n",
      "Validation: Epoch [11], Batch [679/938], Loss: 0.5279219746589661\n",
      "Validation: Epoch [11], Batch [680/938], Loss: 0.7938029170036316\n",
      "Validation: Epoch [11], Batch [681/938], Loss: 0.5386122465133667\n",
      "Validation: Epoch [11], Batch [682/938], Loss: 0.6133682727813721\n",
      "Validation: Epoch [11], Batch [683/938], Loss: 0.7832903861999512\n",
      "Validation: Epoch [11], Batch [684/938], Loss: 0.6900911927223206\n",
      "Validation: Epoch [11], Batch [685/938], Loss: 0.7363623976707458\n",
      "Validation: Epoch [11], Batch [686/938], Loss: 0.8277926445007324\n",
      "Validation: Epoch [11], Batch [687/938], Loss: 0.9482585191726685\n",
      "Validation: Epoch [11], Batch [688/938], Loss: 0.7899042367935181\n",
      "Validation: Epoch [11], Batch [689/938], Loss: 0.8891094923019409\n",
      "Validation: Epoch [11], Batch [690/938], Loss: 0.8910204768180847\n",
      "Validation: Epoch [11], Batch [691/938], Loss: 0.7907788753509521\n",
      "Validation: Epoch [11], Batch [692/938], Loss: 0.7927643060684204\n",
      "Validation: Epoch [11], Batch [693/938], Loss: 0.7735587954521179\n",
      "Validation: Epoch [11], Batch [694/938], Loss: 0.7396824359893799\n",
      "Validation: Epoch [11], Batch [695/938], Loss: 0.5684388875961304\n",
      "Validation: Epoch [11], Batch [696/938], Loss: 0.9161701798439026\n",
      "Validation: Epoch [11], Batch [697/938], Loss: 0.8422489166259766\n",
      "Validation: Epoch [11], Batch [698/938], Loss: 0.7129849195480347\n",
      "Validation: Epoch [11], Batch [699/938], Loss: 0.7604957818984985\n",
      "Validation: Epoch [11], Batch [700/938], Loss: 0.546170711517334\n",
      "Validation: Epoch [11], Batch [701/938], Loss: 0.8867816925048828\n",
      "Validation: Epoch [11], Batch [702/938], Loss: 0.9393437504768372\n",
      "Validation: Epoch [11], Batch [703/938], Loss: 0.7377525568008423\n",
      "Validation: Epoch [11], Batch [704/938], Loss: 0.6064432859420776\n",
      "Validation: Epoch [11], Batch [705/938], Loss: 0.6358253955841064\n",
      "Validation: Epoch [11], Batch [706/938], Loss: 0.7688618898391724\n",
      "Validation: Epoch [11], Batch [707/938], Loss: 1.031226396560669\n",
      "Validation: Epoch [11], Batch [708/938], Loss: 0.8875956535339355\n",
      "Validation: Epoch [11], Batch [709/938], Loss: 0.9055846333503723\n",
      "Validation: Epoch [11], Batch [710/938], Loss: 0.6687662601470947\n",
      "Validation: Epoch [11], Batch [711/938], Loss: 0.751733660697937\n",
      "Validation: Epoch [11], Batch [712/938], Loss: 0.9030259847640991\n",
      "Validation: Epoch [11], Batch [713/938], Loss: 0.8524670004844666\n",
      "Validation: Epoch [11], Batch [714/938], Loss: 0.7895829677581787\n",
      "Validation: Epoch [11], Batch [715/938], Loss: 0.9508249163627625\n",
      "Validation: Epoch [11], Batch [716/938], Loss: 0.8218110799789429\n",
      "Validation: Epoch [11], Batch [717/938], Loss: 0.7757200598716736\n",
      "Validation: Epoch [11], Batch [718/938], Loss: 0.6370947360992432\n",
      "Validation: Epoch [11], Batch [719/938], Loss: 0.6106489300727844\n",
      "Validation: Epoch [11], Batch [720/938], Loss: 0.7967305183410645\n",
      "Validation: Epoch [11], Batch [721/938], Loss: 0.9141362905502319\n",
      "Validation: Epoch [11], Batch [722/938], Loss: 0.7321395874023438\n",
      "Validation: Epoch [11], Batch [723/938], Loss: 0.7259018421173096\n",
      "Validation: Epoch [11], Batch [724/938], Loss: 0.8245964646339417\n",
      "Validation: Epoch [11], Batch [725/938], Loss: 0.8829104900360107\n",
      "Validation: Epoch [11], Batch [726/938], Loss: 0.713657557964325\n",
      "Validation: Epoch [11], Batch [727/938], Loss: 0.732791006565094\n",
      "Validation: Epoch [11], Batch [728/938], Loss: 0.7620231509208679\n",
      "Validation: Epoch [11], Batch [729/938], Loss: 0.7456650137901306\n",
      "Validation: Epoch [11], Batch [730/938], Loss: 0.6364756226539612\n",
      "Validation: Epoch [11], Batch [731/938], Loss: 0.7059856653213501\n",
      "Validation: Epoch [11], Batch [732/938], Loss: 0.8753151893615723\n",
      "Validation: Epoch [11], Batch [733/938], Loss: 0.6599875688552856\n",
      "Validation: Epoch [11], Batch [734/938], Loss: 0.7323281764984131\n",
      "Validation: Epoch [11], Batch [735/938], Loss: 0.6870742440223694\n",
      "Validation: Epoch [11], Batch [736/938], Loss: 0.6555665731430054\n",
      "Validation: Epoch [11], Batch [737/938], Loss: 0.7982977628707886\n",
      "Validation: Epoch [11], Batch [738/938], Loss: 0.9398781061172485\n",
      "Validation: Epoch [11], Batch [739/938], Loss: 0.6673724055290222\n",
      "Validation: Epoch [11], Batch [740/938], Loss: 0.8209993243217468\n",
      "Validation: Epoch [11], Batch [741/938], Loss: 0.613127589225769\n",
      "Validation: Epoch [11], Batch [742/938], Loss: 0.6170425415039062\n",
      "Validation: Epoch [11], Batch [743/938], Loss: 0.8964477181434631\n",
      "Validation: Epoch [11], Batch [744/938], Loss: 0.6820147037506104\n",
      "Validation: Epoch [11], Batch [745/938], Loss: 0.8756556510925293\n",
      "Validation: Epoch [11], Batch [746/938], Loss: 0.7410615682601929\n",
      "Validation: Epoch [11], Batch [747/938], Loss: 0.9773264527320862\n",
      "Validation: Epoch [11], Batch [748/938], Loss: 0.8088254928588867\n",
      "Validation: Epoch [11], Batch [749/938], Loss: 0.810235321521759\n",
      "Validation: Epoch [11], Batch [750/938], Loss: 0.9100428223609924\n",
      "Validation: Epoch [11], Batch [751/938], Loss: 0.5976290702819824\n",
      "Validation: Epoch [11], Batch [752/938], Loss: 0.7347477674484253\n",
      "Validation: Epoch [11], Batch [753/938], Loss: 0.6793385744094849\n",
      "Validation: Epoch [11], Batch [754/938], Loss: 0.7884137630462646\n",
      "Validation: Epoch [11], Batch [755/938], Loss: 1.000197410583496\n",
      "Validation: Epoch [11], Batch [756/938], Loss: 1.011065125465393\n",
      "Validation: Epoch [11], Batch [757/938], Loss: 0.8682804107666016\n",
      "Validation: Epoch [11], Batch [758/938], Loss: 0.7840842008590698\n",
      "Validation: Epoch [11], Batch [759/938], Loss: 0.757803201675415\n",
      "Validation: Epoch [11], Batch [760/938], Loss: 0.8654509782791138\n",
      "Validation: Epoch [11], Batch [761/938], Loss: 0.5991768836975098\n",
      "Validation: Epoch [11], Batch [762/938], Loss: 0.9319112300872803\n",
      "Validation: Epoch [11], Batch [763/938], Loss: 1.1607013940811157\n",
      "Validation: Epoch [11], Batch [764/938], Loss: 0.6958563327789307\n",
      "Validation: Epoch [11], Batch [765/938], Loss: 0.7355789542198181\n",
      "Validation: Epoch [11], Batch [766/938], Loss: 0.7674915194511414\n",
      "Validation: Epoch [11], Batch [767/938], Loss: 0.7263882160186768\n",
      "Validation: Epoch [11], Batch [768/938], Loss: 0.8299453258514404\n",
      "Validation: Epoch [11], Batch [769/938], Loss: 1.0193767547607422\n",
      "Validation: Epoch [11], Batch [770/938], Loss: 0.6465999484062195\n",
      "Validation: Epoch [11], Batch [771/938], Loss: 0.9253479242324829\n",
      "Validation: Epoch [11], Batch [772/938], Loss: 0.697731614112854\n",
      "Validation: Epoch [11], Batch [773/938], Loss: 1.0174996852874756\n",
      "Validation: Epoch [11], Batch [774/938], Loss: 0.7957294583320618\n",
      "Validation: Epoch [11], Batch [775/938], Loss: 0.6488558650016785\n",
      "Validation: Epoch [11], Batch [776/938], Loss: 0.5305464863777161\n",
      "Validation: Epoch [11], Batch [777/938], Loss: 0.7296282052993774\n",
      "Validation: Epoch [11], Batch [778/938], Loss: 0.6582574844360352\n",
      "Validation: Epoch [11], Batch [779/938], Loss: 0.939994215965271\n",
      "Validation: Epoch [11], Batch [780/938], Loss: 1.357193946838379\n",
      "Validation: Epoch [11], Batch [781/938], Loss: 0.9776191115379333\n",
      "Validation: Epoch [11], Batch [782/938], Loss: 0.7718029618263245\n",
      "Validation: Epoch [11], Batch [783/938], Loss: 0.8000375628471375\n",
      "Validation: Epoch [11], Batch [784/938], Loss: 0.6748775243759155\n",
      "Validation: Epoch [11], Batch [785/938], Loss: 0.8324897289276123\n",
      "Validation: Epoch [11], Batch [786/938], Loss: 0.7212788462638855\n",
      "Validation: Epoch [11], Batch [787/938], Loss: 0.7842720150947571\n",
      "Validation: Epoch [11], Batch [788/938], Loss: 0.7069428563117981\n",
      "Validation: Epoch [11], Batch [789/938], Loss: 0.8051355481147766\n",
      "Validation: Epoch [11], Batch [790/938], Loss: 0.6936495304107666\n",
      "Validation: Epoch [11], Batch [791/938], Loss: 0.6230794787406921\n",
      "Validation: Epoch [11], Batch [792/938], Loss: 0.7404635548591614\n",
      "Validation: Epoch [11], Batch [793/938], Loss: 0.9482908844947815\n",
      "Validation: Epoch [11], Batch [794/938], Loss: 1.047256588935852\n",
      "Validation: Epoch [11], Batch [795/938], Loss: 0.5124515891075134\n",
      "Validation: Epoch [11], Batch [796/938], Loss: 0.8149141669273376\n",
      "Validation: Epoch [11], Batch [797/938], Loss: 0.6260620355606079\n",
      "Validation: Epoch [11], Batch [798/938], Loss: 0.6486344933509827\n",
      "Validation: Epoch [11], Batch [799/938], Loss: 0.7454419732093811\n",
      "Validation: Epoch [11], Batch [800/938], Loss: 0.7721226811408997\n",
      "Validation: Epoch [11], Batch [801/938], Loss: 0.8257555961608887\n",
      "Validation: Epoch [11], Batch [802/938], Loss: 0.8692620992660522\n",
      "Validation: Epoch [11], Batch [803/938], Loss: 0.9249626398086548\n",
      "Validation: Epoch [11], Batch [804/938], Loss: 0.6356174945831299\n",
      "Validation: Epoch [11], Batch [805/938], Loss: 0.6761950254440308\n",
      "Validation: Epoch [11], Batch [806/938], Loss: 0.7496057152748108\n",
      "Validation: Epoch [11], Batch [807/938], Loss: 0.7708332538604736\n",
      "Validation: Epoch [11], Batch [808/938], Loss: 0.9359784722328186\n",
      "Validation: Epoch [11], Batch [809/938], Loss: 0.921642541885376\n",
      "Validation: Epoch [11], Batch [810/938], Loss: 0.7996695637702942\n",
      "Validation: Epoch [11], Batch [811/938], Loss: 0.7319937348365784\n",
      "Validation: Epoch [11], Batch [812/938], Loss: 0.7822769284248352\n",
      "Validation: Epoch [11], Batch [813/938], Loss: 0.710369884967804\n",
      "Validation: Epoch [11], Batch [814/938], Loss: 0.6853083968162537\n",
      "Validation: Epoch [11], Batch [815/938], Loss: 0.7743561863899231\n",
      "Validation: Epoch [11], Batch [816/938], Loss: 0.6119076013565063\n",
      "Validation: Epoch [11], Batch [817/938], Loss: 0.49904295802116394\n",
      "Validation: Epoch [11], Batch [818/938], Loss: 0.6123457551002502\n",
      "Validation: Epoch [11], Batch [819/938], Loss: 0.752363383769989\n",
      "Validation: Epoch [11], Batch [820/938], Loss: 0.625487744808197\n",
      "Validation: Epoch [11], Batch [821/938], Loss: 0.6302084922790527\n",
      "Validation: Epoch [11], Batch [822/938], Loss: 0.7676131725311279\n",
      "Validation: Epoch [11], Batch [823/938], Loss: 0.7004623413085938\n",
      "Validation: Epoch [11], Batch [824/938], Loss: 1.0072036981582642\n",
      "Validation: Epoch [11], Batch [825/938], Loss: 0.5516622066497803\n",
      "Validation: Epoch [11], Batch [826/938], Loss: 0.7394413948059082\n",
      "Validation: Epoch [11], Batch [827/938], Loss: 1.0485544204711914\n",
      "Validation: Epoch [11], Batch [828/938], Loss: 0.6080117225646973\n",
      "Validation: Epoch [11], Batch [829/938], Loss: 0.7909226417541504\n",
      "Validation: Epoch [11], Batch [830/938], Loss: 0.8752756714820862\n",
      "Validation: Epoch [11], Batch [831/938], Loss: 0.5527527332305908\n",
      "Validation: Epoch [11], Batch [832/938], Loss: 0.8240000009536743\n",
      "Validation: Epoch [11], Batch [833/938], Loss: 0.8236892223358154\n",
      "Validation: Epoch [11], Batch [834/938], Loss: 0.9164882898330688\n",
      "Validation: Epoch [11], Batch [835/938], Loss: 0.8930003046989441\n",
      "Validation: Epoch [11], Batch [836/938], Loss: 0.8169406056404114\n",
      "Validation: Epoch [11], Batch [837/938], Loss: 0.6463908553123474\n",
      "Validation: Epoch [11], Batch [838/938], Loss: 1.027305006980896\n",
      "Validation: Epoch [11], Batch [839/938], Loss: 0.5407196283340454\n",
      "Validation: Epoch [11], Batch [840/938], Loss: 0.6581676006317139\n",
      "Validation: Epoch [11], Batch [841/938], Loss: 0.6596335172653198\n",
      "Validation: Epoch [11], Batch [842/938], Loss: 0.680306613445282\n",
      "Validation: Epoch [11], Batch [843/938], Loss: 0.8069985508918762\n",
      "Validation: Epoch [11], Batch [844/938], Loss: 1.0804778337478638\n",
      "Validation: Epoch [11], Batch [845/938], Loss: 0.5068708658218384\n",
      "Validation: Epoch [11], Batch [846/938], Loss: 0.7785758972167969\n",
      "Validation: Epoch [11], Batch [847/938], Loss: 0.6196438670158386\n",
      "Validation: Epoch [11], Batch [848/938], Loss: 0.6383044719696045\n",
      "Validation: Epoch [11], Batch [849/938], Loss: 0.6744486093521118\n",
      "Validation: Epoch [11], Batch [850/938], Loss: 0.7372711300849915\n",
      "Validation: Epoch [11], Batch [851/938], Loss: 0.624775230884552\n",
      "Validation: Epoch [11], Batch [852/938], Loss: 0.4519897997379303\n",
      "Validation: Epoch [11], Batch [853/938], Loss: 0.6539351940155029\n",
      "Validation: Epoch [11], Batch [854/938], Loss: 0.5991542935371399\n",
      "Validation: Epoch [11], Batch [855/938], Loss: 0.8477489352226257\n",
      "Validation: Epoch [11], Batch [856/938], Loss: 1.1215356588363647\n",
      "Validation: Epoch [11], Batch [857/938], Loss: 0.5900236964225769\n",
      "Validation: Epoch [11], Batch [858/938], Loss: 0.912390947341919\n",
      "Validation: Epoch [11], Batch [859/938], Loss: 0.7602978348731995\n",
      "Validation: Epoch [11], Batch [860/938], Loss: 0.898729145526886\n",
      "Validation: Epoch [11], Batch [861/938], Loss: 0.9443411231040955\n",
      "Validation: Epoch [11], Batch [862/938], Loss: 0.7707462906837463\n",
      "Validation: Epoch [11], Batch [863/938], Loss: 1.0170987844467163\n",
      "Validation: Epoch [11], Batch [864/938], Loss: 0.888728141784668\n",
      "Validation: Epoch [11], Batch [865/938], Loss: 0.81709885597229\n",
      "Validation: Epoch [11], Batch [866/938], Loss: 0.7834954857826233\n",
      "Validation: Epoch [11], Batch [867/938], Loss: 0.8295552134513855\n",
      "Validation: Epoch [11], Batch [868/938], Loss: 0.7781950235366821\n",
      "Validation: Epoch [11], Batch [869/938], Loss: 0.6095241904258728\n",
      "Validation: Epoch [11], Batch [870/938], Loss: 0.6747404932975769\n",
      "Validation: Epoch [11], Batch [871/938], Loss: 0.6898878216743469\n",
      "Validation: Epoch [11], Batch [872/938], Loss: 0.7153557538986206\n",
      "Validation: Epoch [11], Batch [873/938], Loss: 0.6387317776679993\n",
      "Validation: Epoch [11], Batch [874/938], Loss: 0.829448938369751\n",
      "Validation: Epoch [11], Batch [875/938], Loss: 0.8422523140907288\n",
      "Validation: Epoch [11], Batch [876/938], Loss: 0.8580153584480286\n",
      "Validation: Epoch [11], Batch [877/938], Loss: 0.7884958386421204\n",
      "Validation: Epoch [11], Batch [878/938], Loss: 0.9007983207702637\n",
      "Validation: Epoch [11], Batch [879/938], Loss: 0.7241068482398987\n",
      "Validation: Epoch [11], Batch [880/938], Loss: 0.9734932780265808\n",
      "Validation: Epoch [11], Batch [881/938], Loss: 0.7702788710594177\n",
      "Validation: Epoch [11], Batch [882/938], Loss: 0.7222173810005188\n",
      "Validation: Epoch [11], Batch [883/938], Loss: 0.7033195495605469\n",
      "Validation: Epoch [11], Batch [884/938], Loss: 0.5472347736358643\n",
      "Validation: Epoch [11], Batch [885/938], Loss: 0.8740012049674988\n",
      "Validation: Epoch [11], Batch [886/938], Loss: 0.7215373516082764\n",
      "Validation: Epoch [11], Batch [887/938], Loss: 0.8566082715988159\n",
      "Validation: Epoch [11], Batch [888/938], Loss: 0.9008398652076721\n",
      "Validation: Epoch [11], Batch [889/938], Loss: 0.9749300479888916\n",
      "Validation: Epoch [11], Batch [890/938], Loss: 0.5865062475204468\n",
      "Validation: Epoch [11], Batch [891/938], Loss: 0.7505854368209839\n",
      "Validation: Epoch [11], Batch [892/938], Loss: 0.6946096420288086\n",
      "Validation: Epoch [11], Batch [893/938], Loss: 0.8066564798355103\n",
      "Validation: Epoch [11], Batch [894/938], Loss: 0.8068742752075195\n",
      "Validation: Epoch [11], Batch [895/938], Loss: 0.8116363286972046\n",
      "Validation: Epoch [11], Batch [896/938], Loss: 0.5500178337097168\n",
      "Validation: Epoch [11], Batch [897/938], Loss: 0.6316369771957397\n",
      "Validation: Epoch [11], Batch [898/938], Loss: 0.6785442233085632\n",
      "Validation: Epoch [11], Batch [899/938], Loss: 0.6849998235702515\n",
      "Validation: Epoch [11], Batch [900/938], Loss: 0.834669291973114\n",
      "Validation: Epoch [11], Batch [901/938], Loss: 0.7385043501853943\n",
      "Validation: Epoch [11], Batch [902/938], Loss: 0.9184719324111938\n",
      "Validation: Epoch [11], Batch [903/938], Loss: 0.7002165913581848\n",
      "Validation: Epoch [11], Batch [904/938], Loss: 0.9123090505599976\n",
      "Validation: Epoch [11], Batch [905/938], Loss: 0.873155415058136\n",
      "Validation: Epoch [11], Batch [906/938], Loss: 0.7268372178077698\n",
      "Validation: Epoch [11], Batch [907/938], Loss: 0.7908154726028442\n",
      "Validation: Epoch [11], Batch [908/938], Loss: 1.031873345375061\n",
      "Validation: Epoch [11], Batch [909/938], Loss: 0.9365355968475342\n",
      "Validation: Epoch [11], Batch [910/938], Loss: 0.7114071846008301\n",
      "Validation: Epoch [11], Batch [911/938], Loss: 0.7065334916114807\n",
      "Validation: Epoch [11], Batch [912/938], Loss: 0.7063288688659668\n",
      "Validation: Epoch [11], Batch [913/938], Loss: 0.7540318369865417\n",
      "Validation: Epoch [11], Batch [914/938], Loss: 0.7926554679870605\n",
      "Validation: Epoch [11], Batch [915/938], Loss: 0.7363195419311523\n",
      "Validation: Epoch [11], Batch [916/938], Loss: 0.6180471777915955\n",
      "Validation: Epoch [11], Batch [917/938], Loss: 0.5508721470832825\n",
      "Validation: Epoch [11], Batch [918/938], Loss: 0.660572350025177\n",
      "Validation: Epoch [11], Batch [919/938], Loss: 0.7401232123374939\n",
      "Validation: Epoch [11], Batch [920/938], Loss: 1.007103681564331\n",
      "Validation: Epoch [11], Batch [921/938], Loss: 0.6852021813392639\n",
      "Validation: Epoch [11], Batch [922/938], Loss: 0.7490447163581848\n",
      "Validation: Epoch [11], Batch [923/938], Loss: 0.6888673901557922\n",
      "Validation: Epoch [11], Batch [924/938], Loss: 0.8163164854049683\n",
      "Validation: Epoch [11], Batch [925/938], Loss: 0.6843303442001343\n",
      "Validation: Epoch [11], Batch [926/938], Loss: 0.6757078170776367\n",
      "Validation: Epoch [11], Batch [927/938], Loss: 0.9065536260604858\n",
      "Validation: Epoch [11], Batch [928/938], Loss: 0.636490523815155\n",
      "Validation: Epoch [11], Batch [929/938], Loss: 0.8704187273979187\n",
      "Validation: Epoch [11], Batch [930/938], Loss: 0.719636082649231\n",
      "Validation: Epoch [11], Batch [931/938], Loss: 0.814355731010437\n",
      "Validation: Epoch [11], Batch [932/938], Loss: 0.8123060464859009\n",
      "Validation: Epoch [11], Batch [933/938], Loss: 0.702484667301178\n",
      "Validation: Epoch [11], Batch [934/938], Loss: 0.8641396760940552\n",
      "Validation: Epoch [11], Batch [935/938], Loss: 0.5909931659698486\n",
      "Validation: Epoch [11], Batch [936/938], Loss: 0.7630770802497864\n",
      "Validation: Epoch [11], Batch [937/938], Loss: 1.0816808938980103\n",
      "Validation: Epoch [11], Batch [938/938], Loss: 0.7935201525688171\n",
      "Accuracy of test set: 0.7537333333333334\n",
      "Train: Epoch [12], Batch [1/938], Loss: 0.9450815320014954\n",
      "Train: Epoch [12], Batch [2/938], Loss: 0.6528570652008057\n",
      "Train: Epoch [12], Batch [3/938], Loss: 1.0421788692474365\n",
      "Train: Epoch [12], Batch [4/938], Loss: 0.7634909749031067\n",
      "Train: Epoch [12], Batch [5/938], Loss: 0.7460107207298279\n",
      "Train: Epoch [12], Batch [6/938], Loss: 0.6751357316970825\n",
      "Train: Epoch [12], Batch [7/938], Loss: 0.6258410215377808\n",
      "Train: Epoch [12], Batch [8/938], Loss: 0.7311679720878601\n",
      "Train: Epoch [12], Batch [9/938], Loss: 0.8045812249183655\n",
      "Train: Epoch [12], Batch [10/938], Loss: 0.4552140533924103\n",
      "Train: Epoch [12], Batch [11/938], Loss: 0.8814665079116821\n",
      "Train: Epoch [12], Batch [12/938], Loss: 0.7489349842071533\n",
      "Train: Epoch [12], Batch [13/938], Loss: 0.7972114682197571\n",
      "Train: Epoch [12], Batch [14/938], Loss: 1.2937616109848022\n",
      "Train: Epoch [12], Batch [15/938], Loss: 0.7839942574501038\n",
      "Train: Epoch [12], Batch [16/938], Loss: 0.6932305693626404\n",
      "Train: Epoch [12], Batch [17/938], Loss: 0.7885535955429077\n",
      "Train: Epoch [12], Batch [18/938], Loss: 0.897457480430603\n",
      "Train: Epoch [12], Batch [19/938], Loss: 0.8803830146789551\n",
      "Train: Epoch [12], Batch [20/938], Loss: 0.7709931135177612\n",
      "Train: Epoch [12], Batch [21/938], Loss: 0.8873004913330078\n",
      "Train: Epoch [12], Batch [22/938], Loss: 0.6590763330459595\n",
      "Train: Epoch [12], Batch [23/938], Loss: 0.79356449842453\n",
      "Train: Epoch [12], Batch [24/938], Loss: 0.9873331785202026\n",
      "Train: Epoch [12], Batch [25/938], Loss: 0.7111473083496094\n",
      "Train: Epoch [12], Batch [26/938], Loss: 0.5977821350097656\n",
      "Train: Epoch [12], Batch [27/938], Loss: 0.8929386138916016\n",
      "Train: Epoch [12], Batch [28/938], Loss: 0.5928958654403687\n",
      "Train: Epoch [12], Batch [29/938], Loss: 0.6248049736022949\n",
      "Train: Epoch [12], Batch [30/938], Loss: 0.790935218334198\n",
      "Train: Epoch [12], Batch [31/938], Loss: 1.0670931339263916\n",
      "Train: Epoch [12], Batch [32/938], Loss: 0.758881151676178\n",
      "Train: Epoch [12], Batch [33/938], Loss: 0.9502483606338501\n",
      "Train: Epoch [12], Batch [34/938], Loss: 0.850213885307312\n",
      "Train: Epoch [12], Batch [35/938], Loss: 0.852139413356781\n",
      "Train: Epoch [12], Batch [36/938], Loss: 0.9342359900474548\n",
      "Train: Epoch [12], Batch [37/938], Loss: 0.629144012928009\n",
      "Train: Epoch [12], Batch [38/938], Loss: 0.7987970113754272\n",
      "Train: Epoch [12], Batch [39/938], Loss: 0.6025996208190918\n",
      "Train: Epoch [12], Batch [40/938], Loss: 0.7242593765258789\n",
      "Train: Epoch [12], Batch [41/938], Loss: 0.7883163094520569\n",
      "Train: Epoch [12], Batch [42/938], Loss: 0.4827020764350891\n",
      "Train: Epoch [12], Batch [43/938], Loss: 0.7829580903053284\n",
      "Train: Epoch [12], Batch [44/938], Loss: 0.8307909965515137\n",
      "Train: Epoch [12], Batch [45/938], Loss: 0.8135648369789124\n",
      "Train: Epoch [12], Batch [46/938], Loss: 0.9164087176322937\n",
      "Train: Epoch [12], Batch [47/938], Loss: 0.8395290374755859\n",
      "Train: Epoch [12], Batch [48/938], Loss: 0.6978962421417236\n",
      "Train: Epoch [12], Batch [49/938], Loss: 0.8787283301353455\n",
      "Train: Epoch [12], Batch [50/938], Loss: 0.921201229095459\n",
      "Train: Epoch [12], Batch [51/938], Loss: 0.5795227885246277\n",
      "Train: Epoch [12], Batch [52/938], Loss: 0.7829043865203857\n",
      "Train: Epoch [12], Batch [53/938], Loss: 0.7059826254844666\n",
      "Train: Epoch [12], Batch [54/938], Loss: 0.75450199842453\n",
      "Train: Epoch [12], Batch [55/938], Loss: 0.5112013816833496\n",
      "Train: Epoch [12], Batch [56/938], Loss: 0.8552944660186768\n",
      "Train: Epoch [12], Batch [57/938], Loss: 0.7934212684631348\n",
      "Train: Epoch [12], Batch [58/938], Loss: 0.8976956009864807\n",
      "Train: Epoch [12], Batch [59/938], Loss: 0.5495114922523499\n",
      "Train: Epoch [12], Batch [60/938], Loss: 0.5977301597595215\n",
      "Train: Epoch [12], Batch [61/938], Loss: 0.5260671377182007\n",
      "Train: Epoch [12], Batch [62/938], Loss: 0.6923425197601318\n",
      "Train: Epoch [12], Batch [63/938], Loss: 0.6107885837554932\n",
      "Train: Epoch [12], Batch [64/938], Loss: 0.628624439239502\n",
      "Train: Epoch [12], Batch [65/938], Loss: 0.9351373910903931\n",
      "Train: Epoch [12], Batch [66/938], Loss: 0.7150030136108398\n",
      "Train: Epoch [12], Batch [67/938], Loss: 0.68166184425354\n",
      "Train: Epoch [12], Batch [68/938], Loss: 0.7640004754066467\n",
      "Train: Epoch [12], Batch [69/938], Loss: 0.7176987528800964\n",
      "Train: Epoch [12], Batch [70/938], Loss: 0.862896203994751\n",
      "Train: Epoch [12], Batch [71/938], Loss: 0.8179050087928772\n",
      "Train: Epoch [12], Batch [72/938], Loss: 0.8185964822769165\n",
      "Train: Epoch [12], Batch [73/938], Loss: 0.917069137096405\n",
      "Train: Epoch [12], Batch [74/938], Loss: 0.7217946648597717\n",
      "Train: Epoch [12], Batch [75/938], Loss: 0.8100830316543579\n",
      "Train: Epoch [12], Batch [76/938], Loss: 0.6291196346282959\n",
      "Train: Epoch [12], Batch [77/938], Loss: 0.7613135576248169\n",
      "Train: Epoch [12], Batch [78/938], Loss: 0.7004169225692749\n",
      "Train: Epoch [12], Batch [79/938], Loss: 0.6385191679000854\n",
      "Train: Epoch [12], Batch [80/938], Loss: 0.9479007720947266\n",
      "Train: Epoch [12], Batch [81/938], Loss: 0.8109971880912781\n",
      "Train: Epoch [12], Batch [82/938], Loss: 0.6915738582611084\n",
      "Train: Epoch [12], Batch [83/938], Loss: 0.6009923219680786\n",
      "Train: Epoch [12], Batch [84/938], Loss: 0.6088237762451172\n",
      "Train: Epoch [12], Batch [85/938], Loss: 0.5981627106666565\n",
      "Train: Epoch [12], Batch [86/938], Loss: 0.5168271064758301\n",
      "Train: Epoch [12], Batch [87/938], Loss: 1.0095796585083008\n",
      "Train: Epoch [12], Batch [88/938], Loss: 0.911714494228363\n",
      "Train: Epoch [12], Batch [89/938], Loss: 0.7847620248794556\n",
      "Train: Epoch [12], Batch [90/938], Loss: 0.8124036192893982\n",
      "Train: Epoch [12], Batch [91/938], Loss: 0.7731072902679443\n",
      "Train: Epoch [12], Batch [92/938], Loss: 0.5799278020858765\n",
      "Train: Epoch [12], Batch [93/938], Loss: 0.8653377890586853\n",
      "Train: Epoch [12], Batch [94/938], Loss: 0.9753230810165405\n",
      "Train: Epoch [12], Batch [95/938], Loss: 0.7857953906059265\n",
      "Train: Epoch [12], Batch [96/938], Loss: 0.7360025644302368\n",
      "Train: Epoch [12], Batch [97/938], Loss: 0.5007668137550354\n",
      "Train: Epoch [12], Batch [98/938], Loss: 0.6884191036224365\n",
      "Train: Epoch [12], Batch [99/938], Loss: 1.0310444831848145\n",
      "Train: Epoch [12], Batch [100/938], Loss: 0.8146158456802368\n",
      "Train: Epoch [12], Batch [101/938], Loss: 0.5078136920928955\n",
      "Train: Epoch [12], Batch [102/938], Loss: 0.7401701211929321\n",
      "Train: Epoch [12], Batch [103/938], Loss: 0.8825575113296509\n",
      "Train: Epoch [12], Batch [104/938], Loss: 0.6437732577323914\n",
      "Train: Epoch [12], Batch [105/938], Loss: 0.7990640997886658\n",
      "Train: Epoch [12], Batch [106/938], Loss: 0.6480201482772827\n",
      "Train: Epoch [12], Batch [107/938], Loss: 0.8036764860153198\n",
      "Train: Epoch [12], Batch [108/938], Loss: 0.9716015458106995\n",
      "Train: Epoch [12], Batch [109/938], Loss: 0.8899635672569275\n",
      "Train: Epoch [12], Batch [110/938], Loss: 0.8083266615867615\n",
      "Train: Epoch [12], Batch [111/938], Loss: 0.6261102557182312\n",
      "Train: Epoch [12], Batch [112/938], Loss: 0.7850602865219116\n",
      "Train: Epoch [12], Batch [113/938], Loss: 0.7329772710800171\n",
      "Train: Epoch [12], Batch [114/938], Loss: 0.9830691814422607\n",
      "Train: Epoch [12], Batch [115/938], Loss: 0.9780839085578918\n",
      "Train: Epoch [12], Batch [116/938], Loss: 0.7049177885055542\n",
      "Train: Epoch [12], Batch [117/938], Loss: 0.5785746574401855\n",
      "Train: Epoch [12], Batch [118/938], Loss: 0.6285380721092224\n",
      "Train: Epoch [12], Batch [119/938], Loss: 0.637839138507843\n",
      "Train: Epoch [12], Batch [120/938], Loss: 0.9284217357635498\n",
      "Train: Epoch [12], Batch [121/938], Loss: 0.7026698589324951\n",
      "Train: Epoch [12], Batch [122/938], Loss: 0.8176597952842712\n",
      "Train: Epoch [12], Batch [123/938], Loss: 0.6675503253936768\n",
      "Train: Epoch [12], Batch [124/938], Loss: 0.7768208384513855\n",
      "Train: Epoch [12], Batch [125/938], Loss: 0.9074891805648804\n",
      "Train: Epoch [12], Batch [126/938], Loss: 0.8557993173599243\n",
      "Train: Epoch [12], Batch [127/938], Loss: 0.8558058738708496\n",
      "Train: Epoch [12], Batch [128/938], Loss: 0.6665483117103577\n",
      "Train: Epoch [12], Batch [129/938], Loss: 0.7700096964836121\n",
      "Train: Epoch [12], Batch [130/938], Loss: 0.5765035152435303\n",
      "Train: Epoch [12], Batch [131/938], Loss: 0.70793217420578\n",
      "Train: Epoch [12], Batch [132/938], Loss: 0.7906927466392517\n",
      "Train: Epoch [12], Batch [133/938], Loss: 0.7573592066764832\n",
      "Train: Epoch [12], Batch [134/938], Loss: 0.6524248719215393\n",
      "Train: Epoch [12], Batch [135/938], Loss: 0.7238115072250366\n",
      "Train: Epoch [12], Batch [136/938], Loss: 0.8992187976837158\n",
      "Train: Epoch [12], Batch [137/938], Loss: 0.7163859605789185\n",
      "Train: Epoch [12], Batch [138/938], Loss: 0.9591273665428162\n",
      "Train: Epoch [12], Batch [139/938], Loss: 0.8917213082313538\n",
      "Train: Epoch [12], Batch [140/938], Loss: 0.6850050687789917\n",
      "Train: Epoch [12], Batch [141/938], Loss: 0.5821727514266968\n",
      "Train: Epoch [12], Batch [142/938], Loss: 0.7015700340270996\n",
      "Train: Epoch [12], Batch [143/938], Loss: 0.7668622732162476\n",
      "Train: Epoch [12], Batch [144/938], Loss: 0.4859142601490021\n",
      "Train: Epoch [12], Batch [145/938], Loss: 0.6020753979682922\n",
      "Train: Epoch [12], Batch [146/938], Loss: 0.8078651428222656\n",
      "Train: Epoch [12], Batch [147/938], Loss: 0.6466004252433777\n",
      "Train: Epoch [12], Batch [148/938], Loss: 0.6161239743232727\n",
      "Train: Epoch [12], Batch [149/938], Loss: 0.632386326789856\n",
      "Train: Epoch [12], Batch [150/938], Loss: 0.7987457513809204\n",
      "Train: Epoch [12], Batch [151/938], Loss: 0.7304067015647888\n",
      "Train: Epoch [12], Batch [152/938], Loss: 0.6335311532020569\n",
      "Train: Epoch [12], Batch [153/938], Loss: 0.6720389723777771\n",
      "Train: Epoch [12], Batch [154/938], Loss: 0.6792747974395752\n",
      "Train: Epoch [12], Batch [155/938], Loss: 0.6615485548973083\n",
      "Train: Epoch [12], Batch [156/938], Loss: 0.8337306380271912\n",
      "Train: Epoch [12], Batch [157/938], Loss: 0.6526655554771423\n",
      "Train: Epoch [12], Batch [158/938], Loss: 0.6012240052223206\n",
      "Train: Epoch [12], Batch [159/938], Loss: 0.7793001532554626\n",
      "Train: Epoch [12], Batch [160/938], Loss: 0.8381476402282715\n",
      "Train: Epoch [12], Batch [161/938], Loss: 0.7610490918159485\n",
      "Train: Epoch [12], Batch [162/938], Loss: 0.8248983025550842\n",
      "Train: Epoch [12], Batch [163/938], Loss: 1.0851749181747437\n",
      "Train: Epoch [12], Batch [164/938], Loss: 0.7475214600563049\n",
      "Train: Epoch [12], Batch [165/938], Loss: 0.6953707337379456\n",
      "Train: Epoch [12], Batch [166/938], Loss: 0.7005533576011658\n",
      "Train: Epoch [12], Batch [167/938], Loss: 0.8442684412002563\n",
      "Train: Epoch [12], Batch [168/938], Loss: 1.053977608680725\n",
      "Train: Epoch [12], Batch [169/938], Loss: 0.7819725275039673\n",
      "Train: Epoch [12], Batch [170/938], Loss: 0.9462563991546631\n",
      "Train: Epoch [12], Batch [171/938], Loss: 0.5665082931518555\n",
      "Train: Epoch [12], Batch [172/938], Loss: 0.6794474720954895\n",
      "Train: Epoch [12], Batch [173/938], Loss: 0.6858739852905273\n",
      "Train: Epoch [12], Batch [174/938], Loss: 1.0627385377883911\n",
      "Train: Epoch [12], Batch [175/938], Loss: 0.7992346286773682\n",
      "Train: Epoch [12], Batch [176/938], Loss: 0.7312893867492676\n",
      "Train: Epoch [12], Batch [177/938], Loss: 0.9388881921768188\n",
      "Train: Epoch [12], Batch [178/938], Loss: 0.7912035584449768\n",
      "Train: Epoch [12], Batch [179/938], Loss: 0.6245282888412476\n",
      "Train: Epoch [12], Batch [180/938], Loss: 0.6865254044532776\n",
      "Train: Epoch [12], Batch [181/938], Loss: 0.9124900698661804\n",
      "Train: Epoch [12], Batch [182/938], Loss: 0.7240893244743347\n",
      "Train: Epoch [12], Batch [183/938], Loss: 0.8310792446136475\n",
      "Train: Epoch [12], Batch [184/938], Loss: 0.5013249516487122\n",
      "Train: Epoch [12], Batch [185/938], Loss: 0.9211015105247498\n",
      "Train: Epoch [12], Batch [186/938], Loss: 0.862913966178894\n",
      "Train: Epoch [12], Batch [187/938], Loss: 0.895066499710083\n",
      "Train: Epoch [12], Batch [188/938], Loss: 0.5982322096824646\n",
      "Train: Epoch [12], Batch [189/938], Loss: 0.8115422129631042\n",
      "Train: Epoch [12], Batch [190/938], Loss: 0.6682096123695374\n",
      "Train: Epoch [12], Batch [191/938], Loss: 0.7653433084487915\n",
      "Train: Epoch [12], Batch [192/938], Loss: 0.6346117258071899\n",
      "Train: Epoch [12], Batch [193/938], Loss: 0.5599473714828491\n",
      "Train: Epoch [12], Batch [194/938], Loss: 0.9068921208381653\n",
      "Train: Epoch [12], Batch [195/938], Loss: 0.5833661556243896\n",
      "Train: Epoch [12], Batch [196/938], Loss: 0.7287524938583374\n",
      "Train: Epoch [12], Batch [197/938], Loss: 0.8917552828788757\n",
      "Train: Epoch [12], Batch [198/938], Loss: 0.6289989352226257\n",
      "Train: Epoch [12], Batch [199/938], Loss: 0.7891935706138611\n",
      "Train: Epoch [12], Batch [200/938], Loss: 0.86680006980896\n",
      "Train: Epoch [12], Batch [201/938], Loss: 0.8937515020370483\n",
      "Train: Epoch [12], Batch [202/938], Loss: 0.5395882725715637\n",
      "Train: Epoch [12], Batch [203/938], Loss: 0.7766606211662292\n",
      "Train: Epoch [12], Batch [204/938], Loss: 0.5546376705169678\n",
      "Train: Epoch [12], Batch [205/938], Loss: 0.6336782574653625\n",
      "Train: Epoch [12], Batch [206/938], Loss: 0.6741936206817627\n",
      "Train: Epoch [12], Batch [207/938], Loss: 0.7505106925964355\n",
      "Train: Epoch [12], Batch [208/938], Loss: 0.6996967196464539\n",
      "Train: Epoch [12], Batch [209/938], Loss: 0.7378066778182983\n",
      "Train: Epoch [12], Batch [210/938], Loss: 0.9401457905769348\n",
      "Train: Epoch [12], Batch [211/938], Loss: 0.6005126237869263\n",
      "Train: Epoch [12], Batch [212/938], Loss: 0.6427686810493469\n",
      "Train: Epoch [12], Batch [213/938], Loss: 0.7725319266319275\n",
      "Train: Epoch [12], Batch [214/938], Loss: 0.7051432728767395\n",
      "Train: Epoch [12], Batch [215/938], Loss: 0.913292407989502\n",
      "Train: Epoch [12], Batch [216/938], Loss: 0.7158670425415039\n",
      "Train: Epoch [12], Batch [217/938], Loss: 0.6480618715286255\n",
      "Train: Epoch [12], Batch [218/938], Loss: 0.6899104714393616\n",
      "Train: Epoch [12], Batch [219/938], Loss: 1.2060458660125732\n",
      "Train: Epoch [12], Batch [220/938], Loss: 0.6595736145973206\n",
      "Train: Epoch [12], Batch [221/938], Loss: 0.5817331671714783\n",
      "Train: Epoch [12], Batch [222/938], Loss: 0.9930660128593445\n",
      "Train: Epoch [12], Batch [223/938], Loss: 1.0411231517791748\n",
      "Train: Epoch [12], Batch [224/938], Loss: 0.6845843195915222\n",
      "Train: Epoch [12], Batch [225/938], Loss: 0.6559796333312988\n",
      "Train: Epoch [12], Batch [226/938], Loss: 0.671164870262146\n",
      "Train: Epoch [12], Batch [227/938], Loss: 0.746188223361969\n",
      "Train: Epoch [12], Batch [228/938], Loss: 0.6900327205657959\n",
      "Train: Epoch [12], Batch [229/938], Loss: 0.940501868724823\n",
      "Train: Epoch [12], Batch [230/938], Loss: 0.8423197269439697\n",
      "Train: Epoch [12], Batch [231/938], Loss: 0.8092319369316101\n",
      "Train: Epoch [12], Batch [232/938], Loss: 0.6040599346160889\n",
      "Train: Epoch [12], Batch [233/938], Loss: 0.7687228918075562\n",
      "Train: Epoch [12], Batch [234/938], Loss: 0.9526097774505615\n",
      "Train: Epoch [12], Batch [235/938], Loss: 0.6294115781784058\n",
      "Train: Epoch [12], Batch [236/938], Loss: 0.7130250930786133\n",
      "Train: Epoch [12], Batch [237/938], Loss: 0.808263897895813\n",
      "Train: Epoch [12], Batch [238/938], Loss: 1.0172994136810303\n",
      "Train: Epoch [12], Batch [239/938], Loss: 0.7141016125679016\n",
      "Train: Epoch [12], Batch [240/938], Loss: 0.6774979829788208\n",
      "Train: Epoch [12], Batch [241/938], Loss: 0.7424169778823853\n",
      "Train: Epoch [12], Batch [242/938], Loss: 0.6482863426208496\n",
      "Train: Epoch [12], Batch [243/938], Loss: 0.524990439414978\n",
      "Train: Epoch [12], Batch [244/938], Loss: 0.7546476125717163\n",
      "Train: Epoch [12], Batch [245/938], Loss: 0.5921288132667542\n",
      "Train: Epoch [12], Batch [246/938], Loss: 0.5300354957580566\n",
      "Train: Epoch [12], Batch [247/938], Loss: 0.6732890009880066\n",
      "Train: Epoch [12], Batch [248/938], Loss: 0.9574716687202454\n",
      "Train: Epoch [12], Batch [249/938], Loss: 0.5598737597465515\n",
      "Train: Epoch [12], Batch [250/938], Loss: 0.5626047253608704\n",
      "Train: Epoch [12], Batch [251/938], Loss: 0.8740048408508301\n",
      "Train: Epoch [12], Batch [252/938], Loss: 0.7472512125968933\n",
      "Train: Epoch [12], Batch [253/938], Loss: 0.6947420835494995\n",
      "Train: Epoch [12], Batch [254/938], Loss: 0.5897979736328125\n",
      "Train: Epoch [12], Batch [255/938], Loss: 0.8019285202026367\n",
      "Train: Epoch [12], Batch [256/938], Loss: 0.822949230670929\n",
      "Train: Epoch [12], Batch [257/938], Loss: 0.7187626361846924\n",
      "Train: Epoch [12], Batch [258/938], Loss: 0.8445703983306885\n",
      "Train: Epoch [12], Batch [259/938], Loss: 0.5734554529190063\n",
      "Train: Epoch [12], Batch [260/938], Loss: 0.63225919008255\n",
      "Train: Epoch [12], Batch [261/938], Loss: 0.5876973271369934\n",
      "Train: Epoch [12], Batch [262/938], Loss: 0.6628808379173279\n",
      "Train: Epoch [12], Batch [263/938], Loss: 0.7149825692176819\n",
      "Train: Epoch [12], Batch [264/938], Loss: 0.7624550461769104\n",
      "Train: Epoch [12], Batch [265/938], Loss: 0.9520186185836792\n",
      "Train: Epoch [12], Batch [266/938], Loss: 0.7346895337104797\n",
      "Train: Epoch [12], Batch [267/938], Loss: 0.6394550204277039\n",
      "Train: Epoch [12], Batch [268/938], Loss: 0.6022967100143433\n",
      "Train: Epoch [12], Batch [269/938], Loss: 0.5276435017585754\n",
      "Train: Epoch [12], Batch [270/938], Loss: 0.8683093786239624\n",
      "Train: Epoch [12], Batch [271/938], Loss: 0.6785327196121216\n",
      "Train: Epoch [12], Batch [272/938], Loss: 0.7562049031257629\n",
      "Train: Epoch [12], Batch [273/938], Loss: 0.7545505166053772\n",
      "Train: Epoch [12], Batch [274/938], Loss: 0.5284484624862671\n",
      "Train: Epoch [12], Batch [275/938], Loss: 0.7995050549507141\n",
      "Train: Epoch [12], Batch [276/938], Loss: 0.6401803493499756\n",
      "Train: Epoch [12], Batch [277/938], Loss: 0.8517121076583862\n",
      "Train: Epoch [12], Batch [278/938], Loss: 0.7710285186767578\n",
      "Train: Epoch [12], Batch [279/938], Loss: 0.5553775429725647\n",
      "Train: Epoch [12], Batch [280/938], Loss: 0.5392309427261353\n",
      "Train: Epoch [12], Batch [281/938], Loss: 0.9076766967773438\n",
      "Train: Epoch [12], Batch [282/938], Loss: 0.8218148946762085\n",
      "Train: Epoch [12], Batch [283/938], Loss: 0.7940423488616943\n",
      "Train: Epoch [12], Batch [284/938], Loss: 0.8060165643692017\n",
      "Train: Epoch [12], Batch [285/938], Loss: 0.6121534705162048\n",
      "Train: Epoch [12], Batch [286/938], Loss: 0.9392274618148804\n",
      "Train: Epoch [12], Batch [287/938], Loss: 0.6946431994438171\n",
      "Train: Epoch [12], Batch [288/938], Loss: 0.8475152254104614\n",
      "Train: Epoch [12], Batch [289/938], Loss: 0.6604490280151367\n",
      "Train: Epoch [12], Batch [290/938], Loss: 0.5386918187141418\n",
      "Train: Epoch [12], Batch [291/938], Loss: 0.8303818702697754\n",
      "Train: Epoch [12], Batch [292/938], Loss: 0.7131943702697754\n",
      "Train: Epoch [12], Batch [293/938], Loss: 0.6046200394630432\n",
      "Train: Epoch [12], Batch [294/938], Loss: 0.5808157920837402\n",
      "Train: Epoch [12], Batch [295/938], Loss: 0.6554825305938721\n",
      "Train: Epoch [12], Batch [296/938], Loss: 0.8025341629981995\n",
      "Train: Epoch [12], Batch [297/938], Loss: 0.9233611822128296\n",
      "Train: Epoch [12], Batch [298/938], Loss: 0.6710736751556396\n",
      "Train: Epoch [12], Batch [299/938], Loss: 0.7045059204101562\n",
      "Train: Epoch [12], Batch [300/938], Loss: 0.8815939426422119\n",
      "Train: Epoch [12], Batch [301/938], Loss: 0.8119642734527588\n",
      "Train: Epoch [12], Batch [302/938], Loss: 0.8641559481620789\n",
      "Train: Epoch [12], Batch [303/938], Loss: 0.746626615524292\n",
      "Train: Epoch [12], Batch [304/938], Loss: 0.9097589254379272\n",
      "Train: Epoch [12], Batch [305/938], Loss: 0.8927910327911377\n",
      "Train: Epoch [12], Batch [306/938], Loss: 0.7234470844268799\n",
      "Train: Epoch [12], Batch [307/938], Loss: 0.7580719590187073\n",
      "Train: Epoch [12], Batch [308/938], Loss: 0.6670879125595093\n",
      "Train: Epoch [12], Batch [309/938], Loss: 1.050497055053711\n",
      "Train: Epoch [12], Batch [310/938], Loss: 0.6242116689682007\n",
      "Train: Epoch [12], Batch [311/938], Loss: 0.7145331501960754\n",
      "Train: Epoch [12], Batch [312/938], Loss: 0.8155354857444763\n",
      "Train: Epoch [12], Batch [313/938], Loss: 0.690405547618866\n",
      "Train: Epoch [12], Batch [314/938], Loss: 0.5780569314956665\n",
      "Train: Epoch [12], Batch [315/938], Loss: 0.5944880247116089\n",
      "Train: Epoch [12], Batch [316/938], Loss: 0.6532096862792969\n",
      "Train: Epoch [12], Batch [317/938], Loss: 0.6277564167976379\n",
      "Train: Epoch [12], Batch [318/938], Loss: 0.808538556098938\n",
      "Train: Epoch [12], Batch [319/938], Loss: 0.7336068153381348\n",
      "Train: Epoch [12], Batch [320/938], Loss: 0.7189561724662781\n",
      "Train: Epoch [12], Batch [321/938], Loss: 0.8711761832237244\n",
      "Train: Epoch [12], Batch [322/938], Loss: 1.0415635108947754\n",
      "Train: Epoch [12], Batch [323/938], Loss: 1.0578349828720093\n",
      "Train: Epoch [12], Batch [324/938], Loss: 0.9434863328933716\n",
      "Train: Epoch [12], Batch [325/938], Loss: 0.7752137184143066\n",
      "Train: Epoch [12], Batch [326/938], Loss: 0.6977632641792297\n",
      "Train: Epoch [12], Batch [327/938], Loss: 0.8968105912208557\n",
      "Train: Epoch [12], Batch [328/938], Loss: 0.7063301205635071\n",
      "Train: Epoch [12], Batch [329/938], Loss: 0.754494309425354\n",
      "Train: Epoch [12], Batch [330/938], Loss: 0.6121402978897095\n",
      "Train: Epoch [12], Batch [331/938], Loss: 0.7015948295593262\n",
      "Train: Epoch [12], Batch [332/938], Loss: 0.7574045658111572\n",
      "Train: Epoch [12], Batch [333/938], Loss: 0.9274047017097473\n",
      "Train: Epoch [12], Batch [334/938], Loss: 0.6277037858963013\n",
      "Train: Epoch [12], Batch [335/938], Loss: 0.8846141695976257\n",
      "Train: Epoch [12], Batch [336/938], Loss: 0.8063532114028931\n",
      "Train: Epoch [12], Batch [337/938], Loss: 0.6478970646858215\n",
      "Train: Epoch [12], Batch [338/938], Loss: 0.6281732320785522\n",
      "Train: Epoch [12], Batch [339/938], Loss: 0.8287500739097595\n",
      "Train: Epoch [12], Batch [340/938], Loss: 0.7585745453834534\n",
      "Train: Epoch [12], Batch [341/938], Loss: 0.8256871104240417\n",
      "Train: Epoch [12], Batch [342/938], Loss: 0.7554951906204224\n",
      "Train: Epoch [12], Batch [343/938], Loss: 0.5894220471382141\n",
      "Train: Epoch [12], Batch [344/938], Loss: 0.7080297470092773\n",
      "Train: Epoch [12], Batch [345/938], Loss: 0.9329147338867188\n",
      "Train: Epoch [12], Batch [346/938], Loss: 0.8803465366363525\n",
      "Train: Epoch [12], Batch [347/938], Loss: 0.7532503604888916\n",
      "Train: Epoch [12], Batch [348/938], Loss: 0.7772276401519775\n",
      "Train: Epoch [12], Batch [349/938], Loss: 0.6993173956871033\n",
      "Train: Epoch [12], Batch [350/938], Loss: 0.7337145805358887\n",
      "Train: Epoch [12], Batch [351/938], Loss: 0.7957628965377808\n",
      "Train: Epoch [12], Batch [352/938], Loss: 0.8508577346801758\n",
      "Train: Epoch [12], Batch [353/938], Loss: 0.6205777525901794\n",
      "Train: Epoch [12], Batch [354/938], Loss: 0.6894288659095764\n",
      "Train: Epoch [12], Batch [355/938], Loss: 0.6752090454101562\n",
      "Train: Epoch [12], Batch [356/938], Loss: 0.729101836681366\n",
      "Train: Epoch [12], Batch [357/938], Loss: 0.7214635014533997\n",
      "Train: Epoch [12], Batch [358/938], Loss: 0.6408738493919373\n",
      "Train: Epoch [12], Batch [359/938], Loss: 0.5448494553565979\n",
      "Train: Epoch [12], Batch [360/938], Loss: 0.768098771572113\n",
      "Train: Epoch [12], Batch [361/938], Loss: 0.6861135363578796\n",
      "Train: Epoch [12], Batch [362/938], Loss: 0.7804906964302063\n",
      "Train: Epoch [12], Batch [363/938], Loss: 0.6419363021850586\n",
      "Train: Epoch [12], Batch [364/938], Loss: 0.743492066860199\n",
      "Train: Epoch [12], Batch [365/938], Loss: 0.7236746549606323\n",
      "Train: Epoch [12], Batch [366/938], Loss: 0.6854393482208252\n",
      "Train: Epoch [12], Batch [367/938], Loss: 0.4698057770729065\n",
      "Train: Epoch [12], Batch [368/938], Loss: 0.5580275654792786\n",
      "Train: Epoch [12], Batch [369/938], Loss: 0.5700509548187256\n",
      "Train: Epoch [12], Batch [370/938], Loss: 0.6725232005119324\n",
      "Train: Epoch [12], Batch [371/938], Loss: 0.47331467270851135\n",
      "Train: Epoch [12], Batch [372/938], Loss: 0.7278860211372375\n",
      "Train: Epoch [12], Batch [373/938], Loss: 0.7004514336585999\n",
      "Train: Epoch [12], Batch [374/938], Loss: 0.6498193144798279\n",
      "Train: Epoch [12], Batch [375/938], Loss: 0.7077891230583191\n",
      "Train: Epoch [12], Batch [376/938], Loss: 0.7608112096786499\n",
      "Train: Epoch [12], Batch [377/938], Loss: 0.8122631907463074\n",
      "Train: Epoch [12], Batch [378/938], Loss: 0.8718187212944031\n",
      "Train: Epoch [12], Batch [379/938], Loss: 0.43282121419906616\n",
      "Train: Epoch [12], Batch [380/938], Loss: 0.9246649146080017\n",
      "Train: Epoch [12], Batch [381/938], Loss: 0.5984052419662476\n",
      "Train: Epoch [12], Batch [382/938], Loss: 0.7194072008132935\n",
      "Train: Epoch [12], Batch [383/938], Loss: 0.6844609975814819\n",
      "Train: Epoch [12], Batch [384/938], Loss: 0.8788362741470337\n",
      "Train: Epoch [12], Batch [385/938], Loss: 0.6348024010658264\n",
      "Train: Epoch [12], Batch [386/938], Loss: 0.9445796012878418\n",
      "Train: Epoch [12], Batch [387/938], Loss: 0.6846542954444885\n",
      "Train: Epoch [12], Batch [388/938], Loss: 0.7478755712509155\n",
      "Train: Epoch [12], Batch [389/938], Loss: 0.7035863995552063\n",
      "Train: Epoch [12], Batch [390/938], Loss: 0.7554525136947632\n",
      "Train: Epoch [12], Batch [391/938], Loss: 0.578687310218811\n",
      "Train: Epoch [12], Batch [392/938], Loss: 0.7205948233604431\n",
      "Train: Epoch [12], Batch [393/938], Loss: 1.0062127113342285\n",
      "Train: Epoch [12], Batch [394/938], Loss: 0.8355283737182617\n",
      "Train: Epoch [12], Batch [395/938], Loss: 0.7946568727493286\n",
      "Train: Epoch [12], Batch [396/938], Loss: 0.7715743184089661\n",
      "Train: Epoch [12], Batch [397/938], Loss: 0.6735731363296509\n",
      "Train: Epoch [12], Batch [398/938], Loss: 0.8599632978439331\n",
      "Train: Epoch [12], Batch [399/938], Loss: 0.8528774976730347\n",
      "Train: Epoch [12], Batch [400/938], Loss: 0.7257049083709717\n",
      "Train: Epoch [12], Batch [401/938], Loss: 0.6324403285980225\n",
      "Train: Epoch [12], Batch [402/938], Loss: 0.6806859970092773\n",
      "Train: Epoch [12], Batch [403/938], Loss: 0.7741398215293884\n",
      "Train: Epoch [12], Batch [404/938], Loss: 0.751812219619751\n",
      "Train: Epoch [12], Batch [405/938], Loss: 0.7320834994316101\n",
      "Train: Epoch [12], Batch [406/938], Loss: 0.7204803824424744\n",
      "Train: Epoch [12], Batch [407/938], Loss: 0.4554322361946106\n",
      "Train: Epoch [12], Batch [408/938], Loss: 0.8425084352493286\n",
      "Train: Epoch [12], Batch [409/938], Loss: 0.6696266531944275\n",
      "Train: Epoch [12], Batch [410/938], Loss: 0.7142439484596252\n",
      "Train: Epoch [12], Batch [411/938], Loss: 0.5286622047424316\n",
      "Train: Epoch [12], Batch [412/938], Loss: 0.5956303477287292\n",
      "Train: Epoch [12], Batch [413/938], Loss: 0.8449740409851074\n",
      "Train: Epoch [12], Batch [414/938], Loss: 0.7426989674568176\n",
      "Train: Epoch [12], Batch [415/938], Loss: 0.48681581020355225\n",
      "Train: Epoch [12], Batch [416/938], Loss: 0.8278610110282898\n",
      "Train: Epoch [12], Batch [417/938], Loss: 0.860812783241272\n",
      "Train: Epoch [12], Batch [418/938], Loss: 0.5995948314666748\n",
      "Train: Epoch [12], Batch [419/938], Loss: 0.7530592083930969\n",
      "Train: Epoch [12], Batch [420/938], Loss: 0.43033090233802795\n",
      "Train: Epoch [12], Batch [421/938], Loss: 0.6738042235374451\n",
      "Train: Epoch [12], Batch [422/938], Loss: 0.6898788213729858\n",
      "Train: Epoch [12], Batch [423/938], Loss: 0.8919841051101685\n",
      "Train: Epoch [12], Batch [424/938], Loss: 0.7178511619567871\n",
      "Train: Epoch [12], Batch [425/938], Loss: 0.9102888703346252\n",
      "Train: Epoch [12], Batch [426/938], Loss: 0.7017150521278381\n",
      "Train: Epoch [12], Batch [427/938], Loss: 0.7164834141731262\n",
      "Train: Epoch [12], Batch [428/938], Loss: 0.787729799747467\n",
      "Train: Epoch [12], Batch [429/938], Loss: 0.7569087743759155\n",
      "Train: Epoch [12], Batch [430/938], Loss: 0.534988284111023\n",
      "Train: Epoch [12], Batch [431/938], Loss: 0.7932175993919373\n",
      "Train: Epoch [12], Batch [432/938], Loss: 0.799952507019043\n",
      "Train: Epoch [12], Batch [433/938], Loss: 0.7850431203842163\n",
      "Train: Epoch [12], Batch [434/938], Loss: 0.7969186305999756\n",
      "Train: Epoch [12], Batch [435/938], Loss: 0.6567067503929138\n",
      "Train: Epoch [12], Batch [436/938], Loss: 0.7557475566864014\n",
      "Train: Epoch [12], Batch [437/938], Loss: 0.78556227684021\n",
      "Train: Epoch [12], Batch [438/938], Loss: 0.848719596862793\n",
      "Train: Epoch [12], Batch [439/938], Loss: 0.7654241323471069\n",
      "Train: Epoch [12], Batch [440/938], Loss: 0.6839154362678528\n",
      "Train: Epoch [12], Batch [441/938], Loss: 0.8746482133865356\n",
      "Train: Epoch [12], Batch [442/938], Loss: 1.0778791904449463\n",
      "Train: Epoch [12], Batch [443/938], Loss: 0.7384928464889526\n",
      "Train: Epoch [12], Batch [444/938], Loss: 1.054433822631836\n",
      "Train: Epoch [12], Batch [445/938], Loss: 0.5569331049919128\n",
      "Train: Epoch [12], Batch [446/938], Loss: 0.5840656757354736\n",
      "Train: Epoch [12], Batch [447/938], Loss: 0.8765182495117188\n",
      "Train: Epoch [12], Batch [448/938], Loss: 0.8203113675117493\n",
      "Train: Epoch [12], Batch [449/938], Loss: 0.7438082098960876\n",
      "Train: Epoch [12], Batch [450/938], Loss: 0.9660255908966064\n",
      "Train: Epoch [12], Batch [451/938], Loss: 0.6421617865562439\n",
      "Train: Epoch [12], Batch [452/938], Loss: 0.5483015179634094\n",
      "Train: Epoch [12], Batch [453/938], Loss: 0.6195880770683289\n",
      "Train: Epoch [12], Batch [454/938], Loss: 0.7344546318054199\n",
      "Train: Epoch [12], Batch [455/938], Loss: 0.6896252036094666\n",
      "Train: Epoch [12], Batch [456/938], Loss: 0.4770774245262146\n",
      "Train: Epoch [12], Batch [457/938], Loss: 0.7449694275856018\n",
      "Train: Epoch [12], Batch [458/938], Loss: 0.7439796924591064\n",
      "Train: Epoch [12], Batch [459/938], Loss: 0.5265879034996033\n",
      "Train: Epoch [12], Batch [460/938], Loss: 0.933437705039978\n",
      "Train: Epoch [12], Batch [461/938], Loss: 0.7729007005691528\n",
      "Train: Epoch [12], Batch [462/938], Loss: 0.7819474935531616\n",
      "Train: Epoch [12], Batch [463/938], Loss: 0.7477430701255798\n",
      "Train: Epoch [12], Batch [464/938], Loss: 0.6024926900863647\n",
      "Train: Epoch [12], Batch [465/938], Loss: 0.762822151184082\n",
      "Train: Epoch [12], Batch [466/938], Loss: 0.9190353751182556\n",
      "Train: Epoch [12], Batch [467/938], Loss: 0.6375683546066284\n",
      "Train: Epoch [12], Batch [468/938], Loss: 0.9293755888938904\n",
      "Train: Epoch [12], Batch [469/938], Loss: 0.6650071144104004\n",
      "Train: Epoch [12], Batch [470/938], Loss: 0.6662469506263733\n",
      "Train: Epoch [12], Batch [471/938], Loss: 0.7994238138198853\n",
      "Train: Epoch [12], Batch [472/938], Loss: 0.8538276553153992\n",
      "Train: Epoch [12], Batch [473/938], Loss: 0.6077924370765686\n",
      "Train: Epoch [12], Batch [474/938], Loss: 0.7715339660644531\n",
      "Train: Epoch [12], Batch [475/938], Loss: 0.7973881959915161\n",
      "Train: Epoch [12], Batch [476/938], Loss: 0.7943631410598755\n",
      "Train: Epoch [12], Batch [477/938], Loss: 0.6660148501396179\n",
      "Train: Epoch [12], Batch [478/938], Loss: 0.47169607877731323\n",
      "Train: Epoch [12], Batch [479/938], Loss: 0.7446252107620239\n",
      "Train: Epoch [12], Batch [480/938], Loss: 0.9061592817306519\n",
      "Train: Epoch [12], Batch [481/938], Loss: 0.6179495453834534\n",
      "Train: Epoch [12], Batch [482/938], Loss: 0.7977836728096008\n",
      "Train: Epoch [12], Batch [483/938], Loss: 0.7946586608886719\n",
      "Train: Epoch [12], Batch [484/938], Loss: 0.8519170880317688\n",
      "Train: Epoch [12], Batch [485/938], Loss: 0.5953699946403503\n",
      "Train: Epoch [12], Batch [486/938], Loss: 0.850170910358429\n",
      "Train: Epoch [12], Batch [487/938], Loss: 1.132234811782837\n",
      "Train: Epoch [12], Batch [488/938], Loss: 0.6530392169952393\n",
      "Train: Epoch [12], Batch [489/938], Loss: 0.6785957217216492\n",
      "Train: Epoch [12], Batch [490/938], Loss: 0.9386082887649536\n",
      "Train: Epoch [12], Batch [491/938], Loss: 0.8599117994308472\n",
      "Train: Epoch [12], Batch [492/938], Loss: 1.0112007856369019\n",
      "Train: Epoch [12], Batch [493/938], Loss: 0.8051049709320068\n",
      "Train: Epoch [12], Batch [494/938], Loss: 0.7181394100189209\n",
      "Train: Epoch [12], Batch [495/938], Loss: 0.8296113014221191\n",
      "Train: Epoch [12], Batch [496/938], Loss: 0.7102349996566772\n",
      "Train: Epoch [12], Batch [497/938], Loss: 0.7795891165733337\n",
      "Train: Epoch [12], Batch [498/938], Loss: 0.7991786003112793\n",
      "Train: Epoch [12], Batch [499/938], Loss: 0.6641389727592468\n",
      "Train: Epoch [12], Batch [500/938], Loss: 0.8539524674415588\n",
      "Train: Epoch [12], Batch [501/938], Loss: 0.82016921043396\n",
      "Train: Epoch [12], Batch [502/938], Loss: 0.8477683663368225\n",
      "Train: Epoch [12], Batch [503/938], Loss: 0.7213711142539978\n",
      "Train: Epoch [12], Batch [504/938], Loss: 0.7488312125205994\n",
      "Train: Epoch [12], Batch [505/938], Loss: 0.7881031632423401\n",
      "Train: Epoch [12], Batch [506/938], Loss: 0.750281572341919\n",
      "Train: Epoch [12], Batch [507/938], Loss: 0.39915549755096436\n",
      "Train: Epoch [12], Batch [508/938], Loss: 0.7006077170372009\n",
      "Train: Epoch [12], Batch [509/938], Loss: 0.7275240421295166\n",
      "Train: Epoch [12], Batch [510/938], Loss: 0.9768321514129639\n",
      "Train: Epoch [12], Batch [511/938], Loss: 0.7906333804130554\n",
      "Train: Epoch [12], Batch [512/938], Loss: 0.7778552770614624\n",
      "Train: Epoch [12], Batch [513/938], Loss: 0.7492256164550781\n",
      "Train: Epoch [12], Batch [514/938], Loss: 0.6639156937599182\n",
      "Train: Epoch [12], Batch [515/938], Loss: 0.6090774536132812\n",
      "Train: Epoch [12], Batch [516/938], Loss: 0.7384342551231384\n",
      "Train: Epoch [12], Batch [517/938], Loss: 0.6519427299499512\n",
      "Train: Epoch [12], Batch [518/938], Loss: 0.7215200662612915\n",
      "Train: Epoch [12], Batch [519/938], Loss: 0.9645358920097351\n",
      "Train: Epoch [12], Batch [520/938], Loss: 0.6671040058135986\n",
      "Train: Epoch [12], Batch [521/938], Loss: 0.6735466718673706\n",
      "Train: Epoch [12], Batch [522/938], Loss: 0.42017531394958496\n",
      "Train: Epoch [12], Batch [523/938], Loss: 0.8139930963516235\n",
      "Train: Epoch [12], Batch [524/938], Loss: 0.8111811876296997\n",
      "Train: Epoch [12], Batch [525/938], Loss: 0.6639132499694824\n",
      "Train: Epoch [12], Batch [526/938], Loss: 0.6896010637283325\n",
      "Train: Epoch [12], Batch [527/938], Loss: 0.9249356389045715\n",
      "Train: Epoch [12], Batch [528/938], Loss: 0.8811397552490234\n",
      "Train: Epoch [12], Batch [529/938], Loss: 0.72767174243927\n",
      "Train: Epoch [12], Batch [530/938], Loss: 0.7783410549163818\n",
      "Train: Epoch [12], Batch [531/938], Loss: 0.6040143966674805\n",
      "Train: Epoch [12], Batch [532/938], Loss: 0.7280248403549194\n",
      "Train: Epoch [12], Batch [533/938], Loss: 0.8676208853721619\n",
      "Train: Epoch [12], Batch [534/938], Loss: 0.7063435912132263\n",
      "Train: Epoch [12], Batch [535/938], Loss: 0.7500663995742798\n",
      "Train: Epoch [12], Batch [536/938], Loss: 0.7312442064285278\n",
      "Train: Epoch [12], Batch [537/938], Loss: 0.6396926641464233\n",
      "Train: Epoch [12], Batch [538/938], Loss: 0.7490746974945068\n",
      "Train: Epoch [12], Batch [539/938], Loss: 0.8830782771110535\n",
      "Train: Epoch [12], Batch [540/938], Loss: 0.7128298878669739\n",
      "Train: Epoch [12], Batch [541/938], Loss: 0.7166419625282288\n",
      "Train: Epoch [12], Batch [542/938], Loss: 0.8781225681304932\n",
      "Train: Epoch [12], Batch [543/938], Loss: 0.7402048110961914\n",
      "Train: Epoch [12], Batch [544/938], Loss: 0.5725992918014526\n",
      "Train: Epoch [12], Batch [545/938], Loss: 0.7234964370727539\n",
      "Train: Epoch [12], Batch [546/938], Loss: 0.585071325302124\n",
      "Train: Epoch [12], Batch [547/938], Loss: 0.592171847820282\n",
      "Train: Epoch [12], Batch [548/938], Loss: 0.8458841443061829\n",
      "Train: Epoch [12], Batch [549/938], Loss: 0.7005206942558289\n",
      "Train: Epoch [12], Batch [550/938], Loss: 0.6947574019432068\n",
      "Train: Epoch [12], Batch [551/938], Loss: 0.7354776859283447\n",
      "Train: Epoch [12], Batch [552/938], Loss: 0.7362746596336365\n",
      "Train: Epoch [12], Batch [553/938], Loss: 0.7270028591156006\n",
      "Train: Epoch [12], Batch [554/938], Loss: 0.6998082399368286\n",
      "Train: Epoch [12], Batch [555/938], Loss: 0.7362568974494934\n",
      "Train: Epoch [12], Batch [556/938], Loss: 1.0578869581222534\n",
      "Train: Epoch [12], Batch [557/938], Loss: 0.5938167572021484\n",
      "Train: Epoch [12], Batch [558/938], Loss: 0.7564569711685181\n",
      "Train: Epoch [12], Batch [559/938], Loss: 1.0743520259857178\n",
      "Train: Epoch [12], Batch [560/938], Loss: 0.7101613879203796\n",
      "Train: Epoch [12], Batch [561/938], Loss: 0.8715741038322449\n",
      "Train: Epoch [12], Batch [562/938], Loss: 1.0806530714035034\n",
      "Train: Epoch [12], Batch [563/938], Loss: 0.5690612196922302\n",
      "Train: Epoch [12], Batch [564/938], Loss: 0.7908871173858643\n",
      "Train: Epoch [12], Batch [565/938], Loss: 0.6261202692985535\n",
      "Train: Epoch [12], Batch [566/938], Loss: 0.6519646048545837\n",
      "Train: Epoch [12], Batch [567/938], Loss: 0.6767317652702332\n",
      "Train: Epoch [12], Batch [568/938], Loss: 0.7820386290550232\n",
      "Train: Epoch [12], Batch [569/938], Loss: 0.7713223695755005\n",
      "Train: Epoch [12], Batch [570/938], Loss: 0.6555277705192566\n",
      "Train: Epoch [12], Batch [571/938], Loss: 0.7232769727706909\n",
      "Train: Epoch [12], Batch [572/938], Loss: 0.4786092936992645\n",
      "Train: Epoch [12], Batch [573/938], Loss: 0.5817437767982483\n",
      "Train: Epoch [12], Batch [574/938], Loss: 0.7371099591255188\n",
      "Train: Epoch [12], Batch [575/938], Loss: 0.7704011797904968\n",
      "Train: Epoch [12], Batch [576/938], Loss: 0.731749415397644\n",
      "Train: Epoch [12], Batch [577/938], Loss: 0.6761630773544312\n",
      "Train: Epoch [12], Batch [578/938], Loss: 0.6174840331077576\n",
      "Train: Epoch [12], Batch [579/938], Loss: 0.5693281292915344\n",
      "Train: Epoch [12], Batch [580/938], Loss: 0.7587004899978638\n",
      "Train: Epoch [12], Batch [581/938], Loss: 0.73673015832901\n",
      "Train: Epoch [12], Batch [582/938], Loss: 0.5888655185699463\n",
      "Train: Epoch [12], Batch [583/938], Loss: 0.6958346366882324\n",
      "Train: Epoch [12], Batch [584/938], Loss: 0.5635021924972534\n",
      "Train: Epoch [12], Batch [585/938], Loss: 0.5919121503829956\n",
      "Train: Epoch [12], Batch [586/938], Loss: 0.7794319987297058\n",
      "Train: Epoch [12], Batch [587/938], Loss: 0.6938930153846741\n",
      "Train: Epoch [12], Batch [588/938], Loss: 0.7731853723526001\n",
      "Train: Epoch [12], Batch [589/938], Loss: 0.9824385643005371\n",
      "Train: Epoch [12], Batch [590/938], Loss: 0.845657467842102\n",
      "Train: Epoch [12], Batch [591/938], Loss: 0.6118488907814026\n",
      "Train: Epoch [12], Batch [592/938], Loss: 0.6927691698074341\n",
      "Train: Epoch [12], Batch [593/938], Loss: 0.6293337345123291\n",
      "Train: Epoch [12], Batch [594/938], Loss: 0.7722822427749634\n",
      "Train: Epoch [12], Batch [595/938], Loss: 1.15177583694458\n",
      "Train: Epoch [12], Batch [596/938], Loss: 0.6498998403549194\n",
      "Train: Epoch [12], Batch [597/938], Loss: 0.8188657164573669\n",
      "Train: Epoch [12], Batch [598/938], Loss: 0.7315368056297302\n",
      "Train: Epoch [12], Batch [599/938], Loss: 0.8317984938621521\n",
      "Train: Epoch [12], Batch [600/938], Loss: 0.9678835868835449\n",
      "Train: Epoch [12], Batch [601/938], Loss: 0.608833909034729\n",
      "Train: Epoch [12], Batch [602/938], Loss: 0.9798604249954224\n",
      "Train: Epoch [12], Batch [603/938], Loss: 0.6420170068740845\n",
      "Train: Epoch [12], Batch [604/938], Loss: 0.6529144048690796\n",
      "Train: Epoch [12], Batch [605/938], Loss: 0.6400407552719116\n",
      "Train: Epoch [12], Batch [606/938], Loss: 0.7441380023956299\n",
      "Train: Epoch [12], Batch [607/938], Loss: 0.3808986246585846\n",
      "Train: Epoch [12], Batch [608/938], Loss: 0.7645663022994995\n",
      "Train: Epoch [12], Batch [609/938], Loss: 0.7666540145874023\n",
      "Train: Epoch [12], Batch [610/938], Loss: 0.6712378859519958\n",
      "Train: Epoch [12], Batch [611/938], Loss: 0.7708953022956848\n",
      "Train: Epoch [12], Batch [612/938], Loss: 0.9048792719841003\n",
      "Train: Epoch [12], Batch [613/938], Loss: 0.625913679599762\n",
      "Train: Epoch [12], Batch [614/938], Loss: 0.5012005567550659\n",
      "Train: Epoch [12], Batch [615/938], Loss: 0.633570671081543\n",
      "Train: Epoch [12], Batch [616/938], Loss: 0.6634543538093567\n",
      "Train: Epoch [12], Batch [617/938], Loss: 1.0956792831420898\n",
      "Train: Epoch [12], Batch [618/938], Loss: 0.5848321318626404\n",
      "Train: Epoch [12], Batch [619/938], Loss: 0.6035353541374207\n",
      "Train: Epoch [12], Batch [620/938], Loss: 1.0797004699707031\n",
      "Train: Epoch [12], Batch [621/938], Loss: 1.008495569229126\n",
      "Train: Epoch [12], Batch [622/938], Loss: 0.8463709354400635\n",
      "Train: Epoch [12], Batch [623/938], Loss: 0.8338298797607422\n",
      "Train: Epoch [12], Batch [624/938], Loss: 0.9417160153388977\n",
      "Train: Epoch [12], Batch [625/938], Loss: 0.6138523817062378\n",
      "Train: Epoch [12], Batch [626/938], Loss: 0.7619720101356506\n",
      "Train: Epoch [12], Batch [627/938], Loss: 0.6086985468864441\n",
      "Train: Epoch [12], Batch [628/938], Loss: 0.5614256262779236\n",
      "Train: Epoch [12], Batch [629/938], Loss: 0.6413862705230713\n",
      "Train: Epoch [12], Batch [630/938], Loss: 0.7193566560745239\n",
      "Train: Epoch [12], Batch [631/938], Loss: 0.8146612644195557\n",
      "Train: Epoch [12], Batch [632/938], Loss: 0.7304603457450867\n",
      "Train: Epoch [12], Batch [633/938], Loss: 1.0319702625274658\n",
      "Train: Epoch [12], Batch [634/938], Loss: 0.9032634496688843\n",
      "Train: Epoch [12], Batch [635/938], Loss: 0.8814166784286499\n",
      "Train: Epoch [12], Batch [636/938], Loss: 0.5400936603546143\n",
      "Train: Epoch [12], Batch [637/938], Loss: 0.7777265906333923\n",
      "Train: Epoch [12], Batch [638/938], Loss: 0.9602996706962585\n",
      "Train: Epoch [12], Batch [639/938], Loss: 0.9413233399391174\n",
      "Train: Epoch [12], Batch [640/938], Loss: 0.6725406050682068\n",
      "Train: Epoch [12], Batch [641/938], Loss: 0.78929203748703\n",
      "Train: Epoch [12], Batch [642/938], Loss: 0.9155793786048889\n",
      "Train: Epoch [12], Batch [643/938], Loss: 0.38919195532798767\n",
      "Train: Epoch [12], Batch [644/938], Loss: 0.8921321630477905\n",
      "Train: Epoch [12], Batch [645/938], Loss: 0.9573226571083069\n",
      "Train: Epoch [12], Batch [646/938], Loss: 0.9211123585700989\n",
      "Train: Epoch [12], Batch [647/938], Loss: 0.6063498258590698\n",
      "Train: Epoch [12], Batch [648/938], Loss: 0.7941513061523438\n",
      "Train: Epoch [12], Batch [649/938], Loss: 0.44839000701904297\n",
      "Train: Epoch [12], Batch [650/938], Loss: 0.7190967798233032\n",
      "Train: Epoch [12], Batch [651/938], Loss: 0.8090773224830627\n",
      "Train: Epoch [12], Batch [652/938], Loss: 0.7025561332702637\n",
      "Train: Epoch [12], Batch [653/938], Loss: 0.5813265442848206\n",
      "Train: Epoch [12], Batch [654/938], Loss: 0.7059603929519653\n",
      "Train: Epoch [12], Batch [655/938], Loss: 0.7319766283035278\n",
      "Train: Epoch [12], Batch [656/938], Loss: 0.43067458271980286\n",
      "Train: Epoch [12], Batch [657/938], Loss: 0.7922849655151367\n",
      "Train: Epoch [12], Batch [658/938], Loss: 0.6300297975540161\n",
      "Train: Epoch [12], Batch [659/938], Loss: 0.9231480956077576\n",
      "Train: Epoch [12], Batch [660/938], Loss: 0.5544381737709045\n",
      "Train: Epoch [12], Batch [661/938], Loss: 0.8352742195129395\n",
      "Train: Epoch [12], Batch [662/938], Loss: 0.5625227689743042\n",
      "Train: Epoch [12], Batch [663/938], Loss: 0.8239771127700806\n",
      "Train: Epoch [12], Batch [664/938], Loss: 0.693495512008667\n",
      "Train: Epoch [12], Batch [665/938], Loss: 0.6326701641082764\n",
      "Train: Epoch [12], Batch [666/938], Loss: 0.6687441468238831\n",
      "Train: Epoch [12], Batch [667/938], Loss: 0.8597577810287476\n",
      "Train: Epoch [12], Batch [668/938], Loss: 0.4901837408542633\n",
      "Train: Epoch [12], Batch [669/938], Loss: 0.6682022213935852\n",
      "Train: Epoch [12], Batch [670/938], Loss: 0.7976033091545105\n",
      "Train: Epoch [12], Batch [671/938], Loss: 0.48362046480178833\n",
      "Train: Epoch [12], Batch [672/938], Loss: 0.41313400864601135\n",
      "Train: Epoch [12], Batch [673/938], Loss: 0.5811238884925842\n",
      "Train: Epoch [12], Batch [674/938], Loss: 0.5473064184188843\n",
      "Train: Epoch [12], Batch [675/938], Loss: 0.9275388121604919\n",
      "Train: Epoch [12], Batch [676/938], Loss: 0.7922976016998291\n",
      "Train: Epoch [12], Batch [677/938], Loss: 0.8041788339614868\n",
      "Train: Epoch [12], Batch [678/938], Loss: 0.848010778427124\n",
      "Train: Epoch [12], Batch [679/938], Loss: 0.7628000378608704\n",
      "Train: Epoch [12], Batch [680/938], Loss: 0.5916908383369446\n",
      "Train: Epoch [12], Batch [681/938], Loss: 0.6185733079910278\n",
      "Train: Epoch [12], Batch [682/938], Loss: 0.7521174550056458\n",
      "Train: Epoch [12], Batch [683/938], Loss: 0.6231749057769775\n",
      "Train: Epoch [12], Batch [684/938], Loss: 0.7852763533592224\n",
      "Train: Epoch [12], Batch [685/938], Loss: 0.5314184427261353\n",
      "Train: Epoch [12], Batch [686/938], Loss: 0.7518475651741028\n",
      "Train: Epoch [12], Batch [687/938], Loss: 0.8669991493225098\n",
      "Train: Epoch [12], Batch [688/938], Loss: 0.8003877401351929\n",
      "Train: Epoch [12], Batch [689/938], Loss: 0.6649845838546753\n",
      "Train: Epoch [12], Batch [690/938], Loss: 0.8111550807952881\n",
      "Train: Epoch [12], Batch [691/938], Loss: 1.018850326538086\n",
      "Train: Epoch [12], Batch [692/938], Loss: 0.7382692098617554\n",
      "Train: Epoch [12], Batch [693/938], Loss: 0.7268989086151123\n",
      "Train: Epoch [12], Batch [694/938], Loss: 0.8899715542793274\n",
      "Train: Epoch [12], Batch [695/938], Loss: 0.6460771560668945\n",
      "Train: Epoch [12], Batch [696/938], Loss: 0.5857914686203003\n",
      "Train: Epoch [12], Batch [697/938], Loss: 0.7097501754760742\n",
      "Train: Epoch [12], Batch [698/938], Loss: 0.516319751739502\n",
      "Train: Epoch [12], Batch [699/938], Loss: 0.6484245657920837\n",
      "Train: Epoch [12], Batch [700/938], Loss: 0.6359348297119141\n",
      "Train: Epoch [12], Batch [701/938], Loss: 0.8948242664337158\n",
      "Train: Epoch [12], Batch [702/938], Loss: 0.8433498740196228\n",
      "Train: Epoch [12], Batch [703/938], Loss: 0.8536489605903625\n",
      "Train: Epoch [12], Batch [704/938], Loss: 0.7510481476783752\n",
      "Train: Epoch [12], Batch [705/938], Loss: 0.5269031524658203\n",
      "Train: Epoch [12], Batch [706/938], Loss: 0.7573917508125305\n",
      "Train: Epoch [12], Batch [707/938], Loss: 0.6981446146965027\n",
      "Train: Epoch [12], Batch [708/938], Loss: 0.8159348964691162\n",
      "Train: Epoch [12], Batch [709/938], Loss: 0.7791350483894348\n",
      "Train: Epoch [12], Batch [710/938], Loss: 0.7421754598617554\n",
      "Train: Epoch [12], Batch [711/938], Loss: 0.7447608113288879\n",
      "Train: Epoch [12], Batch [712/938], Loss: 0.6539804935455322\n",
      "Train: Epoch [12], Batch [713/938], Loss: 0.8251128196716309\n",
      "Train: Epoch [12], Batch [714/938], Loss: 0.6740907430648804\n",
      "Train: Epoch [12], Batch [715/938], Loss: 0.5971418023109436\n",
      "Train: Epoch [12], Batch [716/938], Loss: 0.7837244272232056\n",
      "Train: Epoch [12], Batch [717/938], Loss: 0.8036794066429138\n",
      "Train: Epoch [12], Batch [718/938], Loss: 0.8869487643241882\n",
      "Train: Epoch [12], Batch [719/938], Loss: 0.8458389639854431\n",
      "Train: Epoch [12], Batch [720/938], Loss: 0.5300794243812561\n",
      "Train: Epoch [12], Batch [721/938], Loss: 0.8565149307250977\n",
      "Train: Epoch [12], Batch [722/938], Loss: 0.8569377660751343\n",
      "Train: Epoch [12], Batch [723/938], Loss: 0.6457040309906006\n",
      "Train: Epoch [12], Batch [724/938], Loss: 0.5855501890182495\n",
      "Train: Epoch [12], Batch [725/938], Loss: 0.786900520324707\n",
      "Train: Epoch [12], Batch [726/938], Loss: 0.8374122381210327\n",
      "Train: Epoch [12], Batch [727/938], Loss: 0.6148668527603149\n",
      "Train: Epoch [12], Batch [728/938], Loss: 0.8393984436988831\n",
      "Train: Epoch [12], Batch [729/938], Loss: 0.958967387676239\n",
      "Train: Epoch [12], Batch [730/938], Loss: 0.5863791704177856\n",
      "Train: Epoch [12], Batch [731/938], Loss: 0.8116733431816101\n",
      "Train: Epoch [12], Batch [732/938], Loss: 0.8005548119544983\n",
      "Train: Epoch [12], Batch [733/938], Loss: 0.7217687368392944\n",
      "Train: Epoch [12], Batch [734/938], Loss: 0.8011407852172852\n",
      "Train: Epoch [12], Batch [735/938], Loss: 0.7264309525489807\n",
      "Train: Epoch [12], Batch [736/938], Loss: 0.7785254120826721\n",
      "Train: Epoch [12], Batch [737/938], Loss: 0.6518151164054871\n",
      "Train: Epoch [12], Batch [738/938], Loss: 0.7539635300636292\n",
      "Train: Epoch [12], Batch [739/938], Loss: 0.5789788961410522\n",
      "Train: Epoch [12], Batch [740/938], Loss: 0.6859995126724243\n",
      "Train: Epoch [12], Batch [741/938], Loss: 0.660790205001831\n",
      "Train: Epoch [12], Batch [742/938], Loss: 0.6960204839706421\n",
      "Train: Epoch [12], Batch [743/938], Loss: 0.6570637822151184\n",
      "Train: Epoch [12], Batch [744/938], Loss: 0.9627313017845154\n",
      "Train: Epoch [12], Batch [745/938], Loss: 0.6395821571350098\n",
      "Train: Epoch [12], Batch [746/938], Loss: 0.5377436876296997\n",
      "Train: Epoch [12], Batch [747/938], Loss: 0.6949781179428101\n",
      "Train: Epoch [12], Batch [748/938], Loss: 0.7732820510864258\n",
      "Train: Epoch [12], Batch [749/938], Loss: 0.6256633996963501\n",
      "Train: Epoch [12], Batch [750/938], Loss: 0.7643712162971497\n",
      "Train: Epoch [12], Batch [751/938], Loss: 1.068077802658081\n",
      "Train: Epoch [12], Batch [752/938], Loss: 1.1439911127090454\n",
      "Train: Epoch [12], Batch [753/938], Loss: 0.6820892095565796\n",
      "Train: Epoch [12], Batch [754/938], Loss: 0.9551050662994385\n",
      "Train: Epoch [12], Batch [755/938], Loss: 0.9101629257202148\n",
      "Train: Epoch [12], Batch [756/938], Loss: 0.682085394859314\n",
      "Train: Epoch [12], Batch [757/938], Loss: 0.5623657703399658\n",
      "Train: Epoch [12], Batch [758/938], Loss: 0.5844168066978455\n",
      "Train: Epoch [12], Batch [759/938], Loss: 0.7050482034683228\n",
      "Train: Epoch [12], Batch [760/938], Loss: 0.580962598323822\n",
      "Train: Epoch [12], Batch [761/938], Loss: 0.6699112057685852\n",
      "Train: Epoch [12], Batch [762/938], Loss: 0.6081767678260803\n",
      "Train: Epoch [12], Batch [763/938], Loss: 0.8743036985397339\n",
      "Train: Epoch [12], Batch [764/938], Loss: 0.9309494495391846\n",
      "Train: Epoch [12], Batch [765/938], Loss: 0.780468761920929\n",
      "Train: Epoch [12], Batch [766/938], Loss: 0.6526122093200684\n",
      "Train: Epoch [12], Batch [767/938], Loss: 0.6326529383659363\n",
      "Train: Epoch [12], Batch [768/938], Loss: 0.6062575578689575\n",
      "Train: Epoch [12], Batch [769/938], Loss: 0.46586301922798157\n",
      "Train: Epoch [12], Batch [770/938], Loss: 0.7532379627227783\n",
      "Train: Epoch [12], Batch [771/938], Loss: 0.6587092280387878\n",
      "Train: Epoch [12], Batch [772/938], Loss: 0.7334521412849426\n",
      "Train: Epoch [12], Batch [773/938], Loss: 0.6489056348800659\n",
      "Train: Epoch [12], Batch [774/938], Loss: 0.9061063528060913\n",
      "Train: Epoch [12], Batch [775/938], Loss: 0.9405416250228882\n",
      "Train: Epoch [12], Batch [776/938], Loss: 0.95308518409729\n",
      "Train: Epoch [12], Batch [777/938], Loss: 0.9475857019424438\n",
      "Train: Epoch [12], Batch [778/938], Loss: 0.6433184146881104\n",
      "Train: Epoch [12], Batch [779/938], Loss: 0.7162047028541565\n",
      "Train: Epoch [12], Batch [780/938], Loss: 0.8253165483474731\n",
      "Train: Epoch [12], Batch [781/938], Loss: 0.7725464105606079\n",
      "Train: Epoch [12], Batch [782/938], Loss: 0.7509748935699463\n",
      "Train: Epoch [12], Batch [783/938], Loss: 0.49271807074546814\n",
      "Train: Epoch [12], Batch [784/938], Loss: 0.7414190769195557\n",
      "Train: Epoch [12], Batch [785/938], Loss: 0.7649219036102295\n",
      "Train: Epoch [12], Batch [786/938], Loss: 0.5433165431022644\n",
      "Train: Epoch [12], Batch [787/938], Loss: 0.4545208215713501\n",
      "Train: Epoch [12], Batch [788/938], Loss: 0.5971549153327942\n",
      "Train: Epoch [12], Batch [789/938], Loss: 1.0157204866409302\n",
      "Train: Epoch [12], Batch [790/938], Loss: 0.7095946073532104\n",
      "Train: Epoch [12], Batch [791/938], Loss: 0.8484289050102234\n",
      "Train: Epoch [12], Batch [792/938], Loss: 0.7594941258430481\n",
      "Train: Epoch [12], Batch [793/938], Loss: 0.7082962989807129\n",
      "Train: Epoch [12], Batch [794/938], Loss: 0.6754082441329956\n",
      "Train: Epoch [12], Batch [795/938], Loss: 0.699180543422699\n",
      "Train: Epoch [12], Batch [796/938], Loss: 0.8963704109191895\n",
      "Train: Epoch [12], Batch [797/938], Loss: 0.6372054219245911\n",
      "Train: Epoch [12], Batch [798/938], Loss: 0.7412808537483215\n",
      "Train: Epoch [12], Batch [799/938], Loss: 0.8050944805145264\n",
      "Train: Epoch [12], Batch [800/938], Loss: 0.6703693866729736\n",
      "Train: Epoch [12], Batch [801/938], Loss: 0.9266636967658997\n",
      "Train: Epoch [12], Batch [802/938], Loss: 0.6889684200286865\n",
      "Train: Epoch [12], Batch [803/938], Loss: 0.8149574995040894\n",
      "Train: Epoch [12], Batch [804/938], Loss: 0.7982362508773804\n",
      "Train: Epoch [12], Batch [805/938], Loss: 0.7378583550453186\n",
      "Train: Epoch [12], Batch [806/938], Loss: 0.8282524347305298\n",
      "Train: Epoch [12], Batch [807/938], Loss: 0.765071451663971\n",
      "Train: Epoch [12], Batch [808/938], Loss: 0.6595668792724609\n",
      "Train: Epoch [12], Batch [809/938], Loss: 0.59864741563797\n",
      "Train: Epoch [12], Batch [810/938], Loss: 0.7936269044876099\n",
      "Train: Epoch [12], Batch [811/938], Loss: 0.9693860411643982\n",
      "Train: Epoch [12], Batch [812/938], Loss: 0.9720767736434937\n",
      "Train: Epoch [12], Batch [813/938], Loss: 0.6287215948104858\n",
      "Train: Epoch [12], Batch [814/938], Loss: 0.5391473770141602\n",
      "Train: Epoch [12], Batch [815/938], Loss: 1.0032984018325806\n",
      "Train: Epoch [12], Batch [816/938], Loss: 0.6423805952072144\n",
      "Train: Epoch [12], Batch [817/938], Loss: 0.7531782984733582\n",
      "Train: Epoch [12], Batch [818/938], Loss: 0.8456234335899353\n",
      "Train: Epoch [12], Batch [819/938], Loss: 0.7099433541297913\n",
      "Train: Epoch [12], Batch [820/938], Loss: 0.923980712890625\n",
      "Train: Epoch [12], Batch [821/938], Loss: 0.7549760341644287\n",
      "Train: Epoch [12], Batch [822/938], Loss: 0.9091636538505554\n",
      "Train: Epoch [12], Batch [823/938], Loss: 0.8451218008995056\n",
      "Train: Epoch [12], Batch [824/938], Loss: 0.7230392694473267\n",
      "Train: Epoch [12], Batch [825/938], Loss: 0.8801828622817993\n",
      "Train: Epoch [12], Batch [826/938], Loss: 0.6587080955505371\n",
      "Train: Epoch [12], Batch [827/938], Loss: 0.7465804815292358\n",
      "Train: Epoch [12], Batch [828/938], Loss: 0.6565092206001282\n",
      "Train: Epoch [12], Batch [829/938], Loss: 0.5641278028488159\n",
      "Train: Epoch [12], Batch [830/938], Loss: 0.6848952770233154\n",
      "Train: Epoch [12], Batch [831/938], Loss: 0.7614449858665466\n",
      "Train: Epoch [12], Batch [832/938], Loss: 0.6965322494506836\n",
      "Train: Epoch [12], Batch [833/938], Loss: 0.7060261368751526\n",
      "Train: Epoch [12], Batch [834/938], Loss: 0.633187472820282\n",
      "Train: Epoch [12], Batch [835/938], Loss: 0.8057543635368347\n",
      "Train: Epoch [12], Batch [836/938], Loss: 0.7177402377128601\n",
      "Train: Epoch [12], Batch [837/938], Loss: 0.5685206651687622\n",
      "Train: Epoch [12], Batch [838/938], Loss: 0.8082674741744995\n",
      "Train: Epoch [12], Batch [839/938], Loss: 0.672971248626709\n",
      "Train: Epoch [12], Batch [840/938], Loss: 0.7323318719863892\n",
      "Train: Epoch [12], Batch [841/938], Loss: 0.6957711577415466\n",
      "Train: Epoch [12], Batch [842/938], Loss: 0.6118722558021545\n",
      "Train: Epoch [12], Batch [843/938], Loss: 0.7109684944152832\n",
      "Train: Epoch [12], Batch [844/938], Loss: 0.7038361430168152\n",
      "Train: Epoch [12], Batch [845/938], Loss: 0.5671310424804688\n",
      "Train: Epoch [12], Batch [846/938], Loss: 0.6141093373298645\n",
      "Train: Epoch [12], Batch [847/938], Loss: 0.8519596457481384\n",
      "Train: Epoch [12], Batch [848/938], Loss: 0.8989599943161011\n",
      "Train: Epoch [12], Batch [849/938], Loss: 0.780354380607605\n",
      "Train: Epoch [12], Batch [850/938], Loss: 0.5321539640426636\n",
      "Train: Epoch [12], Batch [851/938], Loss: 0.7833174467086792\n",
      "Train: Epoch [12], Batch [852/938], Loss: 0.8783082962036133\n",
      "Train: Epoch [12], Batch [853/938], Loss: 0.723316490650177\n",
      "Train: Epoch [12], Batch [854/938], Loss: 0.7815181016921997\n",
      "Train: Epoch [12], Batch [855/938], Loss: 0.702264666557312\n",
      "Train: Epoch [12], Batch [856/938], Loss: 0.5744261145591736\n",
      "Train: Epoch [12], Batch [857/938], Loss: 0.7342897653579712\n",
      "Train: Epoch [12], Batch [858/938], Loss: 0.8432502150535583\n",
      "Train: Epoch [12], Batch [859/938], Loss: 0.5006459355354309\n",
      "Train: Epoch [12], Batch [860/938], Loss: 0.48390164971351624\n",
      "Train: Epoch [12], Batch [861/938], Loss: 0.7318735122680664\n",
      "Train: Epoch [12], Batch [862/938], Loss: 0.7191547155380249\n",
      "Train: Epoch [12], Batch [863/938], Loss: 0.7884878516197205\n",
      "Train: Epoch [12], Batch [864/938], Loss: 0.7517662644386292\n",
      "Train: Epoch [12], Batch [865/938], Loss: 0.7172554135322571\n",
      "Train: Epoch [12], Batch [866/938], Loss: 0.9099594354629517\n",
      "Train: Epoch [12], Batch [867/938], Loss: 0.5738933682441711\n",
      "Train: Epoch [12], Batch [868/938], Loss: 0.6849461197853088\n",
      "Train: Epoch [12], Batch [869/938], Loss: 0.6015862226486206\n",
      "Train: Epoch [12], Batch [870/938], Loss: 0.5671070218086243\n",
      "Train: Epoch [12], Batch [871/938], Loss: 0.6825165748596191\n",
      "Train: Epoch [12], Batch [872/938], Loss: 1.1082093715667725\n",
      "Train: Epoch [12], Batch [873/938], Loss: 0.8625268936157227\n",
      "Train: Epoch [12], Batch [874/938], Loss: 0.6202131509780884\n",
      "Train: Epoch [12], Batch [875/938], Loss: 0.7299919724464417\n",
      "Train: Epoch [12], Batch [876/938], Loss: 0.8277797698974609\n",
      "Train: Epoch [12], Batch [877/938], Loss: 0.8471275568008423\n",
      "Train: Epoch [12], Batch [878/938], Loss: 0.7893273830413818\n",
      "Train: Epoch [12], Batch [879/938], Loss: 0.5903139710426331\n",
      "Train: Epoch [12], Batch [880/938], Loss: 1.0417166948318481\n",
      "Train: Epoch [12], Batch [881/938], Loss: 0.7886582016944885\n",
      "Train: Epoch [12], Batch [882/938], Loss: 0.8636181354522705\n",
      "Train: Epoch [12], Batch [883/938], Loss: 0.7106695175170898\n",
      "Train: Epoch [12], Batch [884/938], Loss: 0.9342011213302612\n",
      "Train: Epoch [12], Batch [885/938], Loss: 0.6420426368713379\n",
      "Train: Epoch [12], Batch [886/938], Loss: 0.762853741645813\n",
      "Train: Epoch [12], Batch [887/938], Loss: 0.7547064423561096\n",
      "Train: Epoch [12], Batch [888/938], Loss: 0.719468355178833\n",
      "Train: Epoch [12], Batch [889/938], Loss: 0.9652894139289856\n",
      "Train: Epoch [12], Batch [890/938], Loss: 0.8748428821563721\n",
      "Train: Epoch [12], Batch [891/938], Loss: 0.619320273399353\n",
      "Train: Epoch [12], Batch [892/938], Loss: 0.8491315841674805\n",
      "Train: Epoch [12], Batch [893/938], Loss: 0.7695419788360596\n",
      "Train: Epoch [12], Batch [894/938], Loss: 0.7040764689445496\n",
      "Train: Epoch [12], Batch [895/938], Loss: 0.6679931879043579\n",
      "Train: Epoch [12], Batch [896/938], Loss: 0.7437118887901306\n",
      "Train: Epoch [12], Batch [897/938], Loss: 0.6204153895378113\n",
      "Train: Epoch [12], Batch [898/938], Loss: 0.5304688811302185\n",
      "Train: Epoch [12], Batch [899/938], Loss: 0.6482771039009094\n",
      "Train: Epoch [12], Batch [900/938], Loss: 0.8883546590805054\n",
      "Train: Epoch [12], Batch [901/938], Loss: 0.6160880327224731\n",
      "Train: Epoch [12], Batch [902/938], Loss: 0.8776834011077881\n",
      "Train: Epoch [12], Batch [903/938], Loss: 0.7717610597610474\n",
      "Train: Epoch [12], Batch [904/938], Loss: 1.0946124792099\n",
      "Train: Epoch [12], Batch [905/938], Loss: 0.7351594567298889\n",
      "Train: Epoch [12], Batch [906/938], Loss: 0.7642905116081238\n",
      "Train: Epoch [12], Batch [907/938], Loss: 0.8092038631439209\n",
      "Train: Epoch [12], Batch [908/938], Loss: 0.7354198694229126\n",
      "Train: Epoch [12], Batch [909/938], Loss: 0.7711647152900696\n",
      "Train: Epoch [12], Batch [910/938], Loss: 0.8764119148254395\n",
      "Train: Epoch [12], Batch [911/938], Loss: 0.5732461214065552\n",
      "Train: Epoch [12], Batch [912/938], Loss: 1.0938987731933594\n",
      "Train: Epoch [12], Batch [913/938], Loss: 0.6467134356498718\n",
      "Train: Epoch [12], Batch [914/938], Loss: 0.6305033564567566\n",
      "Train: Epoch [12], Batch [915/938], Loss: 0.9541118741035461\n",
      "Train: Epoch [12], Batch [916/938], Loss: 0.9612557291984558\n",
      "Train: Epoch [12], Batch [917/938], Loss: 0.7314648628234863\n",
      "Train: Epoch [12], Batch [918/938], Loss: 0.6685912609100342\n",
      "Train: Epoch [12], Batch [919/938], Loss: 0.6523568630218506\n",
      "Train: Epoch [12], Batch [920/938], Loss: 0.7286339998245239\n",
      "Train: Epoch [12], Batch [921/938], Loss: 0.6327184438705444\n",
      "Train: Epoch [12], Batch [922/938], Loss: 0.6444046497344971\n",
      "Train: Epoch [12], Batch [923/938], Loss: 0.7094343304634094\n",
      "Train: Epoch [12], Batch [924/938], Loss: 0.5045244693756104\n",
      "Train: Epoch [12], Batch [925/938], Loss: 0.8326116800308228\n",
      "Train: Epoch [12], Batch [926/938], Loss: 0.5974022150039673\n",
      "Train: Epoch [12], Batch [927/938], Loss: 0.6712492108345032\n",
      "Train: Epoch [12], Batch [928/938], Loss: 0.7639937996864319\n",
      "Train: Epoch [12], Batch [929/938], Loss: 0.8759967684745789\n",
      "Train: Epoch [12], Batch [930/938], Loss: 0.7133750915527344\n",
      "Train: Epoch [12], Batch [931/938], Loss: 0.827847957611084\n",
      "Train: Epoch [12], Batch [932/938], Loss: 0.6730175018310547\n",
      "Train: Epoch [12], Batch [933/938], Loss: 0.7101746201515198\n",
      "Train: Epoch [12], Batch [934/938], Loss: 0.8564084768295288\n",
      "Train: Epoch [12], Batch [935/938], Loss: 0.7117498517036438\n",
      "Train: Epoch [12], Batch [936/938], Loss: 0.5288515090942383\n",
      "Train: Epoch [12], Batch [937/938], Loss: 0.5748356580734253\n",
      "Train: Epoch [12], Batch [938/938], Loss: 0.45490124821662903\n",
      "Accuracy of train set: 0.77105\n",
      "Validation: Epoch [12], Batch [1/938], Loss: 0.5800476670265198\n",
      "Validation: Epoch [12], Batch [2/938], Loss: 0.7871637940406799\n",
      "Validation: Epoch [12], Batch [3/938], Loss: 0.6963381767272949\n",
      "Validation: Epoch [12], Batch [4/938], Loss: 0.5934359431266785\n",
      "Validation: Epoch [12], Batch [5/938], Loss: 0.6770997643470764\n",
      "Validation: Epoch [12], Batch [6/938], Loss: 0.6955595016479492\n",
      "Validation: Epoch [12], Batch [7/938], Loss: 0.5610694885253906\n",
      "Validation: Epoch [12], Batch [8/938], Loss: 0.7521986961364746\n",
      "Validation: Epoch [12], Batch [9/938], Loss: 0.8493385314941406\n",
      "Validation: Epoch [12], Batch [10/938], Loss: 0.7047228217124939\n",
      "Validation: Epoch [12], Batch [11/938], Loss: 0.7035224437713623\n",
      "Validation: Epoch [12], Batch [12/938], Loss: 0.8030140399932861\n",
      "Validation: Epoch [12], Batch [13/938], Loss: 0.9690473079681396\n",
      "Validation: Epoch [12], Batch [14/938], Loss: 0.7291677594184875\n",
      "Validation: Epoch [12], Batch [15/938], Loss: 0.8066542148590088\n",
      "Validation: Epoch [12], Batch [16/938], Loss: 0.6091150045394897\n",
      "Validation: Epoch [12], Batch [17/938], Loss: 0.7823796272277832\n",
      "Validation: Epoch [12], Batch [18/938], Loss: 0.5421546101570129\n",
      "Validation: Epoch [12], Batch [19/938], Loss: 0.7324371933937073\n",
      "Validation: Epoch [12], Batch [20/938], Loss: 0.8087805509567261\n",
      "Validation: Epoch [12], Batch [21/938], Loss: 0.8446474075317383\n",
      "Validation: Epoch [12], Batch [22/938], Loss: 0.6035312414169312\n",
      "Validation: Epoch [12], Batch [23/938], Loss: 0.4990618824958801\n",
      "Validation: Epoch [12], Batch [24/938], Loss: 0.5653082728385925\n",
      "Validation: Epoch [12], Batch [25/938], Loss: 0.6702254414558411\n",
      "Validation: Epoch [12], Batch [26/938], Loss: 0.7588080167770386\n",
      "Validation: Epoch [12], Batch [27/938], Loss: 0.7028833031654358\n",
      "Validation: Epoch [12], Batch [28/938], Loss: 0.7263432741165161\n",
      "Validation: Epoch [12], Batch [29/938], Loss: 0.5411096811294556\n",
      "Validation: Epoch [12], Batch [30/938], Loss: 0.8121106028556824\n",
      "Validation: Epoch [12], Batch [31/938], Loss: 0.5639206171035767\n",
      "Validation: Epoch [12], Batch [32/938], Loss: 0.5979769229888916\n",
      "Validation: Epoch [12], Batch [33/938], Loss: 0.6022202968597412\n",
      "Validation: Epoch [12], Batch [34/938], Loss: 1.06804358959198\n",
      "Validation: Epoch [12], Batch [35/938], Loss: 0.5945953726768494\n",
      "Validation: Epoch [12], Batch [36/938], Loss: 0.6833459734916687\n",
      "Validation: Epoch [12], Batch [37/938], Loss: 0.6905364394187927\n",
      "Validation: Epoch [12], Batch [38/938], Loss: 0.9696270823478699\n",
      "Validation: Epoch [12], Batch [39/938], Loss: 0.7643184661865234\n",
      "Validation: Epoch [12], Batch [40/938], Loss: 0.7508900165557861\n",
      "Validation: Epoch [12], Batch [41/938], Loss: 0.725806713104248\n",
      "Validation: Epoch [12], Batch [42/938], Loss: 0.6836451292037964\n",
      "Validation: Epoch [12], Batch [43/938], Loss: 0.7870696783065796\n",
      "Validation: Epoch [12], Batch [44/938], Loss: 0.8941140174865723\n",
      "Validation: Epoch [12], Batch [45/938], Loss: 0.8111504316329956\n",
      "Validation: Epoch [12], Batch [46/938], Loss: 0.7679732441902161\n",
      "Validation: Epoch [12], Batch [47/938], Loss: 0.640519380569458\n",
      "Validation: Epoch [12], Batch [48/938], Loss: 0.6863671541213989\n",
      "Validation: Epoch [12], Batch [49/938], Loss: 0.644400954246521\n",
      "Validation: Epoch [12], Batch [50/938], Loss: 0.7875024080276489\n",
      "Validation: Epoch [12], Batch [51/938], Loss: 0.7153283357620239\n",
      "Validation: Epoch [12], Batch [52/938], Loss: 0.5650533437728882\n",
      "Validation: Epoch [12], Batch [53/938], Loss: 0.737265944480896\n",
      "Validation: Epoch [12], Batch [54/938], Loss: 0.7982531189918518\n",
      "Validation: Epoch [12], Batch [55/938], Loss: 0.8089319467544556\n",
      "Validation: Epoch [12], Batch [56/938], Loss: 0.7652249932289124\n",
      "Validation: Epoch [12], Batch [57/938], Loss: 0.7936938405036926\n",
      "Validation: Epoch [12], Batch [58/938], Loss: 0.810403048992157\n",
      "Validation: Epoch [12], Batch [59/938], Loss: 0.5029736757278442\n",
      "Validation: Epoch [12], Batch [60/938], Loss: 0.7346424460411072\n",
      "Validation: Epoch [12], Batch [61/938], Loss: 0.8405715227127075\n",
      "Validation: Epoch [12], Batch [62/938], Loss: 0.7510859966278076\n",
      "Validation: Epoch [12], Batch [63/938], Loss: 0.6577142477035522\n",
      "Validation: Epoch [12], Batch [64/938], Loss: 0.6373775601387024\n",
      "Validation: Epoch [12], Batch [65/938], Loss: 0.764372706413269\n",
      "Validation: Epoch [12], Batch [66/938], Loss: 1.0380923748016357\n",
      "Validation: Epoch [12], Batch [67/938], Loss: 0.7205722332000732\n",
      "Validation: Epoch [12], Batch [68/938], Loss: 0.7320792078971863\n",
      "Validation: Epoch [12], Batch [69/938], Loss: 0.9081164598464966\n",
      "Validation: Epoch [12], Batch [70/938], Loss: 0.6511777639389038\n",
      "Validation: Epoch [12], Batch [71/938], Loss: 0.7599477767944336\n",
      "Validation: Epoch [12], Batch [72/938], Loss: 0.7014753818511963\n",
      "Validation: Epoch [12], Batch [73/938], Loss: 0.5877407789230347\n",
      "Validation: Epoch [12], Batch [74/938], Loss: 0.903565526008606\n",
      "Validation: Epoch [12], Batch [75/938], Loss: 0.4429759383201599\n",
      "Validation: Epoch [12], Batch [76/938], Loss: 0.9151874780654907\n",
      "Validation: Epoch [12], Batch [77/938], Loss: 0.4239298701286316\n",
      "Validation: Epoch [12], Batch [78/938], Loss: 0.7962524890899658\n",
      "Validation: Epoch [12], Batch [79/938], Loss: 0.7782666683197021\n",
      "Validation: Epoch [12], Batch [80/938], Loss: 0.55174320936203\n",
      "Validation: Epoch [12], Batch [81/938], Loss: 0.7194235324859619\n",
      "Validation: Epoch [12], Batch [82/938], Loss: 0.6076829433441162\n",
      "Validation: Epoch [12], Batch [83/938], Loss: 0.7302647829055786\n",
      "Validation: Epoch [12], Batch [84/938], Loss: 0.7450937032699585\n",
      "Validation: Epoch [12], Batch [85/938], Loss: 0.7905303239822388\n",
      "Validation: Epoch [12], Batch [86/938], Loss: 0.7604809403419495\n",
      "Validation: Epoch [12], Batch [87/938], Loss: 0.6917281150817871\n",
      "Validation: Epoch [12], Batch [88/938], Loss: 0.8012863397598267\n",
      "Validation: Epoch [12], Batch [89/938], Loss: 0.7177675366401672\n",
      "Validation: Epoch [12], Batch [90/938], Loss: 0.8003851771354675\n",
      "Validation: Epoch [12], Batch [91/938], Loss: 0.9463856220245361\n",
      "Validation: Epoch [12], Batch [92/938], Loss: 0.5616110563278198\n",
      "Validation: Epoch [12], Batch [93/938], Loss: 0.6667012572288513\n",
      "Validation: Epoch [12], Batch [94/938], Loss: 0.8429222702980042\n",
      "Validation: Epoch [12], Batch [95/938], Loss: 0.7853199243545532\n",
      "Validation: Epoch [12], Batch [96/938], Loss: 0.5080767869949341\n",
      "Validation: Epoch [12], Batch [97/938], Loss: 0.842276394367218\n",
      "Validation: Epoch [12], Batch [98/938], Loss: 0.8560470938682556\n",
      "Validation: Epoch [12], Batch [99/938], Loss: 0.8027064800262451\n",
      "Validation: Epoch [12], Batch [100/938], Loss: 0.5581685304641724\n",
      "Validation: Epoch [12], Batch [101/938], Loss: 0.5389727354049683\n",
      "Validation: Epoch [12], Batch [102/938], Loss: 0.8139398097991943\n",
      "Validation: Epoch [12], Batch [103/938], Loss: 0.8929270505905151\n",
      "Validation: Epoch [12], Batch [104/938], Loss: 0.8784671425819397\n",
      "Validation: Epoch [12], Batch [105/938], Loss: 0.66607666015625\n",
      "Validation: Epoch [12], Batch [106/938], Loss: 0.8860039114952087\n",
      "Validation: Epoch [12], Batch [107/938], Loss: 0.8126057982444763\n",
      "Validation: Epoch [12], Batch [108/938], Loss: 0.9099573493003845\n",
      "Validation: Epoch [12], Batch [109/938], Loss: 0.6702240705490112\n",
      "Validation: Epoch [12], Batch [110/938], Loss: 0.6233901381492615\n",
      "Validation: Epoch [12], Batch [111/938], Loss: 0.779249906539917\n",
      "Validation: Epoch [12], Batch [112/938], Loss: 0.6897902488708496\n",
      "Validation: Epoch [12], Batch [113/938], Loss: 0.5462607741355896\n",
      "Validation: Epoch [12], Batch [114/938], Loss: 0.7240064144134521\n",
      "Validation: Epoch [12], Batch [115/938], Loss: 0.7838719487190247\n",
      "Validation: Epoch [12], Batch [116/938], Loss: 0.9309867024421692\n",
      "Validation: Epoch [12], Batch [117/938], Loss: 0.725597620010376\n",
      "Validation: Epoch [12], Batch [118/938], Loss: 0.6880386471748352\n",
      "Validation: Epoch [12], Batch [119/938], Loss: 0.802922785282135\n",
      "Validation: Epoch [12], Batch [120/938], Loss: 0.6887174844741821\n",
      "Validation: Epoch [12], Batch [121/938], Loss: 0.6758809685707092\n",
      "Validation: Epoch [12], Batch [122/938], Loss: 0.7788050174713135\n",
      "Validation: Epoch [12], Batch [123/938], Loss: 0.6919724345207214\n",
      "Validation: Epoch [12], Batch [124/938], Loss: 0.5868619084358215\n",
      "Validation: Epoch [12], Batch [125/938], Loss: 0.8067751526832581\n",
      "Validation: Epoch [12], Batch [126/938], Loss: 0.9338758587837219\n",
      "Validation: Epoch [12], Batch [127/938], Loss: 0.5773301124572754\n",
      "Validation: Epoch [12], Batch [128/938], Loss: 0.6797186136245728\n",
      "Validation: Epoch [12], Batch [129/938], Loss: 0.8359490036964417\n",
      "Validation: Epoch [12], Batch [130/938], Loss: 1.0455849170684814\n",
      "Validation: Epoch [12], Batch [131/938], Loss: 0.6364011168479919\n",
      "Validation: Epoch [12], Batch [132/938], Loss: 0.5985350012779236\n",
      "Validation: Epoch [12], Batch [133/938], Loss: 0.7600666880607605\n",
      "Validation: Epoch [12], Batch [134/938], Loss: 0.7622888088226318\n",
      "Validation: Epoch [12], Batch [135/938], Loss: 0.6969814300537109\n",
      "Validation: Epoch [12], Batch [136/938], Loss: 0.6991090178489685\n",
      "Validation: Epoch [12], Batch [137/938], Loss: 0.807809591293335\n",
      "Validation: Epoch [12], Batch [138/938], Loss: 0.6776235699653625\n",
      "Validation: Epoch [12], Batch [139/938], Loss: 0.7980438470840454\n",
      "Validation: Epoch [12], Batch [140/938], Loss: 0.9368599653244019\n",
      "Validation: Epoch [12], Batch [141/938], Loss: 0.6746099591255188\n",
      "Validation: Epoch [12], Batch [142/938], Loss: 0.6935255527496338\n",
      "Validation: Epoch [12], Batch [143/938], Loss: 0.6793277859687805\n",
      "Validation: Epoch [12], Batch [144/938], Loss: 0.5969201922416687\n",
      "Validation: Epoch [12], Batch [145/938], Loss: 0.5710912346839905\n",
      "Validation: Epoch [12], Batch [146/938], Loss: 0.8569931983947754\n",
      "Validation: Epoch [12], Batch [147/938], Loss: 0.6397722959518433\n",
      "Validation: Epoch [12], Batch [148/938], Loss: 0.6740315556526184\n",
      "Validation: Epoch [12], Batch [149/938], Loss: 0.5680404901504517\n",
      "Validation: Epoch [12], Batch [150/938], Loss: 0.8142107129096985\n",
      "Validation: Epoch [12], Batch [151/938], Loss: 0.9028759002685547\n",
      "Validation: Epoch [12], Batch [152/938], Loss: 0.8967859745025635\n",
      "Validation: Epoch [12], Batch [153/938], Loss: 0.9998201131820679\n",
      "Validation: Epoch [12], Batch [154/938], Loss: 0.5555993914604187\n",
      "Validation: Epoch [12], Batch [155/938], Loss: 0.7082757353782654\n",
      "Validation: Epoch [12], Batch [156/938], Loss: 0.6506534814834595\n",
      "Validation: Epoch [12], Batch [157/938], Loss: 0.6339531540870667\n",
      "Validation: Epoch [12], Batch [158/938], Loss: 0.6613282561302185\n",
      "Validation: Epoch [12], Batch [159/938], Loss: 0.568293035030365\n",
      "Validation: Epoch [12], Batch [160/938], Loss: 0.8288393020629883\n",
      "Validation: Epoch [12], Batch [161/938], Loss: 0.4970680773258209\n",
      "Validation: Epoch [12], Batch [162/938], Loss: 0.8565119504928589\n",
      "Validation: Epoch [12], Batch [163/938], Loss: 0.7853617668151855\n",
      "Validation: Epoch [12], Batch [164/938], Loss: 0.6225183606147766\n",
      "Validation: Epoch [12], Batch [165/938], Loss: 0.7857412695884705\n",
      "Validation: Epoch [12], Batch [166/938], Loss: 0.8404079079627991\n",
      "Validation: Epoch [12], Batch [167/938], Loss: 0.47735482454299927\n",
      "Validation: Epoch [12], Batch [168/938], Loss: 0.8395746946334839\n",
      "Validation: Epoch [12], Batch [169/938], Loss: 0.7286887764930725\n",
      "Validation: Epoch [12], Batch [170/938], Loss: 0.6130841970443726\n",
      "Validation: Epoch [12], Batch [171/938], Loss: 0.855678379535675\n",
      "Validation: Epoch [12], Batch [172/938], Loss: 0.731083869934082\n",
      "Validation: Epoch [12], Batch [173/938], Loss: 0.8311079740524292\n",
      "Validation: Epoch [12], Batch [174/938], Loss: 0.6993687152862549\n",
      "Validation: Epoch [12], Batch [175/938], Loss: 0.7615600228309631\n",
      "Validation: Epoch [12], Batch [176/938], Loss: 0.712988018989563\n",
      "Validation: Epoch [12], Batch [177/938], Loss: 0.6940585374832153\n",
      "Validation: Epoch [12], Batch [178/938], Loss: 0.5376779437065125\n",
      "Validation: Epoch [12], Batch [179/938], Loss: 0.6255236268043518\n",
      "Validation: Epoch [12], Batch [180/938], Loss: 0.8232095837593079\n",
      "Validation: Epoch [12], Batch [181/938], Loss: 0.6396159529685974\n",
      "Validation: Epoch [12], Batch [182/938], Loss: 0.7250765562057495\n",
      "Validation: Epoch [12], Batch [183/938], Loss: 0.7284571528434753\n",
      "Validation: Epoch [12], Batch [184/938], Loss: 0.6071822047233582\n",
      "Validation: Epoch [12], Batch [185/938], Loss: 0.6756960153579712\n",
      "Validation: Epoch [12], Batch [186/938], Loss: 0.993946373462677\n",
      "Validation: Epoch [12], Batch [187/938], Loss: 0.8872051239013672\n",
      "Validation: Epoch [12], Batch [188/938], Loss: 0.7085365056991577\n",
      "Validation: Epoch [12], Batch [189/938], Loss: 0.453763484954834\n",
      "Validation: Epoch [12], Batch [190/938], Loss: 0.8793637156486511\n",
      "Validation: Epoch [12], Batch [191/938], Loss: 0.7156652808189392\n",
      "Validation: Epoch [12], Batch [192/938], Loss: 0.8402108550071716\n",
      "Validation: Epoch [12], Batch [193/938], Loss: 0.861068844795227\n",
      "Validation: Epoch [12], Batch [194/938], Loss: 0.9657294154167175\n",
      "Validation: Epoch [12], Batch [195/938], Loss: 0.7817453742027283\n",
      "Validation: Epoch [12], Batch [196/938], Loss: 0.7824545502662659\n",
      "Validation: Epoch [12], Batch [197/938], Loss: 0.6348767280578613\n",
      "Validation: Epoch [12], Batch [198/938], Loss: 0.6804365515708923\n",
      "Validation: Epoch [12], Batch [199/938], Loss: 1.0623137950897217\n",
      "Validation: Epoch [12], Batch [200/938], Loss: 1.017272710800171\n",
      "Validation: Epoch [12], Batch [201/938], Loss: 0.963106095790863\n",
      "Validation: Epoch [12], Batch [202/938], Loss: 0.8254282474517822\n",
      "Validation: Epoch [12], Batch [203/938], Loss: 1.1356409788131714\n",
      "Validation: Epoch [12], Batch [204/938], Loss: 0.6100714802742004\n",
      "Validation: Epoch [12], Batch [205/938], Loss: 0.7258663177490234\n",
      "Validation: Epoch [12], Batch [206/938], Loss: 0.6091266870498657\n",
      "Validation: Epoch [12], Batch [207/938], Loss: 0.6159994602203369\n",
      "Validation: Epoch [12], Batch [208/938], Loss: 0.6687997579574585\n",
      "Validation: Epoch [12], Batch [209/938], Loss: 0.7471402883529663\n",
      "Validation: Epoch [12], Batch [210/938], Loss: 0.6050772666931152\n",
      "Validation: Epoch [12], Batch [211/938], Loss: 0.7996655702590942\n",
      "Validation: Epoch [12], Batch [212/938], Loss: 0.5732685923576355\n",
      "Validation: Epoch [12], Batch [213/938], Loss: 0.9229723215103149\n",
      "Validation: Epoch [12], Batch [214/938], Loss: 0.8689153790473938\n",
      "Validation: Epoch [12], Batch [215/938], Loss: 0.6352034211158752\n",
      "Validation: Epoch [12], Batch [216/938], Loss: 0.6426562070846558\n",
      "Validation: Epoch [12], Batch [217/938], Loss: 0.6415379047393799\n",
      "Validation: Epoch [12], Batch [218/938], Loss: 0.7380457520484924\n",
      "Validation: Epoch [12], Batch [219/938], Loss: 0.9361382722854614\n",
      "Validation: Epoch [12], Batch [220/938], Loss: 0.6591275334358215\n",
      "Validation: Epoch [12], Batch [221/938], Loss: 0.7478083968162537\n",
      "Validation: Epoch [12], Batch [222/938], Loss: 0.5264456272125244\n",
      "Validation: Epoch [12], Batch [223/938], Loss: 0.8747824430465698\n",
      "Validation: Epoch [12], Batch [224/938], Loss: 0.5913900136947632\n",
      "Validation: Epoch [12], Batch [225/938], Loss: 0.7938978672027588\n",
      "Validation: Epoch [12], Batch [226/938], Loss: 0.6437869071960449\n",
      "Validation: Epoch [12], Batch [227/938], Loss: 0.932466983795166\n",
      "Validation: Epoch [12], Batch [228/938], Loss: 0.6633450984954834\n",
      "Validation: Epoch [12], Batch [229/938], Loss: 0.8230025768280029\n",
      "Validation: Epoch [12], Batch [230/938], Loss: 0.7388682961463928\n",
      "Validation: Epoch [12], Batch [231/938], Loss: 0.6220740079879761\n",
      "Validation: Epoch [12], Batch [232/938], Loss: 0.6901295185089111\n",
      "Validation: Epoch [12], Batch [233/938], Loss: 0.4102517366409302\n",
      "Validation: Epoch [12], Batch [234/938], Loss: 0.661490261554718\n",
      "Validation: Epoch [12], Batch [235/938], Loss: 0.5847171545028687\n",
      "Validation: Epoch [12], Batch [236/938], Loss: 0.7773187756538391\n",
      "Validation: Epoch [12], Batch [237/938], Loss: 0.5903248190879822\n",
      "Validation: Epoch [12], Batch [238/938], Loss: 0.692725419998169\n",
      "Validation: Epoch [12], Batch [239/938], Loss: 0.5600705742835999\n",
      "Validation: Epoch [12], Batch [240/938], Loss: 0.6359542608261108\n",
      "Validation: Epoch [12], Batch [241/938], Loss: 0.7964566349983215\n",
      "Validation: Epoch [12], Batch [242/938], Loss: 0.823989748954773\n",
      "Validation: Epoch [12], Batch [243/938], Loss: 0.6023781299591064\n",
      "Validation: Epoch [12], Batch [244/938], Loss: 0.7648151516914368\n",
      "Validation: Epoch [12], Batch [245/938], Loss: 0.796140193939209\n",
      "Validation: Epoch [12], Batch [246/938], Loss: 0.7934767603874207\n",
      "Validation: Epoch [12], Batch [247/938], Loss: 0.8232530355453491\n",
      "Validation: Epoch [12], Batch [248/938], Loss: 0.7090269327163696\n",
      "Validation: Epoch [12], Batch [249/938], Loss: 0.8343005776405334\n",
      "Validation: Epoch [12], Batch [250/938], Loss: 0.8757727742195129\n",
      "Validation: Epoch [12], Batch [251/938], Loss: 0.721361517906189\n",
      "Validation: Epoch [12], Batch [252/938], Loss: 0.6343547701835632\n",
      "Validation: Epoch [12], Batch [253/938], Loss: 0.6995219588279724\n",
      "Validation: Epoch [12], Batch [254/938], Loss: 0.7475485801696777\n",
      "Validation: Epoch [12], Batch [255/938], Loss: 0.5222322940826416\n",
      "Validation: Epoch [12], Batch [256/938], Loss: 0.7322092652320862\n",
      "Validation: Epoch [12], Batch [257/938], Loss: 0.7617537379264832\n",
      "Validation: Epoch [12], Batch [258/938], Loss: 0.5343979001045227\n",
      "Validation: Epoch [12], Batch [259/938], Loss: 0.5702054500579834\n",
      "Validation: Epoch [12], Batch [260/938], Loss: 0.7241436243057251\n",
      "Validation: Epoch [12], Batch [261/938], Loss: 0.8089901208877563\n",
      "Validation: Epoch [12], Batch [262/938], Loss: 0.6964107751846313\n",
      "Validation: Epoch [12], Batch [263/938], Loss: 0.8585001826286316\n",
      "Validation: Epoch [12], Batch [264/938], Loss: 0.701120913028717\n",
      "Validation: Epoch [12], Batch [265/938], Loss: 0.7960528135299683\n",
      "Validation: Epoch [12], Batch [266/938], Loss: 0.622261106967926\n",
      "Validation: Epoch [12], Batch [267/938], Loss: 0.7316700220108032\n",
      "Validation: Epoch [12], Batch [268/938], Loss: 0.6713654398918152\n",
      "Validation: Epoch [12], Batch [269/938], Loss: 0.5644698143005371\n",
      "Validation: Epoch [12], Batch [270/938], Loss: 0.6900277137756348\n",
      "Validation: Epoch [12], Batch [271/938], Loss: 0.848122239112854\n",
      "Validation: Epoch [12], Batch [272/938], Loss: 0.7734525203704834\n",
      "Validation: Epoch [12], Batch [273/938], Loss: 0.9449501037597656\n",
      "Validation: Epoch [12], Batch [274/938], Loss: 0.8730677366256714\n",
      "Validation: Epoch [12], Batch [275/938], Loss: 0.7037885785102844\n",
      "Validation: Epoch [12], Batch [276/938], Loss: 0.6067327260971069\n",
      "Validation: Epoch [12], Batch [277/938], Loss: 0.5147767066955566\n",
      "Validation: Epoch [12], Batch [278/938], Loss: 0.7527813911437988\n",
      "Validation: Epoch [12], Batch [279/938], Loss: 0.5340394377708435\n",
      "Validation: Epoch [12], Batch [280/938], Loss: 0.8363962173461914\n",
      "Validation: Epoch [12], Batch [281/938], Loss: 0.6127446889877319\n",
      "Validation: Epoch [12], Batch [282/938], Loss: 0.8770209550857544\n",
      "Validation: Epoch [12], Batch [283/938], Loss: 0.6763055324554443\n",
      "Validation: Epoch [12], Batch [284/938], Loss: 0.714691162109375\n",
      "Validation: Epoch [12], Batch [285/938], Loss: 0.729286789894104\n",
      "Validation: Epoch [12], Batch [286/938], Loss: 0.7787613868713379\n",
      "Validation: Epoch [12], Batch [287/938], Loss: 0.532514750957489\n",
      "Validation: Epoch [12], Batch [288/938], Loss: 0.9429978728294373\n",
      "Validation: Epoch [12], Batch [289/938], Loss: 0.743019163608551\n",
      "Validation: Epoch [12], Batch [290/938], Loss: 0.7442788481712341\n",
      "Validation: Epoch [12], Batch [291/938], Loss: 0.6035556197166443\n",
      "Validation: Epoch [12], Batch [292/938], Loss: 0.7966038584709167\n",
      "Validation: Epoch [12], Batch [293/938], Loss: 0.9325403571128845\n",
      "Validation: Epoch [12], Batch [294/938], Loss: 0.9075972437858582\n",
      "Validation: Epoch [12], Batch [295/938], Loss: 0.7327611446380615\n",
      "Validation: Epoch [12], Batch [296/938], Loss: 0.6843716502189636\n",
      "Validation: Epoch [12], Batch [297/938], Loss: 0.6471647620201111\n",
      "Validation: Epoch [12], Batch [298/938], Loss: 0.8348904848098755\n",
      "Validation: Epoch [12], Batch [299/938], Loss: 0.8279210329055786\n",
      "Validation: Epoch [12], Batch [300/938], Loss: 0.8531146049499512\n",
      "Validation: Epoch [12], Batch [301/938], Loss: 0.6521888971328735\n",
      "Validation: Epoch [12], Batch [302/938], Loss: 0.8134713172912598\n",
      "Validation: Epoch [12], Batch [303/938], Loss: 0.6237468719482422\n",
      "Validation: Epoch [12], Batch [304/938], Loss: 0.7559276223182678\n",
      "Validation: Epoch [12], Batch [305/938], Loss: 0.7098656892776489\n",
      "Validation: Epoch [12], Batch [306/938], Loss: 0.5508741736412048\n",
      "Validation: Epoch [12], Batch [307/938], Loss: 0.768139123916626\n",
      "Validation: Epoch [12], Batch [308/938], Loss: 0.629230797290802\n",
      "Validation: Epoch [12], Batch [309/938], Loss: 0.6264479756355286\n",
      "Validation: Epoch [12], Batch [310/938], Loss: 0.8341663479804993\n",
      "Validation: Epoch [12], Batch [311/938], Loss: 0.5172869563102722\n",
      "Validation: Epoch [12], Batch [312/938], Loss: 0.9310026168823242\n",
      "Validation: Epoch [12], Batch [313/938], Loss: 0.6316578388214111\n",
      "Validation: Epoch [12], Batch [314/938], Loss: 0.7134214639663696\n",
      "Validation: Epoch [12], Batch [315/938], Loss: 0.7238759994506836\n",
      "Validation: Epoch [12], Batch [316/938], Loss: 0.8875840306282043\n",
      "Validation: Epoch [12], Batch [317/938], Loss: 0.5785238146781921\n",
      "Validation: Epoch [12], Batch [318/938], Loss: 0.6833920478820801\n",
      "Validation: Epoch [12], Batch [319/938], Loss: 0.6963602304458618\n",
      "Validation: Epoch [12], Batch [320/938], Loss: 0.8364483714103699\n",
      "Validation: Epoch [12], Batch [321/938], Loss: 0.6323156952857971\n",
      "Validation: Epoch [12], Batch [322/938], Loss: 0.8223742842674255\n",
      "Validation: Epoch [12], Batch [323/938], Loss: 0.7784947156906128\n",
      "Validation: Epoch [12], Batch [324/938], Loss: 0.7954522371292114\n",
      "Validation: Epoch [12], Batch [325/938], Loss: 0.8152113556861877\n",
      "Validation: Epoch [12], Batch [326/938], Loss: 0.5874124765396118\n",
      "Validation: Epoch [12], Batch [327/938], Loss: 0.8580752015113831\n",
      "Validation: Epoch [12], Batch [328/938], Loss: 0.7686514854431152\n",
      "Validation: Epoch [12], Batch [329/938], Loss: 0.6484474539756775\n",
      "Validation: Epoch [12], Batch [330/938], Loss: 0.6589823365211487\n",
      "Validation: Epoch [12], Batch [331/938], Loss: 0.7269701957702637\n",
      "Validation: Epoch [12], Batch [332/938], Loss: 0.6675758957862854\n",
      "Validation: Epoch [12], Batch [333/938], Loss: 0.7736086249351501\n",
      "Validation: Epoch [12], Batch [334/938], Loss: 0.534468412399292\n",
      "Validation: Epoch [12], Batch [335/938], Loss: 0.662270188331604\n",
      "Validation: Epoch [12], Batch [336/938], Loss: 0.7650481462478638\n",
      "Validation: Epoch [12], Batch [337/938], Loss: 0.890307605266571\n",
      "Validation: Epoch [12], Batch [338/938], Loss: 0.8572755455970764\n",
      "Validation: Epoch [12], Batch [339/938], Loss: 0.7907789945602417\n",
      "Validation: Epoch [12], Batch [340/938], Loss: 0.8341832160949707\n",
      "Validation: Epoch [12], Batch [341/938], Loss: 0.7558797597885132\n",
      "Validation: Epoch [12], Batch [342/938], Loss: 0.7567026615142822\n",
      "Validation: Epoch [12], Batch [343/938], Loss: 0.9299936294555664\n",
      "Validation: Epoch [12], Batch [344/938], Loss: 0.708367109298706\n",
      "Validation: Epoch [12], Batch [345/938], Loss: 0.8832860589027405\n",
      "Validation: Epoch [12], Batch [346/938], Loss: 0.8533241748809814\n",
      "Validation: Epoch [12], Batch [347/938], Loss: 0.7644760608673096\n",
      "Validation: Epoch [12], Batch [348/938], Loss: 0.7357057332992554\n",
      "Validation: Epoch [12], Batch [349/938], Loss: 0.7115018367767334\n",
      "Validation: Epoch [12], Batch [350/938], Loss: 0.9174520373344421\n",
      "Validation: Epoch [12], Batch [351/938], Loss: 0.5132639408111572\n",
      "Validation: Epoch [12], Batch [352/938], Loss: 1.0306493043899536\n",
      "Validation: Epoch [12], Batch [353/938], Loss: 0.7752139568328857\n",
      "Validation: Epoch [12], Batch [354/938], Loss: 0.7540939450263977\n",
      "Validation: Epoch [12], Batch [355/938], Loss: 0.5586792230606079\n",
      "Validation: Epoch [12], Batch [356/938], Loss: 0.7222771048545837\n",
      "Validation: Epoch [12], Batch [357/938], Loss: 0.6487282514572144\n",
      "Validation: Epoch [12], Batch [358/938], Loss: 0.8729562759399414\n",
      "Validation: Epoch [12], Batch [359/938], Loss: 0.4835376739501953\n",
      "Validation: Epoch [12], Batch [360/938], Loss: 0.7353471517562866\n",
      "Validation: Epoch [12], Batch [361/938], Loss: 0.6746223568916321\n",
      "Validation: Epoch [12], Batch [362/938], Loss: 0.7798447608947754\n",
      "Validation: Epoch [12], Batch [363/938], Loss: 0.6926999092102051\n",
      "Validation: Epoch [12], Batch [364/938], Loss: 0.804157018661499\n",
      "Validation: Epoch [12], Batch [365/938], Loss: 0.6466203331947327\n",
      "Validation: Epoch [12], Batch [366/938], Loss: 0.6577484011650085\n",
      "Validation: Epoch [12], Batch [367/938], Loss: 0.7030253410339355\n",
      "Validation: Epoch [12], Batch [368/938], Loss: 0.7075074911117554\n",
      "Validation: Epoch [12], Batch [369/938], Loss: 0.6151418685913086\n",
      "Validation: Epoch [12], Batch [370/938], Loss: 0.6991520524024963\n",
      "Validation: Epoch [12], Batch [371/938], Loss: 0.8610415458679199\n",
      "Validation: Epoch [12], Batch [372/938], Loss: 0.938527524471283\n",
      "Validation: Epoch [12], Batch [373/938], Loss: 0.7499544620513916\n",
      "Validation: Epoch [12], Batch [374/938], Loss: 0.6755825281143188\n",
      "Validation: Epoch [12], Batch [375/938], Loss: 0.7930768132209778\n",
      "Validation: Epoch [12], Batch [376/938], Loss: 0.7316051125526428\n",
      "Validation: Epoch [12], Batch [377/938], Loss: 0.617733895778656\n",
      "Validation: Epoch [12], Batch [378/938], Loss: 0.6495324373245239\n",
      "Validation: Epoch [12], Batch [379/938], Loss: 0.5664801001548767\n",
      "Validation: Epoch [12], Batch [380/938], Loss: 0.7862930297851562\n",
      "Validation: Epoch [12], Batch [381/938], Loss: 0.730269730091095\n",
      "Validation: Epoch [12], Batch [382/938], Loss: 0.691910982131958\n",
      "Validation: Epoch [12], Batch [383/938], Loss: 0.7139536142349243\n",
      "Validation: Epoch [12], Batch [384/938], Loss: 0.6673463582992554\n",
      "Validation: Epoch [12], Batch [385/938], Loss: 0.7304787635803223\n",
      "Validation: Epoch [12], Batch [386/938], Loss: 0.7265186905860901\n",
      "Validation: Epoch [12], Batch [387/938], Loss: 0.707412600517273\n",
      "Validation: Epoch [12], Batch [388/938], Loss: 0.8237965106964111\n",
      "Validation: Epoch [12], Batch [389/938], Loss: 0.7685458064079285\n",
      "Validation: Epoch [12], Batch [390/938], Loss: 0.5291719436645508\n",
      "Validation: Epoch [12], Batch [391/938], Loss: 0.7218711376190186\n",
      "Validation: Epoch [12], Batch [392/938], Loss: 0.8753665685653687\n",
      "Validation: Epoch [12], Batch [393/938], Loss: 0.7886922955513\n",
      "Validation: Epoch [12], Batch [394/938], Loss: 0.6451918482780457\n",
      "Validation: Epoch [12], Batch [395/938], Loss: 1.1312319040298462\n",
      "Validation: Epoch [12], Batch [396/938], Loss: 0.6546612977981567\n",
      "Validation: Epoch [12], Batch [397/938], Loss: 0.9059220552444458\n",
      "Validation: Epoch [12], Batch [398/938], Loss: 0.718715488910675\n",
      "Validation: Epoch [12], Batch [399/938], Loss: 0.8058744668960571\n",
      "Validation: Epoch [12], Batch [400/938], Loss: 0.5717494487762451\n",
      "Validation: Epoch [12], Batch [401/938], Loss: 0.7164096236228943\n",
      "Validation: Epoch [12], Batch [402/938], Loss: 0.7192199230194092\n",
      "Validation: Epoch [12], Batch [403/938], Loss: 0.9308330416679382\n",
      "Validation: Epoch [12], Batch [404/938], Loss: 0.49272945523262024\n",
      "Validation: Epoch [12], Batch [405/938], Loss: 0.6546428799629211\n",
      "Validation: Epoch [12], Batch [406/938], Loss: 0.4885578751564026\n",
      "Validation: Epoch [12], Batch [407/938], Loss: 0.9473984241485596\n",
      "Validation: Epoch [12], Batch [408/938], Loss: 0.6653403043746948\n",
      "Validation: Epoch [12], Batch [409/938], Loss: 0.7213727235794067\n",
      "Validation: Epoch [12], Batch [410/938], Loss: 0.6710178852081299\n",
      "Validation: Epoch [12], Batch [411/938], Loss: 0.7930809259414673\n",
      "Validation: Epoch [12], Batch [412/938], Loss: 0.6529174447059631\n",
      "Validation: Epoch [12], Batch [413/938], Loss: 0.7612987160682678\n",
      "Validation: Epoch [12], Batch [414/938], Loss: 0.8486465811729431\n",
      "Validation: Epoch [12], Batch [415/938], Loss: 0.5257740616798401\n",
      "Validation: Epoch [12], Batch [416/938], Loss: 0.6922047138214111\n",
      "Validation: Epoch [12], Batch [417/938], Loss: 0.7782224416732788\n",
      "Validation: Epoch [12], Batch [418/938], Loss: 0.7058817148208618\n",
      "Validation: Epoch [12], Batch [419/938], Loss: 0.6790816187858582\n",
      "Validation: Epoch [12], Batch [420/938], Loss: 0.7334977388381958\n",
      "Validation: Epoch [12], Batch [421/938], Loss: 0.632478654384613\n",
      "Validation: Epoch [12], Batch [422/938], Loss: 0.8771344423294067\n",
      "Validation: Epoch [12], Batch [423/938], Loss: 0.6896442174911499\n",
      "Validation: Epoch [12], Batch [424/938], Loss: 0.7606936693191528\n",
      "Validation: Epoch [12], Batch [425/938], Loss: 0.6011257767677307\n",
      "Validation: Epoch [12], Batch [426/938], Loss: 1.0759762525558472\n",
      "Validation: Epoch [12], Batch [427/938], Loss: 0.6434462070465088\n",
      "Validation: Epoch [12], Batch [428/938], Loss: 0.7883968353271484\n",
      "Validation: Epoch [12], Batch [429/938], Loss: 0.4521138370037079\n",
      "Validation: Epoch [12], Batch [430/938], Loss: 0.7168588042259216\n",
      "Validation: Epoch [12], Batch [431/938], Loss: 0.5973350405693054\n",
      "Validation: Epoch [12], Batch [432/938], Loss: 0.5954098701477051\n",
      "Validation: Epoch [12], Batch [433/938], Loss: 0.8508714437484741\n",
      "Validation: Epoch [12], Batch [434/938], Loss: 0.7456253170967102\n",
      "Validation: Epoch [12], Batch [435/938], Loss: 0.6758958101272583\n",
      "Validation: Epoch [12], Batch [436/938], Loss: 0.4908689260482788\n",
      "Validation: Epoch [12], Batch [437/938], Loss: 0.7317395210266113\n",
      "Validation: Epoch [12], Batch [438/938], Loss: 0.7400521039962769\n",
      "Validation: Epoch [12], Batch [439/938], Loss: 0.703764021396637\n",
      "Validation: Epoch [12], Batch [440/938], Loss: 0.5207774639129639\n",
      "Validation: Epoch [12], Batch [441/938], Loss: 0.9093285799026489\n",
      "Validation: Epoch [12], Batch [442/938], Loss: 0.830444872379303\n",
      "Validation: Epoch [12], Batch [443/938], Loss: 0.8364408016204834\n",
      "Validation: Epoch [12], Batch [444/938], Loss: 0.608795166015625\n",
      "Validation: Epoch [12], Batch [445/938], Loss: 0.7437893748283386\n",
      "Validation: Epoch [12], Batch [446/938], Loss: 0.7970988154411316\n",
      "Validation: Epoch [12], Batch [447/938], Loss: 0.7580324411392212\n",
      "Validation: Epoch [12], Batch [448/938], Loss: 0.49587833881378174\n",
      "Validation: Epoch [12], Batch [449/938], Loss: 0.782214343547821\n",
      "Validation: Epoch [12], Batch [450/938], Loss: 0.7275338172912598\n",
      "Validation: Epoch [12], Batch [451/938], Loss: 0.7573069930076599\n",
      "Validation: Epoch [12], Batch [452/938], Loss: 0.7474493384361267\n",
      "Validation: Epoch [12], Batch [453/938], Loss: 0.6593620181083679\n",
      "Validation: Epoch [12], Batch [454/938], Loss: 0.6318366527557373\n",
      "Validation: Epoch [12], Batch [455/938], Loss: 0.921561062335968\n",
      "Validation: Epoch [12], Batch [456/938], Loss: 0.5617542266845703\n",
      "Validation: Epoch [12], Batch [457/938], Loss: 0.7381918430328369\n",
      "Validation: Epoch [12], Batch [458/938], Loss: 0.6585148572921753\n",
      "Validation: Epoch [12], Batch [459/938], Loss: 0.6404426097869873\n",
      "Validation: Epoch [12], Batch [460/938], Loss: 0.8813230991363525\n",
      "Validation: Epoch [12], Batch [461/938], Loss: 0.5987616181373596\n",
      "Validation: Epoch [12], Batch [462/938], Loss: 0.7065073251724243\n",
      "Validation: Epoch [12], Batch [463/938], Loss: 0.5914342403411865\n",
      "Validation: Epoch [12], Batch [464/938], Loss: 0.6203879117965698\n",
      "Validation: Epoch [12], Batch [465/938], Loss: 1.0105721950531006\n",
      "Validation: Epoch [12], Batch [466/938], Loss: 0.5356173515319824\n",
      "Validation: Epoch [12], Batch [467/938], Loss: 0.5959769487380981\n",
      "Validation: Epoch [12], Batch [468/938], Loss: 0.9152787327766418\n",
      "Validation: Epoch [12], Batch [469/938], Loss: 0.549899160861969\n",
      "Validation: Epoch [12], Batch [470/938], Loss: 0.6747139692306519\n",
      "Validation: Epoch [12], Batch [471/938], Loss: 0.8109641671180725\n",
      "Validation: Epoch [12], Batch [472/938], Loss: 0.6109579801559448\n",
      "Validation: Epoch [12], Batch [473/938], Loss: 0.5245438814163208\n",
      "Validation: Epoch [12], Batch [474/938], Loss: 0.7458456754684448\n",
      "Validation: Epoch [12], Batch [475/938], Loss: 0.5238162279129028\n",
      "Validation: Epoch [12], Batch [476/938], Loss: 0.6785035133361816\n",
      "Validation: Epoch [12], Batch [477/938], Loss: 0.879822313785553\n",
      "Validation: Epoch [12], Batch [478/938], Loss: 0.7401648759841919\n",
      "Validation: Epoch [12], Batch [479/938], Loss: 0.8653051853179932\n",
      "Validation: Epoch [12], Batch [480/938], Loss: 0.6803339719772339\n",
      "Validation: Epoch [12], Batch [481/938], Loss: 0.9803115129470825\n",
      "Validation: Epoch [12], Batch [482/938], Loss: 0.5250757336616516\n",
      "Validation: Epoch [12], Batch [483/938], Loss: 0.8129665851593018\n",
      "Validation: Epoch [12], Batch [484/938], Loss: 0.6264573335647583\n",
      "Validation: Epoch [12], Batch [485/938], Loss: 0.6925203204154968\n",
      "Validation: Epoch [12], Batch [486/938], Loss: 0.4518132209777832\n",
      "Validation: Epoch [12], Batch [487/938], Loss: 0.513838529586792\n",
      "Validation: Epoch [12], Batch [488/938], Loss: 0.566489577293396\n",
      "Validation: Epoch [12], Batch [489/938], Loss: 0.5338637232780457\n",
      "Validation: Epoch [12], Batch [490/938], Loss: 0.9132179021835327\n",
      "Validation: Epoch [12], Batch [491/938], Loss: 0.6956086158752441\n",
      "Validation: Epoch [12], Batch [492/938], Loss: 0.5853026509284973\n",
      "Validation: Epoch [12], Batch [493/938], Loss: 0.577144205570221\n",
      "Validation: Epoch [12], Batch [494/938], Loss: 0.6270577311515808\n",
      "Validation: Epoch [12], Batch [495/938], Loss: 0.5537390112876892\n",
      "Validation: Epoch [12], Batch [496/938], Loss: 0.7570571303367615\n",
      "Validation: Epoch [12], Batch [497/938], Loss: 0.7554893493652344\n",
      "Validation: Epoch [12], Batch [498/938], Loss: 0.8909639120101929\n",
      "Validation: Epoch [12], Batch [499/938], Loss: 0.8038768768310547\n",
      "Validation: Epoch [12], Batch [500/938], Loss: 0.8576890826225281\n",
      "Validation: Epoch [12], Batch [501/938], Loss: 0.6339046955108643\n",
      "Validation: Epoch [12], Batch [502/938], Loss: 0.7470744252204895\n",
      "Validation: Epoch [12], Batch [503/938], Loss: 0.7051712870597839\n",
      "Validation: Epoch [12], Batch [504/938], Loss: 0.6229247450828552\n",
      "Validation: Epoch [12], Batch [505/938], Loss: 0.8886300325393677\n",
      "Validation: Epoch [12], Batch [506/938], Loss: 1.0056333541870117\n",
      "Validation: Epoch [12], Batch [507/938], Loss: 0.7321909070014954\n",
      "Validation: Epoch [12], Batch [508/938], Loss: 0.7573724985122681\n",
      "Validation: Epoch [12], Batch [509/938], Loss: 0.8079805970191956\n",
      "Validation: Epoch [12], Batch [510/938], Loss: 0.6738960146903992\n",
      "Validation: Epoch [12], Batch [511/938], Loss: 0.5399060249328613\n",
      "Validation: Epoch [12], Batch [512/938], Loss: 0.5722187161445618\n",
      "Validation: Epoch [12], Batch [513/938], Loss: 0.5894902944564819\n",
      "Validation: Epoch [12], Batch [514/938], Loss: 0.7852070331573486\n",
      "Validation: Epoch [12], Batch [515/938], Loss: 0.5658840537071228\n",
      "Validation: Epoch [12], Batch [516/938], Loss: 0.7050788998603821\n",
      "Validation: Epoch [12], Batch [517/938], Loss: 0.6677746176719666\n",
      "Validation: Epoch [12], Batch [518/938], Loss: 0.4477968215942383\n",
      "Validation: Epoch [12], Batch [519/938], Loss: 0.832330048084259\n",
      "Validation: Epoch [12], Batch [520/938], Loss: 0.6777746081352234\n",
      "Validation: Epoch [12], Batch [521/938], Loss: 0.7438289523124695\n",
      "Validation: Epoch [12], Batch [522/938], Loss: 0.7209526300430298\n",
      "Validation: Epoch [12], Batch [523/938], Loss: 0.6993350982666016\n",
      "Validation: Epoch [12], Batch [524/938], Loss: 0.8156700134277344\n",
      "Validation: Epoch [12], Batch [525/938], Loss: 0.7399409413337708\n",
      "Validation: Epoch [12], Batch [526/938], Loss: 0.7355257272720337\n",
      "Validation: Epoch [12], Batch [527/938], Loss: 0.8758753538131714\n",
      "Validation: Epoch [12], Batch [528/938], Loss: 0.6830629706382751\n",
      "Validation: Epoch [12], Batch [529/938], Loss: 0.7007480263710022\n",
      "Validation: Epoch [12], Batch [530/938], Loss: 0.6674577593803406\n",
      "Validation: Epoch [12], Batch [531/938], Loss: 0.877058744430542\n",
      "Validation: Epoch [12], Batch [532/938], Loss: 0.6552437543869019\n",
      "Validation: Epoch [12], Batch [533/938], Loss: 0.8189218044281006\n",
      "Validation: Epoch [12], Batch [534/938], Loss: 0.640060544013977\n",
      "Validation: Epoch [12], Batch [535/938], Loss: 0.704519510269165\n",
      "Validation: Epoch [12], Batch [536/938], Loss: 0.7480462789535522\n",
      "Validation: Epoch [12], Batch [537/938], Loss: 0.6832386255264282\n",
      "Validation: Epoch [12], Batch [538/938], Loss: 0.7901331186294556\n",
      "Validation: Epoch [12], Batch [539/938], Loss: 0.9077102541923523\n",
      "Validation: Epoch [12], Batch [540/938], Loss: 0.7809280753135681\n",
      "Validation: Epoch [12], Batch [541/938], Loss: 0.9866014719009399\n",
      "Validation: Epoch [12], Batch [542/938], Loss: 0.4885232448577881\n",
      "Validation: Epoch [12], Batch [543/938], Loss: 0.5397570729255676\n",
      "Validation: Epoch [12], Batch [544/938], Loss: 0.7648537158966064\n",
      "Validation: Epoch [12], Batch [545/938], Loss: 0.9267551898956299\n",
      "Validation: Epoch [12], Batch [546/938], Loss: 0.589105486869812\n",
      "Validation: Epoch [12], Batch [547/938], Loss: 0.782069981098175\n",
      "Validation: Epoch [12], Batch [548/938], Loss: 0.6784073710441589\n",
      "Validation: Epoch [12], Batch [549/938], Loss: 0.7529665231704712\n",
      "Validation: Epoch [12], Batch [550/938], Loss: 0.9028054475784302\n",
      "Validation: Epoch [12], Batch [551/938], Loss: 0.6387665271759033\n",
      "Validation: Epoch [12], Batch [552/938], Loss: 0.4726925492286682\n",
      "Validation: Epoch [12], Batch [553/938], Loss: 0.9789524674415588\n",
      "Validation: Epoch [12], Batch [554/938], Loss: 0.7600241899490356\n",
      "Validation: Epoch [12], Batch [555/938], Loss: 0.9254768490791321\n",
      "Validation: Epoch [12], Batch [556/938], Loss: 0.7021864652633667\n",
      "Validation: Epoch [12], Batch [557/938], Loss: 0.6075329184532166\n",
      "Validation: Epoch [12], Batch [558/938], Loss: 0.9495612382888794\n",
      "Validation: Epoch [12], Batch [559/938], Loss: 1.0020076036453247\n",
      "Validation: Epoch [12], Batch [560/938], Loss: 0.7007885575294495\n",
      "Validation: Epoch [12], Batch [561/938], Loss: 0.5667676329612732\n",
      "Validation: Epoch [12], Batch [562/938], Loss: 0.7981054186820984\n",
      "Validation: Epoch [12], Batch [563/938], Loss: 0.6589587926864624\n",
      "Validation: Epoch [12], Batch [564/938], Loss: 0.8090834617614746\n",
      "Validation: Epoch [12], Batch [565/938], Loss: 0.5208898186683655\n",
      "Validation: Epoch [12], Batch [566/938], Loss: 0.7711102962493896\n",
      "Validation: Epoch [12], Batch [567/938], Loss: 0.6949743628501892\n",
      "Validation: Epoch [12], Batch [568/938], Loss: 0.4552515149116516\n",
      "Validation: Epoch [12], Batch [569/938], Loss: 0.697555422782898\n",
      "Validation: Epoch [12], Batch [570/938], Loss: 0.6902856826782227\n",
      "Validation: Epoch [12], Batch [571/938], Loss: 0.5347893834114075\n",
      "Validation: Epoch [12], Batch [572/938], Loss: 0.563518226146698\n",
      "Validation: Epoch [12], Batch [573/938], Loss: 0.7297344207763672\n",
      "Validation: Epoch [12], Batch [574/938], Loss: 0.9202824831008911\n",
      "Validation: Epoch [12], Batch [575/938], Loss: 0.8457752466201782\n",
      "Validation: Epoch [12], Batch [576/938], Loss: 0.6297298073768616\n",
      "Validation: Epoch [12], Batch [577/938], Loss: 0.6314513683319092\n",
      "Validation: Epoch [12], Batch [578/938], Loss: 0.7467681765556335\n",
      "Validation: Epoch [12], Batch [579/938], Loss: 0.6889239549636841\n",
      "Validation: Epoch [12], Batch [580/938], Loss: 0.6808545589447021\n",
      "Validation: Epoch [12], Batch [581/938], Loss: 0.7695122957229614\n",
      "Validation: Epoch [12], Batch [582/938], Loss: 0.7734191417694092\n",
      "Validation: Epoch [12], Batch [583/938], Loss: 0.7752573490142822\n",
      "Validation: Epoch [12], Batch [584/938], Loss: 0.8741075992584229\n",
      "Validation: Epoch [12], Batch [585/938], Loss: 0.6924020051956177\n",
      "Validation: Epoch [12], Batch [586/938], Loss: 0.6354833841323853\n",
      "Validation: Epoch [12], Batch [587/938], Loss: 0.7106227278709412\n",
      "Validation: Epoch [12], Batch [588/938], Loss: 0.7281520962715149\n",
      "Validation: Epoch [12], Batch [589/938], Loss: 0.726658821105957\n",
      "Validation: Epoch [12], Batch [590/938], Loss: 0.7295374870300293\n",
      "Validation: Epoch [12], Batch [591/938], Loss: 0.729910671710968\n",
      "Validation: Epoch [12], Batch [592/938], Loss: 0.672958493232727\n",
      "Validation: Epoch [12], Batch [593/938], Loss: 0.8974207043647766\n",
      "Validation: Epoch [12], Batch [594/938], Loss: 0.7565572261810303\n",
      "Validation: Epoch [12], Batch [595/938], Loss: 0.550887405872345\n",
      "Validation: Epoch [12], Batch [596/938], Loss: 0.8653683662414551\n",
      "Validation: Epoch [12], Batch [597/938], Loss: 0.7083338499069214\n",
      "Validation: Epoch [12], Batch [598/938], Loss: 0.8329408764839172\n",
      "Validation: Epoch [12], Batch [599/938], Loss: 0.674186110496521\n",
      "Validation: Epoch [12], Batch [600/938], Loss: 0.5176689028739929\n",
      "Validation: Epoch [12], Batch [601/938], Loss: 0.6995518207550049\n",
      "Validation: Epoch [12], Batch [602/938], Loss: 0.8694173097610474\n",
      "Validation: Epoch [12], Batch [603/938], Loss: 0.6921283602714539\n",
      "Validation: Epoch [12], Batch [604/938], Loss: 0.9531628489494324\n",
      "Validation: Epoch [12], Batch [605/938], Loss: 0.6912901401519775\n",
      "Validation: Epoch [12], Batch [606/938], Loss: 0.7254796624183655\n",
      "Validation: Epoch [12], Batch [607/938], Loss: 0.7597907781600952\n",
      "Validation: Epoch [12], Batch [608/938], Loss: 0.7544941306114197\n",
      "Validation: Epoch [12], Batch [609/938], Loss: 0.6261230111122131\n",
      "Validation: Epoch [12], Batch [610/938], Loss: 0.7482331395149231\n",
      "Validation: Epoch [12], Batch [611/938], Loss: 0.577620267868042\n",
      "Validation: Epoch [12], Batch [612/938], Loss: 0.982809841632843\n",
      "Validation: Epoch [12], Batch [613/938], Loss: 0.6315796375274658\n",
      "Validation: Epoch [12], Batch [614/938], Loss: 0.4452318251132965\n",
      "Validation: Epoch [12], Batch [615/938], Loss: 0.8554267883300781\n",
      "Validation: Epoch [12], Batch [616/938], Loss: 0.7400640845298767\n",
      "Validation: Epoch [12], Batch [617/938], Loss: 0.8125828504562378\n",
      "Validation: Epoch [12], Batch [618/938], Loss: 0.6267471313476562\n",
      "Validation: Epoch [12], Batch [619/938], Loss: 0.9738587737083435\n",
      "Validation: Epoch [12], Batch [620/938], Loss: 0.8032913208007812\n",
      "Validation: Epoch [12], Batch [621/938], Loss: 0.6102535128593445\n",
      "Validation: Epoch [12], Batch [622/938], Loss: 0.5801644325256348\n",
      "Validation: Epoch [12], Batch [623/938], Loss: 0.8148624300956726\n",
      "Validation: Epoch [12], Batch [624/938], Loss: 0.8815340995788574\n",
      "Validation: Epoch [12], Batch [625/938], Loss: 0.557030975818634\n",
      "Validation: Epoch [12], Batch [626/938], Loss: 0.6215887665748596\n",
      "Validation: Epoch [12], Batch [627/938], Loss: 0.7751342058181763\n",
      "Validation: Epoch [12], Batch [628/938], Loss: 0.6906105279922485\n",
      "Validation: Epoch [12], Batch [629/938], Loss: 0.610035240650177\n",
      "Validation: Epoch [12], Batch [630/938], Loss: 0.5560228824615479\n",
      "Validation: Epoch [12], Batch [631/938], Loss: 0.5426501631736755\n",
      "Validation: Epoch [12], Batch [632/938], Loss: 0.7383795976638794\n",
      "Validation: Epoch [12], Batch [633/938], Loss: 0.7946250438690186\n",
      "Validation: Epoch [12], Batch [634/938], Loss: 0.909339189529419\n",
      "Validation: Epoch [12], Batch [635/938], Loss: 0.670593798160553\n",
      "Validation: Epoch [12], Batch [636/938], Loss: 0.6508944034576416\n",
      "Validation: Epoch [12], Batch [637/938], Loss: 0.7830978631973267\n",
      "Validation: Epoch [12], Batch [638/938], Loss: 0.9044164419174194\n",
      "Validation: Epoch [12], Batch [639/938], Loss: 0.8908059000968933\n",
      "Validation: Epoch [12], Batch [640/938], Loss: 0.5292596220970154\n",
      "Validation: Epoch [12], Batch [641/938], Loss: 0.8241205811500549\n",
      "Validation: Epoch [12], Batch [642/938], Loss: 0.4862363636493683\n",
      "Validation: Epoch [12], Batch [643/938], Loss: 0.4429594576358795\n",
      "Validation: Epoch [12], Batch [644/938], Loss: 0.6039943695068359\n",
      "Validation: Epoch [12], Batch [645/938], Loss: 0.6324717998504639\n",
      "Validation: Epoch [12], Batch [646/938], Loss: 0.7517954111099243\n",
      "Validation: Epoch [12], Batch [647/938], Loss: 0.5527252554893494\n",
      "Validation: Epoch [12], Batch [648/938], Loss: 0.6530807614326477\n",
      "Validation: Epoch [12], Batch [649/938], Loss: 0.6664362549781799\n",
      "Validation: Epoch [12], Batch [650/938], Loss: 0.6789177656173706\n",
      "Validation: Epoch [12], Batch [651/938], Loss: 0.7811636328697205\n",
      "Validation: Epoch [12], Batch [652/938], Loss: 0.6864080429077148\n",
      "Validation: Epoch [12], Batch [653/938], Loss: 0.779058575630188\n",
      "Validation: Epoch [12], Batch [654/938], Loss: 0.5912389755249023\n",
      "Validation: Epoch [12], Batch [655/938], Loss: 0.6361410021781921\n",
      "Validation: Epoch [12], Batch [656/938], Loss: 0.7368814945220947\n",
      "Validation: Epoch [12], Batch [657/938], Loss: 0.5060970783233643\n",
      "Validation: Epoch [12], Batch [658/938], Loss: 0.5501240491867065\n",
      "Validation: Epoch [12], Batch [659/938], Loss: 0.7752248644828796\n",
      "Validation: Epoch [12], Batch [660/938], Loss: 0.7105945348739624\n",
      "Validation: Epoch [12], Batch [661/938], Loss: 0.7655412554740906\n",
      "Validation: Epoch [12], Batch [662/938], Loss: 0.6740112900733948\n",
      "Validation: Epoch [12], Batch [663/938], Loss: 0.5497899055480957\n",
      "Validation: Epoch [12], Batch [664/938], Loss: 0.7848950028419495\n",
      "Validation: Epoch [12], Batch [665/938], Loss: 0.6122446060180664\n",
      "Validation: Epoch [12], Batch [666/938], Loss: 0.6477252840995789\n",
      "Validation: Epoch [12], Batch [667/938], Loss: 0.5371736288070679\n",
      "Validation: Epoch [12], Batch [668/938], Loss: 0.8225992321968079\n",
      "Validation: Epoch [12], Batch [669/938], Loss: 0.5618237853050232\n",
      "Validation: Epoch [12], Batch [670/938], Loss: 0.6290653347969055\n",
      "Validation: Epoch [12], Batch [671/938], Loss: 0.6890335083007812\n",
      "Validation: Epoch [12], Batch [672/938], Loss: 0.7496642470359802\n",
      "Validation: Epoch [12], Batch [673/938], Loss: 0.5315176844596863\n",
      "Validation: Epoch [12], Batch [674/938], Loss: 0.7975758910179138\n",
      "Validation: Epoch [12], Batch [675/938], Loss: 0.6052258014678955\n",
      "Validation: Epoch [12], Batch [676/938], Loss: 0.7435521483421326\n",
      "Validation: Epoch [12], Batch [677/938], Loss: 0.7547978758811951\n",
      "Validation: Epoch [12], Batch [678/938], Loss: 0.9650949835777283\n",
      "Validation: Epoch [12], Batch [679/938], Loss: 0.7725291848182678\n",
      "Validation: Epoch [12], Batch [680/938], Loss: 0.8818590044975281\n",
      "Validation: Epoch [12], Batch [681/938], Loss: 0.7171478271484375\n",
      "Validation: Epoch [12], Batch [682/938], Loss: 0.653454065322876\n",
      "Validation: Epoch [12], Batch [683/938], Loss: 0.8226447701454163\n",
      "Validation: Epoch [12], Batch [684/938], Loss: 0.6765962839126587\n",
      "Validation: Epoch [12], Batch [685/938], Loss: 0.6561434864997864\n",
      "Validation: Epoch [12], Batch [686/938], Loss: 0.6057273149490356\n",
      "Validation: Epoch [12], Batch [687/938], Loss: 0.7736586928367615\n",
      "Validation: Epoch [12], Batch [688/938], Loss: 0.6321573257446289\n",
      "Validation: Epoch [12], Batch [689/938], Loss: 0.8818851709365845\n",
      "Validation: Epoch [12], Batch [690/938], Loss: 0.6148962378501892\n",
      "Validation: Epoch [12], Batch [691/938], Loss: 0.748485803604126\n",
      "Validation: Epoch [12], Batch [692/938], Loss: 0.7106509804725647\n",
      "Validation: Epoch [12], Batch [693/938], Loss: 0.6512230634689331\n",
      "Validation: Epoch [12], Batch [694/938], Loss: 0.8342344164848328\n",
      "Validation: Epoch [12], Batch [695/938], Loss: 0.6303883194923401\n",
      "Validation: Epoch [12], Batch [696/938], Loss: 0.5909792184829712\n",
      "Validation: Epoch [12], Batch [697/938], Loss: 0.7069822549819946\n",
      "Validation: Epoch [12], Batch [698/938], Loss: 0.5611138343811035\n",
      "Validation: Epoch [12], Batch [699/938], Loss: 0.8074201941490173\n",
      "Validation: Epoch [12], Batch [700/938], Loss: 0.6598243117332458\n",
      "Validation: Epoch [12], Batch [701/938], Loss: 0.8101533651351929\n",
      "Validation: Epoch [12], Batch [702/938], Loss: 0.6105486750602722\n",
      "Validation: Epoch [12], Batch [703/938], Loss: 0.6828840970993042\n",
      "Validation: Epoch [12], Batch [704/938], Loss: 0.8150646686553955\n",
      "Validation: Epoch [12], Batch [705/938], Loss: 0.7973921895027161\n",
      "Validation: Epoch [12], Batch [706/938], Loss: 0.4436369240283966\n",
      "Validation: Epoch [12], Batch [707/938], Loss: 0.750905454158783\n",
      "Validation: Epoch [12], Batch [708/938], Loss: 0.6408252716064453\n",
      "Validation: Epoch [12], Batch [709/938], Loss: 0.6182571649551392\n",
      "Validation: Epoch [12], Batch [710/938], Loss: 0.70740807056427\n",
      "Validation: Epoch [12], Batch [711/938], Loss: 0.7854843139648438\n",
      "Validation: Epoch [12], Batch [712/938], Loss: 0.6918262243270874\n",
      "Validation: Epoch [12], Batch [713/938], Loss: 0.6485716104507446\n",
      "Validation: Epoch [12], Batch [714/938], Loss: 0.6033915281295776\n",
      "Validation: Epoch [12], Batch [715/938], Loss: 0.8236023187637329\n",
      "Validation: Epoch [12], Batch [716/938], Loss: 0.842562735080719\n",
      "Validation: Epoch [12], Batch [717/938], Loss: 0.8084172010421753\n",
      "Validation: Epoch [12], Batch [718/938], Loss: 0.7849434614181519\n",
      "Validation: Epoch [12], Batch [719/938], Loss: 0.4541056454181671\n",
      "Validation: Epoch [12], Batch [720/938], Loss: 0.9206520915031433\n",
      "Validation: Epoch [12], Batch [721/938], Loss: 1.0114998817443848\n",
      "Validation: Epoch [12], Batch [722/938], Loss: 0.7605813145637512\n",
      "Validation: Epoch [12], Batch [723/938], Loss: 0.6206243634223938\n",
      "Validation: Epoch [12], Batch [724/938], Loss: 0.6567466259002686\n",
      "Validation: Epoch [12], Batch [725/938], Loss: 0.6286055445671082\n",
      "Validation: Epoch [12], Batch [726/938], Loss: 0.7455217242240906\n",
      "Validation: Epoch [12], Batch [727/938], Loss: 0.6333855986595154\n",
      "Validation: Epoch [12], Batch [728/938], Loss: 0.799217700958252\n",
      "Validation: Epoch [12], Batch [729/938], Loss: 0.7136358022689819\n",
      "Validation: Epoch [12], Batch [730/938], Loss: 0.6137184500694275\n",
      "Validation: Epoch [12], Batch [731/938], Loss: 0.7510833740234375\n",
      "Validation: Epoch [12], Batch [732/938], Loss: 0.7937856912612915\n",
      "Validation: Epoch [12], Batch [733/938], Loss: 0.5442177057266235\n",
      "Validation: Epoch [12], Batch [734/938], Loss: 0.750998854637146\n",
      "Validation: Epoch [12], Batch [735/938], Loss: 0.8184692859649658\n",
      "Validation: Epoch [12], Batch [736/938], Loss: 0.6312580704689026\n",
      "Validation: Epoch [12], Batch [737/938], Loss: 0.640198826789856\n",
      "Validation: Epoch [12], Batch [738/938], Loss: 0.6648597121238708\n",
      "Validation: Epoch [12], Batch [739/938], Loss: 0.710700511932373\n",
      "Validation: Epoch [12], Batch [740/938], Loss: 0.9391746520996094\n",
      "Validation: Epoch [12], Batch [741/938], Loss: 0.5532715320587158\n",
      "Validation: Epoch [12], Batch [742/938], Loss: 0.8466618657112122\n",
      "Validation: Epoch [12], Batch [743/938], Loss: 0.6047189831733704\n",
      "Validation: Epoch [12], Batch [744/938], Loss: 0.7767111659049988\n",
      "Validation: Epoch [12], Batch [745/938], Loss: 0.645214855670929\n",
      "Validation: Epoch [12], Batch [746/938], Loss: 0.820486843585968\n",
      "Validation: Epoch [12], Batch [747/938], Loss: 0.6140612959861755\n",
      "Validation: Epoch [12], Batch [748/938], Loss: 0.7560477256774902\n",
      "Validation: Epoch [12], Batch [749/938], Loss: 0.5771023035049438\n",
      "Validation: Epoch [12], Batch [750/938], Loss: 0.7433597445487976\n",
      "Validation: Epoch [12], Batch [751/938], Loss: 0.7892479300498962\n",
      "Validation: Epoch [12], Batch [752/938], Loss: 0.8152503371238708\n",
      "Validation: Epoch [12], Batch [753/938], Loss: 0.4476165175437927\n",
      "Validation: Epoch [12], Batch [754/938], Loss: 0.6095482110977173\n",
      "Validation: Epoch [12], Batch [755/938], Loss: 1.141303300857544\n",
      "Validation: Epoch [12], Batch [756/938], Loss: 0.6700493693351746\n",
      "Validation: Epoch [12], Batch [757/938], Loss: 0.658035159111023\n",
      "Validation: Epoch [12], Batch [758/938], Loss: 0.8130001425743103\n",
      "Validation: Epoch [12], Batch [759/938], Loss: 0.41634175181388855\n",
      "Validation: Epoch [12], Batch [760/938], Loss: 1.0168603658676147\n",
      "Validation: Epoch [12], Batch [761/938], Loss: 0.48045095801353455\n",
      "Validation: Epoch [12], Batch [762/938], Loss: 0.5978699326515198\n",
      "Validation: Epoch [12], Batch [763/938], Loss: 0.5324665307998657\n",
      "Validation: Epoch [12], Batch [764/938], Loss: 0.8231214284896851\n",
      "Validation: Epoch [12], Batch [765/938], Loss: 0.8813630938529968\n",
      "Validation: Epoch [12], Batch [766/938], Loss: 0.7839283347129822\n",
      "Validation: Epoch [12], Batch [767/938], Loss: 0.7004343271255493\n",
      "Validation: Epoch [12], Batch [768/938], Loss: 0.6662601232528687\n",
      "Validation: Epoch [12], Batch [769/938], Loss: 0.6141689419746399\n",
      "Validation: Epoch [12], Batch [770/938], Loss: 0.6944580078125\n",
      "Validation: Epoch [12], Batch [771/938], Loss: 0.5712105631828308\n",
      "Validation: Epoch [12], Batch [772/938], Loss: 0.732907772064209\n",
      "Validation: Epoch [12], Batch [773/938], Loss: 0.6419642567634583\n",
      "Validation: Epoch [12], Batch [774/938], Loss: 0.9585901498794556\n",
      "Validation: Epoch [12], Batch [775/938], Loss: 0.5851645469665527\n",
      "Validation: Epoch [12], Batch [776/938], Loss: 0.45071426033973694\n",
      "Validation: Epoch [12], Batch [777/938], Loss: 0.985065221786499\n",
      "Validation: Epoch [12], Batch [778/938], Loss: 0.7471950650215149\n",
      "Validation: Epoch [12], Batch [779/938], Loss: 0.5379133224487305\n",
      "Validation: Epoch [12], Batch [780/938], Loss: 0.686802089214325\n",
      "Validation: Epoch [12], Batch [781/938], Loss: 0.887718915939331\n",
      "Validation: Epoch [12], Batch [782/938], Loss: 0.7961030006408691\n",
      "Validation: Epoch [12], Batch [783/938], Loss: 0.626089334487915\n",
      "Validation: Epoch [12], Batch [784/938], Loss: 0.9707180261611938\n",
      "Validation: Epoch [12], Batch [785/938], Loss: 0.639726996421814\n",
      "Validation: Epoch [12], Batch [786/938], Loss: 0.7247722148895264\n",
      "Validation: Epoch [12], Batch [787/938], Loss: 0.8121389150619507\n",
      "Validation: Epoch [12], Batch [788/938], Loss: 0.7139155268669128\n",
      "Validation: Epoch [12], Batch [789/938], Loss: 0.8386432528495789\n",
      "Validation: Epoch [12], Batch [790/938], Loss: 0.833135187625885\n",
      "Validation: Epoch [12], Batch [791/938], Loss: 0.581397533416748\n",
      "Validation: Epoch [12], Batch [792/938], Loss: 0.6100571155548096\n",
      "Validation: Epoch [12], Batch [793/938], Loss: 0.983882486820221\n",
      "Validation: Epoch [12], Batch [794/938], Loss: 0.6491279006004333\n",
      "Validation: Epoch [12], Batch [795/938], Loss: 0.6775538325309753\n",
      "Validation: Epoch [12], Batch [796/938], Loss: 0.6499746441841125\n",
      "Validation: Epoch [12], Batch [797/938], Loss: 0.9775892496109009\n",
      "Validation: Epoch [12], Batch [798/938], Loss: 0.61075758934021\n",
      "Validation: Epoch [12], Batch [799/938], Loss: 0.6146554350852966\n",
      "Validation: Epoch [12], Batch [800/938], Loss: 0.7459951043128967\n",
      "Validation: Epoch [12], Batch [801/938], Loss: 0.662082314491272\n",
      "Validation: Epoch [12], Batch [802/938], Loss: 0.7803427577018738\n",
      "Validation: Epoch [12], Batch [803/938], Loss: 0.8492705225944519\n",
      "Validation: Epoch [12], Batch [804/938], Loss: 0.6294342875480652\n",
      "Validation: Epoch [12], Batch [805/938], Loss: 0.5849266052246094\n",
      "Validation: Epoch [12], Batch [806/938], Loss: 0.7108027338981628\n",
      "Validation: Epoch [12], Batch [807/938], Loss: 0.7434406876564026\n",
      "Validation: Epoch [12], Batch [808/938], Loss: 0.5821318030357361\n",
      "Validation: Epoch [12], Batch [809/938], Loss: 0.6824285984039307\n",
      "Validation: Epoch [12], Batch [810/938], Loss: 0.7855772972106934\n",
      "Validation: Epoch [12], Batch [811/938], Loss: 0.7436004281044006\n",
      "Validation: Epoch [12], Batch [812/938], Loss: 0.7167243957519531\n",
      "Validation: Epoch [12], Batch [813/938], Loss: 0.6823463439941406\n",
      "Validation: Epoch [12], Batch [814/938], Loss: 1.0104451179504395\n",
      "Validation: Epoch [12], Batch [815/938], Loss: 0.8065522909164429\n",
      "Validation: Epoch [12], Batch [816/938], Loss: 0.8591480255126953\n",
      "Validation: Epoch [12], Batch [817/938], Loss: 0.7683287858963013\n",
      "Validation: Epoch [12], Batch [818/938], Loss: 0.7424831390380859\n",
      "Validation: Epoch [12], Batch [819/938], Loss: 0.7288838028907776\n",
      "Validation: Epoch [12], Batch [820/938], Loss: 0.7726439237594604\n",
      "Validation: Epoch [12], Batch [821/938], Loss: 0.7918429374694824\n",
      "Validation: Epoch [12], Batch [822/938], Loss: 0.645915687084198\n",
      "Validation: Epoch [12], Batch [823/938], Loss: 0.6585203409194946\n",
      "Validation: Epoch [12], Batch [824/938], Loss: 0.7925894260406494\n",
      "Validation: Epoch [12], Batch [825/938], Loss: 0.6437808275222778\n",
      "Validation: Epoch [12], Batch [826/938], Loss: 0.8192415833473206\n",
      "Validation: Epoch [12], Batch [827/938], Loss: 0.5756614804267883\n",
      "Validation: Epoch [12], Batch [828/938], Loss: 0.5883467197418213\n",
      "Validation: Epoch [12], Batch [829/938], Loss: 0.45563870668411255\n",
      "Validation: Epoch [12], Batch [830/938], Loss: 1.028093934059143\n",
      "Validation: Epoch [12], Batch [831/938], Loss: 0.5752715468406677\n",
      "Validation: Epoch [12], Batch [832/938], Loss: 0.7633234858512878\n",
      "Validation: Epoch [12], Batch [833/938], Loss: 0.5523051023483276\n",
      "Validation: Epoch [12], Batch [834/938], Loss: 0.6444748640060425\n",
      "Validation: Epoch [12], Batch [835/938], Loss: 0.6773779392242432\n",
      "Validation: Epoch [12], Batch [836/938], Loss: 0.9236056208610535\n",
      "Validation: Epoch [12], Batch [837/938], Loss: 0.829268217086792\n",
      "Validation: Epoch [12], Batch [838/938], Loss: 0.7740699648857117\n",
      "Validation: Epoch [12], Batch [839/938], Loss: 0.6692333221435547\n",
      "Validation: Epoch [12], Batch [840/938], Loss: 0.6448712348937988\n",
      "Validation: Epoch [12], Batch [841/938], Loss: 0.6315011978149414\n",
      "Validation: Epoch [12], Batch [842/938], Loss: 0.7082192301750183\n",
      "Validation: Epoch [12], Batch [843/938], Loss: 0.7925350069999695\n",
      "Validation: Epoch [12], Batch [844/938], Loss: 0.7352245450019836\n",
      "Validation: Epoch [12], Batch [845/938], Loss: 0.7132426500320435\n",
      "Validation: Epoch [12], Batch [846/938], Loss: 0.8209903240203857\n",
      "Validation: Epoch [12], Batch [847/938], Loss: 0.7954521179199219\n",
      "Validation: Epoch [12], Batch [848/938], Loss: 0.744698166847229\n",
      "Validation: Epoch [12], Batch [849/938], Loss: 0.9536948800086975\n",
      "Validation: Epoch [12], Batch [850/938], Loss: 0.5449349880218506\n",
      "Validation: Epoch [12], Batch [851/938], Loss: 0.840903103351593\n",
      "Validation: Epoch [12], Batch [852/938], Loss: 0.7548506259918213\n",
      "Validation: Epoch [12], Batch [853/938], Loss: 0.8439918160438538\n",
      "Validation: Epoch [12], Batch [854/938], Loss: 0.6929960250854492\n",
      "Validation: Epoch [12], Batch [855/938], Loss: 0.7648663520812988\n",
      "Validation: Epoch [12], Batch [856/938], Loss: 0.7609983682632446\n",
      "Validation: Epoch [12], Batch [857/938], Loss: 0.9725102186203003\n",
      "Validation: Epoch [12], Batch [858/938], Loss: 0.7359862923622131\n",
      "Validation: Epoch [12], Batch [859/938], Loss: 1.0551261901855469\n",
      "Validation: Epoch [12], Batch [860/938], Loss: 0.8094266653060913\n",
      "Validation: Epoch [12], Batch [861/938], Loss: 0.6963799595832825\n",
      "Validation: Epoch [12], Batch [862/938], Loss: 0.742290198802948\n",
      "Validation: Epoch [12], Batch [863/938], Loss: 0.5991075038909912\n",
      "Validation: Epoch [12], Batch [864/938], Loss: 0.5781890153884888\n",
      "Validation: Epoch [12], Batch [865/938], Loss: 0.605294942855835\n",
      "Validation: Epoch [12], Batch [866/938], Loss: 0.7964381575584412\n",
      "Validation: Epoch [12], Batch [867/938], Loss: 0.5463687777519226\n",
      "Validation: Epoch [12], Batch [868/938], Loss: 0.6666017174720764\n",
      "Validation: Epoch [12], Batch [869/938], Loss: 0.7026838660240173\n",
      "Validation: Epoch [12], Batch [870/938], Loss: 0.7023258805274963\n",
      "Validation: Epoch [12], Batch [871/938], Loss: 0.9268584847450256\n",
      "Validation: Epoch [12], Batch [872/938], Loss: 0.8735185861587524\n",
      "Validation: Epoch [12], Batch [873/938], Loss: 0.6970776915550232\n",
      "Validation: Epoch [12], Batch [874/938], Loss: 0.772166907787323\n",
      "Validation: Epoch [12], Batch [875/938], Loss: 0.7630075216293335\n",
      "Validation: Epoch [12], Batch [876/938], Loss: 0.5573977828025818\n",
      "Validation: Epoch [12], Batch [877/938], Loss: 0.6182996034622192\n",
      "Validation: Epoch [12], Batch [878/938], Loss: 0.6098617911338806\n",
      "Validation: Epoch [12], Batch [879/938], Loss: 0.7017861604690552\n",
      "Validation: Epoch [12], Batch [880/938], Loss: 0.7056623697280884\n",
      "Validation: Epoch [12], Batch [881/938], Loss: 0.7424416542053223\n",
      "Validation: Epoch [12], Batch [882/938], Loss: 0.5808100700378418\n",
      "Validation: Epoch [12], Batch [883/938], Loss: 0.5088244676589966\n",
      "Validation: Epoch [12], Batch [884/938], Loss: 0.6509138345718384\n",
      "Validation: Epoch [12], Batch [885/938], Loss: 0.9361138343811035\n",
      "Validation: Epoch [12], Batch [886/938], Loss: 0.6624131202697754\n",
      "Validation: Epoch [12], Batch [887/938], Loss: 0.6339930295944214\n",
      "Validation: Epoch [12], Batch [888/938], Loss: 0.5638652443885803\n",
      "Validation: Epoch [12], Batch [889/938], Loss: 0.6789512634277344\n",
      "Validation: Epoch [12], Batch [890/938], Loss: 0.8489959836006165\n",
      "Validation: Epoch [12], Batch [891/938], Loss: 0.8229517936706543\n",
      "Validation: Epoch [12], Batch [892/938], Loss: 0.7258538603782654\n",
      "Validation: Epoch [12], Batch [893/938], Loss: 0.6825257539749146\n",
      "Validation: Epoch [12], Batch [894/938], Loss: 0.7326906323432922\n",
      "Validation: Epoch [12], Batch [895/938], Loss: 0.7588164210319519\n",
      "Validation: Epoch [12], Batch [896/938], Loss: 0.6711328625679016\n",
      "Validation: Epoch [12], Batch [897/938], Loss: 0.5230579972267151\n",
      "Validation: Epoch [12], Batch [898/938], Loss: 0.735808253288269\n",
      "Validation: Epoch [12], Batch [899/938], Loss: 0.5977704524993896\n",
      "Validation: Epoch [12], Batch [900/938], Loss: 0.8146771192550659\n",
      "Validation: Epoch [12], Batch [901/938], Loss: 0.7455974221229553\n",
      "Validation: Epoch [12], Batch [902/938], Loss: 0.5927461385726929\n",
      "Validation: Epoch [12], Batch [903/938], Loss: 0.7472996711730957\n",
      "Validation: Epoch [12], Batch [904/938], Loss: 0.9218060970306396\n",
      "Validation: Epoch [12], Batch [905/938], Loss: 0.6422856450080872\n",
      "Validation: Epoch [12], Batch [906/938], Loss: 0.7377747297286987\n",
      "Validation: Epoch [12], Batch [907/938], Loss: 0.731808602809906\n",
      "Validation: Epoch [12], Batch [908/938], Loss: 0.8507043123245239\n",
      "Validation: Epoch [12], Batch [909/938], Loss: 0.7747858166694641\n",
      "Validation: Epoch [12], Batch [910/938], Loss: 0.7131539583206177\n",
      "Validation: Epoch [12], Batch [911/938], Loss: 0.5953695178031921\n",
      "Validation: Epoch [12], Batch [912/938], Loss: 0.988478422164917\n",
      "Validation: Epoch [12], Batch [913/938], Loss: 0.5532646179199219\n",
      "Validation: Epoch [12], Batch [914/938], Loss: 0.6797443628311157\n",
      "Validation: Epoch [12], Batch [915/938], Loss: 0.7656043767929077\n",
      "Validation: Epoch [12], Batch [916/938], Loss: 0.49794822931289673\n",
      "Validation: Epoch [12], Batch [917/938], Loss: 0.5581696033477783\n",
      "Validation: Epoch [12], Batch [918/938], Loss: 0.6226093769073486\n",
      "Validation: Epoch [12], Batch [919/938], Loss: 0.7516375780105591\n",
      "Validation: Epoch [12], Batch [920/938], Loss: 0.4339000880718231\n",
      "Validation: Epoch [12], Batch [921/938], Loss: 0.7152557373046875\n",
      "Validation: Epoch [12], Batch [922/938], Loss: 0.6159546375274658\n",
      "Validation: Epoch [12], Batch [923/938], Loss: 0.8684772253036499\n",
      "Validation: Epoch [12], Batch [924/938], Loss: 0.7664691209793091\n",
      "Validation: Epoch [12], Batch [925/938], Loss: 0.9341756701469421\n",
      "Validation: Epoch [12], Batch [926/938], Loss: 0.7001314163208008\n",
      "Validation: Epoch [12], Batch [927/938], Loss: 0.8736054301261902\n",
      "Validation: Epoch [12], Batch [928/938], Loss: 0.953038215637207\n",
      "Validation: Epoch [12], Batch [929/938], Loss: 1.0535414218902588\n",
      "Validation: Epoch [12], Batch [930/938], Loss: 0.8301860094070435\n",
      "Validation: Epoch [12], Batch [931/938], Loss: 0.8731870055198669\n",
      "Validation: Epoch [12], Batch [932/938], Loss: 0.6664021015167236\n",
      "Validation: Epoch [12], Batch [933/938], Loss: 0.8323746919631958\n",
      "Validation: Epoch [12], Batch [934/938], Loss: 0.6458114981651306\n",
      "Validation: Epoch [12], Batch [935/938], Loss: 0.6649454832077026\n",
      "Validation: Epoch [12], Batch [936/938], Loss: 0.7511088252067566\n",
      "Validation: Epoch [12], Batch [937/938], Loss: 0.6774344444274902\n",
      "Validation: Epoch [12], Batch [938/938], Loss: 0.6669945120811462\n",
      "Accuracy of test set: 0.7821\n",
      "Train: Epoch [13], Batch [1/938], Loss: 0.5818424224853516\n",
      "Train: Epoch [13], Batch [2/938], Loss: 0.6616582274436951\n",
      "Train: Epoch [13], Batch [3/938], Loss: 0.5338504910469055\n",
      "Train: Epoch [13], Batch [4/938], Loss: 0.7668524384498596\n",
      "Train: Epoch [13], Batch [5/938], Loss: 0.9728772640228271\n",
      "Train: Epoch [13], Batch [6/938], Loss: 0.6177476048469543\n",
      "Train: Epoch [13], Batch [7/938], Loss: 0.820954442024231\n",
      "Train: Epoch [13], Batch [8/938], Loss: 0.8576325178146362\n",
      "Train: Epoch [13], Batch [9/938], Loss: 0.8493796586990356\n",
      "Train: Epoch [13], Batch [10/938], Loss: 0.6176283955574036\n",
      "Train: Epoch [13], Batch [11/938], Loss: 0.7677929997444153\n",
      "Train: Epoch [13], Batch [12/938], Loss: 1.0671796798706055\n",
      "Train: Epoch [13], Batch [13/938], Loss: 0.6616966724395752\n",
      "Train: Epoch [13], Batch [14/938], Loss: 0.8897362947463989\n",
      "Train: Epoch [13], Batch [15/938], Loss: 0.8372798562049866\n",
      "Train: Epoch [13], Batch [16/938], Loss: 0.8204393982887268\n",
      "Train: Epoch [13], Batch [17/938], Loss: 0.8775016069412231\n",
      "Train: Epoch [13], Batch [18/938], Loss: 1.070298433303833\n",
      "Train: Epoch [13], Batch [19/938], Loss: 0.9644678235054016\n",
      "Train: Epoch [13], Batch [20/938], Loss: 0.5477270483970642\n",
      "Train: Epoch [13], Batch [21/938], Loss: 0.5106920003890991\n",
      "Train: Epoch [13], Batch [22/938], Loss: 0.627862811088562\n",
      "Train: Epoch [13], Batch [23/938], Loss: 0.6073054075241089\n",
      "Train: Epoch [13], Batch [24/938], Loss: 0.7530136108398438\n",
      "Train: Epoch [13], Batch [25/938], Loss: 0.5374405384063721\n",
      "Train: Epoch [13], Batch [26/938], Loss: 0.7446456551551819\n",
      "Train: Epoch [13], Batch [27/938], Loss: 0.8313456773757935\n",
      "Train: Epoch [13], Batch [28/938], Loss: 0.8661596179008484\n",
      "Train: Epoch [13], Batch [29/938], Loss: 0.66559237241745\n",
      "Train: Epoch [13], Batch [30/938], Loss: 0.794671356678009\n",
      "Train: Epoch [13], Batch [31/938], Loss: 0.48614028096199036\n",
      "Train: Epoch [13], Batch [32/938], Loss: 0.5845387578010559\n",
      "Train: Epoch [13], Batch [33/938], Loss: 0.7934752702713013\n",
      "Train: Epoch [13], Batch [34/938], Loss: 0.6989064812660217\n",
      "Train: Epoch [13], Batch [35/938], Loss: 0.7789580821990967\n",
      "Train: Epoch [13], Batch [36/938], Loss: 0.6559221744537354\n",
      "Train: Epoch [13], Batch [37/938], Loss: 0.7404710054397583\n",
      "Train: Epoch [13], Batch [38/938], Loss: 0.5765191316604614\n",
      "Train: Epoch [13], Batch [39/938], Loss: 0.5873289704322815\n",
      "Train: Epoch [13], Batch [40/938], Loss: 0.8399440050125122\n",
      "Train: Epoch [13], Batch [41/938], Loss: 0.6263920068740845\n",
      "Train: Epoch [13], Batch [42/938], Loss: 0.6789437532424927\n",
      "Train: Epoch [13], Batch [43/938], Loss: 0.45348095893859863\n",
      "Train: Epoch [13], Batch [44/938], Loss: 0.7696362733840942\n",
      "Train: Epoch [13], Batch [45/938], Loss: 0.9231750965118408\n",
      "Train: Epoch [13], Batch [46/938], Loss: 0.9055989384651184\n",
      "Train: Epoch [13], Batch [47/938], Loss: 0.5961472392082214\n",
      "Train: Epoch [13], Batch [48/938], Loss: 0.9213337898254395\n",
      "Train: Epoch [13], Batch [49/938], Loss: 0.8381218314170837\n",
      "Train: Epoch [13], Batch [50/938], Loss: 0.7547762393951416\n",
      "Train: Epoch [13], Batch [51/938], Loss: 0.8296388387680054\n",
      "Train: Epoch [13], Batch [52/938], Loss: 0.6611552834510803\n",
      "Train: Epoch [13], Batch [53/938], Loss: 0.8705214858055115\n",
      "Train: Epoch [13], Batch [54/938], Loss: 0.5592129826545715\n",
      "Train: Epoch [13], Batch [55/938], Loss: 0.7565965056419373\n",
      "Train: Epoch [13], Batch [56/938], Loss: 0.6599319577217102\n",
      "Train: Epoch [13], Batch [57/938], Loss: 0.7213518619537354\n",
      "Train: Epoch [13], Batch [58/938], Loss: 0.6745287775993347\n",
      "Train: Epoch [13], Batch [59/938], Loss: 0.7298797369003296\n",
      "Train: Epoch [13], Batch [60/938], Loss: 0.6058375835418701\n",
      "Train: Epoch [13], Batch [61/938], Loss: 0.8160624504089355\n",
      "Train: Epoch [13], Batch [62/938], Loss: 0.8708222508430481\n",
      "Train: Epoch [13], Batch [63/938], Loss: 1.074050784111023\n",
      "Train: Epoch [13], Batch [64/938], Loss: 0.7748525738716125\n",
      "Train: Epoch [13], Batch [65/938], Loss: 0.6484998464584351\n",
      "Train: Epoch [13], Batch [66/938], Loss: 0.7553879022598267\n",
      "Train: Epoch [13], Batch [67/938], Loss: 0.9591169953346252\n",
      "Train: Epoch [13], Batch [68/938], Loss: 0.831045389175415\n",
      "Train: Epoch [13], Batch [69/938], Loss: 0.5583643913269043\n",
      "Train: Epoch [13], Batch [70/938], Loss: 0.7955083847045898\n",
      "Train: Epoch [13], Batch [71/938], Loss: 0.9063557386398315\n",
      "Train: Epoch [13], Batch [72/938], Loss: 0.6890338063240051\n",
      "Train: Epoch [13], Batch [73/938], Loss: 0.8449521064758301\n",
      "Train: Epoch [13], Batch [74/938], Loss: 0.8642216920852661\n",
      "Train: Epoch [13], Batch [75/938], Loss: 0.8141508102416992\n",
      "Train: Epoch [13], Batch [76/938], Loss: 0.6690512895584106\n",
      "Train: Epoch [13], Batch [77/938], Loss: 1.1303834915161133\n",
      "Train: Epoch [13], Batch [78/938], Loss: 0.6275306940078735\n",
      "Train: Epoch [13], Batch [79/938], Loss: 0.6862400770187378\n",
      "Train: Epoch [13], Batch [80/938], Loss: 0.6597763299942017\n",
      "Train: Epoch [13], Batch [81/938], Loss: 0.7667433023452759\n",
      "Train: Epoch [13], Batch [82/938], Loss: 0.5869529247283936\n",
      "Train: Epoch [13], Batch [83/938], Loss: 0.8261146545410156\n",
      "Train: Epoch [13], Batch [84/938], Loss: 0.9581703543663025\n",
      "Train: Epoch [13], Batch [85/938], Loss: 0.6328737735748291\n",
      "Train: Epoch [13], Batch [86/938], Loss: 0.7872781753540039\n",
      "Train: Epoch [13], Batch [87/938], Loss: 0.8800026178359985\n",
      "Train: Epoch [13], Batch [88/938], Loss: 0.9526121020317078\n",
      "Train: Epoch [13], Batch [89/938], Loss: 0.9147244691848755\n",
      "Train: Epoch [13], Batch [90/938], Loss: 0.6132862567901611\n",
      "Train: Epoch [13], Batch [91/938], Loss: 0.6942043900489807\n",
      "Train: Epoch [13], Batch [92/938], Loss: 0.7641045451164246\n",
      "Train: Epoch [13], Batch [93/938], Loss: 0.5800312757492065\n",
      "Train: Epoch [13], Batch [94/938], Loss: 1.0463758707046509\n",
      "Train: Epoch [13], Batch [95/938], Loss: 0.7924275398254395\n",
      "Train: Epoch [13], Batch [96/938], Loss: 0.6920276880264282\n",
      "Train: Epoch [13], Batch [97/938], Loss: 0.7661218643188477\n",
      "Train: Epoch [13], Batch [98/938], Loss: 0.7832147479057312\n",
      "Train: Epoch [13], Batch [99/938], Loss: 1.0650362968444824\n",
      "Train: Epoch [13], Batch [100/938], Loss: 0.46646833419799805\n",
      "Train: Epoch [13], Batch [101/938], Loss: 0.6662968397140503\n",
      "Train: Epoch [13], Batch [102/938], Loss: 0.9316008687019348\n",
      "Train: Epoch [13], Batch [103/938], Loss: 1.0469549894332886\n",
      "Train: Epoch [13], Batch [104/938], Loss: 0.767031729221344\n",
      "Train: Epoch [13], Batch [105/938], Loss: 0.622215986251831\n",
      "Train: Epoch [13], Batch [106/938], Loss: 0.754214882850647\n",
      "Train: Epoch [13], Batch [107/938], Loss: 0.8683187961578369\n",
      "Train: Epoch [13], Batch [108/938], Loss: 0.7451645135879517\n",
      "Train: Epoch [13], Batch [109/938], Loss: 0.6307681202888489\n",
      "Train: Epoch [13], Batch [110/938], Loss: 0.6142597198486328\n",
      "Train: Epoch [13], Batch [111/938], Loss: 0.8848044276237488\n",
      "Train: Epoch [13], Batch [112/938], Loss: 0.6845980882644653\n",
      "Train: Epoch [13], Batch [113/938], Loss: 0.7286132574081421\n",
      "Train: Epoch [13], Batch [114/938], Loss: 0.8084749579429626\n",
      "Train: Epoch [13], Batch [115/938], Loss: 0.582623302936554\n",
      "Train: Epoch [13], Batch [116/938], Loss: 1.018420696258545\n",
      "Train: Epoch [13], Batch [117/938], Loss: 0.5346176624298096\n",
      "Train: Epoch [13], Batch [118/938], Loss: 0.8875075578689575\n",
      "Train: Epoch [13], Batch [119/938], Loss: 0.7109584212303162\n",
      "Train: Epoch [13], Batch [120/938], Loss: 0.7294589281082153\n",
      "Train: Epoch [13], Batch [121/938], Loss: 0.548458993434906\n",
      "Train: Epoch [13], Batch [122/938], Loss: 0.7649706602096558\n",
      "Train: Epoch [13], Batch [123/938], Loss: 0.687265932559967\n",
      "Train: Epoch [13], Batch [124/938], Loss: 0.5493301749229431\n",
      "Train: Epoch [13], Batch [125/938], Loss: 0.638552725315094\n",
      "Train: Epoch [13], Batch [126/938], Loss: 0.5804879069328308\n",
      "Train: Epoch [13], Batch [127/938], Loss: 0.5802054405212402\n",
      "Train: Epoch [13], Batch [128/938], Loss: 0.5935996770858765\n",
      "Train: Epoch [13], Batch [129/938], Loss: 0.7762231230735779\n",
      "Train: Epoch [13], Batch [130/938], Loss: 0.6231356859207153\n",
      "Train: Epoch [13], Batch [131/938], Loss: 0.8101601004600525\n",
      "Train: Epoch [13], Batch [132/938], Loss: 0.7208061218261719\n",
      "Train: Epoch [13], Batch [133/938], Loss: 0.6201274394989014\n",
      "Train: Epoch [13], Batch [134/938], Loss: 0.6955304145812988\n",
      "Train: Epoch [13], Batch [135/938], Loss: 0.6665821671485901\n",
      "Train: Epoch [13], Batch [136/938], Loss: 0.6718279719352722\n",
      "Train: Epoch [13], Batch [137/938], Loss: 0.698017954826355\n",
      "Train: Epoch [13], Batch [138/938], Loss: 0.7159148454666138\n",
      "Train: Epoch [13], Batch [139/938], Loss: 0.8867749571800232\n",
      "Train: Epoch [13], Batch [140/938], Loss: 0.7175798416137695\n",
      "Train: Epoch [13], Batch [141/938], Loss: 0.6845254898071289\n",
      "Train: Epoch [13], Batch [142/938], Loss: 0.5929093956947327\n",
      "Train: Epoch [13], Batch [143/938], Loss: 0.5474745631217957\n",
      "Train: Epoch [13], Batch [144/938], Loss: 0.7415558099746704\n",
      "Train: Epoch [13], Batch [145/938], Loss: 0.7096028923988342\n",
      "Train: Epoch [13], Batch [146/938], Loss: 0.8195008039474487\n",
      "Train: Epoch [13], Batch [147/938], Loss: 0.7985482215881348\n",
      "Train: Epoch [13], Batch [148/938], Loss: 0.6536668539047241\n",
      "Train: Epoch [13], Batch [149/938], Loss: 0.6360410451889038\n",
      "Train: Epoch [13], Batch [150/938], Loss: 0.7493650913238525\n",
      "Train: Epoch [13], Batch [151/938], Loss: 0.545621395111084\n",
      "Train: Epoch [13], Batch [152/938], Loss: 0.5701225996017456\n",
      "Train: Epoch [13], Batch [153/938], Loss: 0.592776358127594\n",
      "Train: Epoch [13], Batch [154/938], Loss: 0.8631690740585327\n",
      "Train: Epoch [13], Batch [155/938], Loss: 0.8543376326560974\n",
      "Train: Epoch [13], Batch [156/938], Loss: 0.646249532699585\n",
      "Train: Epoch [13], Batch [157/938], Loss: 0.8799037933349609\n",
      "Train: Epoch [13], Batch [158/938], Loss: 0.6891434788703918\n",
      "Train: Epoch [13], Batch [159/938], Loss: 0.8338311314582825\n",
      "Train: Epoch [13], Batch [160/938], Loss: 0.5047910809516907\n",
      "Train: Epoch [13], Batch [161/938], Loss: 0.6773216724395752\n",
      "Train: Epoch [13], Batch [162/938], Loss: 0.6256341934204102\n",
      "Train: Epoch [13], Batch [163/938], Loss: 0.47991225123405457\n",
      "Train: Epoch [13], Batch [164/938], Loss: 0.7052874565124512\n",
      "Train: Epoch [13], Batch [165/938], Loss: 0.6264461874961853\n",
      "Train: Epoch [13], Batch [166/938], Loss: 0.729525089263916\n",
      "Train: Epoch [13], Batch [167/938], Loss: 0.7319053411483765\n",
      "Train: Epoch [13], Batch [168/938], Loss: 0.7780857682228088\n",
      "Train: Epoch [13], Batch [169/938], Loss: 0.7731856107711792\n",
      "Train: Epoch [13], Batch [170/938], Loss: 0.5021277070045471\n",
      "Train: Epoch [13], Batch [171/938], Loss: 0.5525816679000854\n",
      "Train: Epoch [13], Batch [172/938], Loss: 0.7895663380622864\n",
      "Train: Epoch [13], Batch [173/938], Loss: 0.7008434534072876\n",
      "Train: Epoch [13], Batch [174/938], Loss: 0.6705778241157532\n",
      "Train: Epoch [13], Batch [175/938], Loss: 1.174412727355957\n",
      "Train: Epoch [13], Batch [176/938], Loss: 0.8627215623855591\n",
      "Train: Epoch [13], Batch [177/938], Loss: 0.44266602396965027\n",
      "Train: Epoch [13], Batch [178/938], Loss: 0.6691902875900269\n",
      "Train: Epoch [13], Batch [179/938], Loss: 0.9319789409637451\n",
      "Train: Epoch [13], Batch [180/938], Loss: 0.7413638234138489\n",
      "Train: Epoch [13], Batch [181/938], Loss: 0.6342178583145142\n",
      "Train: Epoch [13], Batch [182/938], Loss: 0.8928399085998535\n",
      "Train: Epoch [13], Batch [183/938], Loss: 0.6192674040794373\n",
      "Train: Epoch [13], Batch [184/938], Loss: 0.6600659489631653\n",
      "Train: Epoch [13], Batch [185/938], Loss: 0.7381794452667236\n",
      "Train: Epoch [13], Batch [186/938], Loss: 0.7846680283546448\n",
      "Train: Epoch [13], Batch [187/938], Loss: 0.776674747467041\n",
      "Train: Epoch [13], Batch [188/938], Loss: 0.9994242191314697\n",
      "Train: Epoch [13], Batch [189/938], Loss: 0.7236163020133972\n",
      "Train: Epoch [13], Batch [190/938], Loss: 0.8419041633605957\n",
      "Train: Epoch [13], Batch [191/938], Loss: 1.048747181892395\n",
      "Train: Epoch [13], Batch [192/938], Loss: 0.7286810278892517\n",
      "Train: Epoch [13], Batch [193/938], Loss: 0.836063027381897\n",
      "Train: Epoch [13], Batch [194/938], Loss: 0.8667855262756348\n",
      "Train: Epoch [13], Batch [195/938], Loss: 0.6284484267234802\n",
      "Train: Epoch [13], Batch [196/938], Loss: 0.5887326598167419\n",
      "Train: Epoch [13], Batch [197/938], Loss: 0.6148831844329834\n",
      "Train: Epoch [13], Batch [198/938], Loss: 0.9661075472831726\n",
      "Train: Epoch [13], Batch [199/938], Loss: 0.6856290102005005\n",
      "Train: Epoch [13], Batch [200/938], Loss: 0.7888553738594055\n",
      "Train: Epoch [13], Batch [201/938], Loss: 0.6533564329147339\n",
      "Train: Epoch [13], Batch [202/938], Loss: 0.8284174799919128\n",
      "Train: Epoch [13], Batch [203/938], Loss: 0.7089462280273438\n",
      "Train: Epoch [13], Batch [204/938], Loss: 0.9181831479072571\n",
      "Train: Epoch [13], Batch [205/938], Loss: 0.9799030423164368\n",
      "Train: Epoch [13], Batch [206/938], Loss: 0.9317702054977417\n",
      "Train: Epoch [13], Batch [207/938], Loss: 0.7651560306549072\n",
      "Train: Epoch [13], Batch [208/938], Loss: 0.7477121353149414\n",
      "Train: Epoch [13], Batch [209/938], Loss: 0.6360926628112793\n",
      "Train: Epoch [13], Batch [210/938], Loss: 0.47944772243499756\n",
      "Train: Epoch [13], Batch [211/938], Loss: 0.7169047594070435\n",
      "Train: Epoch [13], Batch [212/938], Loss: 0.7820907831192017\n",
      "Train: Epoch [13], Batch [213/938], Loss: 0.8596814274787903\n",
      "Train: Epoch [13], Batch [214/938], Loss: 0.6834740042686462\n",
      "Train: Epoch [13], Batch [215/938], Loss: 0.7607843279838562\n",
      "Train: Epoch [13], Batch [216/938], Loss: 0.6784435510635376\n",
      "Train: Epoch [13], Batch [217/938], Loss: 0.36700576543807983\n",
      "Train: Epoch [13], Batch [218/938], Loss: 0.5272711515426636\n",
      "Train: Epoch [13], Batch [219/938], Loss: 0.6208637952804565\n",
      "Train: Epoch [13], Batch [220/938], Loss: 0.7238512635231018\n",
      "Train: Epoch [13], Batch [221/938], Loss: 0.6872677206993103\n",
      "Train: Epoch [13], Batch [222/938], Loss: 0.870721697807312\n",
      "Train: Epoch [13], Batch [223/938], Loss: 0.7250828146934509\n",
      "Train: Epoch [13], Batch [224/938], Loss: 0.8979374766349792\n",
      "Train: Epoch [13], Batch [225/938], Loss: 1.0144739151000977\n",
      "Train: Epoch [13], Batch [226/938], Loss: 0.8423623442649841\n",
      "Train: Epoch [13], Batch [227/938], Loss: 0.7701265811920166\n",
      "Train: Epoch [13], Batch [228/938], Loss: 0.6784296631813049\n",
      "Train: Epoch [13], Batch [229/938], Loss: 0.710989236831665\n",
      "Train: Epoch [13], Batch [230/938], Loss: 0.5457020998001099\n",
      "Train: Epoch [13], Batch [231/938], Loss: 0.9778541326522827\n",
      "Train: Epoch [13], Batch [232/938], Loss: 0.8925586938858032\n",
      "Train: Epoch [13], Batch [233/938], Loss: 0.8385745882987976\n",
      "Train: Epoch [13], Batch [234/938], Loss: 0.8210601210594177\n",
      "Train: Epoch [13], Batch [235/938], Loss: 0.6166764497756958\n",
      "Train: Epoch [13], Batch [236/938], Loss: 0.7797814607620239\n",
      "Train: Epoch [13], Batch [237/938], Loss: 0.7200989127159119\n",
      "Train: Epoch [13], Batch [238/938], Loss: 0.618320107460022\n",
      "Train: Epoch [13], Batch [239/938], Loss: 0.5490959286689758\n",
      "Train: Epoch [13], Batch [240/938], Loss: 0.6834945678710938\n",
      "Train: Epoch [13], Batch [241/938], Loss: 0.6695890426635742\n",
      "Train: Epoch [13], Batch [242/938], Loss: 0.8022521138191223\n",
      "Train: Epoch [13], Batch [243/938], Loss: 0.7045336961746216\n",
      "Train: Epoch [13], Batch [244/938], Loss: 0.5630117654800415\n",
      "Train: Epoch [13], Batch [245/938], Loss: 0.724166989326477\n",
      "Train: Epoch [13], Batch [246/938], Loss: 0.7285175323486328\n",
      "Train: Epoch [13], Batch [247/938], Loss: 0.8671278357505798\n",
      "Train: Epoch [13], Batch [248/938], Loss: 0.8989715576171875\n",
      "Train: Epoch [13], Batch [249/938], Loss: 0.8184711933135986\n",
      "Train: Epoch [13], Batch [250/938], Loss: 0.7067124247550964\n",
      "Train: Epoch [13], Batch [251/938], Loss: 0.807276725769043\n",
      "Train: Epoch [13], Batch [252/938], Loss: 0.6413778066635132\n",
      "Train: Epoch [13], Batch [253/938], Loss: 0.5872923135757446\n",
      "Train: Epoch [13], Batch [254/938], Loss: 0.5246642231941223\n",
      "Train: Epoch [13], Batch [255/938], Loss: 0.6569718718528748\n",
      "Train: Epoch [13], Batch [256/938], Loss: 0.8039219975471497\n",
      "Train: Epoch [13], Batch [257/938], Loss: 0.5837482213973999\n",
      "Train: Epoch [13], Batch [258/938], Loss: 0.8001376390457153\n",
      "Train: Epoch [13], Batch [259/938], Loss: 0.5762578248977661\n",
      "Train: Epoch [13], Batch [260/938], Loss: 0.7599352598190308\n",
      "Train: Epoch [13], Batch [261/938], Loss: 0.5359970331192017\n",
      "Train: Epoch [13], Batch [262/938], Loss: 0.9802649021148682\n",
      "Train: Epoch [13], Batch [263/938], Loss: 0.6890218257904053\n",
      "Train: Epoch [13], Batch [264/938], Loss: 0.7855332493782043\n",
      "Train: Epoch [13], Batch [265/938], Loss: 0.8230069279670715\n",
      "Train: Epoch [13], Batch [266/938], Loss: 0.7698088884353638\n",
      "Train: Epoch [13], Batch [267/938], Loss: 0.818697988986969\n",
      "Train: Epoch [13], Batch [268/938], Loss: 0.8088042140007019\n",
      "Train: Epoch [13], Batch [269/938], Loss: 0.690026044845581\n",
      "Train: Epoch [13], Batch [270/938], Loss: 0.775826632976532\n",
      "Train: Epoch [13], Batch [271/938], Loss: 0.7784849405288696\n",
      "Train: Epoch [13], Batch [272/938], Loss: 0.549555242061615\n",
      "Train: Epoch [13], Batch [273/938], Loss: 0.709098756313324\n",
      "Train: Epoch [13], Batch [274/938], Loss: 0.7031902074813843\n",
      "Train: Epoch [13], Batch [275/938], Loss: 0.7379347681999207\n",
      "Train: Epoch [13], Batch [276/938], Loss: 0.9423810243606567\n",
      "Train: Epoch [13], Batch [277/938], Loss: 0.7317764163017273\n",
      "Train: Epoch [13], Batch [278/938], Loss: 0.7443246841430664\n",
      "Train: Epoch [13], Batch [279/938], Loss: 0.7654322385787964\n",
      "Train: Epoch [13], Batch [280/938], Loss: 0.7250198721885681\n",
      "Train: Epoch [13], Batch [281/938], Loss: 0.6873927116394043\n",
      "Train: Epoch [13], Batch [282/938], Loss: 0.7002636790275574\n",
      "Train: Epoch [13], Batch [283/938], Loss: 0.5971936583518982\n",
      "Train: Epoch [13], Batch [284/938], Loss: 0.7765635848045349\n",
      "Train: Epoch [13], Batch [285/938], Loss: 0.8175577521324158\n",
      "Train: Epoch [13], Batch [286/938], Loss: 0.5559663772583008\n",
      "Train: Epoch [13], Batch [287/938], Loss: 0.6060530543327332\n",
      "Train: Epoch [13], Batch [288/938], Loss: 0.9196513295173645\n",
      "Train: Epoch [13], Batch [289/938], Loss: 0.7622564435005188\n",
      "Train: Epoch [13], Batch [290/938], Loss: 0.8357236385345459\n",
      "Train: Epoch [13], Batch [291/938], Loss: 0.8035364151000977\n",
      "Train: Epoch [13], Batch [292/938], Loss: 1.0920933485031128\n",
      "Train: Epoch [13], Batch [293/938], Loss: 0.8029245138168335\n",
      "Train: Epoch [13], Batch [294/938], Loss: 0.6070007085800171\n",
      "Train: Epoch [13], Batch [295/938], Loss: 0.7752598524093628\n",
      "Train: Epoch [13], Batch [296/938], Loss: 0.8901119232177734\n",
      "Train: Epoch [13], Batch [297/938], Loss: 0.9972305297851562\n",
      "Train: Epoch [13], Batch [298/938], Loss: 0.8753322958946228\n",
      "Train: Epoch [13], Batch [299/938], Loss: 0.7511906623840332\n",
      "Train: Epoch [13], Batch [300/938], Loss: 0.6734370589256287\n",
      "Train: Epoch [13], Batch [301/938], Loss: 0.72193843126297\n",
      "Train: Epoch [13], Batch [302/938], Loss: 0.5394046306610107\n",
      "Train: Epoch [13], Batch [303/938], Loss: 0.5520758032798767\n",
      "Train: Epoch [13], Batch [304/938], Loss: 0.9038486480712891\n",
      "Train: Epoch [13], Batch [305/938], Loss: 0.8975449204444885\n",
      "Train: Epoch [13], Batch [306/938], Loss: 0.6924457550048828\n",
      "Train: Epoch [13], Batch [307/938], Loss: 0.8005622029304504\n",
      "Train: Epoch [13], Batch [308/938], Loss: 0.6738342642784119\n",
      "Train: Epoch [13], Batch [309/938], Loss: 0.7288426160812378\n",
      "Train: Epoch [13], Batch [310/938], Loss: 0.6898142695426941\n",
      "Train: Epoch [13], Batch [311/938], Loss: 0.693932294845581\n",
      "Train: Epoch [13], Batch [312/938], Loss: 1.0721204280853271\n",
      "Train: Epoch [13], Batch [313/938], Loss: 0.4937112629413605\n",
      "Train: Epoch [13], Batch [314/938], Loss: 0.6943779587745667\n",
      "Train: Epoch [13], Batch [315/938], Loss: 1.0256091356277466\n",
      "Train: Epoch [13], Batch [316/938], Loss: 0.7065052390098572\n",
      "Train: Epoch [13], Batch [317/938], Loss: 0.5455147624015808\n",
      "Train: Epoch [13], Batch [318/938], Loss: 0.623399019241333\n",
      "Train: Epoch [13], Batch [319/938], Loss: 0.9677720665931702\n",
      "Train: Epoch [13], Batch [320/938], Loss: 0.6800678372383118\n",
      "Train: Epoch [13], Batch [321/938], Loss: 0.957546591758728\n",
      "Train: Epoch [13], Batch [322/938], Loss: 0.685931921005249\n",
      "Train: Epoch [13], Batch [323/938], Loss: 0.6835103631019592\n",
      "Train: Epoch [13], Batch [324/938], Loss: 0.632698655128479\n",
      "Train: Epoch [13], Batch [325/938], Loss: 0.7297412157058716\n",
      "Train: Epoch [13], Batch [326/938], Loss: 0.5105264782905579\n",
      "Train: Epoch [13], Batch [327/938], Loss: 0.5940766930580139\n",
      "Train: Epoch [13], Batch [328/938], Loss: 0.6863040328025818\n",
      "Train: Epoch [13], Batch [329/938], Loss: 0.5360549688339233\n",
      "Train: Epoch [13], Batch [330/938], Loss: 0.598334550857544\n",
      "Train: Epoch [13], Batch [331/938], Loss: 0.8112063407897949\n",
      "Train: Epoch [13], Batch [332/938], Loss: 0.6770603060722351\n",
      "Train: Epoch [13], Batch [333/938], Loss: 0.8111670017242432\n",
      "Train: Epoch [13], Batch [334/938], Loss: 0.8133164644241333\n",
      "Train: Epoch [13], Batch [335/938], Loss: 0.981378436088562\n",
      "Train: Epoch [13], Batch [336/938], Loss: 1.0096081495285034\n",
      "Train: Epoch [13], Batch [337/938], Loss: 0.6615440845489502\n",
      "Train: Epoch [13], Batch [338/938], Loss: 0.6460806131362915\n",
      "Train: Epoch [13], Batch [339/938], Loss: 0.726318895816803\n",
      "Train: Epoch [13], Batch [340/938], Loss: 0.7495238780975342\n",
      "Train: Epoch [13], Batch [341/938], Loss: 0.7126473188400269\n",
      "Train: Epoch [13], Batch [342/938], Loss: 0.5406830310821533\n",
      "Train: Epoch [13], Batch [343/938], Loss: 0.7894076704978943\n",
      "Train: Epoch [13], Batch [344/938], Loss: 0.780604898929596\n",
      "Train: Epoch [13], Batch [345/938], Loss: 0.8544528484344482\n",
      "Train: Epoch [13], Batch [346/938], Loss: 0.9926300048828125\n",
      "Train: Epoch [13], Batch [347/938], Loss: 0.88843834400177\n",
      "Train: Epoch [13], Batch [348/938], Loss: 0.9732048511505127\n",
      "Train: Epoch [13], Batch [349/938], Loss: 0.78920578956604\n",
      "Train: Epoch [13], Batch [350/938], Loss: 0.7065383791923523\n",
      "Train: Epoch [13], Batch [351/938], Loss: 0.7792712450027466\n",
      "Train: Epoch [13], Batch [352/938], Loss: 0.9734156131744385\n",
      "Train: Epoch [13], Batch [353/938], Loss: 0.7095127105712891\n",
      "Train: Epoch [13], Batch [354/938], Loss: 0.8349241018295288\n",
      "Train: Epoch [13], Batch [355/938], Loss: 0.5915052890777588\n",
      "Train: Epoch [13], Batch [356/938], Loss: 0.7213303446769714\n",
      "Train: Epoch [13], Batch [357/938], Loss: 0.8785796165466309\n",
      "Train: Epoch [13], Batch [358/938], Loss: 0.6768876910209656\n",
      "Train: Epoch [13], Batch [359/938], Loss: 0.6990492343902588\n",
      "Train: Epoch [13], Batch [360/938], Loss: 0.8568682074546814\n",
      "Train: Epoch [13], Batch [361/938], Loss: 1.0637894868850708\n",
      "Train: Epoch [13], Batch [362/938], Loss: 0.6648398041725159\n",
      "Train: Epoch [13], Batch [363/938], Loss: 0.7461977005004883\n",
      "Train: Epoch [13], Batch [364/938], Loss: 0.8383391499519348\n",
      "Train: Epoch [13], Batch [365/938], Loss: 0.9182941317558289\n",
      "Train: Epoch [13], Batch [366/938], Loss: 0.49696990847587585\n",
      "Train: Epoch [13], Batch [367/938], Loss: 0.7461951971054077\n",
      "Train: Epoch [13], Batch [368/938], Loss: 0.52313631772995\n",
      "Train: Epoch [13], Batch [369/938], Loss: 0.6777887344360352\n",
      "Train: Epoch [13], Batch [370/938], Loss: 0.7939087152481079\n",
      "Train: Epoch [13], Batch [371/938], Loss: 0.5848583579063416\n",
      "Train: Epoch [13], Batch [372/938], Loss: 0.8542141914367676\n",
      "Train: Epoch [13], Batch [373/938], Loss: 0.4688165485858917\n",
      "Train: Epoch [13], Batch [374/938], Loss: 0.6845453977584839\n",
      "Train: Epoch [13], Batch [375/938], Loss: 0.7259758710861206\n",
      "Train: Epoch [13], Batch [376/938], Loss: 0.48421233892440796\n",
      "Train: Epoch [13], Batch [377/938], Loss: 0.7474851608276367\n",
      "Train: Epoch [13], Batch [378/938], Loss: 0.6196001768112183\n",
      "Train: Epoch [13], Batch [379/938], Loss: 1.0623952150344849\n",
      "Train: Epoch [13], Batch [380/938], Loss: 0.8721339702606201\n",
      "Train: Epoch [13], Batch [381/938], Loss: 0.9023873805999756\n",
      "Train: Epoch [13], Batch [382/938], Loss: 0.7121835350990295\n",
      "Train: Epoch [13], Batch [383/938], Loss: 0.8046135306358337\n",
      "Train: Epoch [13], Batch [384/938], Loss: 0.8283629417419434\n",
      "Train: Epoch [13], Batch [385/938], Loss: 0.8829354643821716\n",
      "Train: Epoch [13], Batch [386/938], Loss: 0.521928608417511\n",
      "Train: Epoch [13], Batch [387/938], Loss: 0.5612746477127075\n",
      "Train: Epoch [13], Batch [388/938], Loss: 0.8739326000213623\n",
      "Train: Epoch [13], Batch [389/938], Loss: 0.9606449007987976\n",
      "Train: Epoch [13], Batch [390/938], Loss: 0.712180495262146\n",
      "Train: Epoch [13], Batch [391/938], Loss: 0.7196781635284424\n",
      "Train: Epoch [13], Batch [392/938], Loss: 0.8911896347999573\n",
      "Train: Epoch [13], Batch [393/938], Loss: 0.4788893461227417\n",
      "Train: Epoch [13], Batch [394/938], Loss: 0.6726997494697571\n",
      "Train: Epoch [13], Batch [395/938], Loss: 0.8343864679336548\n",
      "Train: Epoch [13], Batch [396/938], Loss: 0.7412803173065186\n",
      "Train: Epoch [13], Batch [397/938], Loss: 0.7039381265640259\n",
      "Train: Epoch [13], Batch [398/938], Loss: 0.8076732158660889\n",
      "Train: Epoch [13], Batch [399/938], Loss: 0.6739575862884521\n",
      "Train: Epoch [13], Batch [400/938], Loss: 0.8038668632507324\n",
      "Train: Epoch [13], Batch [401/938], Loss: 0.7470202445983887\n",
      "Train: Epoch [13], Batch [402/938], Loss: 0.7302436232566833\n",
      "Train: Epoch [13], Batch [403/938], Loss: 0.7865219116210938\n",
      "Train: Epoch [13], Batch [404/938], Loss: 0.8323957920074463\n",
      "Train: Epoch [13], Batch [405/938], Loss: 0.760607123374939\n",
      "Train: Epoch [13], Batch [406/938], Loss: 0.7759077548980713\n",
      "Train: Epoch [13], Batch [407/938], Loss: 0.7273737192153931\n",
      "Train: Epoch [13], Batch [408/938], Loss: 0.6559663414955139\n",
      "Train: Epoch [13], Batch [409/938], Loss: 0.5047160983085632\n",
      "Train: Epoch [13], Batch [410/938], Loss: 0.6624491214752197\n",
      "Train: Epoch [13], Batch [411/938], Loss: 0.6457820534706116\n",
      "Train: Epoch [13], Batch [412/938], Loss: 0.6761119365692139\n",
      "Train: Epoch [13], Batch [413/938], Loss: 0.707768440246582\n",
      "Train: Epoch [13], Batch [414/938], Loss: 0.742338240146637\n",
      "Train: Epoch [13], Batch [415/938], Loss: 1.000874400138855\n",
      "Train: Epoch [13], Batch [416/938], Loss: 0.5826449394226074\n",
      "Train: Epoch [13], Batch [417/938], Loss: 0.6992509961128235\n",
      "Train: Epoch [13], Batch [418/938], Loss: 0.5842874050140381\n",
      "Train: Epoch [13], Batch [419/938], Loss: 0.5575481653213501\n",
      "Train: Epoch [13], Batch [420/938], Loss: 0.865958034992218\n",
      "Train: Epoch [13], Batch [421/938], Loss: 0.8795422911643982\n",
      "Train: Epoch [13], Batch [422/938], Loss: 0.7157626748085022\n",
      "Train: Epoch [13], Batch [423/938], Loss: 0.6535177230834961\n",
      "Train: Epoch [13], Batch [424/938], Loss: 0.7673435211181641\n",
      "Train: Epoch [13], Batch [425/938], Loss: 0.8875167369842529\n",
      "Train: Epoch [13], Batch [426/938], Loss: 0.5218007564544678\n",
      "Train: Epoch [13], Batch [427/938], Loss: 0.5567096471786499\n",
      "Train: Epoch [13], Batch [428/938], Loss: 0.8066330552101135\n",
      "Train: Epoch [13], Batch [429/938], Loss: 0.7104132771492004\n",
      "Train: Epoch [13], Batch [430/938], Loss: 0.7435749173164368\n",
      "Train: Epoch [13], Batch [431/938], Loss: 0.6387637257575989\n",
      "Train: Epoch [13], Batch [432/938], Loss: 0.8377381563186646\n",
      "Train: Epoch [13], Batch [433/938], Loss: 0.701110303401947\n",
      "Train: Epoch [13], Batch [434/938], Loss: 0.7920119762420654\n",
      "Train: Epoch [13], Batch [435/938], Loss: 0.717453122138977\n",
      "Train: Epoch [13], Batch [436/938], Loss: 0.6407715082168579\n",
      "Train: Epoch [13], Batch [437/938], Loss: 0.7961506843566895\n",
      "Train: Epoch [13], Batch [438/938], Loss: 0.8476370573043823\n",
      "Train: Epoch [13], Batch [439/938], Loss: 0.7516769766807556\n",
      "Train: Epoch [13], Batch [440/938], Loss: 0.5143160820007324\n",
      "Train: Epoch [13], Batch [441/938], Loss: 1.0554920434951782\n",
      "Train: Epoch [13], Batch [442/938], Loss: 0.7846257090568542\n",
      "Train: Epoch [13], Batch [443/938], Loss: 0.7406641840934753\n",
      "Train: Epoch [13], Batch [444/938], Loss: 0.8873277306556702\n",
      "Train: Epoch [13], Batch [445/938], Loss: 0.8150105476379395\n",
      "Train: Epoch [13], Batch [446/938], Loss: 0.7504613399505615\n",
      "Train: Epoch [13], Batch [447/938], Loss: 0.7125175595283508\n",
      "Train: Epoch [13], Batch [448/938], Loss: 1.0114309787750244\n",
      "Train: Epoch [13], Batch [449/938], Loss: 0.6451296806335449\n",
      "Train: Epoch [13], Batch [450/938], Loss: 0.6248634457588196\n",
      "Train: Epoch [13], Batch [451/938], Loss: 0.8089879155158997\n",
      "Train: Epoch [13], Batch [452/938], Loss: 0.9238762259483337\n",
      "Train: Epoch [13], Batch [453/938], Loss: 0.8186043500900269\n",
      "Train: Epoch [13], Batch [454/938], Loss: 0.7225714325904846\n",
      "Train: Epoch [13], Batch [455/938], Loss: 0.6815665364265442\n",
      "Train: Epoch [13], Batch [456/938], Loss: 0.8996541500091553\n",
      "Train: Epoch [13], Batch [457/938], Loss: 0.8219726085662842\n",
      "Train: Epoch [13], Batch [458/938], Loss: 0.7016774415969849\n",
      "Train: Epoch [13], Batch [459/938], Loss: 0.6410319805145264\n",
      "Train: Epoch [13], Batch [460/938], Loss: 0.6303462982177734\n",
      "Train: Epoch [13], Batch [461/938], Loss: 0.9366858601570129\n",
      "Train: Epoch [13], Batch [462/938], Loss: 0.5820251107215881\n",
      "Train: Epoch [13], Batch [463/938], Loss: 0.6370763778686523\n",
      "Train: Epoch [13], Batch [464/938], Loss: 0.569859504699707\n",
      "Train: Epoch [13], Batch [465/938], Loss: 0.6602731347084045\n",
      "Train: Epoch [13], Batch [466/938], Loss: 0.71875\n",
      "Train: Epoch [13], Batch [467/938], Loss: 0.8996815085411072\n",
      "Train: Epoch [13], Batch [468/938], Loss: 0.563209056854248\n",
      "Train: Epoch [13], Batch [469/938], Loss: 0.5052495002746582\n",
      "Train: Epoch [13], Batch [470/938], Loss: 0.8112798929214478\n",
      "Train: Epoch [13], Batch [471/938], Loss: 0.623612642288208\n",
      "Train: Epoch [13], Batch [472/938], Loss: 0.651459276676178\n",
      "Train: Epoch [13], Batch [473/938], Loss: 0.6911725997924805\n",
      "Train: Epoch [13], Batch [474/938], Loss: 0.5918922424316406\n",
      "Train: Epoch [13], Batch [475/938], Loss: 0.8222624659538269\n",
      "Train: Epoch [13], Batch [476/938], Loss: 0.9524325132369995\n",
      "Train: Epoch [13], Batch [477/938], Loss: 0.8972183465957642\n",
      "Train: Epoch [13], Batch [478/938], Loss: 0.6806548237800598\n",
      "Train: Epoch [13], Batch [479/938], Loss: 0.9939514994621277\n",
      "Train: Epoch [13], Batch [480/938], Loss: 0.6587548851966858\n",
      "Train: Epoch [13], Batch [481/938], Loss: 0.7238274216651917\n",
      "Train: Epoch [13], Batch [482/938], Loss: 0.7571181058883667\n",
      "Train: Epoch [13], Batch [483/938], Loss: 0.5487052798271179\n",
      "Train: Epoch [13], Batch [484/938], Loss: 0.6988554000854492\n",
      "Train: Epoch [13], Batch [485/938], Loss: 0.7308197617530823\n",
      "Train: Epoch [13], Batch [486/938], Loss: 0.4359806478023529\n",
      "Train: Epoch [13], Batch [487/938], Loss: 0.8049941658973694\n",
      "Train: Epoch [13], Batch [488/938], Loss: 0.8477868437767029\n",
      "Train: Epoch [13], Batch [489/938], Loss: 0.6483106017112732\n",
      "Train: Epoch [13], Batch [490/938], Loss: 0.6539241671562195\n",
      "Train: Epoch [13], Batch [491/938], Loss: 0.7903061509132385\n",
      "Train: Epoch [13], Batch [492/938], Loss: 0.8723350763320923\n",
      "Train: Epoch [13], Batch [493/938], Loss: 0.7574887275695801\n",
      "Train: Epoch [13], Batch [494/938], Loss: 0.6510669589042664\n",
      "Train: Epoch [13], Batch [495/938], Loss: 0.7857815623283386\n",
      "Train: Epoch [13], Batch [496/938], Loss: 0.9220802783966064\n",
      "Train: Epoch [13], Batch [497/938], Loss: 0.7753998041152954\n",
      "Train: Epoch [13], Batch [498/938], Loss: 0.957625687122345\n",
      "Train: Epoch [13], Batch [499/938], Loss: 0.5846170783042908\n",
      "Train: Epoch [13], Batch [500/938], Loss: 0.6599521636962891\n",
      "Train: Epoch [13], Batch [501/938], Loss: 0.6863210797309875\n",
      "Train: Epoch [13], Batch [502/938], Loss: 0.5346345901489258\n",
      "Train: Epoch [13], Batch [503/938], Loss: 0.8240617513656616\n",
      "Train: Epoch [13], Batch [504/938], Loss: 0.7447863817214966\n",
      "Train: Epoch [13], Batch [505/938], Loss: 0.571651041507721\n",
      "Train: Epoch [13], Batch [506/938], Loss: 0.8612062931060791\n",
      "Train: Epoch [13], Batch [507/938], Loss: 0.6088417172431946\n",
      "Train: Epoch [13], Batch [508/938], Loss: 0.7734902501106262\n",
      "Train: Epoch [13], Batch [509/938], Loss: 0.6708453893661499\n",
      "Train: Epoch [13], Batch [510/938], Loss: 0.9029417037963867\n",
      "Train: Epoch [13], Batch [511/938], Loss: 0.4787325859069824\n",
      "Train: Epoch [13], Batch [512/938], Loss: 0.7778484225273132\n",
      "Train: Epoch [13], Batch [513/938], Loss: 0.7628522515296936\n",
      "Train: Epoch [13], Batch [514/938], Loss: 0.48637253046035767\n",
      "Train: Epoch [13], Batch [515/938], Loss: 0.7982931137084961\n",
      "Train: Epoch [13], Batch [516/938], Loss: 0.7249785661697388\n",
      "Train: Epoch [13], Batch [517/938], Loss: 0.6504309177398682\n",
      "Train: Epoch [13], Batch [518/938], Loss: 0.7688669562339783\n",
      "Train: Epoch [13], Batch [519/938], Loss: 0.6423336863517761\n",
      "Train: Epoch [13], Batch [520/938], Loss: 0.54242342710495\n",
      "Train: Epoch [13], Batch [521/938], Loss: 0.7636914253234863\n",
      "Train: Epoch [13], Batch [522/938], Loss: 0.5778547525405884\n",
      "Train: Epoch [13], Batch [523/938], Loss: 0.6529046893119812\n",
      "Train: Epoch [13], Batch [524/938], Loss: 0.5906465649604797\n",
      "Train: Epoch [13], Batch [525/938], Loss: 0.803400993347168\n",
      "Train: Epoch [13], Batch [526/938], Loss: 0.4854961633682251\n",
      "Train: Epoch [13], Batch [527/938], Loss: 0.6331086158752441\n",
      "Train: Epoch [13], Batch [528/938], Loss: 0.8475912809371948\n",
      "Train: Epoch [13], Batch [529/938], Loss: 0.7460538148880005\n",
      "Train: Epoch [13], Batch [530/938], Loss: 0.912979781627655\n",
      "Train: Epoch [13], Batch [531/938], Loss: 0.9155094623565674\n",
      "Train: Epoch [13], Batch [532/938], Loss: 0.7251536846160889\n",
      "Train: Epoch [13], Batch [533/938], Loss: 0.7041411995887756\n",
      "Train: Epoch [13], Batch [534/938], Loss: 0.7668877243995667\n",
      "Train: Epoch [13], Batch [535/938], Loss: 0.6331192851066589\n",
      "Train: Epoch [13], Batch [536/938], Loss: 0.6180723905563354\n",
      "Train: Epoch [13], Batch [537/938], Loss: 0.5722591280937195\n",
      "Train: Epoch [13], Batch [538/938], Loss: 0.6635963916778564\n",
      "Train: Epoch [13], Batch [539/938], Loss: 0.9034509062767029\n",
      "Train: Epoch [13], Batch [540/938], Loss: 0.6239131093025208\n",
      "Train: Epoch [13], Batch [541/938], Loss: 0.7161969542503357\n",
      "Train: Epoch [13], Batch [542/938], Loss: 0.565157413482666\n",
      "Train: Epoch [13], Batch [543/938], Loss: 0.594624400138855\n",
      "Train: Epoch [13], Batch [544/938], Loss: 0.43423259258270264\n",
      "Train: Epoch [13], Batch [545/938], Loss: 0.6646417379379272\n",
      "Train: Epoch [13], Batch [546/938], Loss: 0.6708571314811707\n",
      "Train: Epoch [13], Batch [547/938], Loss: 0.7336999177932739\n",
      "Train: Epoch [13], Batch [548/938], Loss: 0.6910130381584167\n",
      "Train: Epoch [13], Batch [549/938], Loss: 0.6240041255950928\n",
      "Train: Epoch [13], Batch [550/938], Loss: 0.8045887351036072\n",
      "Train: Epoch [13], Batch [551/938], Loss: 0.7237433195114136\n",
      "Train: Epoch [13], Batch [552/938], Loss: 0.9209280014038086\n",
      "Train: Epoch [13], Batch [553/938], Loss: 0.8061539530754089\n",
      "Train: Epoch [13], Batch [554/938], Loss: 0.5720199942588806\n",
      "Train: Epoch [13], Batch [555/938], Loss: 0.764191210269928\n",
      "Train: Epoch [13], Batch [556/938], Loss: 0.8247094750404358\n",
      "Train: Epoch [13], Batch [557/938], Loss: 0.7531299591064453\n",
      "Train: Epoch [13], Batch [558/938], Loss: 0.7653588056564331\n",
      "Train: Epoch [13], Batch [559/938], Loss: 0.7236837148666382\n",
      "Train: Epoch [13], Batch [560/938], Loss: 0.7211962342262268\n",
      "Train: Epoch [13], Batch [561/938], Loss: 0.9021679162979126\n",
      "Train: Epoch [13], Batch [562/938], Loss: 0.8122274279594421\n",
      "Train: Epoch [13], Batch [563/938], Loss: 0.47786644101142883\n",
      "Train: Epoch [13], Batch [564/938], Loss: 1.0287617444992065\n",
      "Train: Epoch [13], Batch [565/938], Loss: 0.640687108039856\n",
      "Train: Epoch [13], Batch [566/938], Loss: 0.7374039888381958\n",
      "Train: Epoch [13], Batch [567/938], Loss: 0.7356287240982056\n",
      "Train: Epoch [13], Batch [568/938], Loss: 0.6432085037231445\n",
      "Train: Epoch [13], Batch [569/938], Loss: 0.6176238656044006\n",
      "Train: Epoch [13], Batch [570/938], Loss: 0.8143604397773743\n",
      "Train: Epoch [13], Batch [571/938], Loss: 0.7931193113327026\n",
      "Train: Epoch [13], Batch [572/938], Loss: 0.6942501664161682\n",
      "Train: Epoch [13], Batch [573/938], Loss: 0.8089374899864197\n",
      "Train: Epoch [13], Batch [574/938], Loss: 0.8827711343765259\n",
      "Train: Epoch [13], Batch [575/938], Loss: 0.7621569633483887\n",
      "Train: Epoch [13], Batch [576/938], Loss: 1.0023210048675537\n",
      "Train: Epoch [13], Batch [577/938], Loss: 0.6450481414794922\n",
      "Train: Epoch [13], Batch [578/938], Loss: 0.639413595199585\n",
      "Train: Epoch [13], Batch [579/938], Loss: 0.7854776382446289\n",
      "Train: Epoch [13], Batch [580/938], Loss: 0.7858188152313232\n",
      "Train: Epoch [13], Batch [581/938], Loss: 0.8515928983688354\n",
      "Train: Epoch [13], Batch [582/938], Loss: 0.6945753693580627\n",
      "Train: Epoch [13], Batch [583/938], Loss: 0.8087608218193054\n",
      "Train: Epoch [13], Batch [584/938], Loss: 0.6733728647232056\n",
      "Train: Epoch [13], Batch [585/938], Loss: 0.48219969868659973\n",
      "Train: Epoch [13], Batch [586/938], Loss: 0.5879670977592468\n",
      "Train: Epoch [13], Batch [587/938], Loss: 0.6637614369392395\n",
      "Train: Epoch [13], Batch [588/938], Loss: 0.482540488243103\n",
      "Train: Epoch [13], Batch [589/938], Loss: 0.7969931960105896\n",
      "Train: Epoch [13], Batch [590/938], Loss: 0.7828652858734131\n",
      "Train: Epoch [13], Batch [591/938], Loss: 0.7553210258483887\n",
      "Train: Epoch [13], Batch [592/938], Loss: 0.7478371858596802\n",
      "Train: Epoch [13], Batch [593/938], Loss: 0.894827127456665\n",
      "Train: Epoch [13], Batch [594/938], Loss: 0.39756539463996887\n",
      "Train: Epoch [13], Batch [595/938], Loss: 0.6415648460388184\n",
      "Train: Epoch [13], Batch [596/938], Loss: 0.8840345740318298\n",
      "Train: Epoch [13], Batch [597/938], Loss: 0.6514173746109009\n",
      "Train: Epoch [13], Batch [598/938], Loss: 0.8168050646781921\n",
      "Train: Epoch [13], Batch [599/938], Loss: 0.7389057874679565\n",
      "Train: Epoch [13], Batch [600/938], Loss: 0.9161452054977417\n",
      "Train: Epoch [13], Batch [601/938], Loss: 0.6106123924255371\n",
      "Train: Epoch [13], Batch [602/938], Loss: 0.5215409994125366\n",
      "Train: Epoch [13], Batch [603/938], Loss: 0.5936802625656128\n",
      "Train: Epoch [13], Batch [604/938], Loss: 0.8705160021781921\n",
      "Train: Epoch [13], Batch [605/938], Loss: 0.7070634365081787\n",
      "Train: Epoch [13], Batch [606/938], Loss: 0.6867117285728455\n",
      "Train: Epoch [13], Batch [607/938], Loss: 0.7591844797134399\n",
      "Train: Epoch [13], Batch [608/938], Loss: 0.6536599397659302\n",
      "Train: Epoch [13], Batch [609/938], Loss: 0.8300138115882874\n",
      "Train: Epoch [13], Batch [610/938], Loss: 0.602203369140625\n",
      "Train: Epoch [13], Batch [611/938], Loss: 0.6907669901847839\n",
      "Train: Epoch [13], Batch [612/938], Loss: 0.683012843132019\n",
      "Train: Epoch [13], Batch [613/938], Loss: 0.6230216026306152\n",
      "Train: Epoch [13], Batch [614/938], Loss: 0.5782803893089294\n",
      "Train: Epoch [13], Batch [615/938], Loss: 0.7495429515838623\n",
      "Train: Epoch [13], Batch [616/938], Loss: 0.8235377073287964\n",
      "Train: Epoch [13], Batch [617/938], Loss: 0.9371957182884216\n",
      "Train: Epoch [13], Batch [618/938], Loss: 0.831344485282898\n",
      "Train: Epoch [13], Batch [619/938], Loss: 0.7215551733970642\n",
      "Train: Epoch [13], Batch [620/938], Loss: 0.783733606338501\n",
      "Train: Epoch [13], Batch [621/938], Loss: 0.6961213946342468\n",
      "Train: Epoch [13], Batch [622/938], Loss: 0.7829186916351318\n",
      "Train: Epoch [13], Batch [623/938], Loss: 0.5948789715766907\n",
      "Train: Epoch [13], Batch [624/938], Loss: 0.5593794584274292\n",
      "Train: Epoch [13], Batch [625/938], Loss: 0.7083502411842346\n",
      "Train: Epoch [13], Batch [626/938], Loss: 0.7271238565444946\n",
      "Train: Epoch [13], Batch [627/938], Loss: 0.9154099225997925\n",
      "Train: Epoch [13], Batch [628/938], Loss: 0.920438289642334\n",
      "Train: Epoch [13], Batch [629/938], Loss: 0.8456869125366211\n",
      "Train: Epoch [13], Batch [630/938], Loss: 0.5790606737136841\n",
      "Train: Epoch [13], Batch [631/938], Loss: 0.6688932180404663\n",
      "Train: Epoch [13], Batch [632/938], Loss: 0.8163821697235107\n",
      "Train: Epoch [13], Batch [633/938], Loss: 0.8412880301475525\n",
      "Train: Epoch [13], Batch [634/938], Loss: 0.8826563358306885\n",
      "Train: Epoch [13], Batch [635/938], Loss: 0.7442016005516052\n",
      "Train: Epoch [13], Batch [636/938], Loss: 0.6450386047363281\n",
      "Train: Epoch [13], Batch [637/938], Loss: 0.7096608281135559\n",
      "Train: Epoch [13], Batch [638/938], Loss: 0.8573781251907349\n",
      "Train: Epoch [13], Batch [639/938], Loss: 0.7512184977531433\n",
      "Train: Epoch [13], Batch [640/938], Loss: 0.7057385444641113\n",
      "Train: Epoch [13], Batch [641/938], Loss: 0.8183515071868896\n",
      "Train: Epoch [13], Batch [642/938], Loss: 0.7278119325637817\n",
      "Train: Epoch [13], Batch [643/938], Loss: 0.5847494006156921\n",
      "Train: Epoch [13], Batch [644/938], Loss: 0.8616955876350403\n",
      "Train: Epoch [13], Batch [645/938], Loss: 0.8194988965988159\n",
      "Train: Epoch [13], Batch [646/938], Loss: 0.6519795656204224\n",
      "Train: Epoch [13], Batch [647/938], Loss: 0.5637624263763428\n",
      "Train: Epoch [13], Batch [648/938], Loss: 0.7547010779380798\n",
      "Train: Epoch [13], Batch [649/938], Loss: 0.5225906372070312\n",
      "Train: Epoch [13], Batch [650/938], Loss: 0.5834263563156128\n",
      "Train: Epoch [13], Batch [651/938], Loss: 0.699748158454895\n",
      "Train: Epoch [13], Batch [652/938], Loss: 0.7961567044258118\n",
      "Train: Epoch [13], Batch [653/938], Loss: 0.70182204246521\n",
      "Train: Epoch [13], Batch [654/938], Loss: 0.9617277383804321\n",
      "Train: Epoch [13], Batch [655/938], Loss: 0.7082362771034241\n",
      "Train: Epoch [13], Batch [656/938], Loss: 0.7520766258239746\n",
      "Train: Epoch [13], Batch [657/938], Loss: 0.554227888584137\n",
      "Train: Epoch [13], Batch [658/938], Loss: 0.5936693549156189\n",
      "Train: Epoch [13], Batch [659/938], Loss: 0.5034335255622864\n",
      "Train: Epoch [13], Batch [660/938], Loss: 0.7481626868247986\n",
      "Train: Epoch [13], Batch [661/938], Loss: 0.7819292545318604\n",
      "Train: Epoch [13], Batch [662/938], Loss: 0.748115062713623\n",
      "Train: Epoch [13], Batch [663/938], Loss: 0.7173439264297485\n",
      "Train: Epoch [13], Batch [664/938], Loss: 0.6153477430343628\n",
      "Train: Epoch [13], Batch [665/938], Loss: 0.8102521300315857\n",
      "Train: Epoch [13], Batch [666/938], Loss: 0.6423534154891968\n",
      "Train: Epoch [13], Batch [667/938], Loss: 0.6810678243637085\n",
      "Train: Epoch [13], Batch [668/938], Loss: 0.7886244058609009\n",
      "Train: Epoch [13], Batch [669/938], Loss: 0.9795928001403809\n",
      "Train: Epoch [13], Batch [670/938], Loss: 0.5666216611862183\n",
      "Train: Epoch [13], Batch [671/938], Loss: 0.8582793474197388\n",
      "Train: Epoch [13], Batch [672/938], Loss: 0.8895565867424011\n",
      "Train: Epoch [13], Batch [673/938], Loss: 0.7813115119934082\n",
      "Train: Epoch [13], Batch [674/938], Loss: 0.7323186993598938\n",
      "Train: Epoch [13], Batch [675/938], Loss: 0.6811671257019043\n",
      "Train: Epoch [13], Batch [676/938], Loss: 0.9421710968017578\n",
      "Train: Epoch [13], Batch [677/938], Loss: 0.6781026124954224\n",
      "Train: Epoch [13], Batch [678/938], Loss: 0.7005402445793152\n",
      "Train: Epoch [13], Batch [679/938], Loss: 0.6295948624610901\n",
      "Train: Epoch [13], Batch [680/938], Loss: 0.599574089050293\n",
      "Train: Epoch [13], Batch [681/938], Loss: 0.7742969989776611\n",
      "Train: Epoch [13], Batch [682/938], Loss: 0.6825398802757263\n",
      "Train: Epoch [13], Batch [683/938], Loss: 0.5638574361801147\n",
      "Train: Epoch [13], Batch [684/938], Loss: 0.5986738204956055\n",
      "Train: Epoch [13], Batch [685/938], Loss: 0.7352389097213745\n",
      "Train: Epoch [13], Batch [686/938], Loss: 0.5650356411933899\n",
      "Train: Epoch [13], Batch [687/938], Loss: 0.7155125141143799\n",
      "Train: Epoch [13], Batch [688/938], Loss: 0.5543560981750488\n",
      "Train: Epoch [13], Batch [689/938], Loss: 0.8379337191581726\n",
      "Train: Epoch [13], Batch [690/938], Loss: 0.9809656739234924\n",
      "Train: Epoch [13], Batch [691/938], Loss: 0.5358240008354187\n",
      "Train: Epoch [13], Batch [692/938], Loss: 0.7991594672203064\n",
      "Train: Epoch [13], Batch [693/938], Loss: 0.52088862657547\n",
      "Train: Epoch [13], Batch [694/938], Loss: 1.0246763229370117\n",
      "Train: Epoch [13], Batch [695/938], Loss: 0.6020655632019043\n",
      "Train: Epoch [13], Batch [696/938], Loss: 0.7565394639968872\n",
      "Train: Epoch [13], Batch [697/938], Loss: 0.4048093259334564\n",
      "Train: Epoch [13], Batch [698/938], Loss: 0.8415181040763855\n",
      "Train: Epoch [13], Batch [699/938], Loss: 0.8293401002883911\n",
      "Train: Epoch [13], Batch [700/938], Loss: 0.7765882611274719\n",
      "Train: Epoch [13], Batch [701/938], Loss: 0.5922513008117676\n",
      "Train: Epoch [13], Batch [702/938], Loss: 0.8432025909423828\n",
      "Train: Epoch [13], Batch [703/938], Loss: 0.8258029222488403\n",
      "Train: Epoch [13], Batch [704/938], Loss: 0.7785791754722595\n",
      "Train: Epoch [13], Batch [705/938], Loss: 0.5073779225349426\n",
      "Train: Epoch [13], Batch [706/938], Loss: 0.5764053463935852\n",
      "Train: Epoch [13], Batch [707/938], Loss: 0.7725605368614197\n",
      "Train: Epoch [13], Batch [708/938], Loss: 0.9605740904808044\n",
      "Train: Epoch [13], Batch [709/938], Loss: 0.6882819533348083\n",
      "Train: Epoch [13], Batch [710/938], Loss: 0.9095565676689148\n",
      "Train: Epoch [13], Batch [711/938], Loss: 0.7194531559944153\n",
      "Train: Epoch [13], Batch [712/938], Loss: 0.6568590998649597\n",
      "Train: Epoch [13], Batch [713/938], Loss: 0.6006836891174316\n",
      "Train: Epoch [13], Batch [714/938], Loss: 0.7866655588150024\n",
      "Train: Epoch [13], Batch [715/938], Loss: 0.7056599855422974\n",
      "Train: Epoch [13], Batch [716/938], Loss: 0.8889923691749573\n",
      "Train: Epoch [13], Batch [717/938], Loss: 0.6035927534103394\n",
      "Train: Epoch [13], Batch [718/938], Loss: 0.791131854057312\n",
      "Train: Epoch [13], Batch [719/938], Loss: 0.6260594725608826\n",
      "Train: Epoch [13], Batch [720/938], Loss: 0.9452384114265442\n",
      "Train: Epoch [13], Batch [721/938], Loss: 0.6249898672103882\n",
      "Train: Epoch [13], Batch [722/938], Loss: 0.8039544820785522\n",
      "Train: Epoch [13], Batch [723/938], Loss: 0.6075964570045471\n",
      "Train: Epoch [13], Batch [724/938], Loss: 0.7547265887260437\n",
      "Train: Epoch [13], Batch [725/938], Loss: 0.47728508710861206\n",
      "Train: Epoch [13], Batch [726/938], Loss: 0.6137624979019165\n",
      "Train: Epoch [13], Batch [727/938], Loss: 0.5958034992218018\n",
      "Train: Epoch [13], Batch [728/938], Loss: 0.61874920129776\n",
      "Train: Epoch [13], Batch [729/938], Loss: 0.5265007615089417\n",
      "Train: Epoch [13], Batch [730/938], Loss: 0.4129842519760132\n",
      "Train: Epoch [13], Batch [731/938], Loss: 0.8398760557174683\n",
      "Train: Epoch [13], Batch [732/938], Loss: 0.7739830017089844\n",
      "Train: Epoch [13], Batch [733/938], Loss: 0.6948420405387878\n",
      "Train: Epoch [13], Batch [734/938], Loss: 0.8443328142166138\n",
      "Train: Epoch [13], Batch [735/938], Loss: 0.6002275943756104\n",
      "Train: Epoch [13], Batch [736/938], Loss: 0.7704634070396423\n",
      "Train: Epoch [13], Batch [737/938], Loss: 0.904079020023346\n",
      "Train: Epoch [13], Batch [738/938], Loss: 0.6429892182350159\n",
      "Train: Epoch [13], Batch [739/938], Loss: 0.608185350894928\n",
      "Train: Epoch [13], Batch [740/938], Loss: 0.6645960807800293\n",
      "Train: Epoch [13], Batch [741/938], Loss: 0.6169842481613159\n",
      "Train: Epoch [13], Batch [742/938], Loss: 0.6544910669326782\n",
      "Train: Epoch [13], Batch [743/938], Loss: 0.9329281449317932\n",
      "Train: Epoch [13], Batch [744/938], Loss: 0.6846880912780762\n",
      "Train: Epoch [13], Batch [745/938], Loss: 0.7792367935180664\n",
      "Train: Epoch [13], Batch [746/938], Loss: 0.8182637095451355\n",
      "Train: Epoch [13], Batch [747/938], Loss: 0.7017050981521606\n",
      "Train: Epoch [13], Batch [748/938], Loss: 0.637657105922699\n",
      "Train: Epoch [13], Batch [749/938], Loss: 0.8797428607940674\n",
      "Train: Epoch [13], Batch [750/938], Loss: 0.8876019716262817\n",
      "Train: Epoch [13], Batch [751/938], Loss: 0.7486149072647095\n",
      "Train: Epoch [13], Batch [752/938], Loss: 0.7757782936096191\n",
      "Train: Epoch [13], Batch [753/938], Loss: 0.8144626617431641\n",
      "Train: Epoch [13], Batch [754/938], Loss: 0.7423034310340881\n",
      "Train: Epoch [13], Batch [755/938], Loss: 0.7018933296203613\n",
      "Train: Epoch [13], Batch [756/938], Loss: 0.6541711688041687\n",
      "Train: Epoch [13], Batch [757/938], Loss: 0.7748372554779053\n",
      "Train: Epoch [13], Batch [758/938], Loss: 0.7278512120246887\n",
      "Train: Epoch [13], Batch [759/938], Loss: 0.6934515237808228\n",
      "Train: Epoch [13], Batch [760/938], Loss: 0.6847875118255615\n",
      "Train: Epoch [13], Batch [761/938], Loss: 0.6970752477645874\n",
      "Train: Epoch [13], Batch [762/938], Loss: 0.8041024804115295\n",
      "Train: Epoch [13], Batch [763/938], Loss: 0.8014925122261047\n",
      "Train: Epoch [13], Batch [764/938], Loss: 0.7822657823562622\n",
      "Train: Epoch [13], Batch [765/938], Loss: 0.7976205348968506\n",
      "Train: Epoch [13], Batch [766/938], Loss: 0.9619027972221375\n",
      "Train: Epoch [13], Batch [767/938], Loss: 0.5239739418029785\n",
      "Train: Epoch [13], Batch [768/938], Loss: 0.7866138219833374\n",
      "Train: Epoch [13], Batch [769/938], Loss: 0.48996540904045105\n",
      "Train: Epoch [13], Batch [770/938], Loss: 0.6537880301475525\n",
      "Train: Epoch [13], Batch [771/938], Loss: 0.6461530327796936\n",
      "Train: Epoch [13], Batch [772/938], Loss: 0.9303355813026428\n",
      "Train: Epoch [13], Batch [773/938], Loss: 0.6197203993797302\n",
      "Train: Epoch [13], Batch [774/938], Loss: 0.5616040229797363\n",
      "Train: Epoch [13], Batch [775/938], Loss: 0.8641930818557739\n",
      "Train: Epoch [13], Batch [776/938], Loss: 0.8423901200294495\n",
      "Train: Epoch [13], Batch [777/938], Loss: 0.8669400811195374\n",
      "Train: Epoch [13], Batch [778/938], Loss: 0.667137622833252\n",
      "Train: Epoch [13], Batch [779/938], Loss: 0.5714643597602844\n",
      "Train: Epoch [13], Batch [780/938], Loss: 0.8634175062179565\n",
      "Train: Epoch [13], Batch [781/938], Loss: 0.7029164433479309\n",
      "Train: Epoch [13], Batch [782/938], Loss: 0.6780881881713867\n",
      "Train: Epoch [13], Batch [783/938], Loss: 0.6550822257995605\n",
      "Train: Epoch [13], Batch [784/938], Loss: 0.7432202696800232\n",
      "Train: Epoch [13], Batch [785/938], Loss: 0.6772656440734863\n",
      "Train: Epoch [13], Batch [786/938], Loss: 0.670495867729187\n",
      "Train: Epoch [13], Batch [787/938], Loss: 0.9363106489181519\n",
      "Train: Epoch [13], Batch [788/938], Loss: 0.876532256603241\n",
      "Train: Epoch [13], Batch [789/938], Loss: 0.8674938082695007\n",
      "Train: Epoch [13], Batch [790/938], Loss: 0.5743849277496338\n",
      "Train: Epoch [13], Batch [791/938], Loss: 0.695641279220581\n",
      "Train: Epoch [13], Batch [792/938], Loss: 0.898973822593689\n",
      "Train: Epoch [13], Batch [793/938], Loss: 0.792698860168457\n",
      "Train: Epoch [13], Batch [794/938], Loss: 0.5799211263656616\n",
      "Train: Epoch [13], Batch [795/938], Loss: 0.8287680149078369\n",
      "Train: Epoch [13], Batch [796/938], Loss: 0.7875078916549683\n",
      "Train: Epoch [13], Batch [797/938], Loss: 0.6599899530410767\n",
      "Train: Epoch [13], Batch [798/938], Loss: 0.7438318133354187\n",
      "Train: Epoch [13], Batch [799/938], Loss: 0.7324773669242859\n",
      "Train: Epoch [13], Batch [800/938], Loss: 0.7933449745178223\n",
      "Train: Epoch [13], Batch [801/938], Loss: 0.8961930274963379\n",
      "Train: Epoch [13], Batch [802/938], Loss: 0.5262913107872009\n",
      "Train: Epoch [13], Batch [803/938], Loss: 0.6775299310684204\n",
      "Train: Epoch [13], Batch [804/938], Loss: 0.7398505210876465\n",
      "Train: Epoch [13], Batch [805/938], Loss: 0.7612026929855347\n",
      "Train: Epoch [13], Batch [806/938], Loss: 0.7093828916549683\n",
      "Train: Epoch [13], Batch [807/938], Loss: 0.7306874990463257\n",
      "Train: Epoch [13], Batch [808/938], Loss: 0.5094090104103088\n",
      "Train: Epoch [13], Batch [809/938], Loss: 0.6389985084533691\n",
      "Train: Epoch [13], Batch [810/938], Loss: 0.7356807589530945\n",
      "Train: Epoch [13], Batch [811/938], Loss: 0.9123108386993408\n",
      "Train: Epoch [13], Batch [812/938], Loss: 0.6255102157592773\n",
      "Train: Epoch [13], Batch [813/938], Loss: 0.8169848322868347\n",
      "Train: Epoch [13], Batch [814/938], Loss: 0.908516526222229\n",
      "Train: Epoch [13], Batch [815/938], Loss: 0.6844995021820068\n",
      "Train: Epoch [13], Batch [816/938], Loss: 1.1184231042861938\n",
      "Train: Epoch [13], Batch [817/938], Loss: 0.8135480284690857\n",
      "Train: Epoch [13], Batch [818/938], Loss: 0.5918514132499695\n",
      "Train: Epoch [13], Batch [819/938], Loss: 0.7842259407043457\n",
      "Train: Epoch [13], Batch [820/938], Loss: 0.6351751089096069\n",
      "Train: Epoch [13], Batch [821/938], Loss: 0.6724458932876587\n",
      "Train: Epoch [13], Batch [822/938], Loss: 0.7242835164070129\n",
      "Train: Epoch [13], Batch [823/938], Loss: 0.8422555923461914\n",
      "Train: Epoch [13], Batch [824/938], Loss: 0.6476737260818481\n",
      "Train: Epoch [13], Batch [825/938], Loss: 0.8701163530349731\n",
      "Train: Epoch [13], Batch [826/938], Loss: 0.7587614059448242\n",
      "Train: Epoch [13], Batch [827/938], Loss: 0.8108577132225037\n",
      "Train: Epoch [13], Batch [828/938], Loss: 0.6406300663948059\n",
      "Train: Epoch [13], Batch [829/938], Loss: 0.8367165327072144\n",
      "Train: Epoch [13], Batch [830/938], Loss: 0.6554341316223145\n",
      "Train: Epoch [13], Batch [831/938], Loss: 0.8731005191802979\n",
      "Train: Epoch [13], Batch [832/938], Loss: 0.6929983496665955\n",
      "Train: Epoch [13], Batch [833/938], Loss: 0.7201781272888184\n",
      "Train: Epoch [13], Batch [834/938], Loss: 0.7069964408874512\n",
      "Train: Epoch [13], Batch [835/938], Loss: 0.6457173228263855\n",
      "Train: Epoch [13], Batch [836/938], Loss: 0.2956538200378418\n",
      "Train: Epoch [13], Batch [837/938], Loss: 0.7518683671951294\n",
      "Train: Epoch [13], Batch [838/938], Loss: 0.6528592705726624\n",
      "Train: Epoch [13], Batch [839/938], Loss: 0.6126520037651062\n",
      "Train: Epoch [13], Batch [840/938], Loss: 0.5588785409927368\n",
      "Train: Epoch [13], Batch [841/938], Loss: 0.9410461187362671\n",
      "Train: Epoch [13], Batch [842/938], Loss: 0.5476884841918945\n",
      "Train: Epoch [13], Batch [843/938], Loss: 0.656740128993988\n",
      "Train: Epoch [13], Batch [844/938], Loss: 0.7073004245758057\n",
      "Train: Epoch [13], Batch [845/938], Loss: 0.7030650973320007\n",
      "Train: Epoch [13], Batch [846/938], Loss: 0.7434138059616089\n",
      "Train: Epoch [13], Batch [847/938], Loss: 0.6540888547897339\n",
      "Train: Epoch [13], Batch [848/938], Loss: 0.9338202476501465\n",
      "Train: Epoch [13], Batch [849/938], Loss: 0.46259605884552\n",
      "Train: Epoch [13], Batch [850/938], Loss: 0.575171709060669\n",
      "Train: Epoch [13], Batch [851/938], Loss: 0.5095644593238831\n",
      "Train: Epoch [13], Batch [852/938], Loss: 0.7723318338394165\n",
      "Train: Epoch [13], Batch [853/938], Loss: 0.7292162179946899\n",
      "Train: Epoch [13], Batch [854/938], Loss: 0.7211675643920898\n",
      "Train: Epoch [13], Batch [855/938], Loss: 0.7175425887107849\n",
      "Train: Epoch [13], Batch [856/938], Loss: 0.8195005059242249\n",
      "Train: Epoch [13], Batch [857/938], Loss: 0.6341183185577393\n",
      "Train: Epoch [13], Batch [858/938], Loss: 0.6475915908813477\n",
      "Train: Epoch [13], Batch [859/938], Loss: 0.989764928817749\n",
      "Train: Epoch [13], Batch [860/938], Loss: 0.8333387970924377\n",
      "Train: Epoch [13], Batch [861/938], Loss: 0.8641630411148071\n",
      "Train: Epoch [13], Batch [862/938], Loss: 0.4535970389842987\n",
      "Train: Epoch [13], Batch [863/938], Loss: 0.6347799897193909\n",
      "Train: Epoch [13], Batch [864/938], Loss: 0.6522040367126465\n",
      "Train: Epoch [13], Batch [865/938], Loss: 0.6756540536880493\n",
      "Train: Epoch [13], Batch [866/938], Loss: 0.6056827306747437\n",
      "Train: Epoch [13], Batch [867/938], Loss: 0.6152024865150452\n",
      "Train: Epoch [13], Batch [868/938], Loss: 0.7346504926681519\n",
      "Train: Epoch [13], Batch [869/938], Loss: 0.6522112488746643\n",
      "Train: Epoch [13], Batch [870/938], Loss: 0.48214951157569885\n",
      "Train: Epoch [13], Batch [871/938], Loss: 0.6448728442192078\n",
      "Train: Epoch [13], Batch [872/938], Loss: 0.6208890080451965\n",
      "Train: Epoch [13], Batch [873/938], Loss: 0.6788383722305298\n",
      "Train: Epoch [13], Batch [874/938], Loss: 0.6350972652435303\n",
      "Train: Epoch [13], Batch [875/938], Loss: 0.9110743999481201\n",
      "Train: Epoch [13], Batch [876/938], Loss: 0.5784461498260498\n",
      "Train: Epoch [13], Batch [877/938], Loss: 0.7631140351295471\n",
      "Train: Epoch [13], Batch [878/938], Loss: 0.5574807524681091\n",
      "Train: Epoch [13], Batch [879/938], Loss: 0.7014693021774292\n",
      "Train: Epoch [13], Batch [880/938], Loss: 0.8386363983154297\n",
      "Train: Epoch [13], Batch [881/938], Loss: 0.684093713760376\n",
      "Train: Epoch [13], Batch [882/938], Loss: 0.7786070108413696\n",
      "Train: Epoch [13], Batch [883/938], Loss: 0.6118801236152649\n",
      "Train: Epoch [13], Batch [884/938], Loss: 0.9476990699768066\n",
      "Train: Epoch [13], Batch [885/938], Loss: 0.6979227662086487\n",
      "Train: Epoch [13], Batch [886/938], Loss: 0.7337117195129395\n",
      "Train: Epoch [13], Batch [887/938], Loss: 0.853758692741394\n",
      "Train: Epoch [13], Batch [888/938], Loss: 0.9234899878501892\n",
      "Train: Epoch [13], Batch [889/938], Loss: 0.8191090822219849\n",
      "Train: Epoch [13], Batch [890/938], Loss: 0.734340250492096\n",
      "Train: Epoch [13], Batch [891/938], Loss: 0.5866079330444336\n",
      "Train: Epoch [13], Batch [892/938], Loss: 0.7446597218513489\n",
      "Train: Epoch [13], Batch [893/938], Loss: 0.6161015629768372\n",
      "Train: Epoch [13], Batch [894/938], Loss: 0.7228882312774658\n",
      "Train: Epoch [13], Batch [895/938], Loss: 0.644400417804718\n",
      "Train: Epoch [13], Batch [896/938], Loss: 0.6409429311752319\n",
      "Train: Epoch [13], Batch [897/938], Loss: 0.5995482206344604\n",
      "Train: Epoch [13], Batch [898/938], Loss: 0.6875858306884766\n",
      "Train: Epoch [13], Batch [899/938], Loss: 0.7722327709197998\n",
      "Train: Epoch [13], Batch [900/938], Loss: 0.42114555835723877\n",
      "Train: Epoch [13], Batch [901/938], Loss: 0.713769793510437\n",
      "Train: Epoch [13], Batch [902/938], Loss: 0.6440085768699646\n",
      "Train: Epoch [13], Batch [903/938], Loss: 0.8244859576225281\n",
      "Train: Epoch [13], Batch [904/938], Loss: 0.5606851577758789\n",
      "Train: Epoch [13], Batch [905/938], Loss: 0.7697010040283203\n",
      "Train: Epoch [13], Batch [906/938], Loss: 0.6091771721839905\n",
      "Train: Epoch [13], Batch [907/938], Loss: 0.4950654208660126\n",
      "Train: Epoch [13], Batch [908/938], Loss: 0.5993150472640991\n",
      "Train: Epoch [13], Batch [909/938], Loss: 0.8975956439971924\n",
      "Train: Epoch [13], Batch [910/938], Loss: 0.5924966335296631\n",
      "Train: Epoch [13], Batch [911/938], Loss: 0.4523416757583618\n",
      "Train: Epoch [13], Batch [912/938], Loss: 0.9233227968215942\n",
      "Train: Epoch [13], Batch [913/938], Loss: 0.6952013373374939\n",
      "Train: Epoch [13], Batch [914/938], Loss: 0.7390860319137573\n",
      "Train: Epoch [13], Batch [915/938], Loss: 0.6671305298805237\n",
      "Train: Epoch [13], Batch [916/938], Loss: 0.5651019811630249\n",
      "Train: Epoch [13], Batch [917/938], Loss: 0.5261925458908081\n",
      "Train: Epoch [13], Batch [918/938], Loss: 0.8625184297561646\n",
      "Train: Epoch [13], Batch [919/938], Loss: 0.8279078602790833\n",
      "Train: Epoch [13], Batch [920/938], Loss: 0.6724752187728882\n",
      "Train: Epoch [13], Batch [921/938], Loss: 0.6477274894714355\n",
      "Train: Epoch [13], Batch [922/938], Loss: 0.7263076305389404\n",
      "Train: Epoch [13], Batch [923/938], Loss: 0.8224020004272461\n",
      "Train: Epoch [13], Batch [924/938], Loss: 0.6636873483657837\n",
      "Train: Epoch [13], Batch [925/938], Loss: 0.5608983635902405\n",
      "Train: Epoch [13], Batch [926/938], Loss: 0.42917266488075256\n",
      "Train: Epoch [13], Batch [927/938], Loss: 0.662602961063385\n",
      "Train: Epoch [13], Batch [928/938], Loss: 0.7554923892021179\n",
      "Train: Epoch [13], Batch [929/938], Loss: 0.7385798096656799\n",
      "Train: Epoch [13], Batch [930/938], Loss: 0.8013706207275391\n",
      "Train: Epoch [13], Batch [931/938], Loss: 0.6496678590774536\n",
      "Train: Epoch [13], Batch [932/938], Loss: 0.7687097787857056\n",
      "Train: Epoch [13], Batch [933/938], Loss: 0.8300259113311768\n",
      "Train: Epoch [13], Batch [934/938], Loss: 0.8100714087486267\n",
      "Train: Epoch [13], Batch [935/938], Loss: 0.7723070383071899\n",
      "Train: Epoch [13], Batch [936/938], Loss: 0.6851428747177124\n",
      "Train: Epoch [13], Batch [937/938], Loss: 0.7953762412071228\n",
      "Train: Epoch [13], Batch [938/938], Loss: 1.0040297508239746\n",
      "Accuracy of train set: 0.7781833333333333\n",
      "Validation: Epoch [13], Batch [1/938], Loss: 0.5584585070610046\n",
      "Validation: Epoch [13], Batch [2/938], Loss: 0.8137146234512329\n",
      "Validation: Epoch [13], Batch [3/938], Loss: 0.5507793426513672\n",
      "Validation: Epoch [13], Batch [4/938], Loss: 0.6722885370254517\n",
      "Validation: Epoch [13], Batch [5/938], Loss: 0.9096552133560181\n",
      "Validation: Epoch [13], Batch [6/938], Loss: 0.7499126195907593\n",
      "Validation: Epoch [13], Batch [7/938], Loss: 0.36099332571029663\n",
      "Validation: Epoch [13], Batch [8/938], Loss: 0.893606960773468\n",
      "Validation: Epoch [13], Batch [9/938], Loss: 0.7747642993927002\n",
      "Validation: Epoch [13], Batch [10/938], Loss: 0.6219162940979004\n",
      "Validation: Epoch [13], Batch [11/938], Loss: 0.5482242703437805\n",
      "Validation: Epoch [13], Batch [12/938], Loss: 0.7333911657333374\n",
      "Validation: Epoch [13], Batch [13/938], Loss: 0.7136421203613281\n",
      "Validation: Epoch [13], Batch [14/938], Loss: 0.6837266683578491\n",
      "Validation: Epoch [13], Batch [15/938], Loss: 0.6664538383483887\n",
      "Validation: Epoch [13], Batch [16/938], Loss: 0.7315903902053833\n",
      "Validation: Epoch [13], Batch [17/938], Loss: 0.48301276564598083\n",
      "Validation: Epoch [13], Batch [18/938], Loss: 0.7343708872795105\n",
      "Validation: Epoch [13], Batch [19/938], Loss: 0.5924674272537231\n",
      "Validation: Epoch [13], Batch [20/938], Loss: 0.9850456714630127\n",
      "Validation: Epoch [13], Batch [21/938], Loss: 0.623767614364624\n",
      "Validation: Epoch [13], Batch [22/938], Loss: 0.640129804611206\n",
      "Validation: Epoch [13], Batch [23/938], Loss: 0.48057013750076294\n",
      "Validation: Epoch [13], Batch [24/938], Loss: 0.6425557732582092\n",
      "Validation: Epoch [13], Batch [25/938], Loss: 0.6415033340454102\n",
      "Validation: Epoch [13], Batch [26/938], Loss: 0.6186387538909912\n",
      "Validation: Epoch [13], Batch [27/938], Loss: 0.7234960794448853\n",
      "Validation: Epoch [13], Batch [28/938], Loss: 0.9524877071380615\n",
      "Validation: Epoch [13], Batch [29/938], Loss: 0.6889063715934753\n",
      "Validation: Epoch [13], Batch [30/938], Loss: 0.6375516653060913\n",
      "Validation: Epoch [13], Batch [31/938], Loss: 0.6919124126434326\n",
      "Validation: Epoch [13], Batch [32/938], Loss: 0.5033108592033386\n",
      "Validation: Epoch [13], Batch [33/938], Loss: 0.663367748260498\n",
      "Validation: Epoch [13], Batch [34/938], Loss: 0.754467785358429\n",
      "Validation: Epoch [13], Batch [35/938], Loss: 0.5901479721069336\n",
      "Validation: Epoch [13], Batch [36/938], Loss: 0.7367152571678162\n",
      "Validation: Epoch [13], Batch [37/938], Loss: 0.7089698910713196\n",
      "Validation: Epoch [13], Batch [38/938], Loss: 0.5259609222412109\n",
      "Validation: Epoch [13], Batch [39/938], Loss: 0.7449533343315125\n",
      "Validation: Epoch [13], Batch [40/938], Loss: 0.7854374647140503\n",
      "Validation: Epoch [13], Batch [41/938], Loss: 0.8172281980514526\n",
      "Validation: Epoch [13], Batch [42/938], Loss: 0.6888134479522705\n",
      "Validation: Epoch [13], Batch [43/938], Loss: 0.6073583364486694\n",
      "Validation: Epoch [13], Batch [44/938], Loss: 0.8013763427734375\n",
      "Validation: Epoch [13], Batch [45/938], Loss: 0.6899906396865845\n",
      "Validation: Epoch [13], Batch [46/938], Loss: 0.8341174721717834\n",
      "Validation: Epoch [13], Batch [47/938], Loss: 0.6387695670127869\n",
      "Validation: Epoch [13], Batch [48/938], Loss: 0.5561861395835876\n",
      "Validation: Epoch [13], Batch [49/938], Loss: 0.6169291734695435\n",
      "Validation: Epoch [13], Batch [50/938], Loss: 0.8646650910377502\n",
      "Validation: Epoch [13], Batch [51/938], Loss: 0.9961938858032227\n",
      "Validation: Epoch [13], Batch [52/938], Loss: 0.5886270403862\n",
      "Validation: Epoch [13], Batch [53/938], Loss: 0.6087989211082458\n",
      "Validation: Epoch [13], Batch [54/938], Loss: 0.9279452562332153\n",
      "Validation: Epoch [13], Batch [55/938], Loss: 0.48584479093551636\n",
      "Validation: Epoch [13], Batch [56/938], Loss: 0.6260348558425903\n",
      "Validation: Epoch [13], Batch [57/938], Loss: 0.6886662244796753\n",
      "Validation: Epoch [13], Batch [58/938], Loss: 0.5975184440612793\n",
      "Validation: Epoch [13], Batch [59/938], Loss: 0.5836367011070251\n",
      "Validation: Epoch [13], Batch [60/938], Loss: 0.8591229319572449\n",
      "Validation: Epoch [13], Batch [61/938], Loss: 0.3960610628128052\n",
      "Validation: Epoch [13], Batch [62/938], Loss: 0.6612429022789001\n",
      "Validation: Epoch [13], Batch [63/938], Loss: 0.5770140886306763\n",
      "Validation: Epoch [13], Batch [64/938], Loss: 0.6319989562034607\n",
      "Validation: Epoch [13], Batch [65/938], Loss: 0.8234703540802002\n",
      "Validation: Epoch [13], Batch [66/938], Loss: 0.7792539596557617\n",
      "Validation: Epoch [13], Batch [67/938], Loss: 0.7226207852363586\n",
      "Validation: Epoch [13], Batch [68/938], Loss: 0.9329080581665039\n",
      "Validation: Epoch [13], Batch [69/938], Loss: 0.6763520836830139\n",
      "Validation: Epoch [13], Batch [70/938], Loss: 0.6732427477836609\n",
      "Validation: Epoch [13], Batch [71/938], Loss: 0.6154466867446899\n",
      "Validation: Epoch [13], Batch [72/938], Loss: 0.954872190952301\n",
      "Validation: Epoch [13], Batch [73/938], Loss: 0.8276593685150146\n",
      "Validation: Epoch [13], Batch [74/938], Loss: 0.7776714563369751\n",
      "Validation: Epoch [13], Batch [75/938], Loss: 0.5194442272186279\n",
      "Validation: Epoch [13], Batch [76/938], Loss: 0.9521931409835815\n",
      "Validation: Epoch [13], Batch [77/938], Loss: 0.6767405271530151\n",
      "Validation: Epoch [13], Batch [78/938], Loss: 0.7070689797401428\n",
      "Validation: Epoch [13], Batch [79/938], Loss: 0.5730851292610168\n",
      "Validation: Epoch [13], Batch [80/938], Loss: 0.777639627456665\n",
      "Validation: Epoch [13], Batch [81/938], Loss: 0.6043059229850769\n",
      "Validation: Epoch [13], Batch [82/938], Loss: 0.7205889225006104\n",
      "Validation: Epoch [13], Batch [83/938], Loss: 0.6940211057662964\n",
      "Validation: Epoch [13], Batch [84/938], Loss: 0.4212628901004791\n",
      "Validation: Epoch [13], Batch [85/938], Loss: 0.8998175263404846\n",
      "Validation: Epoch [13], Batch [86/938], Loss: 0.6926733255386353\n",
      "Validation: Epoch [13], Batch [87/938], Loss: 0.6923899054527283\n",
      "Validation: Epoch [13], Batch [88/938], Loss: 0.823712170124054\n",
      "Validation: Epoch [13], Batch [89/938], Loss: 1.006901502609253\n",
      "Validation: Epoch [13], Batch [90/938], Loss: 1.0046905279159546\n",
      "Validation: Epoch [13], Batch [91/938], Loss: 0.5982089638710022\n",
      "Validation: Epoch [13], Batch [92/938], Loss: 0.7041211724281311\n",
      "Validation: Epoch [13], Batch [93/938], Loss: 0.6367919445037842\n",
      "Validation: Epoch [13], Batch [94/938], Loss: 0.6347076296806335\n",
      "Validation: Epoch [13], Batch [95/938], Loss: 0.7003346085548401\n",
      "Validation: Epoch [13], Batch [96/938], Loss: 0.6431692838668823\n",
      "Validation: Epoch [13], Batch [97/938], Loss: 0.8834603428840637\n",
      "Validation: Epoch [13], Batch [98/938], Loss: 1.1332765817642212\n",
      "Validation: Epoch [13], Batch [99/938], Loss: 0.8076595664024353\n",
      "Validation: Epoch [13], Batch [100/938], Loss: 0.6950669288635254\n",
      "Validation: Epoch [13], Batch [101/938], Loss: 0.641965925693512\n",
      "Validation: Epoch [13], Batch [102/938], Loss: 0.8415370583534241\n",
      "Validation: Epoch [13], Batch [103/938], Loss: 0.6730213761329651\n",
      "Validation: Epoch [13], Batch [104/938], Loss: 0.6241386532783508\n",
      "Validation: Epoch [13], Batch [105/938], Loss: 0.6940354704856873\n",
      "Validation: Epoch [13], Batch [106/938], Loss: 0.966525673866272\n",
      "Validation: Epoch [13], Batch [107/938], Loss: 0.6817300319671631\n",
      "Validation: Epoch [13], Batch [108/938], Loss: 0.792057454586029\n",
      "Validation: Epoch [13], Batch [109/938], Loss: 0.8239744901657104\n",
      "Validation: Epoch [13], Batch [110/938], Loss: 0.7983030080795288\n",
      "Validation: Epoch [13], Batch [111/938], Loss: 0.6490244269371033\n",
      "Validation: Epoch [13], Batch [112/938], Loss: 0.8321453332901001\n",
      "Validation: Epoch [13], Batch [113/938], Loss: 0.7717748880386353\n",
      "Validation: Epoch [13], Batch [114/938], Loss: 0.7168347835540771\n",
      "Validation: Epoch [13], Batch [115/938], Loss: 0.8550361394882202\n",
      "Validation: Epoch [13], Batch [116/938], Loss: 0.8615014553070068\n",
      "Validation: Epoch [13], Batch [117/938], Loss: 0.6149571537971497\n",
      "Validation: Epoch [13], Batch [118/938], Loss: 0.6645888090133667\n",
      "Validation: Epoch [13], Batch [119/938], Loss: 0.7088770866394043\n",
      "Validation: Epoch [13], Batch [120/938], Loss: 0.6919058561325073\n",
      "Validation: Epoch [13], Batch [121/938], Loss: 0.8644568920135498\n",
      "Validation: Epoch [13], Batch [122/938], Loss: 0.859227180480957\n",
      "Validation: Epoch [13], Batch [123/938], Loss: 0.6499454379081726\n",
      "Validation: Epoch [13], Batch [124/938], Loss: 0.6745167970657349\n",
      "Validation: Epoch [13], Batch [125/938], Loss: 0.7560392618179321\n",
      "Validation: Epoch [13], Batch [126/938], Loss: 0.8895230293273926\n",
      "Validation: Epoch [13], Batch [127/938], Loss: 0.8908075094223022\n",
      "Validation: Epoch [13], Batch [128/938], Loss: 0.740850031375885\n",
      "Validation: Epoch [13], Batch [129/938], Loss: 0.6894769072532654\n",
      "Validation: Epoch [13], Batch [130/938], Loss: 0.8455240726470947\n",
      "Validation: Epoch [13], Batch [131/938], Loss: 0.8513967394828796\n",
      "Validation: Epoch [13], Batch [132/938], Loss: 0.9294095039367676\n",
      "Validation: Epoch [13], Batch [133/938], Loss: 0.876917839050293\n",
      "Validation: Epoch [13], Batch [134/938], Loss: 0.9314693212509155\n",
      "Validation: Epoch [13], Batch [135/938], Loss: 0.6591974496841431\n",
      "Validation: Epoch [13], Batch [136/938], Loss: 0.7282904386520386\n",
      "Validation: Epoch [13], Batch [137/938], Loss: 0.9673188328742981\n",
      "Validation: Epoch [13], Batch [138/938], Loss: 0.5971848368644714\n",
      "Validation: Epoch [13], Batch [139/938], Loss: 0.9247999787330627\n",
      "Validation: Epoch [13], Batch [140/938], Loss: 0.8412020802497864\n",
      "Validation: Epoch [13], Batch [141/938], Loss: 0.6051868796348572\n",
      "Validation: Epoch [13], Batch [142/938], Loss: 0.654205322265625\n",
      "Validation: Epoch [13], Batch [143/938], Loss: 0.6838899850845337\n",
      "Validation: Epoch [13], Batch [144/938], Loss: 0.6453797221183777\n",
      "Validation: Epoch [13], Batch [145/938], Loss: 0.7102683782577515\n",
      "Validation: Epoch [13], Batch [146/938], Loss: 0.753473699092865\n",
      "Validation: Epoch [13], Batch [147/938], Loss: 0.5806823372840881\n",
      "Validation: Epoch [13], Batch [148/938], Loss: 0.95734041929245\n",
      "Validation: Epoch [13], Batch [149/938], Loss: 0.3950830399990082\n",
      "Validation: Epoch [13], Batch [150/938], Loss: 0.9059027433395386\n",
      "Validation: Epoch [13], Batch [151/938], Loss: 0.5884922742843628\n",
      "Validation: Epoch [13], Batch [152/938], Loss: 0.5282652378082275\n",
      "Validation: Epoch [13], Batch [153/938], Loss: 0.5135551691055298\n",
      "Validation: Epoch [13], Batch [154/938], Loss: 0.5569117069244385\n",
      "Validation: Epoch [13], Batch [155/938], Loss: 0.5137124061584473\n",
      "Validation: Epoch [13], Batch [156/938], Loss: 0.8587416410446167\n",
      "Validation: Epoch [13], Batch [157/938], Loss: 0.6898288130760193\n",
      "Validation: Epoch [13], Batch [158/938], Loss: 0.4789912700653076\n",
      "Validation: Epoch [13], Batch [159/938], Loss: 0.5781660676002502\n",
      "Validation: Epoch [13], Batch [160/938], Loss: 0.7822848558425903\n",
      "Validation: Epoch [13], Batch [161/938], Loss: 0.715867280960083\n",
      "Validation: Epoch [13], Batch [162/938], Loss: 0.5060521960258484\n",
      "Validation: Epoch [13], Batch [163/938], Loss: 0.6136167645454407\n",
      "Validation: Epoch [13], Batch [164/938], Loss: 0.636072039604187\n",
      "Validation: Epoch [13], Batch [165/938], Loss: 0.606147289276123\n",
      "Validation: Epoch [13], Batch [166/938], Loss: 0.7382444143295288\n",
      "Validation: Epoch [13], Batch [167/938], Loss: 0.8455148935317993\n",
      "Validation: Epoch [13], Batch [168/938], Loss: 0.6377348899841309\n",
      "Validation: Epoch [13], Batch [169/938], Loss: 0.6584717035293579\n",
      "Validation: Epoch [13], Batch [170/938], Loss: 0.8734667301177979\n",
      "Validation: Epoch [13], Batch [171/938], Loss: 0.7698953747749329\n",
      "Validation: Epoch [13], Batch [172/938], Loss: 0.6916113495826721\n",
      "Validation: Epoch [13], Batch [173/938], Loss: 0.5237749814987183\n",
      "Validation: Epoch [13], Batch [174/938], Loss: 0.46734240651130676\n",
      "Validation: Epoch [13], Batch [175/938], Loss: 0.8660688400268555\n",
      "Validation: Epoch [13], Batch [176/938], Loss: 0.7212660908699036\n",
      "Validation: Epoch [13], Batch [177/938], Loss: 0.6484066843986511\n",
      "Validation: Epoch [13], Batch [178/938], Loss: 0.9198200106620789\n",
      "Validation: Epoch [13], Batch [179/938], Loss: 0.6906788349151611\n",
      "Validation: Epoch [13], Batch [180/938], Loss: 0.7364648580551147\n",
      "Validation: Epoch [13], Batch [181/938], Loss: 0.7406468987464905\n",
      "Validation: Epoch [13], Batch [182/938], Loss: 0.8729068040847778\n",
      "Validation: Epoch [13], Batch [183/938], Loss: 0.7968782782554626\n",
      "Validation: Epoch [13], Batch [184/938], Loss: 0.6614065766334534\n",
      "Validation: Epoch [13], Batch [185/938], Loss: 0.9234464764595032\n",
      "Validation: Epoch [13], Batch [186/938], Loss: 0.7246583104133606\n",
      "Validation: Epoch [13], Batch [187/938], Loss: 0.8475714325904846\n",
      "Validation: Epoch [13], Batch [188/938], Loss: 0.7374800443649292\n",
      "Validation: Epoch [13], Batch [189/938], Loss: 0.47754940390586853\n",
      "Validation: Epoch [13], Batch [190/938], Loss: 0.7331531047821045\n",
      "Validation: Epoch [13], Batch [191/938], Loss: 0.4549820125102997\n",
      "Validation: Epoch [13], Batch [192/938], Loss: 0.8518781661987305\n",
      "Validation: Epoch [13], Batch [193/938], Loss: 0.5293325185775757\n",
      "Validation: Epoch [13], Batch [194/938], Loss: 0.7094311714172363\n",
      "Validation: Epoch [13], Batch [195/938], Loss: 0.6231579780578613\n",
      "Validation: Epoch [13], Batch [196/938], Loss: 0.9185827970504761\n",
      "Validation: Epoch [13], Batch [197/938], Loss: 0.60108882188797\n",
      "Validation: Epoch [13], Batch [198/938], Loss: 0.6649417877197266\n",
      "Validation: Epoch [13], Batch [199/938], Loss: 0.6191506385803223\n",
      "Validation: Epoch [13], Batch [200/938], Loss: 0.7370145320892334\n",
      "Validation: Epoch [13], Batch [201/938], Loss: 0.6689401865005493\n",
      "Validation: Epoch [13], Batch [202/938], Loss: 0.8095396161079407\n",
      "Validation: Epoch [13], Batch [203/938], Loss: 0.6837160587310791\n",
      "Validation: Epoch [13], Batch [204/938], Loss: 0.5711121559143066\n",
      "Validation: Epoch [13], Batch [205/938], Loss: 0.7901816368103027\n",
      "Validation: Epoch [13], Batch [206/938], Loss: 0.4460301399230957\n",
      "Validation: Epoch [13], Batch [207/938], Loss: 0.6790096759796143\n",
      "Validation: Epoch [13], Batch [208/938], Loss: 0.7669625878334045\n",
      "Validation: Epoch [13], Batch [209/938], Loss: 0.8646765947341919\n",
      "Validation: Epoch [13], Batch [210/938], Loss: 0.8280391693115234\n",
      "Validation: Epoch [13], Batch [211/938], Loss: 0.5712762475013733\n",
      "Validation: Epoch [13], Batch [212/938], Loss: 0.6843812465667725\n",
      "Validation: Epoch [13], Batch [213/938], Loss: 0.6151589155197144\n",
      "Validation: Epoch [13], Batch [214/938], Loss: 0.7690789103507996\n",
      "Validation: Epoch [13], Batch [215/938], Loss: 0.6621174812316895\n",
      "Validation: Epoch [13], Batch [216/938], Loss: 0.815837562084198\n",
      "Validation: Epoch [13], Batch [217/938], Loss: 0.8616959452629089\n",
      "Validation: Epoch [13], Batch [218/938], Loss: 0.8783908486366272\n",
      "Validation: Epoch [13], Batch [219/938], Loss: 0.7602290511131287\n",
      "Validation: Epoch [13], Batch [220/938], Loss: 0.6316065788269043\n",
      "Validation: Epoch [13], Batch [221/938], Loss: 0.8580063581466675\n",
      "Validation: Epoch [13], Batch [222/938], Loss: 0.7390820980072021\n",
      "Validation: Epoch [13], Batch [223/938], Loss: 0.7984411716461182\n",
      "Validation: Epoch [13], Batch [224/938], Loss: 0.8034740090370178\n",
      "Validation: Epoch [13], Batch [225/938], Loss: 0.5794886350631714\n",
      "Validation: Epoch [13], Batch [226/938], Loss: 0.9388697147369385\n",
      "Validation: Epoch [13], Batch [227/938], Loss: 0.7301692962646484\n",
      "Validation: Epoch [13], Batch [228/938], Loss: 0.5041849613189697\n",
      "Validation: Epoch [13], Batch [229/938], Loss: 0.6256064176559448\n",
      "Validation: Epoch [13], Batch [230/938], Loss: 0.6900386810302734\n",
      "Validation: Epoch [13], Batch [231/938], Loss: 0.6709835529327393\n",
      "Validation: Epoch [13], Batch [232/938], Loss: 0.8508432507514954\n",
      "Validation: Epoch [13], Batch [233/938], Loss: 0.8750669956207275\n",
      "Validation: Epoch [13], Batch [234/938], Loss: 0.6396679878234863\n",
      "Validation: Epoch [13], Batch [235/938], Loss: 0.6124513149261475\n",
      "Validation: Epoch [13], Batch [236/938], Loss: 0.6473667621612549\n",
      "Validation: Epoch [13], Batch [237/938], Loss: 0.6304659843444824\n",
      "Validation: Epoch [13], Batch [238/938], Loss: 0.6695508360862732\n",
      "Validation: Epoch [13], Batch [239/938], Loss: 0.6806861162185669\n",
      "Validation: Epoch [13], Batch [240/938], Loss: 0.9395235776901245\n",
      "Validation: Epoch [13], Batch [241/938], Loss: 1.0071735382080078\n",
      "Validation: Epoch [13], Batch [242/938], Loss: 0.8103361129760742\n",
      "Validation: Epoch [13], Batch [243/938], Loss: 0.5792222023010254\n",
      "Validation: Epoch [13], Batch [244/938], Loss: 0.8696072101593018\n",
      "Validation: Epoch [13], Batch [245/938], Loss: 0.7401576042175293\n",
      "Validation: Epoch [13], Batch [246/938], Loss: 0.6719740033149719\n",
      "Validation: Epoch [13], Batch [247/938], Loss: 0.7794841527938843\n",
      "Validation: Epoch [13], Batch [248/938], Loss: 0.768345296382904\n",
      "Validation: Epoch [13], Batch [249/938], Loss: 0.7599982619285583\n",
      "Validation: Epoch [13], Batch [250/938], Loss: 0.8011943101882935\n",
      "Validation: Epoch [13], Batch [251/938], Loss: 0.7866107821464539\n",
      "Validation: Epoch [13], Batch [252/938], Loss: 0.7235352993011475\n",
      "Validation: Epoch [13], Batch [253/938], Loss: 0.6405450701713562\n",
      "Validation: Epoch [13], Batch [254/938], Loss: 0.4045161008834839\n",
      "Validation: Epoch [13], Batch [255/938], Loss: 0.7168794274330139\n",
      "Validation: Epoch [13], Batch [256/938], Loss: 0.7036448121070862\n",
      "Validation: Epoch [13], Batch [257/938], Loss: 0.7202293276786804\n",
      "Validation: Epoch [13], Batch [258/938], Loss: 0.5143454074859619\n",
      "Validation: Epoch [13], Batch [259/938], Loss: 0.7587702870368958\n",
      "Validation: Epoch [13], Batch [260/938], Loss: 0.5165973901748657\n",
      "Validation: Epoch [13], Batch [261/938], Loss: 0.7185841202735901\n",
      "Validation: Epoch [13], Batch [262/938], Loss: 0.8040175437927246\n",
      "Validation: Epoch [13], Batch [263/938], Loss: 0.7880620956420898\n",
      "Validation: Epoch [13], Batch [264/938], Loss: 0.5649483799934387\n",
      "Validation: Epoch [13], Batch [265/938], Loss: 0.9636453986167908\n",
      "Validation: Epoch [13], Batch [266/938], Loss: 0.6764762997627258\n",
      "Validation: Epoch [13], Batch [267/938], Loss: 0.789915144443512\n",
      "Validation: Epoch [13], Batch [268/938], Loss: 0.6057314872741699\n",
      "Validation: Epoch [13], Batch [269/938], Loss: 0.4462267756462097\n",
      "Validation: Epoch [13], Batch [270/938], Loss: 0.8038089871406555\n",
      "Validation: Epoch [13], Batch [271/938], Loss: 0.8981902599334717\n",
      "Validation: Epoch [13], Batch [272/938], Loss: 0.7805731296539307\n",
      "Validation: Epoch [13], Batch [273/938], Loss: 0.513197124004364\n",
      "Validation: Epoch [13], Batch [274/938], Loss: 0.708710789680481\n",
      "Validation: Epoch [13], Batch [275/938], Loss: 0.691247820854187\n",
      "Validation: Epoch [13], Batch [276/938], Loss: 0.7314602732658386\n",
      "Validation: Epoch [13], Batch [277/938], Loss: 0.691526472568512\n",
      "Validation: Epoch [13], Batch [278/938], Loss: 0.7637426853179932\n",
      "Validation: Epoch [13], Batch [279/938], Loss: 0.9139792919158936\n",
      "Validation: Epoch [13], Batch [280/938], Loss: 0.9496888518333435\n",
      "Validation: Epoch [13], Batch [281/938], Loss: 0.7490077018737793\n",
      "Validation: Epoch [13], Batch [282/938], Loss: 0.7458539605140686\n",
      "Validation: Epoch [13], Batch [283/938], Loss: 0.6043468713760376\n",
      "Validation: Epoch [13], Batch [284/938], Loss: 0.8627982139587402\n",
      "Validation: Epoch [13], Batch [285/938], Loss: 0.6695558428764343\n",
      "Validation: Epoch [13], Batch [286/938], Loss: 0.732926607131958\n",
      "Validation: Epoch [13], Batch [287/938], Loss: 0.6661054491996765\n",
      "Validation: Epoch [13], Batch [288/938], Loss: 0.8239693641662598\n",
      "Validation: Epoch [13], Batch [289/938], Loss: 0.8615898489952087\n",
      "Validation: Epoch [13], Batch [290/938], Loss: 0.9002140164375305\n",
      "Validation: Epoch [13], Batch [291/938], Loss: 0.569259524345398\n",
      "Validation: Epoch [13], Batch [292/938], Loss: 0.8284812569618225\n",
      "Validation: Epoch [13], Batch [293/938], Loss: 0.6550549268722534\n",
      "Validation: Epoch [13], Batch [294/938], Loss: 0.6234078407287598\n",
      "Validation: Epoch [13], Batch [295/938], Loss: 0.6692360043525696\n",
      "Validation: Epoch [13], Batch [296/938], Loss: 0.7155572175979614\n",
      "Validation: Epoch [13], Batch [297/938], Loss: 0.7925969362258911\n",
      "Validation: Epoch [13], Batch [298/938], Loss: 0.7296960353851318\n",
      "Validation: Epoch [13], Batch [299/938], Loss: 0.7979708909988403\n",
      "Validation: Epoch [13], Batch [300/938], Loss: 0.5601389408111572\n",
      "Validation: Epoch [13], Batch [301/938], Loss: 0.7626219391822815\n",
      "Validation: Epoch [13], Batch [302/938], Loss: 0.5956695079803467\n",
      "Validation: Epoch [13], Batch [303/938], Loss: 0.8179675340652466\n",
      "Validation: Epoch [13], Batch [304/938], Loss: 0.5461261868476868\n",
      "Validation: Epoch [13], Batch [305/938], Loss: 0.8316187262535095\n",
      "Validation: Epoch [13], Batch [306/938], Loss: 0.9202563762664795\n",
      "Validation: Epoch [13], Batch [307/938], Loss: 0.650696873664856\n",
      "Validation: Epoch [13], Batch [308/938], Loss: 0.7428812384605408\n",
      "Validation: Epoch [13], Batch [309/938], Loss: 0.9117723107337952\n",
      "Validation: Epoch [13], Batch [310/938], Loss: 0.670371413230896\n",
      "Validation: Epoch [13], Batch [311/938], Loss: 0.6288003325462341\n",
      "Validation: Epoch [13], Batch [312/938], Loss: 0.715501070022583\n",
      "Validation: Epoch [13], Batch [313/938], Loss: 0.6254758834838867\n",
      "Validation: Epoch [13], Batch [314/938], Loss: 0.5808968544006348\n",
      "Validation: Epoch [13], Batch [315/938], Loss: 0.8028362393379211\n",
      "Validation: Epoch [13], Batch [316/938], Loss: 0.7617573142051697\n",
      "Validation: Epoch [13], Batch [317/938], Loss: 0.87217777967453\n",
      "Validation: Epoch [13], Batch [318/938], Loss: 0.552153468132019\n",
      "Validation: Epoch [13], Batch [319/938], Loss: 0.7235831618309021\n",
      "Validation: Epoch [13], Batch [320/938], Loss: 0.684728741645813\n",
      "Validation: Epoch [13], Batch [321/938], Loss: 0.6731677055358887\n",
      "Validation: Epoch [13], Batch [322/938], Loss: 0.7156729102134705\n",
      "Validation: Epoch [13], Batch [323/938], Loss: 0.822454035282135\n",
      "Validation: Epoch [13], Batch [324/938], Loss: 0.6902609467506409\n",
      "Validation: Epoch [13], Batch [325/938], Loss: 0.9030793309211731\n",
      "Validation: Epoch [13], Batch [326/938], Loss: 0.8213412761688232\n",
      "Validation: Epoch [13], Batch [327/938], Loss: 0.6148062348365784\n",
      "Validation: Epoch [13], Batch [328/938], Loss: 0.6543657779693604\n",
      "Validation: Epoch [13], Batch [329/938], Loss: 0.6662130355834961\n",
      "Validation: Epoch [13], Batch [330/938], Loss: 0.5300902128219604\n",
      "Validation: Epoch [13], Batch [331/938], Loss: 0.5827437043190002\n",
      "Validation: Epoch [13], Batch [332/938], Loss: 0.7515792846679688\n",
      "Validation: Epoch [13], Batch [333/938], Loss: 0.7189376950263977\n",
      "Validation: Epoch [13], Batch [334/938], Loss: 0.6037046909332275\n",
      "Validation: Epoch [13], Batch [335/938], Loss: 0.7505519390106201\n",
      "Validation: Epoch [13], Batch [336/938], Loss: 0.5753234028816223\n",
      "Validation: Epoch [13], Batch [337/938], Loss: 0.6740661859512329\n",
      "Validation: Epoch [13], Batch [338/938], Loss: 0.49912941455841064\n",
      "Validation: Epoch [13], Batch [339/938], Loss: 0.8318489193916321\n",
      "Validation: Epoch [13], Batch [340/938], Loss: 0.669722318649292\n",
      "Validation: Epoch [13], Batch [341/938], Loss: 0.7106897830963135\n",
      "Validation: Epoch [13], Batch [342/938], Loss: 0.5258690714836121\n",
      "Validation: Epoch [13], Batch [343/938], Loss: 0.7142399549484253\n",
      "Validation: Epoch [13], Batch [344/938], Loss: 0.7704266309738159\n",
      "Validation: Epoch [13], Batch [345/938], Loss: 0.6840303540229797\n",
      "Validation: Epoch [13], Batch [346/938], Loss: 0.8127281665802002\n",
      "Validation: Epoch [13], Batch [347/938], Loss: 0.6871598958969116\n",
      "Validation: Epoch [13], Batch [348/938], Loss: 0.7746351361274719\n",
      "Validation: Epoch [13], Batch [349/938], Loss: 0.8017354607582092\n",
      "Validation: Epoch [13], Batch [350/938], Loss: 0.7774744629859924\n",
      "Validation: Epoch [13], Batch [351/938], Loss: 0.43275314569473267\n",
      "Validation: Epoch [13], Batch [352/938], Loss: 0.740102231502533\n",
      "Validation: Epoch [13], Batch [353/938], Loss: 0.5100080370903015\n",
      "Validation: Epoch [13], Batch [354/938], Loss: 0.962005615234375\n",
      "Validation: Epoch [13], Batch [355/938], Loss: 0.83697110414505\n",
      "Validation: Epoch [13], Batch [356/938], Loss: 0.7567070722579956\n",
      "Validation: Epoch [13], Batch [357/938], Loss: 0.7415724396705627\n",
      "Validation: Epoch [13], Batch [358/938], Loss: 0.8175328969955444\n",
      "Validation: Epoch [13], Batch [359/938], Loss: 0.46764326095581055\n",
      "Validation: Epoch [13], Batch [360/938], Loss: 0.696395754814148\n",
      "Validation: Epoch [13], Batch [361/938], Loss: 0.7213333249092102\n",
      "Validation: Epoch [13], Batch [362/938], Loss: 0.6614500880241394\n",
      "Validation: Epoch [13], Batch [363/938], Loss: 0.48368167877197266\n",
      "Validation: Epoch [13], Batch [364/938], Loss: 0.5714348554611206\n",
      "Validation: Epoch [13], Batch [365/938], Loss: 0.6804344058036804\n",
      "Validation: Epoch [13], Batch [366/938], Loss: 0.695861279964447\n",
      "Validation: Epoch [13], Batch [367/938], Loss: 0.5636317133903503\n",
      "Validation: Epoch [13], Batch [368/938], Loss: 0.923946738243103\n",
      "Validation: Epoch [13], Batch [369/938], Loss: 0.8695209622383118\n",
      "Validation: Epoch [13], Batch [370/938], Loss: 0.5803309082984924\n",
      "Validation: Epoch [13], Batch [371/938], Loss: 0.9165864586830139\n",
      "Validation: Epoch [13], Batch [372/938], Loss: 1.0657483339309692\n",
      "Validation: Epoch [13], Batch [373/938], Loss: 0.7545539736747742\n",
      "Validation: Epoch [13], Batch [374/938], Loss: 0.560178279876709\n",
      "Validation: Epoch [13], Batch [375/938], Loss: 0.6510558724403381\n",
      "Validation: Epoch [13], Batch [376/938], Loss: 0.7464707493782043\n",
      "Validation: Epoch [13], Batch [377/938], Loss: 0.8110086917877197\n",
      "Validation: Epoch [13], Batch [378/938], Loss: 0.6933680772781372\n",
      "Validation: Epoch [13], Batch [379/938], Loss: 0.6040651798248291\n",
      "Validation: Epoch [13], Batch [380/938], Loss: 0.6265042424201965\n",
      "Validation: Epoch [13], Batch [381/938], Loss: 0.7469913959503174\n",
      "Validation: Epoch [13], Batch [382/938], Loss: 0.7318140864372253\n",
      "Validation: Epoch [13], Batch [383/938], Loss: 0.8406016826629639\n",
      "Validation: Epoch [13], Batch [384/938], Loss: 0.803750216960907\n",
      "Validation: Epoch [13], Batch [385/938], Loss: 0.6266043186187744\n",
      "Validation: Epoch [13], Batch [386/938], Loss: 0.7539029121398926\n",
      "Validation: Epoch [13], Batch [387/938], Loss: 0.7908772826194763\n",
      "Validation: Epoch [13], Batch [388/938], Loss: 0.6336721777915955\n",
      "Validation: Epoch [13], Batch [389/938], Loss: 0.5661085844039917\n",
      "Validation: Epoch [13], Batch [390/938], Loss: 0.5401586890220642\n",
      "Validation: Epoch [13], Batch [391/938], Loss: 0.6750955581665039\n",
      "Validation: Epoch [13], Batch [392/938], Loss: 0.7010502219200134\n",
      "Validation: Epoch [13], Batch [393/938], Loss: 0.8258633613586426\n",
      "Validation: Epoch [13], Batch [394/938], Loss: 0.6584122180938721\n",
      "Validation: Epoch [13], Batch [395/938], Loss: 0.918880045413971\n",
      "Validation: Epoch [13], Batch [396/938], Loss: 0.7356720566749573\n",
      "Validation: Epoch [13], Batch [397/938], Loss: 0.6813791394233704\n",
      "Validation: Epoch [13], Batch [398/938], Loss: 0.6581543684005737\n",
      "Validation: Epoch [13], Batch [399/938], Loss: 0.918007493019104\n",
      "Validation: Epoch [13], Batch [400/938], Loss: 0.6807535886764526\n",
      "Validation: Epoch [13], Batch [401/938], Loss: 0.9360101222991943\n",
      "Validation: Epoch [13], Batch [402/938], Loss: 0.6607922911643982\n",
      "Validation: Epoch [13], Batch [403/938], Loss: 0.8177284598350525\n",
      "Validation: Epoch [13], Batch [404/938], Loss: 0.8299013376235962\n",
      "Validation: Epoch [13], Batch [405/938], Loss: 0.6077902913093567\n",
      "Validation: Epoch [13], Batch [406/938], Loss: 0.6711005568504333\n",
      "Validation: Epoch [13], Batch [407/938], Loss: 0.6326144337654114\n",
      "Validation: Epoch [13], Batch [408/938], Loss: 0.8146190047264099\n",
      "Validation: Epoch [13], Batch [409/938], Loss: 0.7883132696151733\n",
      "Validation: Epoch [13], Batch [410/938], Loss: 0.8258863091468811\n",
      "Validation: Epoch [13], Batch [411/938], Loss: 0.7801212668418884\n",
      "Validation: Epoch [13], Batch [412/938], Loss: 0.7077897191047668\n",
      "Validation: Epoch [13], Batch [413/938], Loss: 0.7194622159004211\n",
      "Validation: Epoch [13], Batch [414/938], Loss: 0.6981887817382812\n",
      "Validation: Epoch [13], Batch [415/938], Loss: 0.7594373822212219\n",
      "Validation: Epoch [13], Batch [416/938], Loss: 0.5565613508224487\n",
      "Validation: Epoch [13], Batch [417/938], Loss: 0.612768292427063\n",
      "Validation: Epoch [13], Batch [418/938], Loss: 0.6252318620681763\n",
      "Validation: Epoch [13], Batch [419/938], Loss: 0.8644695281982422\n",
      "Validation: Epoch [13], Batch [420/938], Loss: 0.7275562286376953\n",
      "Validation: Epoch [13], Batch [421/938], Loss: 0.7158089876174927\n",
      "Validation: Epoch [13], Batch [422/938], Loss: 0.6588837504386902\n",
      "Validation: Epoch [13], Batch [423/938], Loss: 0.4938652515411377\n",
      "Validation: Epoch [13], Batch [424/938], Loss: 0.6403307914733887\n",
      "Validation: Epoch [13], Batch [425/938], Loss: 0.8361416459083557\n",
      "Validation: Epoch [13], Batch [426/938], Loss: 0.8533911108970642\n",
      "Validation: Epoch [13], Batch [427/938], Loss: 0.5125477313995361\n",
      "Validation: Epoch [13], Batch [428/938], Loss: 0.41974762082099915\n",
      "Validation: Epoch [13], Batch [429/938], Loss: 0.5437539219856262\n",
      "Validation: Epoch [13], Batch [430/938], Loss: 0.8886643648147583\n",
      "Validation: Epoch [13], Batch [431/938], Loss: 0.7824508547782898\n",
      "Validation: Epoch [13], Batch [432/938], Loss: 0.8154374361038208\n",
      "Validation: Epoch [13], Batch [433/938], Loss: 0.887084424495697\n",
      "Validation: Epoch [13], Batch [434/938], Loss: 0.7772305011749268\n",
      "Validation: Epoch [13], Batch [435/938], Loss: 0.828103244304657\n",
      "Validation: Epoch [13], Batch [436/938], Loss: 0.7753704786300659\n",
      "Validation: Epoch [13], Batch [437/938], Loss: 0.7501457929611206\n",
      "Validation: Epoch [13], Batch [438/938], Loss: 0.8580375909805298\n",
      "Validation: Epoch [13], Batch [439/938], Loss: 0.9022311568260193\n",
      "Validation: Epoch [13], Batch [440/938], Loss: 0.7713748216629028\n",
      "Validation: Epoch [13], Batch [441/938], Loss: 0.7773552536964417\n",
      "Validation: Epoch [13], Batch [442/938], Loss: 0.8759568333625793\n",
      "Validation: Epoch [13], Batch [443/938], Loss: 0.5095261931419373\n",
      "Validation: Epoch [13], Batch [444/938], Loss: 0.8115452527999878\n",
      "Validation: Epoch [13], Batch [445/938], Loss: 0.7088392972946167\n",
      "Validation: Epoch [13], Batch [446/938], Loss: 0.7356500029563904\n",
      "Validation: Epoch [13], Batch [447/938], Loss: 0.7857582569122314\n",
      "Validation: Epoch [13], Batch [448/938], Loss: 0.7271680235862732\n",
      "Validation: Epoch [13], Batch [449/938], Loss: 0.51609867811203\n",
      "Validation: Epoch [13], Batch [450/938], Loss: 0.7247813940048218\n",
      "Validation: Epoch [13], Batch [451/938], Loss: 0.5960543155670166\n",
      "Validation: Epoch [13], Batch [452/938], Loss: 0.6873316764831543\n",
      "Validation: Epoch [13], Batch [453/938], Loss: 0.6843417882919312\n",
      "Validation: Epoch [13], Batch [454/938], Loss: 0.6742792129516602\n",
      "Validation: Epoch [13], Batch [455/938], Loss: 0.5671525001525879\n",
      "Validation: Epoch [13], Batch [456/938], Loss: 0.6448783874511719\n",
      "Validation: Epoch [13], Batch [457/938], Loss: 0.5187825560569763\n",
      "Validation: Epoch [13], Batch [458/938], Loss: 0.6891834735870361\n",
      "Validation: Epoch [13], Batch [459/938], Loss: 0.7663984894752502\n",
      "Validation: Epoch [13], Batch [460/938], Loss: 0.8249310851097107\n",
      "Validation: Epoch [13], Batch [461/938], Loss: 0.6611430048942566\n",
      "Validation: Epoch [13], Batch [462/938], Loss: 0.7756870985031128\n",
      "Validation: Epoch [13], Batch [463/938], Loss: 0.804707407951355\n",
      "Validation: Epoch [13], Batch [464/938], Loss: 0.6136497855186462\n",
      "Validation: Epoch [13], Batch [465/938], Loss: 0.722052812576294\n",
      "Validation: Epoch [13], Batch [466/938], Loss: 0.8411311507225037\n",
      "Validation: Epoch [13], Batch [467/938], Loss: 0.5347458124160767\n",
      "Validation: Epoch [13], Batch [468/938], Loss: 0.8063991665840149\n",
      "Validation: Epoch [13], Batch [469/938], Loss: 0.7061526775360107\n",
      "Validation: Epoch [13], Batch [470/938], Loss: 0.9086384177207947\n",
      "Validation: Epoch [13], Batch [471/938], Loss: 0.82961106300354\n",
      "Validation: Epoch [13], Batch [472/938], Loss: 0.9178215861320496\n",
      "Validation: Epoch [13], Batch [473/938], Loss: 0.7571215629577637\n",
      "Validation: Epoch [13], Batch [474/938], Loss: 0.7501729726791382\n",
      "Validation: Epoch [13], Batch [475/938], Loss: 0.5964492559432983\n",
      "Validation: Epoch [13], Batch [476/938], Loss: 0.8388991951942444\n",
      "Validation: Epoch [13], Batch [477/938], Loss: 0.4548414349555969\n",
      "Validation: Epoch [13], Batch [478/938], Loss: 0.6709047555923462\n",
      "Validation: Epoch [13], Batch [479/938], Loss: 0.8043183088302612\n",
      "Validation: Epoch [13], Batch [480/938], Loss: 0.868262767791748\n",
      "Validation: Epoch [13], Batch [481/938], Loss: 0.5057202577590942\n",
      "Validation: Epoch [13], Batch [482/938], Loss: 0.8645992279052734\n",
      "Validation: Epoch [13], Batch [483/938], Loss: 0.788066029548645\n",
      "Validation: Epoch [13], Batch [484/938], Loss: 0.7192643880844116\n",
      "Validation: Epoch [13], Batch [485/938], Loss: 0.7165845036506653\n",
      "Validation: Epoch [13], Batch [486/938], Loss: 0.8303542137145996\n",
      "Validation: Epoch [13], Batch [487/938], Loss: 0.8061197996139526\n",
      "Validation: Epoch [13], Batch [488/938], Loss: 0.8274283409118652\n",
      "Validation: Epoch [13], Batch [489/938], Loss: 0.47393372654914856\n",
      "Validation: Epoch [13], Batch [490/938], Loss: 0.5204180479049683\n",
      "Validation: Epoch [13], Batch [491/938], Loss: 0.7034787535667419\n",
      "Validation: Epoch [13], Batch [492/938], Loss: 0.7306287884712219\n",
      "Validation: Epoch [13], Batch [493/938], Loss: 0.764104425907135\n",
      "Validation: Epoch [13], Batch [494/938], Loss: 0.6379963159561157\n",
      "Validation: Epoch [13], Batch [495/938], Loss: 0.8620387315750122\n",
      "Validation: Epoch [13], Batch [496/938], Loss: 0.7888872623443604\n",
      "Validation: Epoch [13], Batch [497/938], Loss: 0.4849478006362915\n",
      "Validation: Epoch [13], Batch [498/938], Loss: 0.9556687474250793\n",
      "Validation: Epoch [13], Batch [499/938], Loss: 0.7472631931304932\n",
      "Validation: Epoch [13], Batch [500/938], Loss: 0.6907133460044861\n",
      "Validation: Epoch [13], Batch [501/938], Loss: 0.7440365552902222\n",
      "Validation: Epoch [13], Batch [502/938], Loss: 0.6316482424736023\n",
      "Validation: Epoch [13], Batch [503/938], Loss: 0.7355356216430664\n",
      "Validation: Epoch [13], Batch [504/938], Loss: 0.6976046562194824\n",
      "Validation: Epoch [13], Batch [505/938], Loss: 0.573206901550293\n",
      "Validation: Epoch [13], Batch [506/938], Loss: 0.6364477276802063\n",
      "Validation: Epoch [13], Batch [507/938], Loss: 0.6744598746299744\n",
      "Validation: Epoch [13], Batch [508/938], Loss: 0.6810012459754944\n",
      "Validation: Epoch [13], Batch [509/938], Loss: 0.9258625507354736\n",
      "Validation: Epoch [13], Batch [510/938], Loss: 0.7719336748123169\n",
      "Validation: Epoch [13], Batch [511/938], Loss: 0.5416247248649597\n",
      "Validation: Epoch [13], Batch [512/938], Loss: 0.626460075378418\n",
      "Validation: Epoch [13], Batch [513/938], Loss: 0.8298618793487549\n",
      "Validation: Epoch [13], Batch [514/938], Loss: 0.9221054315567017\n",
      "Validation: Epoch [13], Batch [515/938], Loss: 0.9399686455726624\n",
      "Validation: Epoch [13], Batch [516/938], Loss: 0.5615269541740417\n",
      "Validation: Epoch [13], Batch [517/938], Loss: 0.7851710319519043\n",
      "Validation: Epoch [13], Batch [518/938], Loss: 0.7135721445083618\n",
      "Validation: Epoch [13], Batch [519/938], Loss: 0.6844403743743896\n",
      "Validation: Epoch [13], Batch [520/938], Loss: 0.6572508811950684\n",
      "Validation: Epoch [13], Batch [521/938], Loss: 0.4419642984867096\n",
      "Validation: Epoch [13], Batch [522/938], Loss: 0.7178035378456116\n",
      "Validation: Epoch [13], Batch [523/938], Loss: 0.9205353856086731\n",
      "Validation: Epoch [13], Batch [524/938], Loss: 0.8105850219726562\n",
      "Validation: Epoch [13], Batch [525/938], Loss: 0.5895565748214722\n",
      "Validation: Epoch [13], Batch [526/938], Loss: 0.5747199058532715\n",
      "Validation: Epoch [13], Batch [527/938], Loss: 0.7659140825271606\n",
      "Validation: Epoch [13], Batch [528/938], Loss: 0.9504051208496094\n",
      "Validation: Epoch [13], Batch [529/938], Loss: 0.7127848863601685\n",
      "Validation: Epoch [13], Batch [530/938], Loss: 0.7668063640594482\n",
      "Validation: Epoch [13], Batch [531/938], Loss: 0.6139278411865234\n",
      "Validation: Epoch [13], Batch [532/938], Loss: 0.6468318700790405\n",
      "Validation: Epoch [13], Batch [533/938], Loss: 0.8097053170204163\n",
      "Validation: Epoch [13], Batch [534/938], Loss: 0.9591654539108276\n",
      "Validation: Epoch [13], Batch [535/938], Loss: 0.7405242323875427\n",
      "Validation: Epoch [13], Batch [536/938], Loss: 0.825930118560791\n",
      "Validation: Epoch [13], Batch [537/938], Loss: 0.596548318862915\n",
      "Validation: Epoch [13], Batch [538/938], Loss: 0.701251208782196\n",
      "Validation: Epoch [13], Batch [539/938], Loss: 0.6204628348350525\n",
      "Validation: Epoch [13], Batch [540/938], Loss: 0.6784013509750366\n",
      "Validation: Epoch [13], Batch [541/938], Loss: 0.383006751537323\n",
      "Validation: Epoch [13], Batch [542/938], Loss: 0.9645090103149414\n",
      "Validation: Epoch [13], Batch [543/938], Loss: 0.6365672945976257\n",
      "Validation: Epoch [13], Batch [544/938], Loss: 0.621344268321991\n",
      "Validation: Epoch [13], Batch [545/938], Loss: 0.7149712443351746\n",
      "Validation: Epoch [13], Batch [546/938], Loss: 0.5416545867919922\n",
      "Validation: Epoch [13], Batch [547/938], Loss: 0.6938655972480774\n",
      "Validation: Epoch [13], Batch [548/938], Loss: 0.8162429332733154\n",
      "Validation: Epoch [13], Batch [549/938], Loss: 0.7487584948539734\n",
      "Validation: Epoch [13], Batch [550/938], Loss: 0.7221634387969971\n",
      "Validation: Epoch [13], Batch [551/938], Loss: 0.886687159538269\n",
      "Validation: Epoch [13], Batch [552/938], Loss: 0.5868868231773376\n",
      "Validation: Epoch [13], Batch [553/938], Loss: 0.5442618131637573\n",
      "Validation: Epoch [13], Batch [554/938], Loss: 0.6415643692016602\n",
      "Validation: Epoch [13], Batch [555/938], Loss: 0.5731619596481323\n",
      "Validation: Epoch [13], Batch [556/938], Loss: 0.6080633401870728\n",
      "Validation: Epoch [13], Batch [557/938], Loss: 0.7517597079277039\n",
      "Validation: Epoch [13], Batch [558/938], Loss: 0.7058055400848389\n",
      "Validation: Epoch [13], Batch [559/938], Loss: 0.6774234771728516\n",
      "Validation: Epoch [13], Batch [560/938], Loss: 0.7843239903450012\n",
      "Validation: Epoch [13], Batch [561/938], Loss: 0.6749824285507202\n",
      "Validation: Epoch [13], Batch [562/938], Loss: 0.7933349609375\n",
      "Validation: Epoch [13], Batch [563/938], Loss: 0.7263506650924683\n",
      "Validation: Epoch [13], Batch [564/938], Loss: 0.551384449005127\n",
      "Validation: Epoch [13], Batch [565/938], Loss: 0.6055171489715576\n",
      "Validation: Epoch [13], Batch [566/938], Loss: 0.7999527454376221\n",
      "Validation: Epoch [13], Batch [567/938], Loss: 0.6306796073913574\n",
      "Validation: Epoch [13], Batch [568/938], Loss: 0.7742496132850647\n",
      "Validation: Epoch [13], Batch [569/938], Loss: 0.7795353531837463\n",
      "Validation: Epoch [13], Batch [570/938], Loss: 0.5942774415016174\n",
      "Validation: Epoch [13], Batch [571/938], Loss: 0.7056668400764465\n",
      "Validation: Epoch [13], Batch [572/938], Loss: 0.4936425983905792\n",
      "Validation: Epoch [13], Batch [573/938], Loss: 0.6616104245185852\n",
      "Validation: Epoch [13], Batch [574/938], Loss: 0.8225085735321045\n",
      "Validation: Epoch [13], Batch [575/938], Loss: 0.5838145613670349\n",
      "Validation: Epoch [13], Batch [576/938], Loss: 0.5602747201919556\n",
      "Validation: Epoch [13], Batch [577/938], Loss: 0.5813441276550293\n",
      "Validation: Epoch [13], Batch [578/938], Loss: 0.5381205677986145\n",
      "Validation: Epoch [13], Batch [579/938], Loss: 0.9216421842575073\n",
      "Validation: Epoch [13], Batch [580/938], Loss: 0.6849637627601624\n",
      "Validation: Epoch [13], Batch [581/938], Loss: 1.0208208560943604\n",
      "Validation: Epoch [13], Batch [582/938], Loss: 0.6810586452484131\n",
      "Validation: Epoch [13], Batch [583/938], Loss: 0.5424177646636963\n",
      "Validation: Epoch [13], Batch [584/938], Loss: 0.7028369307518005\n",
      "Validation: Epoch [13], Batch [585/938], Loss: 0.5586733818054199\n",
      "Validation: Epoch [13], Batch [586/938], Loss: 0.5413468480110168\n",
      "Validation: Epoch [13], Batch [587/938], Loss: 0.6684592962265015\n",
      "Validation: Epoch [13], Batch [588/938], Loss: 0.7096405625343323\n",
      "Validation: Epoch [13], Batch [589/938], Loss: 0.7156783938407898\n",
      "Validation: Epoch [13], Batch [590/938], Loss: 0.623696506023407\n",
      "Validation: Epoch [13], Batch [591/938], Loss: 0.809114933013916\n",
      "Validation: Epoch [13], Batch [592/938], Loss: 0.9027373790740967\n",
      "Validation: Epoch [13], Batch [593/938], Loss: 0.6176161170005798\n",
      "Validation: Epoch [13], Batch [594/938], Loss: 0.6487652063369751\n",
      "Validation: Epoch [13], Batch [595/938], Loss: 0.6953356266021729\n",
      "Validation: Epoch [13], Batch [596/938], Loss: 0.8546181917190552\n",
      "Validation: Epoch [13], Batch [597/938], Loss: 0.696355938911438\n",
      "Validation: Epoch [13], Batch [598/938], Loss: 0.5835531949996948\n",
      "Validation: Epoch [13], Batch [599/938], Loss: 0.6351041793823242\n",
      "Validation: Epoch [13], Batch [600/938], Loss: 0.610815167427063\n",
      "Validation: Epoch [13], Batch [601/938], Loss: 0.7575535774230957\n",
      "Validation: Epoch [13], Batch [602/938], Loss: 0.5446004271507263\n",
      "Validation: Epoch [13], Batch [603/938], Loss: 0.39126187562942505\n",
      "Validation: Epoch [13], Batch [604/938], Loss: 0.63785320520401\n",
      "Validation: Epoch [13], Batch [605/938], Loss: 0.5076252222061157\n",
      "Validation: Epoch [13], Batch [606/938], Loss: 0.6808122396469116\n",
      "Validation: Epoch [13], Batch [607/938], Loss: 0.8054394125938416\n",
      "Validation: Epoch [13], Batch [608/938], Loss: 0.8764582276344299\n",
      "Validation: Epoch [13], Batch [609/938], Loss: 0.8434700965881348\n",
      "Validation: Epoch [13], Batch [610/938], Loss: 0.5231320261955261\n",
      "Validation: Epoch [13], Batch [611/938], Loss: 0.6864138841629028\n",
      "Validation: Epoch [13], Batch [612/938], Loss: 0.6685342788696289\n",
      "Validation: Epoch [13], Batch [613/938], Loss: 0.5199548006057739\n",
      "Validation: Epoch [13], Batch [614/938], Loss: 0.6867342591285706\n",
      "Validation: Epoch [13], Batch [615/938], Loss: 0.8159986734390259\n",
      "Validation: Epoch [13], Batch [616/938], Loss: 0.49017882347106934\n",
      "Validation: Epoch [13], Batch [617/938], Loss: 0.6733829975128174\n",
      "Validation: Epoch [13], Batch [618/938], Loss: 0.7580360174179077\n",
      "Validation: Epoch [13], Batch [619/938], Loss: 0.6948067545890808\n",
      "Validation: Epoch [13], Batch [620/938], Loss: 0.6359683275222778\n",
      "Validation: Epoch [13], Batch [621/938], Loss: 0.7582783699035645\n",
      "Validation: Epoch [13], Batch [622/938], Loss: 0.6586548089981079\n",
      "Validation: Epoch [13], Batch [623/938], Loss: 0.7636944651603699\n",
      "Validation: Epoch [13], Batch [624/938], Loss: 0.7579980492591858\n",
      "Validation: Epoch [13], Batch [625/938], Loss: 0.6465237140655518\n",
      "Validation: Epoch [13], Batch [626/938], Loss: 0.6757587194442749\n",
      "Validation: Epoch [13], Batch [627/938], Loss: 0.6651482582092285\n",
      "Validation: Epoch [13], Batch [628/938], Loss: 1.0290534496307373\n",
      "Validation: Epoch [13], Batch [629/938], Loss: 0.6543992161750793\n",
      "Validation: Epoch [13], Batch [630/938], Loss: 0.7107393741607666\n",
      "Validation: Epoch [13], Batch [631/938], Loss: 0.7177640199661255\n",
      "Validation: Epoch [13], Batch [632/938], Loss: 0.8886576890945435\n",
      "Validation: Epoch [13], Batch [633/938], Loss: 0.8398962616920471\n",
      "Validation: Epoch [13], Batch [634/938], Loss: 0.7033839225769043\n",
      "Validation: Epoch [13], Batch [635/938], Loss: 0.632761538028717\n",
      "Validation: Epoch [13], Batch [636/938], Loss: 0.779674768447876\n",
      "Validation: Epoch [13], Batch [637/938], Loss: 0.6866054534912109\n",
      "Validation: Epoch [13], Batch [638/938], Loss: 0.6602733731269836\n",
      "Validation: Epoch [13], Batch [639/938], Loss: 0.7818435430526733\n",
      "Validation: Epoch [13], Batch [640/938], Loss: 0.9293410181999207\n",
      "Validation: Epoch [13], Batch [641/938], Loss: 0.857346773147583\n",
      "Validation: Epoch [13], Batch [642/938], Loss: 0.6964350342750549\n",
      "Validation: Epoch [13], Batch [643/938], Loss: 0.6844951510429382\n",
      "Validation: Epoch [13], Batch [644/938], Loss: 0.8531498908996582\n",
      "Validation: Epoch [13], Batch [645/938], Loss: 0.7594163417816162\n",
      "Validation: Epoch [13], Batch [646/938], Loss: 0.6767719388008118\n",
      "Validation: Epoch [13], Batch [647/938], Loss: 0.6124692559242249\n",
      "Validation: Epoch [13], Batch [648/938], Loss: 0.6456832885742188\n",
      "Validation: Epoch [13], Batch [649/938], Loss: 0.8387506008148193\n",
      "Validation: Epoch [13], Batch [650/938], Loss: 0.9235264658927917\n",
      "Validation: Epoch [13], Batch [651/938], Loss: 0.8536046147346497\n",
      "Validation: Epoch [13], Batch [652/938], Loss: 0.6086211204528809\n",
      "Validation: Epoch [13], Batch [653/938], Loss: 0.8215914368629456\n",
      "Validation: Epoch [13], Batch [654/938], Loss: 0.7221846580505371\n",
      "Validation: Epoch [13], Batch [655/938], Loss: 0.7327585220336914\n",
      "Validation: Epoch [13], Batch [656/938], Loss: 0.830907940864563\n",
      "Validation: Epoch [13], Batch [657/938], Loss: 0.652005672454834\n",
      "Validation: Epoch [13], Batch [658/938], Loss: 0.5343238115310669\n",
      "Validation: Epoch [13], Batch [659/938], Loss: 0.7886937260627747\n",
      "Validation: Epoch [13], Batch [660/938], Loss: 0.6319499015808105\n",
      "Validation: Epoch [13], Batch [661/938], Loss: 0.5441833138465881\n",
      "Validation: Epoch [13], Batch [662/938], Loss: 0.6667963266372681\n",
      "Validation: Epoch [13], Batch [663/938], Loss: 0.6029550433158875\n",
      "Validation: Epoch [13], Batch [664/938], Loss: 0.7385103702545166\n",
      "Validation: Epoch [13], Batch [665/938], Loss: 0.7620761394500732\n",
      "Validation: Epoch [13], Batch [666/938], Loss: 0.5872480869293213\n",
      "Validation: Epoch [13], Batch [667/938], Loss: 0.9567348957061768\n",
      "Validation: Epoch [13], Batch [668/938], Loss: 0.6201443672180176\n",
      "Validation: Epoch [13], Batch [669/938], Loss: 0.5625478625297546\n",
      "Validation: Epoch [13], Batch [670/938], Loss: 0.6185749769210815\n",
      "Validation: Epoch [13], Batch [671/938], Loss: 0.7691283226013184\n",
      "Validation: Epoch [13], Batch [672/938], Loss: 0.5547003149986267\n",
      "Validation: Epoch [13], Batch [673/938], Loss: 0.6190260648727417\n",
      "Validation: Epoch [13], Batch [674/938], Loss: 0.8716486692428589\n",
      "Validation: Epoch [13], Batch [675/938], Loss: 0.5152801275253296\n",
      "Validation: Epoch [13], Batch [676/938], Loss: 0.8797010779380798\n",
      "Validation: Epoch [13], Batch [677/938], Loss: 0.683068037033081\n",
      "Validation: Epoch [13], Batch [678/938], Loss: 0.6051769256591797\n",
      "Validation: Epoch [13], Batch [679/938], Loss: 0.7737752795219421\n",
      "Validation: Epoch [13], Batch [680/938], Loss: 0.8767621517181396\n",
      "Validation: Epoch [13], Batch [681/938], Loss: 0.7902654409408569\n",
      "Validation: Epoch [13], Batch [682/938], Loss: 0.8172415494918823\n",
      "Validation: Epoch [13], Batch [683/938], Loss: 0.7546632885932922\n",
      "Validation: Epoch [13], Batch [684/938], Loss: 0.7443703413009644\n",
      "Validation: Epoch [13], Batch [685/938], Loss: 0.958318829536438\n",
      "Validation: Epoch [13], Batch [686/938], Loss: 0.8475050330162048\n",
      "Validation: Epoch [13], Batch [687/938], Loss: 0.7052584290504456\n",
      "Validation: Epoch [13], Batch [688/938], Loss: 0.7954397797584534\n",
      "Validation: Epoch [13], Batch [689/938], Loss: 0.8225476145744324\n",
      "Validation: Epoch [13], Batch [690/938], Loss: 0.7364088892936707\n",
      "Validation: Epoch [13], Batch [691/938], Loss: 0.7519218325614929\n",
      "Validation: Epoch [13], Batch [692/938], Loss: 0.807432234287262\n",
      "Validation: Epoch [13], Batch [693/938], Loss: 0.7306197881698608\n",
      "Validation: Epoch [13], Batch [694/938], Loss: 0.8528193831443787\n",
      "Validation: Epoch [13], Batch [695/938], Loss: 0.8490168452262878\n",
      "Validation: Epoch [13], Batch [696/938], Loss: 0.47274720668792725\n",
      "Validation: Epoch [13], Batch [697/938], Loss: 0.5487805604934692\n",
      "Validation: Epoch [13], Batch [698/938], Loss: 0.896609902381897\n",
      "Validation: Epoch [13], Batch [699/938], Loss: 0.5257865786552429\n",
      "Validation: Epoch [13], Batch [700/938], Loss: 0.6361871361732483\n",
      "Validation: Epoch [13], Batch [701/938], Loss: 0.6795908808708191\n",
      "Validation: Epoch [13], Batch [702/938], Loss: 0.606839120388031\n",
      "Validation: Epoch [13], Batch [703/938], Loss: 0.7124744653701782\n",
      "Validation: Epoch [13], Batch [704/938], Loss: 0.6736363172531128\n",
      "Validation: Epoch [13], Batch [705/938], Loss: 0.782042920589447\n",
      "Validation: Epoch [13], Batch [706/938], Loss: 0.7071962356567383\n",
      "Validation: Epoch [13], Batch [707/938], Loss: 0.6639686822891235\n",
      "Validation: Epoch [13], Batch [708/938], Loss: 0.5282041430473328\n",
      "Validation: Epoch [13], Batch [709/938], Loss: 0.4089291989803314\n",
      "Validation: Epoch [13], Batch [710/938], Loss: 0.6480704545974731\n",
      "Validation: Epoch [13], Batch [711/938], Loss: 0.5364236235618591\n",
      "Validation: Epoch [13], Batch [712/938], Loss: 0.5302291512489319\n",
      "Validation: Epoch [13], Batch [713/938], Loss: 0.60053950548172\n",
      "Validation: Epoch [13], Batch [714/938], Loss: 0.6501274108886719\n",
      "Validation: Epoch [13], Batch [715/938], Loss: 0.8280945420265198\n",
      "Validation: Epoch [13], Batch [716/938], Loss: 0.6124629974365234\n",
      "Validation: Epoch [13], Batch [717/938], Loss: 0.4134955406188965\n",
      "Validation: Epoch [13], Batch [718/938], Loss: 0.6059294939041138\n",
      "Validation: Epoch [13], Batch [719/938], Loss: 0.8660174012184143\n",
      "Validation: Epoch [13], Batch [720/938], Loss: 0.7930052876472473\n",
      "Validation: Epoch [13], Batch [721/938], Loss: 0.6095099449157715\n",
      "Validation: Epoch [13], Batch [722/938], Loss: 0.6329807639122009\n",
      "Validation: Epoch [13], Batch [723/938], Loss: 0.8118493556976318\n",
      "Validation: Epoch [13], Batch [724/938], Loss: 0.5631878972053528\n",
      "Validation: Epoch [13], Batch [725/938], Loss: 0.9438188076019287\n",
      "Validation: Epoch [13], Batch [726/938], Loss: 0.7851610779762268\n",
      "Validation: Epoch [13], Batch [727/938], Loss: 0.3788216710090637\n",
      "Validation: Epoch [13], Batch [728/938], Loss: 0.8227506875991821\n",
      "Validation: Epoch [13], Batch [729/938], Loss: 0.6325256824493408\n",
      "Validation: Epoch [13], Batch [730/938], Loss: 0.7537011504173279\n",
      "Validation: Epoch [13], Batch [731/938], Loss: 0.7450846433639526\n",
      "Validation: Epoch [13], Batch [732/938], Loss: 0.6556333303451538\n",
      "Validation: Epoch [13], Batch [733/938], Loss: 0.8197478652000427\n",
      "Validation: Epoch [13], Batch [734/938], Loss: 0.7539660334587097\n",
      "Validation: Epoch [13], Batch [735/938], Loss: 0.7104296684265137\n",
      "Validation: Epoch [13], Batch [736/938], Loss: 0.758001983165741\n",
      "Validation: Epoch [13], Batch [737/938], Loss: 0.7117832899093628\n",
      "Validation: Epoch [13], Batch [738/938], Loss: 0.6454668045043945\n",
      "Validation: Epoch [13], Batch [739/938], Loss: 0.5849339962005615\n",
      "Validation: Epoch [13], Batch [740/938], Loss: 0.6986931562423706\n",
      "Validation: Epoch [13], Batch [741/938], Loss: 0.7369915246963501\n",
      "Validation: Epoch [13], Batch [742/938], Loss: 0.744560956954956\n",
      "Validation: Epoch [13], Batch [743/938], Loss: 0.9228212833404541\n",
      "Validation: Epoch [13], Batch [744/938], Loss: 0.5688559412956238\n",
      "Validation: Epoch [13], Batch [745/938], Loss: 0.672009289264679\n",
      "Validation: Epoch [13], Batch [746/938], Loss: 0.7920250296592712\n",
      "Validation: Epoch [13], Batch [747/938], Loss: 0.6353515982627869\n",
      "Validation: Epoch [13], Batch [748/938], Loss: 0.7256689071655273\n",
      "Validation: Epoch [13], Batch [749/938], Loss: 0.6111451387405396\n",
      "Validation: Epoch [13], Batch [750/938], Loss: 0.40216708183288574\n",
      "Validation: Epoch [13], Batch [751/938], Loss: 0.7104036808013916\n",
      "Validation: Epoch [13], Batch [752/938], Loss: 0.734883189201355\n",
      "Validation: Epoch [13], Batch [753/938], Loss: 0.5654525756835938\n",
      "Validation: Epoch [13], Batch [754/938], Loss: 0.5659721493721008\n",
      "Validation: Epoch [13], Batch [755/938], Loss: 0.5042704343795776\n",
      "Validation: Epoch [13], Batch [756/938], Loss: 0.8419787287712097\n",
      "Validation: Epoch [13], Batch [757/938], Loss: 0.9444005489349365\n",
      "Validation: Epoch [13], Batch [758/938], Loss: 0.6815011501312256\n",
      "Validation: Epoch [13], Batch [759/938], Loss: 0.7470536828041077\n",
      "Validation: Epoch [13], Batch [760/938], Loss: 0.6368891000747681\n",
      "Validation: Epoch [13], Batch [761/938], Loss: 0.8741230964660645\n",
      "Validation: Epoch [13], Batch [762/938], Loss: 0.6471298933029175\n",
      "Validation: Epoch [13], Batch [763/938], Loss: 0.4872036278247833\n",
      "Validation: Epoch [13], Batch [764/938], Loss: 0.859182596206665\n",
      "Validation: Epoch [13], Batch [765/938], Loss: 0.695199728012085\n",
      "Validation: Epoch [13], Batch [766/938], Loss: 0.6374621987342834\n",
      "Validation: Epoch [13], Batch [767/938], Loss: 0.5388409495353699\n",
      "Validation: Epoch [13], Batch [768/938], Loss: 0.6003018021583557\n",
      "Validation: Epoch [13], Batch [769/938], Loss: 0.5501897931098938\n",
      "Validation: Epoch [13], Batch [770/938], Loss: 0.47411733865737915\n",
      "Validation: Epoch [13], Batch [771/938], Loss: 0.6085422039031982\n",
      "Validation: Epoch [13], Batch [772/938], Loss: 0.7134295105934143\n",
      "Validation: Epoch [13], Batch [773/938], Loss: 0.7327282428741455\n",
      "Validation: Epoch [13], Batch [774/938], Loss: 0.7060266137123108\n",
      "Validation: Epoch [13], Batch [775/938], Loss: 0.7207924127578735\n",
      "Validation: Epoch [13], Batch [776/938], Loss: 0.6833829879760742\n",
      "Validation: Epoch [13], Batch [777/938], Loss: 0.6549920439720154\n",
      "Validation: Epoch [13], Batch [778/938], Loss: 0.788415789604187\n",
      "Validation: Epoch [13], Batch [779/938], Loss: 0.9572831392288208\n",
      "Validation: Epoch [13], Batch [780/938], Loss: 0.5838361978530884\n",
      "Validation: Epoch [13], Batch [781/938], Loss: 0.6958456635475159\n",
      "Validation: Epoch [13], Batch [782/938], Loss: 0.6910733580589294\n",
      "Validation: Epoch [13], Batch [783/938], Loss: 0.8887687921524048\n",
      "Validation: Epoch [13], Batch [784/938], Loss: 0.5884094834327698\n",
      "Validation: Epoch [13], Batch [785/938], Loss: 0.735102117061615\n",
      "Validation: Epoch [13], Batch [786/938], Loss: 0.5888045430183411\n",
      "Validation: Epoch [13], Batch [787/938], Loss: 0.9194862842559814\n",
      "Validation: Epoch [13], Batch [788/938], Loss: 0.6205649971961975\n",
      "Validation: Epoch [13], Batch [789/938], Loss: 0.5176460146903992\n",
      "Validation: Epoch [13], Batch [790/938], Loss: 0.5705841183662415\n",
      "Validation: Epoch [13], Batch [791/938], Loss: 0.7489975690841675\n",
      "Validation: Epoch [13], Batch [792/938], Loss: 0.8029975295066833\n",
      "Validation: Epoch [13], Batch [793/938], Loss: 0.6901456117630005\n",
      "Validation: Epoch [13], Batch [794/938], Loss: 0.9513325095176697\n",
      "Validation: Epoch [13], Batch [795/938], Loss: 0.8891388177871704\n",
      "Validation: Epoch [13], Batch [796/938], Loss: 0.5717472434043884\n",
      "Validation: Epoch [13], Batch [797/938], Loss: 0.81525719165802\n",
      "Validation: Epoch [13], Batch [798/938], Loss: 0.6538252830505371\n",
      "Validation: Epoch [13], Batch [799/938], Loss: 0.5318390130996704\n",
      "Validation: Epoch [13], Batch [800/938], Loss: 0.682150661945343\n",
      "Validation: Epoch [13], Batch [801/938], Loss: 0.5450447797775269\n",
      "Validation: Epoch [13], Batch [802/938], Loss: 0.8360223174095154\n",
      "Validation: Epoch [13], Batch [803/938], Loss: 0.721362292766571\n",
      "Validation: Epoch [13], Batch [804/938], Loss: 0.56926029920578\n",
      "Validation: Epoch [13], Batch [805/938], Loss: 0.6001836061477661\n",
      "Validation: Epoch [13], Batch [806/938], Loss: 0.5644320845603943\n",
      "Validation: Epoch [13], Batch [807/938], Loss: 0.8653784990310669\n",
      "Validation: Epoch [13], Batch [808/938], Loss: 0.7652714252471924\n",
      "Validation: Epoch [13], Batch [809/938], Loss: 0.5743188858032227\n",
      "Validation: Epoch [13], Batch [810/938], Loss: 0.6306156516075134\n",
      "Validation: Epoch [13], Batch [811/938], Loss: 0.6469312906265259\n",
      "Validation: Epoch [13], Batch [812/938], Loss: 0.969345211982727\n",
      "Validation: Epoch [13], Batch [813/938], Loss: 0.7344186902046204\n",
      "Validation: Epoch [13], Batch [814/938], Loss: 0.9814954400062561\n",
      "Validation: Epoch [13], Batch [815/938], Loss: 0.8538850545883179\n",
      "Validation: Epoch [13], Batch [816/938], Loss: 0.8619011640548706\n",
      "Validation: Epoch [13], Batch [817/938], Loss: 0.531101644039154\n",
      "Validation: Epoch [13], Batch [818/938], Loss: 0.7446126341819763\n",
      "Validation: Epoch [13], Batch [819/938], Loss: 0.5807294249534607\n",
      "Validation: Epoch [13], Batch [820/938], Loss: 0.6780394911766052\n",
      "Validation: Epoch [13], Batch [821/938], Loss: 0.6552128791809082\n",
      "Validation: Epoch [13], Batch [822/938], Loss: 0.7736310958862305\n",
      "Validation: Epoch [13], Batch [823/938], Loss: 0.585593581199646\n",
      "Validation: Epoch [13], Batch [824/938], Loss: 0.7831442356109619\n",
      "Validation: Epoch [13], Batch [825/938], Loss: 0.8061496019363403\n",
      "Validation: Epoch [13], Batch [826/938], Loss: 0.5414212346076965\n",
      "Validation: Epoch [13], Batch [827/938], Loss: 0.8049980998039246\n",
      "Validation: Epoch [13], Batch [828/938], Loss: 0.7588225603103638\n",
      "Validation: Epoch [13], Batch [829/938], Loss: 0.723149836063385\n",
      "Validation: Epoch [13], Batch [830/938], Loss: 0.54896479845047\n",
      "Validation: Epoch [13], Batch [831/938], Loss: 0.7553983330726624\n",
      "Validation: Epoch [13], Batch [832/938], Loss: 0.6636377573013306\n",
      "Validation: Epoch [13], Batch [833/938], Loss: 0.8277517557144165\n",
      "Validation: Epoch [13], Batch [834/938], Loss: 0.5960111618041992\n",
      "Validation: Epoch [13], Batch [835/938], Loss: 0.7623048424720764\n",
      "Validation: Epoch [13], Batch [836/938], Loss: 0.6688583493232727\n",
      "Validation: Epoch [13], Batch [837/938], Loss: 0.46374279260635376\n",
      "Validation: Epoch [13], Batch [838/938], Loss: 0.5164613723754883\n",
      "Validation: Epoch [13], Batch [839/938], Loss: 0.596043586730957\n",
      "Validation: Epoch [13], Batch [840/938], Loss: 0.8147221803665161\n",
      "Validation: Epoch [13], Batch [841/938], Loss: 0.7106374502182007\n",
      "Validation: Epoch [13], Batch [842/938], Loss: 1.065411925315857\n",
      "Validation: Epoch [13], Batch [843/938], Loss: 0.6732924580574036\n",
      "Validation: Epoch [13], Batch [844/938], Loss: 1.011877179145813\n",
      "Validation: Epoch [13], Batch [845/938], Loss: 0.4912275969982147\n",
      "Validation: Epoch [13], Batch [846/938], Loss: 0.7776803970336914\n",
      "Validation: Epoch [13], Batch [847/938], Loss: 0.5921856164932251\n",
      "Validation: Epoch [13], Batch [848/938], Loss: 0.798582911491394\n",
      "Validation: Epoch [13], Batch [849/938], Loss: 0.6903541088104248\n",
      "Validation: Epoch [13], Batch [850/938], Loss: 0.9175509810447693\n",
      "Validation: Epoch [13], Batch [851/938], Loss: 0.7437210083007812\n",
      "Validation: Epoch [13], Batch [852/938], Loss: 0.7787653803825378\n",
      "Validation: Epoch [13], Batch [853/938], Loss: 0.654723584651947\n",
      "Validation: Epoch [13], Batch [854/938], Loss: 0.9394045472145081\n",
      "Validation: Epoch [13], Batch [855/938], Loss: 0.6320189833641052\n",
      "Validation: Epoch [13], Batch [856/938], Loss: 0.7318373918533325\n",
      "Validation: Epoch [13], Batch [857/938], Loss: 0.44292670488357544\n",
      "Validation: Epoch [13], Batch [858/938], Loss: 0.7665596604347229\n",
      "Validation: Epoch [13], Batch [859/938], Loss: 0.6973598599433899\n",
      "Validation: Epoch [13], Batch [860/938], Loss: 0.5023806095123291\n",
      "Validation: Epoch [13], Batch [861/938], Loss: 0.8280946016311646\n",
      "Validation: Epoch [13], Batch [862/938], Loss: 0.4607437551021576\n",
      "Validation: Epoch [13], Batch [863/938], Loss: 0.8255128264427185\n",
      "Validation: Epoch [13], Batch [864/938], Loss: 0.9104785323143005\n",
      "Validation: Epoch [13], Batch [865/938], Loss: 0.8318753838539124\n",
      "Validation: Epoch [13], Batch [866/938], Loss: 0.701962947845459\n",
      "Validation: Epoch [13], Batch [867/938], Loss: 0.8158939480781555\n",
      "Validation: Epoch [13], Batch [868/938], Loss: 0.8948513269424438\n",
      "Validation: Epoch [13], Batch [869/938], Loss: 0.8406209349632263\n",
      "Validation: Epoch [13], Batch [870/938], Loss: 0.7198415994644165\n",
      "Validation: Epoch [13], Batch [871/938], Loss: 0.7857787013053894\n",
      "Validation: Epoch [13], Batch [872/938], Loss: 0.5692713856697083\n",
      "Validation: Epoch [13], Batch [873/938], Loss: 0.6962864398956299\n",
      "Validation: Epoch [13], Batch [874/938], Loss: 0.5100480318069458\n",
      "Validation: Epoch [13], Batch [875/938], Loss: 0.7782588005065918\n",
      "Validation: Epoch [13], Batch [876/938], Loss: 0.693935751914978\n",
      "Validation: Epoch [13], Batch [877/938], Loss: 0.5208114981651306\n",
      "Validation: Epoch [13], Batch [878/938], Loss: 0.6707046031951904\n",
      "Validation: Epoch [13], Batch [879/938], Loss: 0.8240487575531006\n",
      "Validation: Epoch [13], Batch [880/938], Loss: 0.8426686525344849\n",
      "Validation: Epoch [13], Batch [881/938], Loss: 0.7796918153762817\n",
      "Validation: Epoch [13], Batch [882/938], Loss: 0.8689265251159668\n",
      "Validation: Epoch [13], Batch [883/938], Loss: 0.764968991279602\n",
      "Validation: Epoch [13], Batch [884/938], Loss: 0.7014103531837463\n",
      "Validation: Epoch [13], Batch [885/938], Loss: 0.6220315098762512\n",
      "Validation: Epoch [13], Batch [886/938], Loss: 0.5036201477050781\n",
      "Validation: Epoch [13], Batch [887/938], Loss: 0.9605417251586914\n",
      "Validation: Epoch [13], Batch [888/938], Loss: 0.8706247806549072\n",
      "Validation: Epoch [13], Batch [889/938], Loss: 0.7265747785568237\n",
      "Validation: Epoch [13], Batch [890/938], Loss: 1.0582537651062012\n",
      "Validation: Epoch [13], Batch [891/938], Loss: 0.6926243305206299\n",
      "Validation: Epoch [13], Batch [892/938], Loss: 0.5268592834472656\n",
      "Validation: Epoch [13], Batch [893/938], Loss: 0.6816786527633667\n",
      "Validation: Epoch [13], Batch [894/938], Loss: 0.5939270853996277\n",
      "Validation: Epoch [13], Batch [895/938], Loss: 0.7953580021858215\n",
      "Validation: Epoch [13], Batch [896/938], Loss: 0.6662300825119019\n",
      "Validation: Epoch [13], Batch [897/938], Loss: 0.8945791721343994\n",
      "Validation: Epoch [13], Batch [898/938], Loss: 0.8301886320114136\n",
      "Validation: Epoch [13], Batch [899/938], Loss: 0.6285233497619629\n",
      "Validation: Epoch [13], Batch [900/938], Loss: 0.6407421827316284\n",
      "Validation: Epoch [13], Batch [901/938], Loss: 0.6410964727401733\n",
      "Validation: Epoch [13], Batch [902/938], Loss: 0.6561307907104492\n",
      "Validation: Epoch [13], Batch [903/938], Loss: 0.6948396563529968\n",
      "Validation: Epoch [13], Batch [904/938], Loss: 0.693697988986969\n",
      "Validation: Epoch [13], Batch [905/938], Loss: 0.9330500960350037\n",
      "Validation: Epoch [13], Batch [906/938], Loss: 0.7527127861976624\n",
      "Validation: Epoch [13], Batch [907/938], Loss: 0.46419182419776917\n",
      "Validation: Epoch [13], Batch [908/938], Loss: 0.7118136286735535\n",
      "Validation: Epoch [13], Batch [909/938], Loss: 0.7462884783744812\n",
      "Validation: Epoch [13], Batch [910/938], Loss: 0.5076157450675964\n",
      "Validation: Epoch [13], Batch [911/938], Loss: 0.761708676815033\n",
      "Validation: Epoch [13], Batch [912/938], Loss: 0.6772632598876953\n",
      "Validation: Epoch [13], Batch [913/938], Loss: 0.6950164437294006\n",
      "Validation: Epoch [13], Batch [914/938], Loss: 0.7396913766860962\n",
      "Validation: Epoch [13], Batch [915/938], Loss: 0.7862693667411804\n",
      "Validation: Epoch [13], Batch [916/938], Loss: 0.9678502082824707\n",
      "Validation: Epoch [13], Batch [917/938], Loss: 0.7352895140647888\n",
      "Validation: Epoch [13], Batch [918/938], Loss: 0.642817497253418\n",
      "Validation: Epoch [13], Batch [919/938], Loss: 0.8531463742256165\n",
      "Validation: Epoch [13], Batch [920/938], Loss: 0.861088216304779\n",
      "Validation: Epoch [13], Batch [921/938], Loss: 0.6630090475082397\n",
      "Validation: Epoch [13], Batch [922/938], Loss: 0.6821866631507874\n",
      "Validation: Epoch [13], Batch [923/938], Loss: 0.46597781777381897\n",
      "Validation: Epoch [13], Batch [924/938], Loss: 0.7525469660758972\n",
      "Validation: Epoch [13], Batch [925/938], Loss: 0.5413011312484741\n",
      "Validation: Epoch [13], Batch [926/938], Loss: 0.9460111856460571\n",
      "Validation: Epoch [13], Batch [927/938], Loss: 0.7129347324371338\n",
      "Validation: Epoch [13], Batch [928/938], Loss: 0.5673093795776367\n",
      "Validation: Epoch [13], Batch [929/938], Loss: 0.5901987552642822\n",
      "Validation: Epoch [13], Batch [930/938], Loss: 0.8598877191543579\n",
      "Validation: Epoch [13], Batch [931/938], Loss: 0.7650284171104431\n",
      "Validation: Epoch [13], Batch [932/938], Loss: 0.7227007746696472\n",
      "Validation: Epoch [13], Batch [933/938], Loss: 0.5486732721328735\n",
      "Validation: Epoch [13], Batch [934/938], Loss: 0.6133646368980408\n",
      "Validation: Epoch [13], Batch [935/938], Loss: 0.7215383648872375\n",
      "Validation: Epoch [13], Batch [936/938], Loss: 0.9792086482048035\n",
      "Validation: Epoch [13], Batch [937/938], Loss: 0.5341461300849915\n",
      "Validation: Epoch [13], Batch [938/938], Loss: 0.7639424204826355\n",
      "Accuracy of test set: 0.7892166666666667\n",
      "Train: Epoch [14], Batch [1/938], Loss: 0.642884373664856\n",
      "Train: Epoch [14], Batch [2/938], Loss: 0.7363092303276062\n",
      "Train: Epoch [14], Batch [3/938], Loss: 0.8232524394989014\n",
      "Train: Epoch [14], Batch [4/938], Loss: 0.8894951343536377\n",
      "Train: Epoch [14], Batch [5/938], Loss: 0.6274459362030029\n",
      "Train: Epoch [14], Batch [6/938], Loss: 0.5884847640991211\n",
      "Train: Epoch [14], Batch [7/938], Loss: 0.8348546028137207\n",
      "Train: Epoch [14], Batch [8/938], Loss: 0.5792319178581238\n",
      "Train: Epoch [14], Batch [9/938], Loss: 0.6956614851951599\n",
      "Train: Epoch [14], Batch [10/938], Loss: 0.8269665837287903\n",
      "Train: Epoch [14], Batch [11/938], Loss: 0.7030937671661377\n",
      "Train: Epoch [14], Batch [12/938], Loss: 0.9149038195610046\n",
      "Train: Epoch [14], Batch [13/938], Loss: 0.9939799904823303\n",
      "Train: Epoch [14], Batch [14/938], Loss: 0.5810527801513672\n",
      "Train: Epoch [14], Batch [15/938], Loss: 0.8157691955566406\n",
      "Train: Epoch [14], Batch [16/938], Loss: 0.7534184455871582\n",
      "Train: Epoch [14], Batch [17/938], Loss: 0.7136947512626648\n",
      "Train: Epoch [14], Batch [18/938], Loss: 0.5514199137687683\n",
      "Train: Epoch [14], Batch [19/938], Loss: 0.795458197593689\n",
      "Train: Epoch [14], Batch [20/938], Loss: 1.1647357940673828\n",
      "Train: Epoch [14], Batch [21/938], Loss: 0.6626853942871094\n",
      "Train: Epoch [14], Batch [22/938], Loss: 0.8285556435585022\n",
      "Train: Epoch [14], Batch [23/938], Loss: 0.8418471813201904\n",
      "Train: Epoch [14], Batch [24/938], Loss: 0.6316996812820435\n",
      "Train: Epoch [14], Batch [25/938], Loss: 0.7692932486534119\n",
      "Train: Epoch [14], Batch [26/938], Loss: 0.9315782785415649\n",
      "Train: Epoch [14], Batch [27/938], Loss: 0.4892275333404541\n",
      "Train: Epoch [14], Batch [28/938], Loss: 0.5951064229011536\n",
      "Train: Epoch [14], Batch [29/938], Loss: 0.6751483678817749\n",
      "Train: Epoch [14], Batch [30/938], Loss: 0.6141104102134705\n",
      "Train: Epoch [14], Batch [31/938], Loss: 0.7234284281730652\n",
      "Train: Epoch [14], Batch [32/938], Loss: 0.6613328456878662\n",
      "Train: Epoch [14], Batch [33/938], Loss: 0.5040774345397949\n",
      "Train: Epoch [14], Batch [34/938], Loss: 0.5749110579490662\n",
      "Train: Epoch [14], Batch [35/938], Loss: 0.6615427136421204\n",
      "Train: Epoch [14], Batch [36/938], Loss: 0.9044253826141357\n",
      "Train: Epoch [14], Batch [37/938], Loss: 0.6226056814193726\n",
      "Train: Epoch [14], Batch [38/938], Loss: 0.7794049382209778\n",
      "Train: Epoch [14], Batch [39/938], Loss: 0.9845682382583618\n",
      "Train: Epoch [14], Batch [40/938], Loss: 0.5797871351242065\n",
      "Train: Epoch [14], Batch [41/938], Loss: 0.975050687789917\n",
      "Train: Epoch [14], Batch [42/938], Loss: 0.7280221581459045\n",
      "Train: Epoch [14], Batch [43/938], Loss: 0.8223581910133362\n",
      "Train: Epoch [14], Batch [44/938], Loss: 0.913701593875885\n",
      "Train: Epoch [14], Batch [45/938], Loss: 0.7081571221351624\n",
      "Train: Epoch [14], Batch [46/938], Loss: 0.5246210694313049\n",
      "Train: Epoch [14], Batch [47/938], Loss: 0.8731304407119751\n",
      "Train: Epoch [14], Batch [48/938], Loss: 0.5253794193267822\n",
      "Train: Epoch [14], Batch [49/938], Loss: 0.6677163243293762\n",
      "Train: Epoch [14], Batch [50/938], Loss: 0.6048446297645569\n",
      "Train: Epoch [14], Batch [51/938], Loss: 0.5880720615386963\n",
      "Train: Epoch [14], Batch [52/938], Loss: 0.6911332011222839\n",
      "Train: Epoch [14], Batch [53/938], Loss: 0.7521321177482605\n",
      "Train: Epoch [14], Batch [54/938], Loss: 0.8228940963745117\n",
      "Train: Epoch [14], Batch [55/938], Loss: 0.6168217062950134\n",
      "Train: Epoch [14], Batch [56/938], Loss: 0.6005599498748779\n",
      "Train: Epoch [14], Batch [57/938], Loss: 0.7172892093658447\n",
      "Train: Epoch [14], Batch [58/938], Loss: 0.8409885168075562\n",
      "Train: Epoch [14], Batch [59/938], Loss: 0.38960835337638855\n",
      "Train: Epoch [14], Batch [60/938], Loss: 0.6948075294494629\n",
      "Train: Epoch [14], Batch [61/938], Loss: 0.5208328366279602\n",
      "Train: Epoch [14], Batch [62/938], Loss: 0.6480967402458191\n",
      "Train: Epoch [14], Batch [63/938], Loss: 0.7094106078147888\n",
      "Train: Epoch [14], Batch [64/938], Loss: 0.6602230072021484\n",
      "Train: Epoch [14], Batch [65/938], Loss: 0.7287250757217407\n",
      "Train: Epoch [14], Batch [66/938], Loss: 0.7346419095993042\n",
      "Train: Epoch [14], Batch [67/938], Loss: 0.8145833015441895\n",
      "Train: Epoch [14], Batch [68/938], Loss: 0.8627791404724121\n",
      "Train: Epoch [14], Batch [69/938], Loss: 0.8888890743255615\n",
      "Train: Epoch [14], Batch [70/938], Loss: 0.8530534505844116\n",
      "Train: Epoch [14], Batch [71/938], Loss: 0.5438992977142334\n",
      "Train: Epoch [14], Batch [72/938], Loss: 0.7556689381599426\n",
      "Train: Epoch [14], Batch [73/938], Loss: 0.7287588119506836\n",
      "Train: Epoch [14], Batch [74/938], Loss: 0.6691158413887024\n",
      "Train: Epoch [14], Batch [75/938], Loss: 0.5927425026893616\n",
      "Train: Epoch [14], Batch [76/938], Loss: 0.7745582461357117\n",
      "Train: Epoch [14], Batch [77/938], Loss: 0.5745532512664795\n",
      "Train: Epoch [14], Batch [78/938], Loss: 0.4580341577529907\n",
      "Train: Epoch [14], Batch [79/938], Loss: 0.5173888206481934\n",
      "Train: Epoch [14], Batch [80/938], Loss: 0.6315194368362427\n",
      "Train: Epoch [14], Batch [81/938], Loss: 0.5439000725746155\n",
      "Train: Epoch [14], Batch [82/938], Loss: 0.6535637974739075\n",
      "Train: Epoch [14], Batch [83/938], Loss: 0.7000703811645508\n",
      "Train: Epoch [14], Batch [84/938], Loss: 0.5149074792861938\n",
      "Train: Epoch [14], Batch [85/938], Loss: 0.7334510684013367\n",
      "Train: Epoch [14], Batch [86/938], Loss: 0.7999651432037354\n",
      "Train: Epoch [14], Batch [87/938], Loss: 0.6901194453239441\n",
      "Train: Epoch [14], Batch [88/938], Loss: 1.1622010469436646\n",
      "Train: Epoch [14], Batch [89/938], Loss: 0.7423914074897766\n",
      "Train: Epoch [14], Batch [90/938], Loss: 0.8807764649391174\n",
      "Train: Epoch [14], Batch [91/938], Loss: 0.677666962146759\n",
      "Train: Epoch [14], Batch [92/938], Loss: 0.887030303478241\n",
      "Train: Epoch [14], Batch [93/938], Loss: 0.7231824398040771\n",
      "Train: Epoch [14], Batch [94/938], Loss: 0.8168851733207703\n",
      "Train: Epoch [14], Batch [95/938], Loss: 0.5073344707489014\n",
      "Train: Epoch [14], Batch [96/938], Loss: 0.6593626141548157\n",
      "Train: Epoch [14], Batch [97/938], Loss: 0.7349350452423096\n",
      "Train: Epoch [14], Batch [98/938], Loss: 0.5800343751907349\n",
      "Train: Epoch [14], Batch [99/938], Loss: 0.8047378659248352\n",
      "Train: Epoch [14], Batch [100/938], Loss: 0.7381479144096375\n",
      "Train: Epoch [14], Batch [101/938], Loss: 0.6431478261947632\n",
      "Train: Epoch [14], Batch [102/938], Loss: 0.784140408039093\n",
      "Train: Epoch [14], Batch [103/938], Loss: 0.6308860778808594\n",
      "Train: Epoch [14], Batch [104/938], Loss: 1.0045644044876099\n",
      "Train: Epoch [14], Batch [105/938], Loss: 0.477030485868454\n",
      "Train: Epoch [14], Batch [106/938], Loss: 0.7967436909675598\n",
      "Train: Epoch [14], Batch [107/938], Loss: 0.6399476528167725\n",
      "Train: Epoch [14], Batch [108/938], Loss: 0.6822603940963745\n",
      "Train: Epoch [14], Batch [109/938], Loss: 0.6430928111076355\n",
      "Train: Epoch [14], Batch [110/938], Loss: 0.6926097869873047\n",
      "Train: Epoch [14], Batch [111/938], Loss: 0.7275579571723938\n",
      "Train: Epoch [14], Batch [112/938], Loss: 0.8859391808509827\n",
      "Train: Epoch [14], Batch [113/938], Loss: 0.631889283657074\n",
      "Train: Epoch [14], Batch [114/938], Loss: 0.872627317905426\n",
      "Train: Epoch [14], Batch [115/938], Loss: 0.9578096866607666\n",
      "Train: Epoch [14], Batch [116/938], Loss: 0.7852115035057068\n",
      "Train: Epoch [14], Batch [117/938], Loss: 0.7252887487411499\n",
      "Train: Epoch [14], Batch [118/938], Loss: 0.8443502187728882\n",
      "Train: Epoch [14], Batch [119/938], Loss: 0.884810745716095\n",
      "Train: Epoch [14], Batch [120/938], Loss: 0.7257903814315796\n",
      "Train: Epoch [14], Batch [121/938], Loss: 0.7234625816345215\n",
      "Train: Epoch [14], Batch [122/938], Loss: 0.8444490432739258\n",
      "Train: Epoch [14], Batch [123/938], Loss: 0.9486485719680786\n",
      "Train: Epoch [14], Batch [124/938], Loss: 0.735004723072052\n",
      "Train: Epoch [14], Batch [125/938], Loss: 0.6530411839485168\n",
      "Train: Epoch [14], Batch [126/938], Loss: 0.41329893469810486\n",
      "Train: Epoch [14], Batch [127/938], Loss: 0.42830294370651245\n",
      "Train: Epoch [14], Batch [128/938], Loss: 0.6478216052055359\n",
      "Train: Epoch [14], Batch [129/938], Loss: 0.5460034608840942\n",
      "Train: Epoch [14], Batch [130/938], Loss: 0.7890073657035828\n",
      "Train: Epoch [14], Batch [131/938], Loss: 0.9092272520065308\n",
      "Train: Epoch [14], Batch [132/938], Loss: 0.7344928979873657\n",
      "Train: Epoch [14], Batch [133/938], Loss: 0.5083181858062744\n",
      "Train: Epoch [14], Batch [134/938], Loss: 0.7866202592849731\n",
      "Train: Epoch [14], Batch [135/938], Loss: 0.5885900259017944\n",
      "Train: Epoch [14], Batch [136/938], Loss: 0.5399353504180908\n",
      "Train: Epoch [14], Batch [137/938], Loss: 0.5817372798919678\n",
      "Train: Epoch [14], Batch [138/938], Loss: 0.4665811061859131\n",
      "Train: Epoch [14], Batch [139/938], Loss: 0.39840906858444214\n",
      "Train: Epoch [14], Batch [140/938], Loss: 0.6179258823394775\n",
      "Train: Epoch [14], Batch [141/938], Loss: 0.4816826581954956\n",
      "Train: Epoch [14], Batch [142/938], Loss: 0.708519697189331\n",
      "Train: Epoch [14], Batch [143/938], Loss: 0.7882959246635437\n",
      "Train: Epoch [14], Batch [144/938], Loss: 0.5937944054603577\n",
      "Train: Epoch [14], Batch [145/938], Loss: 0.5383221507072449\n",
      "Train: Epoch [14], Batch [146/938], Loss: 0.7372982501983643\n",
      "Train: Epoch [14], Batch [147/938], Loss: 0.9476385712623596\n",
      "Train: Epoch [14], Batch [148/938], Loss: 1.0855650901794434\n",
      "Train: Epoch [14], Batch [149/938], Loss: 0.7711252570152283\n",
      "Train: Epoch [14], Batch [150/938], Loss: 0.542794406414032\n",
      "Train: Epoch [14], Batch [151/938], Loss: 0.5555737018585205\n",
      "Train: Epoch [14], Batch [152/938], Loss: 0.522847056388855\n",
      "Train: Epoch [14], Batch [153/938], Loss: 0.8352205753326416\n",
      "Train: Epoch [14], Batch [154/938], Loss: 0.9533601999282837\n",
      "Train: Epoch [14], Batch [155/938], Loss: 0.6544150710105896\n",
      "Train: Epoch [14], Batch [156/938], Loss: 0.48045337200164795\n",
      "Train: Epoch [14], Batch [157/938], Loss: 0.8567969799041748\n",
      "Train: Epoch [14], Batch [158/938], Loss: 0.6963552236557007\n",
      "Train: Epoch [14], Batch [159/938], Loss: 0.9745904207229614\n",
      "Train: Epoch [14], Batch [160/938], Loss: 0.6443262100219727\n",
      "Train: Epoch [14], Batch [161/938], Loss: 0.5641421675682068\n",
      "Train: Epoch [14], Batch [162/938], Loss: 0.4183294177055359\n",
      "Train: Epoch [14], Batch [163/938], Loss: 0.8199501633644104\n",
      "Train: Epoch [14], Batch [164/938], Loss: 0.993894636631012\n",
      "Train: Epoch [14], Batch [165/938], Loss: 0.9399476647377014\n",
      "Train: Epoch [14], Batch [166/938], Loss: 0.6267653703689575\n",
      "Train: Epoch [14], Batch [167/938], Loss: 0.8288145065307617\n",
      "Train: Epoch [14], Batch [168/938], Loss: 0.7615528702735901\n",
      "Train: Epoch [14], Batch [169/938], Loss: 0.7125698328018188\n",
      "Train: Epoch [14], Batch [170/938], Loss: 0.9164873957633972\n",
      "Train: Epoch [14], Batch [171/938], Loss: 0.7598468661308289\n",
      "Train: Epoch [14], Batch [172/938], Loss: 0.7206453084945679\n",
      "Train: Epoch [14], Batch [173/938], Loss: 0.9169614315032959\n",
      "Train: Epoch [14], Batch [174/938], Loss: 0.5953203439712524\n",
      "Train: Epoch [14], Batch [175/938], Loss: 0.6540268659591675\n",
      "Train: Epoch [14], Batch [176/938], Loss: 0.5203855037689209\n",
      "Train: Epoch [14], Batch [177/938], Loss: 0.6362398862838745\n",
      "Train: Epoch [14], Batch [178/938], Loss: 0.7973490357398987\n",
      "Train: Epoch [14], Batch [179/938], Loss: 0.8459700345993042\n",
      "Train: Epoch [14], Batch [180/938], Loss: 0.6931333541870117\n",
      "Train: Epoch [14], Batch [181/938], Loss: 0.8176602125167847\n",
      "Train: Epoch [14], Batch [182/938], Loss: 0.6676172018051147\n",
      "Train: Epoch [14], Batch [183/938], Loss: 0.6918657422065735\n",
      "Train: Epoch [14], Batch [184/938], Loss: 0.9972866773605347\n",
      "Train: Epoch [14], Batch [185/938], Loss: 0.9597651958465576\n",
      "Train: Epoch [14], Batch [186/938], Loss: 0.7435511350631714\n",
      "Train: Epoch [14], Batch [187/938], Loss: 0.7362269759178162\n",
      "Train: Epoch [14], Batch [188/938], Loss: 0.729586660861969\n",
      "Train: Epoch [14], Batch [189/938], Loss: 0.7216589450836182\n",
      "Train: Epoch [14], Batch [190/938], Loss: 0.5407397150993347\n",
      "Train: Epoch [14], Batch [191/938], Loss: 0.6543810367584229\n",
      "Train: Epoch [14], Batch [192/938], Loss: 0.7850034832954407\n",
      "Train: Epoch [14], Batch [193/938], Loss: 0.9149078130722046\n",
      "Train: Epoch [14], Batch [194/938], Loss: 0.7485264539718628\n",
      "Train: Epoch [14], Batch [195/938], Loss: 0.8054505586624146\n",
      "Train: Epoch [14], Batch [196/938], Loss: 0.5899194478988647\n",
      "Train: Epoch [14], Batch [197/938], Loss: 0.5248271226882935\n",
      "Train: Epoch [14], Batch [198/938], Loss: 0.5390158891677856\n",
      "Train: Epoch [14], Batch [199/938], Loss: 0.670572817325592\n",
      "Train: Epoch [14], Batch [200/938], Loss: 0.7815864682197571\n",
      "Train: Epoch [14], Batch [201/938], Loss: 0.8429152369499207\n",
      "Train: Epoch [14], Batch [202/938], Loss: 0.5835898518562317\n",
      "Train: Epoch [14], Batch [203/938], Loss: 0.6081895232200623\n",
      "Train: Epoch [14], Batch [204/938], Loss: 0.7289261817932129\n",
      "Train: Epoch [14], Batch [205/938], Loss: 0.7394797801971436\n",
      "Train: Epoch [14], Batch [206/938], Loss: 0.6344116926193237\n",
      "Train: Epoch [14], Batch [207/938], Loss: 0.6257339119911194\n",
      "Train: Epoch [14], Batch [208/938], Loss: 0.8089334964752197\n",
      "Train: Epoch [14], Batch [209/938], Loss: 1.0975371599197388\n",
      "Train: Epoch [14], Batch [210/938], Loss: 0.7241597175598145\n",
      "Train: Epoch [14], Batch [211/938], Loss: 0.794150173664093\n",
      "Train: Epoch [14], Batch [212/938], Loss: 0.6425799131393433\n",
      "Train: Epoch [14], Batch [213/938], Loss: 0.7205352783203125\n",
      "Train: Epoch [14], Batch [214/938], Loss: 0.8899175524711609\n",
      "Train: Epoch [14], Batch [215/938], Loss: 0.5996504426002502\n",
      "Train: Epoch [14], Batch [216/938], Loss: 1.0040980577468872\n",
      "Train: Epoch [14], Batch [217/938], Loss: 0.5551838874816895\n",
      "Train: Epoch [14], Batch [218/938], Loss: 0.8687524795532227\n",
      "Train: Epoch [14], Batch [219/938], Loss: 0.7325006723403931\n",
      "Train: Epoch [14], Batch [220/938], Loss: 0.40723738074302673\n",
      "Train: Epoch [14], Batch [221/938], Loss: 0.6541553735733032\n",
      "Train: Epoch [14], Batch [222/938], Loss: 0.6813775897026062\n",
      "Train: Epoch [14], Batch [223/938], Loss: 0.7670413255691528\n",
      "Train: Epoch [14], Batch [224/938], Loss: 0.8758713006973267\n",
      "Train: Epoch [14], Batch [225/938], Loss: 0.9400430917739868\n",
      "Train: Epoch [14], Batch [226/938], Loss: 0.5831089615821838\n",
      "Train: Epoch [14], Batch [227/938], Loss: 0.7348033785820007\n",
      "Train: Epoch [14], Batch [228/938], Loss: 0.5810544490814209\n",
      "Train: Epoch [14], Batch [229/938], Loss: 0.6426860690116882\n",
      "Train: Epoch [14], Batch [230/938], Loss: 0.5583035349845886\n",
      "Train: Epoch [14], Batch [231/938], Loss: 0.8099197149276733\n",
      "Train: Epoch [14], Batch [232/938], Loss: 0.5086231231689453\n",
      "Train: Epoch [14], Batch [233/938], Loss: 0.6669020652770996\n",
      "Train: Epoch [14], Batch [234/938], Loss: 0.9629690647125244\n",
      "Train: Epoch [14], Batch [235/938], Loss: 0.6908179521560669\n",
      "Train: Epoch [14], Batch [236/938], Loss: 0.8430586457252502\n",
      "Train: Epoch [14], Batch [237/938], Loss: 0.6414794325828552\n",
      "Train: Epoch [14], Batch [238/938], Loss: 0.7120533585548401\n",
      "Train: Epoch [14], Batch [239/938], Loss: 0.9327422380447388\n",
      "Train: Epoch [14], Batch [240/938], Loss: 0.7935795187950134\n",
      "Train: Epoch [14], Batch [241/938], Loss: 0.8316084742546082\n",
      "Train: Epoch [14], Batch [242/938], Loss: 0.7335296273231506\n",
      "Train: Epoch [14], Batch [243/938], Loss: 0.8938409090042114\n",
      "Train: Epoch [14], Batch [244/938], Loss: 0.7656499743461609\n",
      "Train: Epoch [14], Batch [245/938], Loss: 0.7053844928741455\n",
      "Train: Epoch [14], Batch [246/938], Loss: 0.5891082882881165\n",
      "Train: Epoch [14], Batch [247/938], Loss: 0.5870333313941956\n",
      "Train: Epoch [14], Batch [248/938], Loss: 0.7616460919380188\n",
      "Train: Epoch [14], Batch [249/938], Loss: 0.5661836266517639\n",
      "Train: Epoch [14], Batch [250/938], Loss: 0.5948426127433777\n",
      "Train: Epoch [14], Batch [251/938], Loss: 0.6423186659812927\n",
      "Train: Epoch [14], Batch [252/938], Loss: 0.7219709753990173\n",
      "Train: Epoch [14], Batch [253/938], Loss: 0.683793306350708\n",
      "Train: Epoch [14], Batch [254/938], Loss: 1.0806605815887451\n",
      "Train: Epoch [14], Batch [255/938], Loss: 0.7243736982345581\n",
      "Train: Epoch [14], Batch [256/938], Loss: 0.7796049118041992\n",
      "Train: Epoch [14], Batch [257/938], Loss: 0.9162232875823975\n",
      "Train: Epoch [14], Batch [258/938], Loss: 0.65803462266922\n",
      "Train: Epoch [14], Batch [259/938], Loss: 0.7875639200210571\n",
      "Train: Epoch [14], Batch [260/938], Loss: 0.765993058681488\n",
      "Train: Epoch [14], Batch [261/938], Loss: 0.6613413691520691\n",
      "Train: Epoch [14], Batch [262/938], Loss: 0.5998120903968811\n",
      "Train: Epoch [14], Batch [263/938], Loss: 0.6097287535667419\n",
      "Train: Epoch [14], Batch [264/938], Loss: 0.7432515621185303\n",
      "Train: Epoch [14], Batch [265/938], Loss: 0.7658651471138\n",
      "Train: Epoch [14], Batch [266/938], Loss: 0.7768295407295227\n",
      "Train: Epoch [14], Batch [267/938], Loss: 0.7963789105415344\n",
      "Train: Epoch [14], Batch [268/938], Loss: 0.5965372920036316\n",
      "Train: Epoch [14], Batch [269/938], Loss: 0.9109416007995605\n",
      "Train: Epoch [14], Batch [270/938], Loss: 0.6394789218902588\n",
      "Train: Epoch [14], Batch [271/938], Loss: 0.5597250461578369\n",
      "Train: Epoch [14], Batch [272/938], Loss: 0.7374393939971924\n",
      "Train: Epoch [14], Batch [273/938], Loss: 0.772652268409729\n",
      "Train: Epoch [14], Batch [274/938], Loss: 0.736028790473938\n",
      "Train: Epoch [14], Batch [275/938], Loss: 0.6964820623397827\n",
      "Train: Epoch [14], Batch [276/938], Loss: 0.645914614200592\n",
      "Train: Epoch [14], Batch [277/938], Loss: 0.674992561340332\n",
      "Train: Epoch [14], Batch [278/938], Loss: 0.6049402952194214\n",
      "Train: Epoch [14], Batch [279/938], Loss: 0.8167520761489868\n",
      "Train: Epoch [14], Batch [280/938], Loss: 0.47771915793418884\n",
      "Train: Epoch [14], Batch [281/938], Loss: 0.8068878054618835\n",
      "Train: Epoch [14], Batch [282/938], Loss: 0.7324961423873901\n",
      "Train: Epoch [14], Batch [283/938], Loss: 0.7984076738357544\n",
      "Train: Epoch [14], Batch [284/938], Loss: 0.7158583998680115\n",
      "Train: Epoch [14], Batch [285/938], Loss: 0.6937471628189087\n",
      "Train: Epoch [14], Batch [286/938], Loss: 0.7894043922424316\n",
      "Train: Epoch [14], Batch [287/938], Loss: 0.5917149782180786\n",
      "Train: Epoch [14], Batch [288/938], Loss: 0.4624132215976715\n",
      "Train: Epoch [14], Batch [289/938], Loss: 0.7992368936538696\n",
      "Train: Epoch [14], Batch [290/938], Loss: 0.8136681318283081\n",
      "Train: Epoch [14], Batch [291/938], Loss: 0.6632042527198792\n",
      "Train: Epoch [14], Batch [292/938], Loss: 0.6720138788223267\n",
      "Train: Epoch [14], Batch [293/938], Loss: 0.6983118057250977\n",
      "Train: Epoch [14], Batch [294/938], Loss: 0.7953401207923889\n",
      "Train: Epoch [14], Batch [295/938], Loss: 0.8506581783294678\n",
      "Train: Epoch [14], Batch [296/938], Loss: 0.5625165700912476\n",
      "Train: Epoch [14], Batch [297/938], Loss: 0.8004949688911438\n",
      "Train: Epoch [14], Batch [298/938], Loss: 0.7763134241104126\n",
      "Train: Epoch [14], Batch [299/938], Loss: 0.900902509689331\n",
      "Train: Epoch [14], Batch [300/938], Loss: 0.7366107702255249\n",
      "Train: Epoch [14], Batch [301/938], Loss: 0.7307267785072327\n",
      "Train: Epoch [14], Batch [302/938], Loss: 0.6444037556648254\n",
      "Train: Epoch [14], Batch [303/938], Loss: 0.8260654211044312\n",
      "Train: Epoch [14], Batch [304/938], Loss: 0.7557990550994873\n",
      "Train: Epoch [14], Batch [305/938], Loss: 0.683213472366333\n",
      "Train: Epoch [14], Batch [306/938], Loss: 0.906539261341095\n",
      "Train: Epoch [14], Batch [307/938], Loss: 0.7241332530975342\n",
      "Train: Epoch [14], Batch [308/938], Loss: 0.6532113552093506\n",
      "Train: Epoch [14], Batch [309/938], Loss: 0.6477208137512207\n",
      "Train: Epoch [14], Batch [310/938], Loss: 0.7015348672866821\n",
      "Train: Epoch [14], Batch [311/938], Loss: 0.5040915012359619\n",
      "Train: Epoch [14], Batch [312/938], Loss: 0.688497006893158\n",
      "Train: Epoch [14], Batch [313/938], Loss: 0.7662710547447205\n",
      "Train: Epoch [14], Batch [314/938], Loss: 0.5462695360183716\n",
      "Train: Epoch [14], Batch [315/938], Loss: 0.5946023464202881\n",
      "Train: Epoch [14], Batch [316/938], Loss: 0.7725974321365356\n",
      "Train: Epoch [14], Batch [317/938], Loss: 0.5622230768203735\n",
      "Train: Epoch [14], Batch [318/938], Loss: 0.5948400497436523\n",
      "Train: Epoch [14], Batch [319/938], Loss: 0.8340114951133728\n",
      "Train: Epoch [14], Batch [320/938], Loss: 0.6127748489379883\n",
      "Train: Epoch [14], Batch [321/938], Loss: 0.7005147337913513\n",
      "Train: Epoch [14], Batch [322/938], Loss: 0.5757439136505127\n",
      "Train: Epoch [14], Batch [323/938], Loss: 0.7278251647949219\n",
      "Train: Epoch [14], Batch [324/938], Loss: 0.9061840772628784\n",
      "Train: Epoch [14], Batch [325/938], Loss: 0.6073347926139832\n",
      "Train: Epoch [14], Batch [326/938], Loss: 0.7074569463729858\n",
      "Train: Epoch [14], Batch [327/938], Loss: 0.6880046129226685\n",
      "Train: Epoch [14], Batch [328/938], Loss: 0.5524259209632874\n",
      "Train: Epoch [14], Batch [329/938], Loss: 0.5248029828071594\n",
      "Train: Epoch [14], Batch [330/938], Loss: 0.672377347946167\n",
      "Train: Epoch [14], Batch [331/938], Loss: 0.6312144994735718\n",
      "Train: Epoch [14], Batch [332/938], Loss: 0.4245523512363434\n",
      "Train: Epoch [14], Batch [333/938], Loss: 0.8512174487113953\n",
      "Train: Epoch [14], Batch [334/938], Loss: 1.0666571855545044\n",
      "Train: Epoch [14], Batch [335/938], Loss: 0.8256068229675293\n",
      "Train: Epoch [14], Batch [336/938], Loss: 0.710927426815033\n",
      "Train: Epoch [14], Batch [337/938], Loss: 1.0093097686767578\n",
      "Train: Epoch [14], Batch [338/938], Loss: 0.812515914440155\n",
      "Train: Epoch [14], Batch [339/938], Loss: 0.706291139125824\n",
      "Train: Epoch [14], Batch [340/938], Loss: 0.7806124091148376\n",
      "Train: Epoch [14], Batch [341/938], Loss: 0.6139116287231445\n",
      "Train: Epoch [14], Batch [342/938], Loss: 0.7293307781219482\n",
      "Train: Epoch [14], Batch [343/938], Loss: 0.7038151621818542\n",
      "Train: Epoch [14], Batch [344/938], Loss: 0.6045957207679749\n",
      "Train: Epoch [14], Batch [345/938], Loss: 0.6739829778671265\n",
      "Train: Epoch [14], Batch [346/938], Loss: 0.6613780856132507\n",
      "Train: Epoch [14], Batch [347/938], Loss: 0.597983717918396\n",
      "Train: Epoch [14], Batch [348/938], Loss: 0.5252923965454102\n",
      "Train: Epoch [14], Batch [349/938], Loss: 0.7345861792564392\n",
      "Train: Epoch [14], Batch [350/938], Loss: 1.094264030456543\n",
      "Train: Epoch [14], Batch [351/938], Loss: 0.5537698268890381\n",
      "Train: Epoch [14], Batch [352/938], Loss: 0.6292238235473633\n",
      "Train: Epoch [14], Batch [353/938], Loss: 0.5746952891349792\n",
      "Train: Epoch [14], Batch [354/938], Loss: 0.9168558120727539\n",
      "Train: Epoch [14], Batch [355/938], Loss: 0.7551554441452026\n",
      "Train: Epoch [14], Batch [356/938], Loss: 0.8165127635002136\n",
      "Train: Epoch [14], Batch [357/938], Loss: 0.5234001874923706\n",
      "Train: Epoch [14], Batch [358/938], Loss: 0.5751490592956543\n",
      "Train: Epoch [14], Batch [359/938], Loss: 0.5700110197067261\n",
      "Train: Epoch [14], Batch [360/938], Loss: 0.7156566381454468\n",
      "Train: Epoch [14], Batch [361/938], Loss: 0.6967511177062988\n",
      "Train: Epoch [14], Batch [362/938], Loss: 0.7938704490661621\n",
      "Train: Epoch [14], Batch [363/938], Loss: 0.6984450221061707\n",
      "Train: Epoch [14], Batch [364/938], Loss: 0.7497507333755493\n",
      "Train: Epoch [14], Batch [365/938], Loss: 0.6739555597305298\n",
      "Train: Epoch [14], Batch [366/938], Loss: 0.7753124237060547\n",
      "Train: Epoch [14], Batch [367/938], Loss: 0.6746470928192139\n",
      "Train: Epoch [14], Batch [368/938], Loss: 0.8693316578865051\n",
      "Train: Epoch [14], Batch [369/938], Loss: 0.6989144682884216\n",
      "Train: Epoch [14], Batch [370/938], Loss: 0.7522711157798767\n",
      "Train: Epoch [14], Batch [371/938], Loss: 0.776293933391571\n",
      "Train: Epoch [14], Batch [372/938], Loss: 0.6967555284500122\n",
      "Train: Epoch [14], Batch [373/938], Loss: 0.6852691769599915\n",
      "Train: Epoch [14], Batch [374/938], Loss: 0.7167078852653503\n",
      "Train: Epoch [14], Batch [375/938], Loss: 0.8002325892448425\n",
      "Train: Epoch [14], Batch [376/938], Loss: 0.6967380046844482\n",
      "Train: Epoch [14], Batch [377/938], Loss: 0.7108827829360962\n",
      "Train: Epoch [14], Batch [378/938], Loss: 0.7989375591278076\n",
      "Train: Epoch [14], Batch [379/938], Loss: 0.5306384563446045\n",
      "Train: Epoch [14], Batch [380/938], Loss: 0.5968519449234009\n",
      "Train: Epoch [14], Batch [381/938], Loss: 0.6272287964820862\n",
      "Train: Epoch [14], Batch [382/938], Loss: 0.6244396567344666\n",
      "Train: Epoch [14], Batch [383/938], Loss: 0.859351634979248\n",
      "Train: Epoch [14], Batch [384/938], Loss: 0.7702054381370544\n",
      "Train: Epoch [14], Batch [385/938], Loss: 0.8212347030639648\n",
      "Train: Epoch [14], Batch [386/938], Loss: 0.6741103529930115\n",
      "Train: Epoch [14], Batch [387/938], Loss: 0.8142394423484802\n",
      "Train: Epoch [14], Batch [388/938], Loss: 0.4967757761478424\n",
      "Train: Epoch [14], Batch [389/938], Loss: 0.5889380574226379\n",
      "Train: Epoch [14], Batch [390/938], Loss: 0.6882599592208862\n",
      "Train: Epoch [14], Batch [391/938], Loss: 0.666199803352356\n",
      "Train: Epoch [14], Batch [392/938], Loss: 0.812258780002594\n",
      "Train: Epoch [14], Batch [393/938], Loss: 0.6155162453651428\n",
      "Train: Epoch [14], Batch [394/938], Loss: 0.7966946959495544\n",
      "Train: Epoch [14], Batch [395/938], Loss: 0.7863008975982666\n",
      "Train: Epoch [14], Batch [396/938], Loss: 0.7382386922836304\n",
      "Train: Epoch [14], Batch [397/938], Loss: 0.6359370350837708\n",
      "Train: Epoch [14], Batch [398/938], Loss: 0.8219327926635742\n",
      "Train: Epoch [14], Batch [399/938], Loss: 0.6724280118942261\n",
      "Train: Epoch [14], Batch [400/938], Loss: 0.8121214509010315\n",
      "Train: Epoch [14], Batch [401/938], Loss: 0.7627759575843811\n",
      "Train: Epoch [14], Batch [402/938], Loss: 0.44749900698661804\n",
      "Train: Epoch [14], Batch [403/938], Loss: 0.7192792892456055\n",
      "Train: Epoch [14], Batch [404/938], Loss: 0.7389392852783203\n",
      "Train: Epoch [14], Batch [405/938], Loss: 1.0159988403320312\n",
      "Train: Epoch [14], Batch [406/938], Loss: 0.6908871531486511\n",
      "Train: Epoch [14], Batch [407/938], Loss: 0.7517613172531128\n",
      "Train: Epoch [14], Batch [408/938], Loss: 0.6117786169052124\n",
      "Train: Epoch [14], Batch [409/938], Loss: 0.9533970355987549\n",
      "Train: Epoch [14], Batch [410/938], Loss: 0.8374192118644714\n",
      "Train: Epoch [14], Batch [411/938], Loss: 0.8254165053367615\n",
      "Train: Epoch [14], Batch [412/938], Loss: 0.8053272366523743\n",
      "Train: Epoch [14], Batch [413/938], Loss: 0.6379482746124268\n",
      "Train: Epoch [14], Batch [414/938], Loss: 0.7516182661056519\n",
      "Train: Epoch [14], Batch [415/938], Loss: 0.5496076941490173\n",
      "Train: Epoch [14], Batch [416/938], Loss: 0.5104023814201355\n",
      "Train: Epoch [14], Batch [417/938], Loss: 0.6079410910606384\n",
      "Train: Epoch [14], Batch [418/938], Loss: 0.4995157718658447\n",
      "Train: Epoch [14], Batch [419/938], Loss: 0.7377309203147888\n",
      "Train: Epoch [14], Batch [420/938], Loss: 0.7433945536613464\n",
      "Train: Epoch [14], Batch [421/938], Loss: 0.9052802324295044\n",
      "Train: Epoch [14], Batch [422/938], Loss: 0.8530759811401367\n",
      "Train: Epoch [14], Batch [423/938], Loss: 0.6732948422431946\n",
      "Train: Epoch [14], Batch [424/938], Loss: 0.6557076573371887\n",
      "Train: Epoch [14], Batch [425/938], Loss: 0.8161261677742004\n",
      "Train: Epoch [14], Batch [426/938], Loss: 0.8466169834136963\n",
      "Train: Epoch [14], Batch [427/938], Loss: 0.5919281840324402\n",
      "Train: Epoch [14], Batch [428/938], Loss: 0.7331233620643616\n",
      "Train: Epoch [14], Batch [429/938], Loss: 0.9321622252464294\n",
      "Train: Epoch [14], Batch [430/938], Loss: 0.8169589638710022\n",
      "Train: Epoch [14], Batch [431/938], Loss: 0.6416956782341003\n",
      "Train: Epoch [14], Batch [432/938], Loss: 0.727016031742096\n",
      "Train: Epoch [14], Batch [433/938], Loss: 0.8584089279174805\n",
      "Train: Epoch [14], Batch [434/938], Loss: 0.4831855595111847\n",
      "Train: Epoch [14], Batch [435/938], Loss: 0.91307532787323\n",
      "Train: Epoch [14], Batch [436/938], Loss: 0.5771690607070923\n",
      "Train: Epoch [14], Batch [437/938], Loss: 0.6173840165138245\n",
      "Train: Epoch [14], Batch [438/938], Loss: 0.8007351160049438\n",
      "Train: Epoch [14], Batch [439/938], Loss: 0.7345336675643921\n",
      "Train: Epoch [14], Batch [440/938], Loss: 0.8672735691070557\n",
      "Train: Epoch [14], Batch [441/938], Loss: 0.6597467660903931\n",
      "Train: Epoch [14], Batch [442/938], Loss: 0.834608793258667\n",
      "Train: Epoch [14], Batch [443/938], Loss: 0.6950225830078125\n",
      "Train: Epoch [14], Batch [444/938], Loss: 0.8985364437103271\n",
      "Train: Epoch [14], Batch [445/938], Loss: 0.7337774038314819\n",
      "Train: Epoch [14], Batch [446/938], Loss: 0.8037657141685486\n",
      "Train: Epoch [14], Batch [447/938], Loss: 0.6215524077415466\n",
      "Train: Epoch [14], Batch [448/938], Loss: 0.701362133026123\n",
      "Train: Epoch [14], Batch [449/938], Loss: 1.0177905559539795\n",
      "Train: Epoch [14], Batch [450/938], Loss: 0.54081791639328\n",
      "Train: Epoch [14], Batch [451/938], Loss: 0.755500078201294\n",
      "Train: Epoch [14], Batch [452/938], Loss: 0.8400564789772034\n",
      "Train: Epoch [14], Batch [453/938], Loss: 0.6214734315872192\n",
      "Train: Epoch [14], Batch [454/938], Loss: 0.7263085842132568\n",
      "Train: Epoch [14], Batch [455/938], Loss: 0.6992791891098022\n",
      "Train: Epoch [14], Batch [456/938], Loss: 0.7335578203201294\n",
      "Train: Epoch [14], Batch [457/938], Loss: 0.6683942079544067\n",
      "Train: Epoch [14], Batch [458/938], Loss: 0.5948451161384583\n",
      "Train: Epoch [14], Batch [459/938], Loss: 0.828889012336731\n",
      "Train: Epoch [14], Batch [460/938], Loss: 0.9092352390289307\n",
      "Train: Epoch [14], Batch [461/938], Loss: 0.6697654128074646\n",
      "Train: Epoch [14], Batch [462/938], Loss: 0.6728429794311523\n",
      "Train: Epoch [14], Batch [463/938], Loss: 0.590610921382904\n",
      "Train: Epoch [14], Batch [464/938], Loss: 0.8420453071594238\n",
      "Train: Epoch [14], Batch [465/938], Loss: 1.2351280450820923\n",
      "Train: Epoch [14], Batch [466/938], Loss: 0.8224365711212158\n",
      "Train: Epoch [14], Batch [467/938], Loss: 0.7624310255050659\n",
      "Train: Epoch [14], Batch [468/938], Loss: 0.7402127385139465\n",
      "Train: Epoch [14], Batch [469/938], Loss: 0.42742305994033813\n",
      "Train: Epoch [14], Batch [470/938], Loss: 0.5697504878044128\n",
      "Train: Epoch [14], Batch [471/938], Loss: 0.4684378206729889\n",
      "Train: Epoch [14], Batch [472/938], Loss: 0.9235895872116089\n",
      "Train: Epoch [14], Batch [473/938], Loss: 0.6776517629623413\n",
      "Train: Epoch [14], Batch [474/938], Loss: 0.8126316070556641\n",
      "Train: Epoch [14], Batch [475/938], Loss: 0.6694188117980957\n",
      "Train: Epoch [14], Batch [476/938], Loss: 0.5814199447631836\n",
      "Train: Epoch [14], Batch [477/938], Loss: 0.6611566543579102\n",
      "Train: Epoch [14], Batch [478/938], Loss: 0.5052326321601868\n",
      "Train: Epoch [14], Batch [479/938], Loss: 0.6877044439315796\n",
      "Train: Epoch [14], Batch [480/938], Loss: 0.6900520324707031\n",
      "Train: Epoch [14], Batch [481/938], Loss: 0.5960929989814758\n",
      "Train: Epoch [14], Batch [482/938], Loss: 0.6765571236610413\n",
      "Train: Epoch [14], Batch [483/938], Loss: 0.7838643789291382\n",
      "Train: Epoch [14], Batch [484/938], Loss: 0.6655712723731995\n",
      "Train: Epoch [14], Batch [485/938], Loss: 0.5177406668663025\n",
      "Train: Epoch [14], Batch [486/938], Loss: 0.8343124389648438\n",
      "Train: Epoch [14], Batch [487/938], Loss: 0.9759060144424438\n",
      "Train: Epoch [14], Batch [488/938], Loss: 0.614868700504303\n",
      "Train: Epoch [14], Batch [489/938], Loss: 0.578682541847229\n",
      "Train: Epoch [14], Batch [490/938], Loss: 0.7685360908508301\n",
      "Train: Epoch [14], Batch [491/938], Loss: 0.8870011568069458\n",
      "Train: Epoch [14], Batch [492/938], Loss: 0.6364421248435974\n",
      "Train: Epoch [14], Batch [493/938], Loss: 0.7152219414710999\n",
      "Train: Epoch [14], Batch [494/938], Loss: 0.6505371332168579\n",
      "Train: Epoch [14], Batch [495/938], Loss: 0.6533192992210388\n",
      "Train: Epoch [14], Batch [496/938], Loss: 0.6654461026191711\n",
      "Train: Epoch [14], Batch [497/938], Loss: 0.900537371635437\n",
      "Train: Epoch [14], Batch [498/938], Loss: 0.8024111390113831\n",
      "Train: Epoch [14], Batch [499/938], Loss: 0.8834708333015442\n",
      "Train: Epoch [14], Batch [500/938], Loss: 0.8187607526779175\n",
      "Train: Epoch [14], Batch [501/938], Loss: 0.823060929775238\n",
      "Train: Epoch [14], Batch [502/938], Loss: 0.6742476224899292\n",
      "Train: Epoch [14], Batch [503/938], Loss: 0.510998010635376\n",
      "Train: Epoch [14], Batch [504/938], Loss: 0.5601235628128052\n",
      "Train: Epoch [14], Batch [505/938], Loss: 0.6616825461387634\n",
      "Train: Epoch [14], Batch [506/938], Loss: 0.897516131401062\n",
      "Train: Epoch [14], Batch [507/938], Loss: 0.681035578250885\n",
      "Train: Epoch [14], Batch [508/938], Loss: 0.6291559338569641\n",
      "Train: Epoch [14], Batch [509/938], Loss: 1.0247067213058472\n",
      "Train: Epoch [14], Batch [510/938], Loss: 0.7790709137916565\n",
      "Train: Epoch [14], Batch [511/938], Loss: 0.8450947999954224\n",
      "Train: Epoch [14], Batch [512/938], Loss: 0.7399880290031433\n",
      "Train: Epoch [14], Batch [513/938], Loss: 0.5723423957824707\n",
      "Train: Epoch [14], Batch [514/938], Loss: 0.7689424753189087\n",
      "Train: Epoch [14], Batch [515/938], Loss: 0.7333939671516418\n",
      "Train: Epoch [14], Batch [516/938], Loss: 0.5922225713729858\n",
      "Train: Epoch [14], Batch [517/938], Loss: 0.7895383834838867\n",
      "Train: Epoch [14], Batch [518/938], Loss: 0.7891970872879028\n",
      "Train: Epoch [14], Batch [519/938], Loss: 0.8558173775672913\n",
      "Train: Epoch [14], Batch [520/938], Loss: 0.6892349123954773\n",
      "Train: Epoch [14], Batch [521/938], Loss: 0.5984428524971008\n",
      "Train: Epoch [14], Batch [522/938], Loss: 0.7003644704818726\n",
      "Train: Epoch [14], Batch [523/938], Loss: 0.9196303486824036\n",
      "Train: Epoch [14], Batch [524/938], Loss: 0.555493950843811\n",
      "Train: Epoch [14], Batch [525/938], Loss: 0.6976054310798645\n",
      "Train: Epoch [14], Batch [526/938], Loss: 0.870669960975647\n",
      "Train: Epoch [14], Batch [527/938], Loss: 0.8001258373260498\n",
      "Train: Epoch [14], Batch [528/938], Loss: 0.6748971939086914\n",
      "Train: Epoch [14], Batch [529/938], Loss: 0.6301777362823486\n",
      "Train: Epoch [14], Batch [530/938], Loss: 0.7944703102111816\n",
      "Train: Epoch [14], Batch [531/938], Loss: 0.8992207050323486\n",
      "Train: Epoch [14], Batch [532/938], Loss: 0.5325913429260254\n",
      "Train: Epoch [14], Batch [533/938], Loss: 0.792605996131897\n",
      "Train: Epoch [14], Batch [534/938], Loss: 0.563654899597168\n",
      "Train: Epoch [14], Batch [535/938], Loss: 0.8790189027786255\n",
      "Train: Epoch [14], Batch [536/938], Loss: 0.6194832921028137\n",
      "Train: Epoch [14], Batch [537/938], Loss: 0.6264960765838623\n",
      "Train: Epoch [14], Batch [538/938], Loss: 0.8070301413536072\n",
      "Train: Epoch [14], Batch [539/938], Loss: 0.6843432784080505\n",
      "Train: Epoch [14], Batch [540/938], Loss: 0.7023298740386963\n",
      "Train: Epoch [14], Batch [541/938], Loss: 0.6066439151763916\n",
      "Train: Epoch [14], Batch [542/938], Loss: 0.8556230068206787\n",
      "Train: Epoch [14], Batch [543/938], Loss: 0.7808982133865356\n",
      "Train: Epoch [14], Batch [544/938], Loss: 0.8124873638153076\n",
      "Train: Epoch [14], Batch [545/938], Loss: 0.6349003911018372\n",
      "Train: Epoch [14], Batch [546/938], Loss: 0.7159534096717834\n",
      "Train: Epoch [14], Batch [547/938], Loss: 0.5942367911338806\n",
      "Train: Epoch [14], Batch [548/938], Loss: 0.604402482509613\n",
      "Train: Epoch [14], Batch [549/938], Loss: 0.6031116843223572\n",
      "Train: Epoch [14], Batch [550/938], Loss: 0.6417897939682007\n",
      "Train: Epoch [14], Batch [551/938], Loss: 0.7954221963882446\n",
      "Train: Epoch [14], Batch [552/938], Loss: 0.7190977931022644\n",
      "Train: Epoch [14], Batch [553/938], Loss: 0.8686814904212952\n",
      "Train: Epoch [14], Batch [554/938], Loss: 0.7212942838668823\n",
      "Train: Epoch [14], Batch [555/938], Loss: 0.7897271513938904\n",
      "Train: Epoch [14], Batch [556/938], Loss: 0.6117674112319946\n",
      "Train: Epoch [14], Batch [557/938], Loss: 0.653440535068512\n",
      "Train: Epoch [14], Batch [558/938], Loss: 0.7310953736305237\n",
      "Train: Epoch [14], Batch [559/938], Loss: 0.8555805683135986\n",
      "Train: Epoch [14], Batch [560/938], Loss: 0.7891613841056824\n",
      "Train: Epoch [14], Batch [561/938], Loss: 0.5381816029548645\n",
      "Train: Epoch [14], Batch [562/938], Loss: 0.5718519687652588\n",
      "Train: Epoch [14], Batch [563/938], Loss: 0.7116115689277649\n",
      "Train: Epoch [14], Batch [564/938], Loss: 0.5963625311851501\n",
      "Train: Epoch [14], Batch [565/938], Loss: 0.6498149037361145\n",
      "Train: Epoch [14], Batch [566/938], Loss: 0.6768950819969177\n",
      "Train: Epoch [14], Batch [567/938], Loss: 0.5360288023948669\n",
      "Train: Epoch [14], Batch [568/938], Loss: 0.7360544204711914\n",
      "Train: Epoch [14], Batch [569/938], Loss: 0.8040602207183838\n",
      "Train: Epoch [14], Batch [570/938], Loss: 0.8724879026412964\n",
      "Train: Epoch [14], Batch [571/938], Loss: 0.8429827690124512\n",
      "Train: Epoch [14], Batch [572/938], Loss: 0.7130910158157349\n",
      "Train: Epoch [14], Batch [573/938], Loss: 0.702858030796051\n",
      "Train: Epoch [14], Batch [574/938], Loss: 0.7037014961242676\n",
      "Train: Epoch [14], Batch [575/938], Loss: 0.785606324672699\n",
      "Train: Epoch [14], Batch [576/938], Loss: 0.6434900760650635\n",
      "Train: Epoch [14], Batch [577/938], Loss: 0.7436054348945618\n",
      "Train: Epoch [14], Batch [578/938], Loss: 0.7137370109558105\n",
      "Train: Epoch [14], Batch [579/938], Loss: 0.816914975643158\n",
      "Train: Epoch [14], Batch [580/938], Loss: 0.598073422908783\n",
      "Train: Epoch [14], Batch [581/938], Loss: 0.9830886721611023\n",
      "Train: Epoch [14], Batch [582/938], Loss: 0.5973865985870361\n",
      "Train: Epoch [14], Batch [583/938], Loss: 0.6799665689468384\n",
      "Train: Epoch [14], Batch [584/938], Loss: 0.7653979659080505\n",
      "Train: Epoch [14], Batch [585/938], Loss: 0.6612839698791504\n",
      "Train: Epoch [14], Batch [586/938], Loss: 0.707984447479248\n",
      "Train: Epoch [14], Batch [587/938], Loss: 0.7422150373458862\n",
      "Train: Epoch [14], Batch [588/938], Loss: 0.8053349256515503\n",
      "Train: Epoch [14], Batch [589/938], Loss: 0.697523295879364\n",
      "Train: Epoch [14], Batch [590/938], Loss: 0.8732645511627197\n",
      "Train: Epoch [14], Batch [591/938], Loss: 0.7652122378349304\n",
      "Train: Epoch [14], Batch [592/938], Loss: 0.7026275396347046\n",
      "Train: Epoch [14], Batch [593/938], Loss: 1.1023660898208618\n",
      "Train: Epoch [14], Batch [594/938], Loss: 0.7017258405685425\n",
      "Train: Epoch [14], Batch [595/938], Loss: 0.6312283873558044\n",
      "Train: Epoch [14], Batch [596/938], Loss: 0.7194492220878601\n",
      "Train: Epoch [14], Batch [597/938], Loss: 0.788239598274231\n",
      "Train: Epoch [14], Batch [598/938], Loss: 0.4724002480506897\n",
      "Train: Epoch [14], Batch [599/938], Loss: 0.6265360116958618\n",
      "Train: Epoch [14], Batch [600/938], Loss: 0.7104107141494751\n",
      "Train: Epoch [14], Batch [601/938], Loss: 0.6553918123245239\n",
      "Train: Epoch [14], Batch [602/938], Loss: 0.724921703338623\n",
      "Train: Epoch [14], Batch [603/938], Loss: 0.6759451627731323\n",
      "Train: Epoch [14], Batch [604/938], Loss: 0.609230637550354\n",
      "Train: Epoch [14], Batch [605/938], Loss: 0.6233268976211548\n",
      "Train: Epoch [14], Batch [606/938], Loss: 0.599452793598175\n",
      "Train: Epoch [14], Batch [607/938], Loss: 0.7599601745605469\n",
      "Train: Epoch [14], Batch [608/938], Loss: 0.739622175693512\n",
      "Train: Epoch [14], Batch [609/938], Loss: 0.6444609761238098\n",
      "Train: Epoch [14], Batch [610/938], Loss: 0.8531140685081482\n",
      "Train: Epoch [14], Batch [611/938], Loss: 0.6876970529556274\n",
      "Train: Epoch [14], Batch [612/938], Loss: 0.7905480265617371\n",
      "Train: Epoch [14], Batch [613/938], Loss: 0.8462881445884705\n",
      "Train: Epoch [14], Batch [614/938], Loss: 0.8080896139144897\n",
      "Train: Epoch [14], Batch [615/938], Loss: 0.6980289220809937\n",
      "Train: Epoch [14], Batch [616/938], Loss: 0.6656051874160767\n",
      "Train: Epoch [14], Batch [617/938], Loss: 0.7173200845718384\n",
      "Train: Epoch [14], Batch [618/938], Loss: 0.6437302827835083\n",
      "Train: Epoch [14], Batch [619/938], Loss: 0.5980486869812012\n",
      "Train: Epoch [14], Batch [620/938], Loss: 0.6356315612792969\n",
      "Train: Epoch [14], Batch [621/938], Loss: 1.0530418157577515\n",
      "Train: Epoch [14], Batch [622/938], Loss: 0.5876095294952393\n",
      "Train: Epoch [14], Batch [623/938], Loss: 0.8238608837127686\n",
      "Train: Epoch [14], Batch [624/938], Loss: 0.6875704526901245\n",
      "Train: Epoch [14], Batch [625/938], Loss: 0.7959590554237366\n",
      "Train: Epoch [14], Batch [626/938], Loss: 0.7031617760658264\n",
      "Train: Epoch [14], Batch [627/938], Loss: 0.604938268661499\n",
      "Train: Epoch [14], Batch [628/938], Loss: 0.5174924731254578\n",
      "Train: Epoch [14], Batch [629/938], Loss: 0.4910956919193268\n",
      "Train: Epoch [14], Batch [630/938], Loss: 0.6594964265823364\n",
      "Train: Epoch [14], Batch [631/938], Loss: 0.8276344537734985\n",
      "Train: Epoch [14], Batch [632/938], Loss: 0.7117795944213867\n",
      "Train: Epoch [14], Batch [633/938], Loss: 0.9197813272476196\n",
      "Train: Epoch [14], Batch [634/938], Loss: 0.8413052558898926\n",
      "Train: Epoch [14], Batch [635/938], Loss: 0.65421462059021\n",
      "Train: Epoch [14], Batch [636/938], Loss: 0.7541275024414062\n",
      "Train: Epoch [14], Batch [637/938], Loss: 0.7151420712471008\n",
      "Train: Epoch [14], Batch [638/938], Loss: 0.5775700807571411\n",
      "Train: Epoch [14], Batch [639/938], Loss: 0.5475597977638245\n",
      "Train: Epoch [14], Batch [640/938], Loss: 0.5071918368339539\n",
      "Train: Epoch [14], Batch [641/938], Loss: 0.8141591548919678\n",
      "Train: Epoch [14], Batch [642/938], Loss: 0.573835551738739\n",
      "Train: Epoch [14], Batch [643/938], Loss: 0.8970419764518738\n",
      "Train: Epoch [14], Batch [644/938], Loss: 0.8207210898399353\n",
      "Train: Epoch [14], Batch [645/938], Loss: 0.5396539568901062\n",
      "Train: Epoch [14], Batch [646/938], Loss: 0.602079451084137\n",
      "Train: Epoch [14], Batch [647/938], Loss: 0.6118298768997192\n",
      "Train: Epoch [14], Batch [648/938], Loss: 0.7226014733314514\n",
      "Train: Epoch [14], Batch [649/938], Loss: 0.5763609409332275\n",
      "Train: Epoch [14], Batch [650/938], Loss: 0.7492129802703857\n",
      "Train: Epoch [14], Batch [651/938], Loss: 0.7713052034378052\n",
      "Train: Epoch [14], Batch [652/938], Loss: 0.7777117490768433\n",
      "Train: Epoch [14], Batch [653/938], Loss: 0.7459602952003479\n",
      "Train: Epoch [14], Batch [654/938], Loss: 0.8835116624832153\n",
      "Train: Epoch [14], Batch [655/938], Loss: 0.5108336806297302\n",
      "Train: Epoch [14], Batch [656/938], Loss: 0.6597499847412109\n",
      "Train: Epoch [14], Batch [657/938], Loss: 0.8337109088897705\n",
      "Train: Epoch [14], Batch [658/938], Loss: 0.54719078540802\n",
      "Train: Epoch [14], Batch [659/938], Loss: 0.6589990258216858\n",
      "Train: Epoch [14], Batch [660/938], Loss: 0.6209661364555359\n",
      "Train: Epoch [14], Batch [661/938], Loss: 0.7988169193267822\n",
      "Train: Epoch [14], Batch [662/938], Loss: 0.4895962178707123\n",
      "Train: Epoch [14], Batch [663/938], Loss: 0.8735157251358032\n",
      "Train: Epoch [14], Batch [664/938], Loss: 0.6429749131202698\n",
      "Train: Epoch [14], Batch [665/938], Loss: 0.7581973671913147\n",
      "Train: Epoch [14], Batch [666/938], Loss: 0.7737377882003784\n",
      "Train: Epoch [14], Batch [667/938], Loss: 0.7989500761032104\n",
      "Train: Epoch [14], Batch [668/938], Loss: 0.6075536012649536\n",
      "Train: Epoch [14], Batch [669/938], Loss: 0.8097190856933594\n",
      "Train: Epoch [14], Batch [670/938], Loss: 0.6352850198745728\n",
      "Train: Epoch [14], Batch [671/938], Loss: 0.5917037725448608\n",
      "Train: Epoch [14], Batch [672/938], Loss: 0.49867093563079834\n",
      "Train: Epoch [14], Batch [673/938], Loss: 0.6106306910514832\n",
      "Train: Epoch [14], Batch [674/938], Loss: 0.6445180773735046\n",
      "Train: Epoch [14], Batch [675/938], Loss: 0.8767204284667969\n",
      "Train: Epoch [14], Batch [676/938], Loss: 0.5524301528930664\n",
      "Train: Epoch [14], Batch [677/938], Loss: 0.705177903175354\n",
      "Train: Epoch [14], Batch [678/938], Loss: 0.8244897723197937\n",
      "Train: Epoch [14], Batch [679/938], Loss: 0.6139856576919556\n",
      "Train: Epoch [14], Batch [680/938], Loss: 0.9477639198303223\n",
      "Train: Epoch [14], Batch [681/938], Loss: 0.8626940846443176\n",
      "Train: Epoch [14], Batch [682/938], Loss: 0.3774546980857849\n",
      "Train: Epoch [14], Batch [683/938], Loss: 0.7549744248390198\n",
      "Train: Epoch [14], Batch [684/938], Loss: 0.6947137117385864\n",
      "Train: Epoch [14], Batch [685/938], Loss: 0.974565863609314\n",
      "Train: Epoch [14], Batch [686/938], Loss: 0.8107271790504456\n",
      "Train: Epoch [14], Batch [687/938], Loss: 0.4108150005340576\n",
      "Train: Epoch [14], Batch [688/938], Loss: 0.6431008577346802\n",
      "Train: Epoch [14], Batch [689/938], Loss: 0.7130215167999268\n",
      "Train: Epoch [14], Batch [690/938], Loss: 0.4988321363925934\n",
      "Train: Epoch [14], Batch [691/938], Loss: 0.5651650428771973\n",
      "Train: Epoch [14], Batch [692/938], Loss: 0.7505785822868347\n",
      "Train: Epoch [14], Batch [693/938], Loss: 0.9217466115951538\n",
      "Train: Epoch [14], Batch [694/938], Loss: 0.877534031867981\n",
      "Train: Epoch [14], Batch [695/938], Loss: 0.9230684041976929\n",
      "Train: Epoch [14], Batch [696/938], Loss: 0.6597471833229065\n",
      "Train: Epoch [14], Batch [697/938], Loss: 0.7736805081367493\n",
      "Train: Epoch [14], Batch [698/938], Loss: 0.7775071859359741\n",
      "Train: Epoch [14], Batch [699/938], Loss: 0.7751812934875488\n",
      "Train: Epoch [14], Batch [700/938], Loss: 0.9912524223327637\n",
      "Train: Epoch [14], Batch [701/938], Loss: 0.5675328969955444\n",
      "Train: Epoch [14], Batch [702/938], Loss: 0.659628689289093\n",
      "Train: Epoch [14], Batch [703/938], Loss: 0.6283007860183716\n",
      "Train: Epoch [14], Batch [704/938], Loss: 0.7352684140205383\n",
      "Train: Epoch [14], Batch [705/938], Loss: 0.9146125912666321\n",
      "Train: Epoch [14], Batch [706/938], Loss: 0.9118875861167908\n",
      "Train: Epoch [14], Batch [707/938], Loss: 0.9690802097320557\n",
      "Train: Epoch [14], Batch [708/938], Loss: 0.9361723065376282\n",
      "Train: Epoch [14], Batch [709/938], Loss: 0.6163187623023987\n",
      "Train: Epoch [14], Batch [710/938], Loss: 0.7642965316772461\n",
      "Train: Epoch [14], Batch [711/938], Loss: 0.7740851044654846\n",
      "Train: Epoch [14], Batch [712/938], Loss: 0.5743412971496582\n",
      "Train: Epoch [14], Batch [713/938], Loss: 0.6137601733207703\n",
      "Train: Epoch [14], Batch [714/938], Loss: 0.9831763505935669\n",
      "Train: Epoch [14], Batch [715/938], Loss: 0.7680218815803528\n",
      "Train: Epoch [14], Batch [716/938], Loss: 0.6924396753311157\n",
      "Train: Epoch [14], Batch [717/938], Loss: 0.9617283344268799\n",
      "Train: Epoch [14], Batch [718/938], Loss: 0.8374319076538086\n",
      "Train: Epoch [14], Batch [719/938], Loss: 0.6199500560760498\n",
      "Train: Epoch [14], Batch [720/938], Loss: 0.6788650751113892\n",
      "Train: Epoch [14], Batch [721/938], Loss: 0.63542640209198\n",
      "Train: Epoch [14], Batch [722/938], Loss: 0.5539836287498474\n",
      "Train: Epoch [14], Batch [723/938], Loss: 0.7138145565986633\n",
      "Train: Epoch [14], Batch [724/938], Loss: 0.735271692276001\n",
      "Train: Epoch [14], Batch [725/938], Loss: 0.77237468957901\n",
      "Train: Epoch [14], Batch [726/938], Loss: 0.8463152647018433\n",
      "Train: Epoch [14], Batch [727/938], Loss: 0.757939875125885\n",
      "Train: Epoch [14], Batch [728/938], Loss: 0.5438036918640137\n",
      "Train: Epoch [14], Batch [729/938], Loss: 0.6506820321083069\n",
      "Train: Epoch [14], Batch [730/938], Loss: 0.8290324211120605\n",
      "Train: Epoch [14], Batch [731/938], Loss: 0.61838299036026\n",
      "Train: Epoch [14], Batch [732/938], Loss: 0.7568954229354858\n",
      "Train: Epoch [14], Batch [733/938], Loss: 0.5982317924499512\n",
      "Train: Epoch [14], Batch [734/938], Loss: 0.38660529255867004\n",
      "Train: Epoch [14], Batch [735/938], Loss: 0.9273118376731873\n",
      "Train: Epoch [14], Batch [736/938], Loss: 0.5865471959114075\n",
      "Train: Epoch [14], Batch [737/938], Loss: 0.6890490651130676\n",
      "Train: Epoch [14], Batch [738/938], Loss: 0.8241720795631409\n",
      "Train: Epoch [14], Batch [739/938], Loss: 0.7913884520530701\n",
      "Train: Epoch [14], Batch [740/938], Loss: 0.7043460607528687\n",
      "Train: Epoch [14], Batch [741/938], Loss: 0.6554189324378967\n",
      "Train: Epoch [14], Batch [742/938], Loss: 0.7799739837646484\n",
      "Train: Epoch [14], Batch [743/938], Loss: 0.6624116897583008\n",
      "Train: Epoch [14], Batch [744/938], Loss: 0.6935814023017883\n",
      "Train: Epoch [14], Batch [745/938], Loss: 0.6506351828575134\n",
      "Train: Epoch [14], Batch [746/938], Loss: 0.6344475746154785\n",
      "Train: Epoch [14], Batch [747/938], Loss: 0.5881698727607727\n",
      "Train: Epoch [14], Batch [748/938], Loss: 0.7357721924781799\n",
      "Train: Epoch [14], Batch [749/938], Loss: 0.7856408953666687\n",
      "Train: Epoch [14], Batch [750/938], Loss: 0.8777384161949158\n",
      "Train: Epoch [14], Batch [751/938], Loss: 0.7111269235610962\n",
      "Train: Epoch [14], Batch [752/938], Loss: 0.6868706941604614\n",
      "Train: Epoch [14], Batch [753/938], Loss: 0.608430802822113\n",
      "Train: Epoch [14], Batch [754/938], Loss: 0.7246488928794861\n",
      "Train: Epoch [14], Batch [755/938], Loss: 0.9691616892814636\n",
      "Train: Epoch [14], Batch [756/938], Loss: 0.6620237231254578\n",
      "Train: Epoch [14], Batch [757/938], Loss: 0.6063933372497559\n",
      "Train: Epoch [14], Batch [758/938], Loss: 0.6605854630470276\n",
      "Train: Epoch [14], Batch [759/938], Loss: 0.7411383390426636\n",
      "Train: Epoch [14], Batch [760/938], Loss: 0.7928865551948547\n",
      "Train: Epoch [14], Batch [761/938], Loss: 0.7635095119476318\n",
      "Train: Epoch [14], Batch [762/938], Loss: 0.7348406910896301\n",
      "Train: Epoch [14], Batch [763/938], Loss: 0.8336095213890076\n",
      "Train: Epoch [14], Batch [764/938], Loss: 0.8718835711479187\n",
      "Train: Epoch [14], Batch [765/938], Loss: 0.8271852731704712\n",
      "Train: Epoch [14], Batch [766/938], Loss: 0.9451842308044434\n",
      "Train: Epoch [14], Batch [767/938], Loss: 0.7680546641349792\n",
      "Train: Epoch [14], Batch [768/938], Loss: 1.0572984218597412\n",
      "Train: Epoch [14], Batch [769/938], Loss: 0.5583556294441223\n",
      "Train: Epoch [14], Batch [770/938], Loss: 0.6183302402496338\n",
      "Train: Epoch [14], Batch [771/938], Loss: 0.9189289808273315\n",
      "Train: Epoch [14], Batch [772/938], Loss: 0.7694976329803467\n",
      "Train: Epoch [14], Batch [773/938], Loss: 0.9631972312927246\n",
      "Train: Epoch [14], Batch [774/938], Loss: 0.7951868772506714\n",
      "Train: Epoch [14], Batch [775/938], Loss: 0.42808887362480164\n",
      "Train: Epoch [14], Batch [776/938], Loss: 0.631828784942627\n",
      "Train: Epoch [14], Batch [777/938], Loss: 0.8475194573402405\n",
      "Train: Epoch [14], Batch [778/938], Loss: 0.48943957686424255\n",
      "Train: Epoch [14], Batch [779/938], Loss: 0.6253015398979187\n",
      "Train: Epoch [14], Batch [780/938], Loss: 0.9518743753433228\n",
      "Train: Epoch [14], Batch [781/938], Loss: 0.6939743161201477\n",
      "Train: Epoch [14], Batch [782/938], Loss: 0.6541894674301147\n",
      "Train: Epoch [14], Batch [783/938], Loss: 0.819129228591919\n",
      "Train: Epoch [14], Batch [784/938], Loss: 0.6360869407653809\n",
      "Train: Epoch [14], Batch [785/938], Loss: 0.6650162935256958\n",
      "Train: Epoch [14], Batch [786/938], Loss: 0.7363415956497192\n",
      "Train: Epoch [14], Batch [787/938], Loss: 0.8735112547874451\n",
      "Train: Epoch [14], Batch [788/938], Loss: 0.8853618502616882\n",
      "Train: Epoch [14], Batch [789/938], Loss: 0.592675507068634\n",
      "Train: Epoch [14], Batch [790/938], Loss: 0.6933354139328003\n",
      "Train: Epoch [14], Batch [791/938], Loss: 0.738642156124115\n",
      "Train: Epoch [14], Batch [792/938], Loss: 0.7474702000617981\n",
      "Train: Epoch [14], Batch [793/938], Loss: 0.7628113031387329\n",
      "Train: Epoch [14], Batch [794/938], Loss: 0.7303228378295898\n",
      "Train: Epoch [14], Batch [795/938], Loss: 0.6096837520599365\n",
      "Train: Epoch [14], Batch [796/938], Loss: 0.6747061610221863\n",
      "Train: Epoch [14], Batch [797/938], Loss: 0.8820635676383972\n",
      "Train: Epoch [14], Batch [798/938], Loss: 0.7108159065246582\n",
      "Train: Epoch [14], Batch [799/938], Loss: 0.7743088006973267\n",
      "Train: Epoch [14], Batch [800/938], Loss: 0.8189360499382019\n",
      "Train: Epoch [14], Batch [801/938], Loss: 0.9948945641517639\n",
      "Train: Epoch [14], Batch [802/938], Loss: 0.6514244079589844\n",
      "Train: Epoch [14], Batch [803/938], Loss: 0.5894402265548706\n",
      "Train: Epoch [14], Batch [804/938], Loss: 0.8193460702896118\n",
      "Train: Epoch [14], Batch [805/938], Loss: 0.7104722857475281\n",
      "Train: Epoch [14], Batch [806/938], Loss: 0.942359209060669\n",
      "Train: Epoch [14], Batch [807/938], Loss: 0.5285391807556152\n",
      "Train: Epoch [14], Batch [808/938], Loss: 0.8423795700073242\n",
      "Train: Epoch [14], Batch [809/938], Loss: 0.595944881439209\n",
      "Train: Epoch [14], Batch [810/938], Loss: 0.7105821967124939\n",
      "Train: Epoch [14], Batch [811/938], Loss: 0.7799073457717896\n",
      "Train: Epoch [14], Batch [812/938], Loss: 0.5710586905479431\n",
      "Train: Epoch [14], Batch [813/938], Loss: 0.6625922918319702\n",
      "Train: Epoch [14], Batch [814/938], Loss: 0.9690913558006287\n",
      "Train: Epoch [14], Batch [815/938], Loss: 0.6811134815216064\n",
      "Train: Epoch [14], Batch [816/938], Loss: 0.5678221583366394\n",
      "Train: Epoch [14], Batch [817/938], Loss: 0.6168879270553589\n",
      "Train: Epoch [14], Batch [818/938], Loss: 0.6650756001472473\n",
      "Train: Epoch [14], Batch [819/938], Loss: 0.7190392017364502\n",
      "Train: Epoch [14], Batch [820/938], Loss: 0.5529976487159729\n",
      "Train: Epoch [14], Batch [821/938], Loss: 0.7136497497558594\n",
      "Train: Epoch [14], Batch [822/938], Loss: 0.8745982646942139\n",
      "Train: Epoch [14], Batch [823/938], Loss: 0.6925545930862427\n",
      "Train: Epoch [14], Batch [824/938], Loss: 0.7471206784248352\n",
      "Train: Epoch [14], Batch [825/938], Loss: 0.9313355088233948\n",
      "Train: Epoch [14], Batch [826/938], Loss: 0.7885342836380005\n",
      "Train: Epoch [14], Batch [827/938], Loss: 1.0049035549163818\n",
      "Train: Epoch [14], Batch [828/938], Loss: 0.9321982860565186\n",
      "Train: Epoch [14], Batch [829/938], Loss: 0.8886390924453735\n",
      "Train: Epoch [14], Batch [830/938], Loss: 0.884378969669342\n",
      "Train: Epoch [14], Batch [831/938], Loss: 0.7271386981010437\n",
      "Train: Epoch [14], Batch [832/938], Loss: 0.5950882434844971\n",
      "Train: Epoch [14], Batch [833/938], Loss: 0.6752042770385742\n",
      "Train: Epoch [14], Batch [834/938], Loss: 0.7186102271080017\n",
      "Train: Epoch [14], Batch [835/938], Loss: 0.6223608255386353\n",
      "Train: Epoch [14], Batch [836/938], Loss: 0.5900550484657288\n",
      "Train: Epoch [14], Batch [837/938], Loss: 0.6568409204483032\n",
      "Train: Epoch [14], Batch [838/938], Loss: 0.6858396530151367\n",
      "Train: Epoch [14], Batch [839/938], Loss: 0.538161039352417\n",
      "Train: Epoch [14], Batch [840/938], Loss: 0.9083060026168823\n",
      "Train: Epoch [14], Batch [841/938], Loss: 0.608741044998169\n",
      "Train: Epoch [14], Batch [842/938], Loss: 0.6626232862472534\n",
      "Train: Epoch [14], Batch [843/938], Loss: 0.6921281218528748\n",
      "Train: Epoch [14], Batch [844/938], Loss: 0.6102696061134338\n",
      "Train: Epoch [14], Batch [845/938], Loss: 0.7858144044876099\n",
      "Train: Epoch [14], Batch [846/938], Loss: 0.5190979242324829\n",
      "Train: Epoch [14], Batch [847/938], Loss: 0.4440738558769226\n",
      "Train: Epoch [14], Batch [848/938], Loss: 0.6687179803848267\n",
      "Train: Epoch [14], Batch [849/938], Loss: 0.7006141543388367\n",
      "Train: Epoch [14], Batch [850/938], Loss: 0.7053331136703491\n",
      "Train: Epoch [14], Batch [851/938], Loss: 0.6901023387908936\n",
      "Train: Epoch [14], Batch [852/938], Loss: 0.5303086042404175\n",
      "Train: Epoch [14], Batch [853/938], Loss: 0.5117418169975281\n",
      "Train: Epoch [14], Batch [854/938], Loss: 0.8369783759117126\n",
      "Train: Epoch [14], Batch [855/938], Loss: 0.9769136905670166\n",
      "Train: Epoch [14], Batch [856/938], Loss: 0.49031201004981995\n",
      "Train: Epoch [14], Batch [857/938], Loss: 1.0239148139953613\n",
      "Train: Epoch [14], Batch [858/938], Loss: 0.7039828300476074\n",
      "Train: Epoch [14], Batch [859/938], Loss: 0.7814996838569641\n",
      "Train: Epoch [14], Batch [860/938], Loss: 0.7264732122421265\n",
      "Train: Epoch [14], Batch [861/938], Loss: 0.6605494618415833\n",
      "Train: Epoch [14], Batch [862/938], Loss: 0.7130981087684631\n",
      "Train: Epoch [14], Batch [863/938], Loss: 0.8205771446228027\n",
      "Train: Epoch [14], Batch [864/938], Loss: 0.6340703964233398\n",
      "Train: Epoch [14], Batch [865/938], Loss: 0.8234829902648926\n",
      "Train: Epoch [14], Batch [866/938], Loss: 0.6689851880073547\n",
      "Train: Epoch [14], Batch [867/938], Loss: 0.9033035635948181\n",
      "Train: Epoch [14], Batch [868/938], Loss: 0.725357711315155\n",
      "Train: Epoch [14], Batch [869/938], Loss: 0.6513493657112122\n",
      "Train: Epoch [14], Batch [870/938], Loss: 0.6890097856521606\n",
      "Train: Epoch [14], Batch [871/938], Loss: 0.9713804125785828\n",
      "Train: Epoch [14], Batch [872/938], Loss: 0.9307114481925964\n",
      "Train: Epoch [14], Batch [873/938], Loss: 0.7333022952079773\n",
      "Train: Epoch [14], Batch [874/938], Loss: 0.7818841934204102\n",
      "Train: Epoch [14], Batch [875/938], Loss: 0.7492259740829468\n",
      "Train: Epoch [14], Batch [876/938], Loss: 0.5626614689826965\n",
      "Train: Epoch [14], Batch [877/938], Loss: 0.8843479156494141\n",
      "Train: Epoch [14], Batch [878/938], Loss: 0.7191668748855591\n",
      "Train: Epoch [14], Batch [879/938], Loss: 0.7458324432373047\n",
      "Train: Epoch [14], Batch [880/938], Loss: 0.4024852216243744\n",
      "Train: Epoch [14], Batch [881/938], Loss: 0.6744418740272522\n",
      "Train: Epoch [14], Batch [882/938], Loss: 0.7290442585945129\n",
      "Train: Epoch [14], Batch [883/938], Loss: 0.8650595545768738\n",
      "Train: Epoch [14], Batch [884/938], Loss: 0.5449102520942688\n",
      "Train: Epoch [14], Batch [885/938], Loss: 0.7659715414047241\n",
      "Train: Epoch [14], Batch [886/938], Loss: 0.5707913637161255\n",
      "Train: Epoch [14], Batch [887/938], Loss: 0.6642870903015137\n",
      "Train: Epoch [14], Batch [888/938], Loss: 0.6686369776725769\n",
      "Train: Epoch [14], Batch [889/938], Loss: 0.9021749496459961\n",
      "Train: Epoch [14], Batch [890/938], Loss: 0.6662765145301819\n",
      "Train: Epoch [14], Batch [891/938], Loss: 0.5294249653816223\n",
      "Train: Epoch [14], Batch [892/938], Loss: 0.6843542456626892\n",
      "Train: Epoch [14], Batch [893/938], Loss: 0.8021659851074219\n",
      "Train: Epoch [14], Batch [894/938], Loss: 0.58141028881073\n",
      "Train: Epoch [14], Batch [895/938], Loss: 0.6395322680473328\n",
      "Train: Epoch [14], Batch [896/938], Loss: 0.5771810412406921\n",
      "Train: Epoch [14], Batch [897/938], Loss: 0.6063429713249207\n",
      "Train: Epoch [14], Batch [898/938], Loss: 0.47278428077697754\n",
      "Train: Epoch [14], Batch [899/938], Loss: 0.6907371282577515\n",
      "Train: Epoch [14], Batch [900/938], Loss: 0.5435082912445068\n",
      "Train: Epoch [14], Batch [901/938], Loss: 0.6721028089523315\n",
      "Train: Epoch [14], Batch [902/938], Loss: 0.6003844738006592\n",
      "Train: Epoch [14], Batch [903/938], Loss: 0.6065042018890381\n",
      "Train: Epoch [14], Batch [904/938], Loss: 0.7089489698410034\n",
      "Train: Epoch [14], Batch [905/938], Loss: 0.8148465752601624\n",
      "Train: Epoch [14], Batch [906/938], Loss: 0.6416824460029602\n",
      "Train: Epoch [14], Batch [907/938], Loss: 0.7676522731781006\n",
      "Train: Epoch [14], Batch [908/938], Loss: 0.5456799268722534\n",
      "Train: Epoch [14], Batch [909/938], Loss: 0.49379536509513855\n",
      "Train: Epoch [14], Batch [910/938], Loss: 0.584827721118927\n",
      "Train: Epoch [14], Batch [911/938], Loss: 0.5682322978973389\n",
      "Train: Epoch [14], Batch [912/938], Loss: 0.905302107334137\n",
      "Train: Epoch [14], Batch [913/938], Loss: 0.5128383040428162\n",
      "Train: Epoch [14], Batch [914/938], Loss: 0.7775973081588745\n",
      "Train: Epoch [14], Batch [915/938], Loss: 0.6922102570533752\n",
      "Train: Epoch [14], Batch [916/938], Loss: 0.8008871674537659\n",
      "Train: Epoch [14], Batch [917/938], Loss: 0.7854666113853455\n",
      "Train: Epoch [14], Batch [918/938], Loss: 0.6131331920623779\n",
      "Train: Epoch [14], Batch [919/938], Loss: 0.6541297435760498\n",
      "Train: Epoch [14], Batch [920/938], Loss: 0.7617807984352112\n",
      "Train: Epoch [14], Batch [921/938], Loss: 0.6455507278442383\n",
      "Train: Epoch [14], Batch [922/938], Loss: 0.6697463989257812\n",
      "Train: Epoch [14], Batch [923/938], Loss: 0.517882764339447\n",
      "Train: Epoch [14], Batch [924/938], Loss: 0.6547043323516846\n",
      "Train: Epoch [14], Batch [925/938], Loss: 0.6293677687644958\n",
      "Train: Epoch [14], Batch [926/938], Loss: 0.8951490521430969\n",
      "Train: Epoch [14], Batch [927/938], Loss: 0.6678851246833801\n",
      "Train: Epoch [14], Batch [928/938], Loss: 0.621427595615387\n",
      "Train: Epoch [14], Batch [929/938], Loss: 0.5459270477294922\n",
      "Train: Epoch [14], Batch [930/938], Loss: 0.6533781290054321\n",
      "Train: Epoch [14], Batch [931/938], Loss: 0.5478488802909851\n",
      "Train: Epoch [14], Batch [932/938], Loss: 0.5043318867683411\n",
      "Train: Epoch [14], Batch [933/938], Loss: 0.5576329231262207\n",
      "Train: Epoch [14], Batch [934/938], Loss: 0.6094003319740295\n",
      "Train: Epoch [14], Batch [935/938], Loss: 0.8337410688400269\n",
      "Train: Epoch [14], Batch [936/938], Loss: 0.7390440702438354\n",
      "Train: Epoch [14], Batch [937/938], Loss: 0.676171600818634\n",
      "Train: Epoch [14], Batch [938/938], Loss: 0.946578860282898\n",
      "Accuracy of train set: 0.7840333333333334\n",
      "Validation: Epoch [14], Batch [1/938], Loss: 0.6742103695869446\n",
      "Validation: Epoch [14], Batch [2/938], Loss: 0.7966045141220093\n",
      "Validation: Epoch [14], Batch [3/938], Loss: 0.7946794033050537\n",
      "Validation: Epoch [14], Batch [4/938], Loss: 0.5136712789535522\n",
      "Validation: Epoch [14], Batch [5/938], Loss: 0.7619832158088684\n",
      "Validation: Epoch [14], Batch [6/938], Loss: 0.6092160940170288\n",
      "Validation: Epoch [14], Batch [7/938], Loss: 0.9978928565979004\n",
      "Validation: Epoch [14], Batch [8/938], Loss: 0.812839925289154\n",
      "Validation: Epoch [14], Batch [9/938], Loss: 0.7096186280250549\n",
      "Validation: Epoch [14], Batch [10/938], Loss: 0.604182779788971\n",
      "Validation: Epoch [14], Batch [11/938], Loss: 0.6739084720611572\n",
      "Validation: Epoch [14], Batch [12/938], Loss: 0.7046093940734863\n",
      "Validation: Epoch [14], Batch [13/938], Loss: 0.4713250696659088\n",
      "Validation: Epoch [14], Batch [14/938], Loss: 0.5844724774360657\n",
      "Validation: Epoch [14], Batch [15/938], Loss: 0.8485354781150818\n",
      "Validation: Epoch [14], Batch [16/938], Loss: 0.8454300165176392\n",
      "Validation: Epoch [14], Batch [17/938], Loss: 0.6627705097198486\n",
      "Validation: Epoch [14], Batch [18/938], Loss: 0.777025043964386\n",
      "Validation: Epoch [14], Batch [19/938], Loss: 0.6182853579521179\n",
      "Validation: Epoch [14], Batch [20/938], Loss: 0.7208066582679749\n",
      "Validation: Epoch [14], Batch [21/938], Loss: 0.5864428281784058\n",
      "Validation: Epoch [14], Batch [22/938], Loss: 0.9415942430496216\n",
      "Validation: Epoch [14], Batch [23/938], Loss: 0.7032663226127625\n",
      "Validation: Epoch [14], Batch [24/938], Loss: 0.7642402648925781\n",
      "Validation: Epoch [14], Batch [25/938], Loss: 0.9402446150779724\n",
      "Validation: Epoch [14], Batch [26/938], Loss: 0.9089428186416626\n",
      "Validation: Epoch [14], Batch [27/938], Loss: 0.8061969876289368\n",
      "Validation: Epoch [14], Batch [28/938], Loss: 0.8138118386268616\n",
      "Validation: Epoch [14], Batch [29/938], Loss: 0.8190882205963135\n",
      "Validation: Epoch [14], Batch [30/938], Loss: 0.6237618923187256\n",
      "Validation: Epoch [14], Batch [31/938], Loss: 0.7719287872314453\n",
      "Validation: Epoch [14], Batch [32/938], Loss: 0.6213086843490601\n",
      "Validation: Epoch [14], Batch [33/938], Loss: 0.8601403832435608\n",
      "Validation: Epoch [14], Batch [34/938], Loss: 0.8496490120887756\n",
      "Validation: Epoch [14], Batch [35/938], Loss: 0.6910218000411987\n",
      "Validation: Epoch [14], Batch [36/938], Loss: 0.8215422630310059\n",
      "Validation: Epoch [14], Batch [37/938], Loss: 0.8728732466697693\n",
      "Validation: Epoch [14], Batch [38/938], Loss: 0.5455611944198608\n",
      "Validation: Epoch [14], Batch [39/938], Loss: 0.6245440244674683\n",
      "Validation: Epoch [14], Batch [40/938], Loss: 0.7216117978096008\n",
      "Validation: Epoch [14], Batch [41/938], Loss: 0.6129109859466553\n",
      "Validation: Epoch [14], Batch [42/938], Loss: 0.9758018851280212\n",
      "Validation: Epoch [14], Batch [43/938], Loss: 0.6566980481147766\n",
      "Validation: Epoch [14], Batch [44/938], Loss: 0.9115028977394104\n",
      "Validation: Epoch [14], Batch [45/938], Loss: 0.9080038070678711\n",
      "Validation: Epoch [14], Batch [46/938], Loss: 0.7726593613624573\n",
      "Validation: Epoch [14], Batch [47/938], Loss: 0.8515354990959167\n",
      "Validation: Epoch [14], Batch [48/938], Loss: 0.7174111604690552\n",
      "Validation: Epoch [14], Batch [49/938], Loss: 0.7414736151695251\n",
      "Validation: Epoch [14], Batch [50/938], Loss: 0.7270243763923645\n",
      "Validation: Epoch [14], Batch [51/938], Loss: 0.7330634593963623\n",
      "Validation: Epoch [14], Batch [52/938], Loss: 0.8889025449752808\n",
      "Validation: Epoch [14], Batch [53/938], Loss: 0.7418167591094971\n",
      "Validation: Epoch [14], Batch [54/938], Loss: 0.6197052001953125\n",
      "Validation: Epoch [14], Batch [55/938], Loss: 0.93770831823349\n",
      "Validation: Epoch [14], Batch [56/938], Loss: 0.8599706888198853\n",
      "Validation: Epoch [14], Batch [57/938], Loss: 0.5087190866470337\n",
      "Validation: Epoch [14], Batch [58/938], Loss: 0.7293382883071899\n",
      "Validation: Epoch [14], Batch [59/938], Loss: 0.8015050888061523\n",
      "Validation: Epoch [14], Batch [60/938], Loss: 0.6895925998687744\n",
      "Validation: Epoch [14], Batch [61/938], Loss: 0.5231119394302368\n",
      "Validation: Epoch [14], Batch [62/938], Loss: 0.6898971796035767\n",
      "Validation: Epoch [14], Batch [63/938], Loss: 0.762243926525116\n",
      "Validation: Epoch [14], Batch [64/938], Loss: 0.668789803981781\n",
      "Validation: Epoch [14], Batch [65/938], Loss: 0.9601605534553528\n",
      "Validation: Epoch [14], Batch [66/938], Loss: 0.7481970191001892\n",
      "Validation: Epoch [14], Batch [67/938], Loss: 0.680479884147644\n",
      "Validation: Epoch [14], Batch [68/938], Loss: 0.7077268362045288\n",
      "Validation: Epoch [14], Batch [69/938], Loss: 1.010090708732605\n",
      "Validation: Epoch [14], Batch [70/938], Loss: 0.6949802041053772\n",
      "Validation: Epoch [14], Batch [71/938], Loss: 1.0742442607879639\n",
      "Validation: Epoch [14], Batch [72/938], Loss: 0.7073546648025513\n",
      "Validation: Epoch [14], Batch [73/938], Loss: 0.7441460490226746\n",
      "Validation: Epoch [14], Batch [74/938], Loss: 0.5951734185218811\n",
      "Validation: Epoch [14], Batch [75/938], Loss: 0.6636543273925781\n",
      "Validation: Epoch [14], Batch [76/938], Loss: 0.774132251739502\n",
      "Validation: Epoch [14], Batch [77/938], Loss: 0.6732761859893799\n",
      "Validation: Epoch [14], Batch [78/938], Loss: 0.7907809615135193\n",
      "Validation: Epoch [14], Batch [79/938], Loss: 0.7986363172531128\n",
      "Validation: Epoch [14], Batch [80/938], Loss: 0.4284006655216217\n",
      "Validation: Epoch [14], Batch [81/938], Loss: 0.6431955099105835\n",
      "Validation: Epoch [14], Batch [82/938], Loss: 0.7201919555664062\n",
      "Validation: Epoch [14], Batch [83/938], Loss: 0.7341431379318237\n",
      "Validation: Epoch [14], Batch [84/938], Loss: 0.716528594493866\n",
      "Validation: Epoch [14], Batch [85/938], Loss: 0.849907636642456\n",
      "Validation: Epoch [14], Batch [86/938], Loss: 0.985851526260376\n",
      "Validation: Epoch [14], Batch [87/938], Loss: 0.895361065864563\n",
      "Validation: Epoch [14], Batch [88/938], Loss: 0.9042046070098877\n",
      "Validation: Epoch [14], Batch [89/938], Loss: 0.6258261799812317\n",
      "Validation: Epoch [14], Batch [90/938], Loss: 0.7523853182792664\n",
      "Validation: Epoch [14], Batch [91/938], Loss: 1.0992560386657715\n",
      "Validation: Epoch [14], Batch [92/938], Loss: 0.5802518129348755\n",
      "Validation: Epoch [14], Batch [93/938], Loss: 0.7638276815414429\n",
      "Validation: Epoch [14], Batch [94/938], Loss: 0.545397162437439\n",
      "Validation: Epoch [14], Batch [95/938], Loss: 0.6823017001152039\n",
      "Validation: Epoch [14], Batch [96/938], Loss: 0.9013640284538269\n",
      "Validation: Epoch [14], Batch [97/938], Loss: 0.8238735198974609\n",
      "Validation: Epoch [14], Batch [98/938], Loss: 0.6912199258804321\n",
      "Validation: Epoch [14], Batch [99/938], Loss: 0.6011781096458435\n",
      "Validation: Epoch [14], Batch [100/938], Loss: 0.835030198097229\n",
      "Validation: Epoch [14], Batch [101/938], Loss: 0.7461717128753662\n",
      "Validation: Epoch [14], Batch [102/938], Loss: 1.0153549909591675\n",
      "Validation: Epoch [14], Batch [103/938], Loss: 0.6643772125244141\n",
      "Validation: Epoch [14], Batch [104/938], Loss: 0.9432329535484314\n",
      "Validation: Epoch [14], Batch [105/938], Loss: 0.6383593082427979\n",
      "Validation: Epoch [14], Batch [106/938], Loss: 1.0496121644973755\n",
      "Validation: Epoch [14], Batch [107/938], Loss: 0.6267138719558716\n",
      "Validation: Epoch [14], Batch [108/938], Loss: 0.8055185079574585\n",
      "Validation: Epoch [14], Batch [109/938], Loss: 0.9535527229309082\n",
      "Validation: Epoch [14], Batch [110/938], Loss: 0.8967734575271606\n",
      "Validation: Epoch [14], Batch [111/938], Loss: 0.9740214943885803\n",
      "Validation: Epoch [14], Batch [112/938], Loss: 0.4364748001098633\n",
      "Validation: Epoch [14], Batch [113/938], Loss: 0.8763805627822876\n",
      "Validation: Epoch [14], Batch [114/938], Loss: 0.7678948044776917\n",
      "Validation: Epoch [14], Batch [115/938], Loss: 0.9574366807937622\n",
      "Validation: Epoch [14], Batch [116/938], Loss: 0.7334959506988525\n",
      "Validation: Epoch [14], Batch [117/938], Loss: 0.5212642550468445\n",
      "Validation: Epoch [14], Batch [118/938], Loss: 0.6938101053237915\n",
      "Validation: Epoch [14], Batch [119/938], Loss: 0.8461489677429199\n",
      "Validation: Epoch [14], Batch [120/938], Loss: 0.614992618560791\n",
      "Validation: Epoch [14], Batch [121/938], Loss: 0.7162120342254639\n",
      "Validation: Epoch [14], Batch [122/938], Loss: 0.7006036043167114\n",
      "Validation: Epoch [14], Batch [123/938], Loss: 0.8324256539344788\n",
      "Validation: Epoch [14], Batch [124/938], Loss: 0.7289043068885803\n",
      "Validation: Epoch [14], Batch [125/938], Loss: 0.807422399520874\n",
      "Validation: Epoch [14], Batch [126/938], Loss: 0.7772011756896973\n",
      "Validation: Epoch [14], Batch [127/938], Loss: 0.9710550308227539\n",
      "Validation: Epoch [14], Batch [128/938], Loss: 0.9513753652572632\n",
      "Validation: Epoch [14], Batch [129/938], Loss: 0.7197344899177551\n",
      "Validation: Epoch [14], Batch [130/938], Loss: 0.7298622131347656\n",
      "Validation: Epoch [14], Batch [131/938], Loss: 0.5902442932128906\n",
      "Validation: Epoch [14], Batch [132/938], Loss: 0.8587129712104797\n",
      "Validation: Epoch [14], Batch [133/938], Loss: 0.8224919438362122\n",
      "Validation: Epoch [14], Batch [134/938], Loss: 0.5314652919769287\n",
      "Validation: Epoch [14], Batch [135/938], Loss: 0.6312693357467651\n",
      "Validation: Epoch [14], Batch [136/938], Loss: 0.5108304023742676\n",
      "Validation: Epoch [14], Batch [137/938], Loss: 0.650606632232666\n",
      "Validation: Epoch [14], Batch [138/938], Loss: 0.5447476506233215\n",
      "Validation: Epoch [14], Batch [139/938], Loss: 1.1012139320373535\n",
      "Validation: Epoch [14], Batch [140/938], Loss: 0.8703588247299194\n",
      "Validation: Epoch [14], Batch [141/938], Loss: 0.9096564650535583\n",
      "Validation: Epoch [14], Batch [142/938], Loss: 0.945857048034668\n",
      "Validation: Epoch [14], Batch [143/938], Loss: 0.6906142234802246\n",
      "Validation: Epoch [14], Batch [144/938], Loss: 0.9544917345046997\n",
      "Validation: Epoch [14], Batch [145/938], Loss: 0.7722592353820801\n",
      "Validation: Epoch [14], Batch [146/938], Loss: 0.5746166706085205\n",
      "Validation: Epoch [14], Batch [147/938], Loss: 0.9952447414398193\n",
      "Validation: Epoch [14], Batch [148/938], Loss: 0.5652604103088379\n",
      "Validation: Epoch [14], Batch [149/938], Loss: 0.5453474521636963\n",
      "Validation: Epoch [14], Batch [150/938], Loss: 0.5607143640518188\n",
      "Validation: Epoch [14], Batch [151/938], Loss: 0.6765561103820801\n",
      "Validation: Epoch [14], Batch [152/938], Loss: 0.899863600730896\n",
      "Validation: Epoch [14], Batch [153/938], Loss: 0.7289365530014038\n",
      "Validation: Epoch [14], Batch [154/938], Loss: 0.7365149259567261\n",
      "Validation: Epoch [14], Batch [155/938], Loss: 0.899796187877655\n",
      "Validation: Epoch [14], Batch [156/938], Loss: 0.8340969085693359\n",
      "Validation: Epoch [14], Batch [157/938], Loss: 0.8583566546440125\n",
      "Validation: Epoch [14], Batch [158/938], Loss: 0.5624825358390808\n",
      "Validation: Epoch [14], Batch [159/938], Loss: 0.7957255840301514\n",
      "Validation: Epoch [14], Batch [160/938], Loss: 0.7119340896606445\n",
      "Validation: Epoch [14], Batch [161/938], Loss: 0.7721471786499023\n",
      "Validation: Epoch [14], Batch [162/938], Loss: 0.9116700887680054\n",
      "Validation: Epoch [14], Batch [163/938], Loss: 1.0014172792434692\n",
      "Validation: Epoch [14], Batch [164/938], Loss: 0.5093871355056763\n",
      "Validation: Epoch [14], Batch [165/938], Loss: 0.6765463948249817\n",
      "Validation: Epoch [14], Batch [166/938], Loss: 0.789046049118042\n",
      "Validation: Epoch [14], Batch [167/938], Loss: 0.8962709903717041\n",
      "Validation: Epoch [14], Batch [168/938], Loss: 0.6207572817802429\n",
      "Validation: Epoch [14], Batch [169/938], Loss: 0.671592116355896\n",
      "Validation: Epoch [14], Batch [170/938], Loss: 0.9867879748344421\n",
      "Validation: Epoch [14], Batch [171/938], Loss: 0.9041146039962769\n",
      "Validation: Epoch [14], Batch [172/938], Loss: 0.7245986461639404\n",
      "Validation: Epoch [14], Batch [173/938], Loss: 0.589453935623169\n",
      "Validation: Epoch [14], Batch [174/938], Loss: 0.9484411478042603\n",
      "Validation: Epoch [14], Batch [175/938], Loss: 0.9612371921539307\n",
      "Validation: Epoch [14], Batch [176/938], Loss: 0.9407424330711365\n",
      "Validation: Epoch [14], Batch [177/938], Loss: 0.5629234313964844\n",
      "Validation: Epoch [14], Batch [178/938], Loss: 0.7042953968048096\n",
      "Validation: Epoch [14], Batch [179/938], Loss: 0.6519926190376282\n",
      "Validation: Epoch [14], Batch [180/938], Loss: 0.9834886193275452\n",
      "Validation: Epoch [14], Batch [181/938], Loss: 0.6513300538063049\n",
      "Validation: Epoch [14], Batch [182/938], Loss: 0.9811111092567444\n",
      "Validation: Epoch [14], Batch [183/938], Loss: 0.5975667834281921\n",
      "Validation: Epoch [14], Batch [184/938], Loss: 0.6808301210403442\n",
      "Validation: Epoch [14], Batch [185/938], Loss: 0.7186731696128845\n",
      "Validation: Epoch [14], Batch [186/938], Loss: 0.9631155133247375\n",
      "Validation: Epoch [14], Batch [187/938], Loss: 0.6779834032058716\n",
      "Validation: Epoch [14], Batch [188/938], Loss: 0.7194395661354065\n",
      "Validation: Epoch [14], Batch [189/938], Loss: 0.800053060054779\n",
      "Validation: Epoch [14], Batch [190/938], Loss: 0.7489945292472839\n",
      "Validation: Epoch [14], Batch [191/938], Loss: 0.6167715787887573\n",
      "Validation: Epoch [14], Batch [192/938], Loss: 0.5051135420799255\n",
      "Validation: Epoch [14], Batch [193/938], Loss: 0.603032112121582\n",
      "Validation: Epoch [14], Batch [194/938], Loss: 0.7189694046974182\n",
      "Validation: Epoch [14], Batch [195/938], Loss: 0.6282859444618225\n",
      "Validation: Epoch [14], Batch [196/938], Loss: 0.6340346336364746\n",
      "Validation: Epoch [14], Batch [197/938], Loss: 0.5582975745201111\n",
      "Validation: Epoch [14], Batch [198/938], Loss: 0.8538362979888916\n",
      "Validation: Epoch [14], Batch [199/938], Loss: 0.8095908761024475\n",
      "Validation: Epoch [14], Batch [200/938], Loss: 1.0239726305007935\n",
      "Validation: Epoch [14], Batch [201/938], Loss: 0.7063481211662292\n",
      "Validation: Epoch [14], Batch [202/938], Loss: 0.7083462476730347\n",
      "Validation: Epoch [14], Batch [203/938], Loss: 0.8557683229446411\n",
      "Validation: Epoch [14], Batch [204/938], Loss: 0.5776447057723999\n",
      "Validation: Epoch [14], Batch [205/938], Loss: 0.609311580657959\n",
      "Validation: Epoch [14], Batch [206/938], Loss: 0.721623420715332\n",
      "Validation: Epoch [14], Batch [207/938], Loss: 0.6303087472915649\n",
      "Validation: Epoch [14], Batch [208/938], Loss: 1.0453437566757202\n",
      "Validation: Epoch [14], Batch [209/938], Loss: 0.677611768245697\n",
      "Validation: Epoch [14], Batch [210/938], Loss: 0.6535230278968811\n",
      "Validation: Epoch [14], Batch [211/938], Loss: 0.798236608505249\n",
      "Validation: Epoch [14], Batch [212/938], Loss: 0.9083333015441895\n",
      "Validation: Epoch [14], Batch [213/938], Loss: 0.6320382952690125\n",
      "Validation: Epoch [14], Batch [214/938], Loss: 0.7393208146095276\n",
      "Validation: Epoch [14], Batch [215/938], Loss: 0.6698063015937805\n",
      "Validation: Epoch [14], Batch [216/938], Loss: 0.7531052231788635\n",
      "Validation: Epoch [14], Batch [217/938], Loss: 0.859790563583374\n",
      "Validation: Epoch [14], Batch [218/938], Loss: 0.6055797338485718\n",
      "Validation: Epoch [14], Batch [219/938], Loss: 0.622002124786377\n",
      "Validation: Epoch [14], Batch [220/938], Loss: 0.6978130340576172\n",
      "Validation: Epoch [14], Batch [221/938], Loss: 0.8525812029838562\n",
      "Validation: Epoch [14], Batch [222/938], Loss: 0.78371262550354\n",
      "Validation: Epoch [14], Batch [223/938], Loss: 0.6583887934684753\n",
      "Validation: Epoch [14], Batch [224/938], Loss: 0.554415762424469\n",
      "Validation: Epoch [14], Batch [225/938], Loss: 0.8121786117553711\n",
      "Validation: Epoch [14], Batch [226/938], Loss: 0.6599236130714417\n",
      "Validation: Epoch [14], Batch [227/938], Loss: 0.8127896785736084\n",
      "Validation: Epoch [14], Batch [228/938], Loss: 0.8505756258964539\n",
      "Validation: Epoch [14], Batch [229/938], Loss: 0.49998509883880615\n",
      "Validation: Epoch [14], Batch [230/938], Loss: 0.7497177720069885\n",
      "Validation: Epoch [14], Batch [231/938], Loss: 0.8027227520942688\n",
      "Validation: Epoch [14], Batch [232/938], Loss: 0.7249944806098938\n",
      "Validation: Epoch [14], Batch [233/938], Loss: 0.5021299719810486\n",
      "Validation: Epoch [14], Batch [234/938], Loss: 0.4791008234024048\n",
      "Validation: Epoch [14], Batch [235/938], Loss: 0.5713736414909363\n",
      "Validation: Epoch [14], Batch [236/938], Loss: 0.5803271532058716\n",
      "Validation: Epoch [14], Batch [237/938], Loss: 0.956373393535614\n",
      "Validation: Epoch [14], Batch [238/938], Loss: 0.8404982686042786\n",
      "Validation: Epoch [14], Batch [239/938], Loss: 0.8938660025596619\n",
      "Validation: Epoch [14], Batch [240/938], Loss: 0.6100925803184509\n",
      "Validation: Epoch [14], Batch [241/938], Loss: 0.8285581469535828\n",
      "Validation: Epoch [14], Batch [242/938], Loss: 0.6134270429611206\n",
      "Validation: Epoch [14], Batch [243/938], Loss: 0.7971469163894653\n",
      "Validation: Epoch [14], Batch [244/938], Loss: 0.7542089223861694\n",
      "Validation: Epoch [14], Batch [245/938], Loss: 0.674937903881073\n",
      "Validation: Epoch [14], Batch [246/938], Loss: 0.6068847179412842\n",
      "Validation: Epoch [14], Batch [247/938], Loss: 0.7482856512069702\n",
      "Validation: Epoch [14], Batch [248/938], Loss: 0.5667001008987427\n",
      "Validation: Epoch [14], Batch [249/938], Loss: 0.43077489733695984\n",
      "Validation: Epoch [14], Batch [250/938], Loss: 0.8010439872741699\n",
      "Validation: Epoch [14], Batch [251/938], Loss: 0.6080745458602905\n",
      "Validation: Epoch [14], Batch [252/938], Loss: 0.8082175254821777\n",
      "Validation: Epoch [14], Batch [253/938], Loss: 0.7522386908531189\n",
      "Validation: Epoch [14], Batch [254/938], Loss: 0.8866240978240967\n",
      "Validation: Epoch [14], Batch [255/938], Loss: 0.6480637192726135\n",
      "Validation: Epoch [14], Batch [256/938], Loss: 1.1598761081695557\n",
      "Validation: Epoch [14], Batch [257/938], Loss: 0.9555485248565674\n",
      "Validation: Epoch [14], Batch [258/938], Loss: 1.0701960325241089\n",
      "Validation: Epoch [14], Batch [259/938], Loss: 0.9706007838249207\n",
      "Validation: Epoch [14], Batch [260/938], Loss: 0.8616912364959717\n",
      "Validation: Epoch [14], Batch [261/938], Loss: 0.7291589975357056\n",
      "Validation: Epoch [14], Batch [262/938], Loss: 0.8718105554580688\n",
      "Validation: Epoch [14], Batch [263/938], Loss: 0.7038085460662842\n",
      "Validation: Epoch [14], Batch [264/938], Loss: 1.061322569847107\n",
      "Validation: Epoch [14], Batch [265/938], Loss: 0.7933281660079956\n",
      "Validation: Epoch [14], Batch [266/938], Loss: 0.7286062240600586\n",
      "Validation: Epoch [14], Batch [267/938], Loss: 0.8698382377624512\n",
      "Validation: Epoch [14], Batch [268/938], Loss: 0.6589324474334717\n",
      "Validation: Epoch [14], Batch [269/938], Loss: 0.9094114899635315\n",
      "Validation: Epoch [14], Batch [270/938], Loss: 0.4717392027378082\n",
      "Validation: Epoch [14], Batch [271/938], Loss: 0.8210591077804565\n",
      "Validation: Epoch [14], Batch [272/938], Loss: 0.41681331396102905\n",
      "Validation: Epoch [14], Batch [273/938], Loss: 0.9465194940567017\n",
      "Validation: Epoch [14], Batch [274/938], Loss: 0.6157957315444946\n",
      "Validation: Epoch [14], Batch [275/938], Loss: 0.8382999897003174\n",
      "Validation: Epoch [14], Batch [276/938], Loss: 0.8022026419639587\n",
      "Validation: Epoch [14], Batch [277/938], Loss: 0.38694894313812256\n",
      "Validation: Epoch [14], Batch [278/938], Loss: 0.835691511631012\n",
      "Validation: Epoch [14], Batch [279/938], Loss: 0.6600876450538635\n",
      "Validation: Epoch [14], Batch [280/938], Loss: 0.7541947960853577\n",
      "Validation: Epoch [14], Batch [281/938], Loss: 0.5681045055389404\n",
      "Validation: Epoch [14], Batch [282/938], Loss: 0.9316830039024353\n",
      "Validation: Epoch [14], Batch [283/938], Loss: 0.9634368419647217\n",
      "Validation: Epoch [14], Batch [284/938], Loss: 0.656476616859436\n",
      "Validation: Epoch [14], Batch [285/938], Loss: 0.7785663604736328\n",
      "Validation: Epoch [14], Batch [286/938], Loss: 0.6973861455917358\n",
      "Validation: Epoch [14], Batch [287/938], Loss: 0.6499457955360413\n",
      "Validation: Epoch [14], Batch [288/938], Loss: 0.7795677185058594\n",
      "Validation: Epoch [14], Batch [289/938], Loss: 0.6399877667427063\n",
      "Validation: Epoch [14], Batch [290/938], Loss: 0.7783581018447876\n",
      "Validation: Epoch [14], Batch [291/938], Loss: 0.752778172492981\n",
      "Validation: Epoch [14], Batch [292/938], Loss: 0.856951892375946\n",
      "Validation: Epoch [14], Batch [293/938], Loss: 0.7000209093093872\n",
      "Validation: Epoch [14], Batch [294/938], Loss: 0.958649218082428\n",
      "Validation: Epoch [14], Batch [295/938], Loss: 0.7679211497306824\n",
      "Validation: Epoch [14], Batch [296/938], Loss: 0.6076902747154236\n",
      "Validation: Epoch [14], Batch [297/938], Loss: 0.7375434637069702\n",
      "Validation: Epoch [14], Batch [298/938], Loss: 0.730427622795105\n",
      "Validation: Epoch [14], Batch [299/938], Loss: 0.6379456520080566\n",
      "Validation: Epoch [14], Batch [300/938], Loss: 0.9038670659065247\n",
      "Validation: Epoch [14], Batch [301/938], Loss: 0.6152071952819824\n",
      "Validation: Epoch [14], Batch [302/938], Loss: 0.5857756733894348\n",
      "Validation: Epoch [14], Batch [303/938], Loss: 0.8718792796134949\n",
      "Validation: Epoch [14], Batch [304/938], Loss: 0.8411906361579895\n",
      "Validation: Epoch [14], Batch [305/938], Loss: 0.7184206247329712\n",
      "Validation: Epoch [14], Batch [306/938], Loss: 0.8425720930099487\n",
      "Validation: Epoch [14], Batch [307/938], Loss: 0.7650992274284363\n",
      "Validation: Epoch [14], Batch [308/938], Loss: 0.6868879795074463\n",
      "Validation: Epoch [14], Batch [309/938], Loss: 0.5531920790672302\n",
      "Validation: Epoch [14], Batch [310/938], Loss: 1.2929646968841553\n",
      "Validation: Epoch [14], Batch [311/938], Loss: 0.8521131277084351\n",
      "Validation: Epoch [14], Batch [312/938], Loss: 0.7229292392730713\n",
      "Validation: Epoch [14], Batch [313/938], Loss: 0.8946965336799622\n",
      "Validation: Epoch [14], Batch [314/938], Loss: 0.9619695544242859\n",
      "Validation: Epoch [14], Batch [315/938], Loss: 0.8210994005203247\n",
      "Validation: Epoch [14], Batch [316/938], Loss: 0.885237991809845\n",
      "Validation: Epoch [14], Batch [317/938], Loss: 0.8033362030982971\n",
      "Validation: Epoch [14], Batch [318/938], Loss: 0.6215901374816895\n",
      "Validation: Epoch [14], Batch [319/938], Loss: 0.6490437984466553\n",
      "Validation: Epoch [14], Batch [320/938], Loss: 0.8047673106193542\n",
      "Validation: Epoch [14], Batch [321/938], Loss: 0.8922044038772583\n",
      "Validation: Epoch [14], Batch [322/938], Loss: 0.7732518315315247\n",
      "Validation: Epoch [14], Batch [323/938], Loss: 0.7194796204566956\n",
      "Validation: Epoch [14], Batch [324/938], Loss: 0.7028340697288513\n",
      "Validation: Epoch [14], Batch [325/938], Loss: 0.9103924036026001\n",
      "Validation: Epoch [14], Batch [326/938], Loss: 0.806178867816925\n",
      "Validation: Epoch [14], Batch [327/938], Loss: 0.6076413989067078\n",
      "Validation: Epoch [14], Batch [328/938], Loss: 0.7324662804603577\n",
      "Validation: Epoch [14], Batch [329/938], Loss: 0.9072244167327881\n",
      "Validation: Epoch [14], Batch [330/938], Loss: 0.6469758749008179\n",
      "Validation: Epoch [14], Batch [331/938], Loss: 0.7489910125732422\n",
      "Validation: Epoch [14], Batch [332/938], Loss: 0.6754758954048157\n",
      "Validation: Epoch [14], Batch [333/938], Loss: 0.737678587436676\n",
      "Validation: Epoch [14], Batch [334/938], Loss: 0.749191164970398\n",
      "Validation: Epoch [14], Batch [335/938], Loss: 0.8043582439422607\n",
      "Validation: Epoch [14], Batch [336/938], Loss: 0.7834250926971436\n",
      "Validation: Epoch [14], Batch [337/938], Loss: 0.8016494512557983\n",
      "Validation: Epoch [14], Batch [338/938], Loss: 0.8462503552436829\n",
      "Validation: Epoch [14], Batch [339/938], Loss: 0.6282104849815369\n",
      "Validation: Epoch [14], Batch [340/938], Loss: 0.6386574506759644\n",
      "Validation: Epoch [14], Batch [341/938], Loss: 0.7902536392211914\n",
      "Validation: Epoch [14], Batch [342/938], Loss: 0.7462592124938965\n",
      "Validation: Epoch [14], Batch [343/938], Loss: 0.6362587213516235\n",
      "Validation: Epoch [14], Batch [344/938], Loss: 0.6711102724075317\n",
      "Validation: Epoch [14], Batch [345/938], Loss: 0.6050019264221191\n",
      "Validation: Epoch [14], Batch [346/938], Loss: 0.8276965618133545\n",
      "Validation: Epoch [14], Batch [347/938], Loss: 0.8141646385192871\n",
      "Validation: Epoch [14], Batch [348/938], Loss: 0.802098274230957\n",
      "Validation: Epoch [14], Batch [349/938], Loss: 0.43224263191223145\n",
      "Validation: Epoch [14], Batch [350/938], Loss: 0.7716431021690369\n",
      "Validation: Epoch [14], Batch [351/938], Loss: 0.8999287486076355\n",
      "Validation: Epoch [14], Batch [352/938], Loss: 0.8189445734024048\n",
      "Validation: Epoch [14], Batch [353/938], Loss: 0.6573560237884521\n",
      "Validation: Epoch [14], Batch [354/938], Loss: 0.8031086325645447\n",
      "Validation: Epoch [14], Batch [355/938], Loss: 0.8007339835166931\n",
      "Validation: Epoch [14], Batch [356/938], Loss: 0.9068844318389893\n",
      "Validation: Epoch [14], Batch [357/938], Loss: 0.7942482233047485\n",
      "Validation: Epoch [14], Batch [358/938], Loss: 0.6079685688018799\n",
      "Validation: Epoch [14], Batch [359/938], Loss: 1.0263830423355103\n",
      "Validation: Epoch [14], Batch [360/938], Loss: 0.7885737419128418\n",
      "Validation: Epoch [14], Batch [361/938], Loss: 0.7275204062461853\n",
      "Validation: Epoch [14], Batch [362/938], Loss: 0.7109702229499817\n",
      "Validation: Epoch [14], Batch [363/938], Loss: 0.7445803880691528\n",
      "Validation: Epoch [14], Batch [364/938], Loss: 0.76251220703125\n",
      "Validation: Epoch [14], Batch [365/938], Loss: 0.9026225805282593\n",
      "Validation: Epoch [14], Batch [366/938], Loss: 0.9955976605415344\n",
      "Validation: Epoch [14], Batch [367/938], Loss: 0.7739341259002686\n",
      "Validation: Epoch [14], Batch [368/938], Loss: 1.1820383071899414\n",
      "Validation: Epoch [14], Batch [369/938], Loss: 0.9519136548042297\n",
      "Validation: Epoch [14], Batch [370/938], Loss: 0.9580775499343872\n",
      "Validation: Epoch [14], Batch [371/938], Loss: 0.6276596188545227\n",
      "Validation: Epoch [14], Batch [372/938], Loss: 0.740145742893219\n",
      "Validation: Epoch [14], Batch [373/938], Loss: 0.8329945206642151\n",
      "Validation: Epoch [14], Batch [374/938], Loss: 0.6952610611915588\n",
      "Validation: Epoch [14], Batch [375/938], Loss: 0.6860547661781311\n",
      "Validation: Epoch [14], Batch [376/938], Loss: 0.8042452931404114\n",
      "Validation: Epoch [14], Batch [377/938], Loss: 0.8286842703819275\n",
      "Validation: Epoch [14], Batch [378/938], Loss: 0.7301211357116699\n",
      "Validation: Epoch [14], Batch [379/938], Loss: 0.6081618070602417\n",
      "Validation: Epoch [14], Batch [380/938], Loss: 0.7539082169532776\n",
      "Validation: Epoch [14], Batch [381/938], Loss: 0.7999597191810608\n",
      "Validation: Epoch [14], Batch [382/938], Loss: 0.7107950448989868\n",
      "Validation: Epoch [14], Batch [383/938], Loss: 0.6525669097900391\n",
      "Validation: Epoch [14], Batch [384/938], Loss: 0.623399555683136\n",
      "Validation: Epoch [14], Batch [385/938], Loss: 0.7220203876495361\n",
      "Validation: Epoch [14], Batch [386/938], Loss: 0.6693291664123535\n",
      "Validation: Epoch [14], Batch [387/938], Loss: 0.8438677191734314\n",
      "Validation: Epoch [14], Batch [388/938], Loss: 0.6839599013328552\n",
      "Validation: Epoch [14], Batch [389/938], Loss: 0.7519106268882751\n",
      "Validation: Epoch [14], Batch [390/938], Loss: 0.43288296461105347\n",
      "Validation: Epoch [14], Batch [391/938], Loss: 0.8613802194595337\n",
      "Validation: Epoch [14], Batch [392/938], Loss: 0.7572405934333801\n",
      "Validation: Epoch [14], Batch [393/938], Loss: 0.5901921987533569\n",
      "Validation: Epoch [14], Batch [394/938], Loss: 0.5166876316070557\n",
      "Validation: Epoch [14], Batch [395/938], Loss: 0.8360306024551392\n",
      "Validation: Epoch [14], Batch [396/938], Loss: 0.9955503344535828\n",
      "Validation: Epoch [14], Batch [397/938], Loss: 0.7520668506622314\n",
      "Validation: Epoch [14], Batch [398/938], Loss: 0.7281163930892944\n",
      "Validation: Epoch [14], Batch [399/938], Loss: 0.7091620564460754\n",
      "Validation: Epoch [14], Batch [400/938], Loss: 0.9905570149421692\n",
      "Validation: Epoch [14], Batch [401/938], Loss: 0.9438454508781433\n",
      "Validation: Epoch [14], Batch [402/938], Loss: 0.6400036811828613\n",
      "Validation: Epoch [14], Batch [403/938], Loss: 0.7837070226669312\n",
      "Validation: Epoch [14], Batch [404/938], Loss: 0.7729003429412842\n",
      "Validation: Epoch [14], Batch [405/938], Loss: 0.7395010590553284\n",
      "Validation: Epoch [14], Batch [406/938], Loss: 0.7911564707756042\n",
      "Validation: Epoch [14], Batch [407/938], Loss: 0.6931948661804199\n",
      "Validation: Epoch [14], Batch [408/938], Loss: 0.5543581247329712\n",
      "Validation: Epoch [14], Batch [409/938], Loss: 0.5681692361831665\n",
      "Validation: Epoch [14], Batch [410/938], Loss: 0.8598588705062866\n",
      "Validation: Epoch [14], Batch [411/938], Loss: 0.6766272783279419\n",
      "Validation: Epoch [14], Batch [412/938], Loss: 0.6064887642860413\n",
      "Validation: Epoch [14], Batch [413/938], Loss: 0.9227217435836792\n",
      "Validation: Epoch [14], Batch [414/938], Loss: 0.7206266522407532\n",
      "Validation: Epoch [14], Batch [415/938], Loss: 0.934434711933136\n",
      "Validation: Epoch [14], Batch [416/938], Loss: 0.9604291915893555\n",
      "Validation: Epoch [14], Batch [417/938], Loss: 0.774694561958313\n",
      "Validation: Epoch [14], Batch [418/938], Loss: 0.6904720664024353\n",
      "Validation: Epoch [14], Batch [419/938], Loss: 0.7963246703147888\n",
      "Validation: Epoch [14], Batch [420/938], Loss: 0.7226161956787109\n",
      "Validation: Epoch [14], Batch [421/938], Loss: 0.8702288866043091\n",
      "Validation: Epoch [14], Batch [422/938], Loss: 0.7597289085388184\n",
      "Validation: Epoch [14], Batch [423/938], Loss: 0.7979540824890137\n",
      "Validation: Epoch [14], Batch [424/938], Loss: 0.8772727251052856\n",
      "Validation: Epoch [14], Batch [425/938], Loss: 0.690525233745575\n",
      "Validation: Epoch [14], Batch [426/938], Loss: 0.8512651920318604\n",
      "Validation: Epoch [14], Batch [427/938], Loss: 0.7859334349632263\n",
      "Validation: Epoch [14], Batch [428/938], Loss: 0.6251563429832458\n",
      "Validation: Epoch [14], Batch [429/938], Loss: 0.7058672904968262\n",
      "Validation: Epoch [14], Batch [430/938], Loss: 0.9644381403923035\n",
      "Validation: Epoch [14], Batch [431/938], Loss: 0.9387341737747192\n",
      "Validation: Epoch [14], Batch [432/938], Loss: 0.7326861023902893\n",
      "Validation: Epoch [14], Batch [433/938], Loss: 0.6642438173294067\n",
      "Validation: Epoch [14], Batch [434/938], Loss: 0.44216421246528625\n",
      "Validation: Epoch [14], Batch [435/938], Loss: 0.6055410504341125\n",
      "Validation: Epoch [14], Batch [436/938], Loss: 0.6235308051109314\n",
      "Validation: Epoch [14], Batch [437/938], Loss: 0.6973126530647278\n",
      "Validation: Epoch [14], Batch [438/938], Loss: 0.6216087341308594\n",
      "Validation: Epoch [14], Batch [439/938], Loss: 0.6678650975227356\n",
      "Validation: Epoch [14], Batch [440/938], Loss: 0.7069060802459717\n",
      "Validation: Epoch [14], Batch [441/938], Loss: 0.8340150117874146\n",
      "Validation: Epoch [14], Batch [442/938], Loss: 0.8194283246994019\n",
      "Validation: Epoch [14], Batch [443/938], Loss: 0.9498454928398132\n",
      "Validation: Epoch [14], Batch [444/938], Loss: 0.6995914578437805\n",
      "Validation: Epoch [14], Batch [445/938], Loss: 0.9408607482910156\n",
      "Validation: Epoch [14], Batch [446/938], Loss: 0.7467008829116821\n",
      "Validation: Epoch [14], Batch [447/938], Loss: 0.7968720197677612\n",
      "Validation: Epoch [14], Batch [448/938], Loss: 0.8122984170913696\n",
      "Validation: Epoch [14], Batch [449/938], Loss: 0.7785554528236389\n",
      "Validation: Epoch [14], Batch [450/938], Loss: 0.5485953688621521\n",
      "Validation: Epoch [14], Batch [451/938], Loss: 0.8783746957778931\n",
      "Validation: Epoch [14], Batch [452/938], Loss: 0.8703768849372864\n",
      "Validation: Epoch [14], Batch [453/938], Loss: 0.6562013626098633\n",
      "Validation: Epoch [14], Batch [454/938], Loss: 0.7106339931488037\n",
      "Validation: Epoch [14], Batch [455/938], Loss: 0.7198532819747925\n",
      "Validation: Epoch [14], Batch [456/938], Loss: 0.6889681220054626\n",
      "Validation: Epoch [14], Batch [457/938], Loss: 0.9483642578125\n",
      "Validation: Epoch [14], Batch [458/938], Loss: 0.6950930953025818\n",
      "Validation: Epoch [14], Batch [459/938], Loss: 0.813361644744873\n",
      "Validation: Epoch [14], Batch [460/938], Loss: 0.6892036199569702\n",
      "Validation: Epoch [14], Batch [461/938], Loss: 0.7875678539276123\n",
      "Validation: Epoch [14], Batch [462/938], Loss: 0.5530853867530823\n",
      "Validation: Epoch [14], Batch [463/938], Loss: 0.642253041267395\n",
      "Validation: Epoch [14], Batch [464/938], Loss: 0.704425573348999\n",
      "Validation: Epoch [14], Batch [465/938], Loss: 0.9469588994979858\n",
      "Validation: Epoch [14], Batch [466/938], Loss: 0.7144727110862732\n",
      "Validation: Epoch [14], Batch [467/938], Loss: 0.8265842199325562\n",
      "Validation: Epoch [14], Batch [468/938], Loss: 0.8182126879692078\n",
      "Validation: Epoch [14], Batch [469/938], Loss: 0.7512574195861816\n",
      "Validation: Epoch [14], Batch [470/938], Loss: 0.5270124673843384\n",
      "Validation: Epoch [14], Batch [471/938], Loss: 0.7690369486808777\n",
      "Validation: Epoch [14], Batch [472/938], Loss: 1.0117865800857544\n",
      "Validation: Epoch [14], Batch [473/938], Loss: 0.6176073551177979\n",
      "Validation: Epoch [14], Batch [474/938], Loss: 0.6512549519538879\n",
      "Validation: Epoch [14], Batch [475/938], Loss: 0.7356021404266357\n",
      "Validation: Epoch [14], Batch [476/938], Loss: 0.6863312721252441\n",
      "Validation: Epoch [14], Batch [477/938], Loss: 1.118273377418518\n",
      "Validation: Epoch [14], Batch [478/938], Loss: 0.7670244574546814\n",
      "Validation: Epoch [14], Batch [479/938], Loss: 0.7425598502159119\n",
      "Validation: Epoch [14], Batch [480/938], Loss: 0.6406144499778748\n",
      "Validation: Epoch [14], Batch [481/938], Loss: 0.7975751757621765\n",
      "Validation: Epoch [14], Batch [482/938], Loss: 0.8547781705856323\n",
      "Validation: Epoch [14], Batch [483/938], Loss: 0.5132500529289246\n",
      "Validation: Epoch [14], Batch [484/938], Loss: 0.6146950721740723\n",
      "Validation: Epoch [14], Batch [485/938], Loss: 0.9567644596099854\n",
      "Validation: Epoch [14], Batch [486/938], Loss: 0.7039362192153931\n",
      "Validation: Epoch [14], Batch [487/938], Loss: 0.591614305973053\n",
      "Validation: Epoch [14], Batch [488/938], Loss: 0.9999947547912598\n",
      "Validation: Epoch [14], Batch [489/938], Loss: 0.9834943413734436\n",
      "Validation: Epoch [14], Batch [490/938], Loss: 0.5544179677963257\n",
      "Validation: Epoch [14], Batch [491/938], Loss: 0.6284993886947632\n",
      "Validation: Epoch [14], Batch [492/938], Loss: 0.8054336309432983\n",
      "Validation: Epoch [14], Batch [493/938], Loss: 0.7917625308036804\n",
      "Validation: Epoch [14], Batch [494/938], Loss: 0.9806504845619202\n",
      "Validation: Epoch [14], Batch [495/938], Loss: 0.7200947403907776\n",
      "Validation: Epoch [14], Batch [496/938], Loss: 0.8097022175788879\n",
      "Validation: Epoch [14], Batch [497/938], Loss: 0.8862243890762329\n",
      "Validation: Epoch [14], Batch [498/938], Loss: 0.760739266872406\n",
      "Validation: Epoch [14], Batch [499/938], Loss: 0.8283209800720215\n",
      "Validation: Epoch [14], Batch [500/938], Loss: 0.611900806427002\n",
      "Validation: Epoch [14], Batch [501/938], Loss: 0.6487593054771423\n",
      "Validation: Epoch [14], Batch [502/938], Loss: 0.5024280548095703\n",
      "Validation: Epoch [14], Batch [503/938], Loss: 0.5614083409309387\n",
      "Validation: Epoch [14], Batch [504/938], Loss: 0.9577347636222839\n",
      "Validation: Epoch [14], Batch [505/938], Loss: 0.7184613943099976\n",
      "Validation: Epoch [14], Batch [506/938], Loss: 0.5892838835716248\n",
      "Validation: Epoch [14], Batch [507/938], Loss: 0.7476708292961121\n",
      "Validation: Epoch [14], Batch [508/938], Loss: 0.535416841506958\n",
      "Validation: Epoch [14], Batch [509/938], Loss: 0.7319197654724121\n",
      "Validation: Epoch [14], Batch [510/938], Loss: 0.8061830997467041\n",
      "Validation: Epoch [14], Batch [511/938], Loss: 0.7054641842842102\n",
      "Validation: Epoch [14], Batch [512/938], Loss: 0.5127048492431641\n",
      "Validation: Epoch [14], Batch [513/938], Loss: 0.9080854654312134\n",
      "Validation: Epoch [14], Batch [514/938], Loss: 0.9383737444877625\n",
      "Validation: Epoch [14], Batch [515/938], Loss: 0.8800272941589355\n",
      "Validation: Epoch [14], Batch [516/938], Loss: 0.6382943391799927\n",
      "Validation: Epoch [14], Batch [517/938], Loss: 0.6751293540000916\n",
      "Validation: Epoch [14], Batch [518/938], Loss: 0.8576504588127136\n",
      "Validation: Epoch [14], Batch [519/938], Loss: 0.8804718255996704\n",
      "Validation: Epoch [14], Batch [520/938], Loss: 0.9161174893379211\n",
      "Validation: Epoch [14], Batch [521/938], Loss: 0.681026041507721\n",
      "Validation: Epoch [14], Batch [522/938], Loss: 0.8155266046524048\n",
      "Validation: Epoch [14], Batch [523/938], Loss: 0.7995975613594055\n",
      "Validation: Epoch [14], Batch [524/938], Loss: 0.759161114692688\n",
      "Validation: Epoch [14], Batch [525/938], Loss: 0.7768478393554688\n",
      "Validation: Epoch [14], Batch [526/938], Loss: 0.7606880068778992\n",
      "Validation: Epoch [14], Batch [527/938], Loss: 0.8902337551116943\n",
      "Validation: Epoch [14], Batch [528/938], Loss: 0.6966649293899536\n",
      "Validation: Epoch [14], Batch [529/938], Loss: 0.7680521011352539\n",
      "Validation: Epoch [14], Batch [530/938], Loss: 0.6682913303375244\n",
      "Validation: Epoch [14], Batch [531/938], Loss: 0.49426618218421936\n",
      "Validation: Epoch [14], Batch [532/938], Loss: 0.5233464241027832\n",
      "Validation: Epoch [14], Batch [533/938], Loss: 0.9212671518325806\n",
      "Validation: Epoch [14], Batch [534/938], Loss: 0.6263645887374878\n",
      "Validation: Epoch [14], Batch [535/938], Loss: 0.705326497554779\n",
      "Validation: Epoch [14], Batch [536/938], Loss: 0.5489473342895508\n",
      "Validation: Epoch [14], Batch [537/938], Loss: 0.6547205448150635\n",
      "Validation: Epoch [14], Batch [538/938], Loss: 0.9397357106208801\n",
      "Validation: Epoch [14], Batch [539/938], Loss: 0.8304125666618347\n",
      "Validation: Epoch [14], Batch [540/938], Loss: 0.6058205366134644\n",
      "Validation: Epoch [14], Batch [541/938], Loss: 0.896648108959198\n",
      "Validation: Epoch [14], Batch [542/938], Loss: 0.7035180330276489\n",
      "Validation: Epoch [14], Batch [543/938], Loss: 0.784844160079956\n",
      "Validation: Epoch [14], Batch [544/938], Loss: 0.8458808660507202\n",
      "Validation: Epoch [14], Batch [545/938], Loss: 0.8743005990982056\n",
      "Validation: Epoch [14], Batch [546/938], Loss: 0.7787851095199585\n",
      "Validation: Epoch [14], Batch [547/938], Loss: 0.7775840163230896\n",
      "Validation: Epoch [14], Batch [548/938], Loss: 1.0135632753372192\n",
      "Validation: Epoch [14], Batch [549/938], Loss: 0.734652042388916\n",
      "Validation: Epoch [14], Batch [550/938], Loss: 0.7271875739097595\n",
      "Validation: Epoch [14], Batch [551/938], Loss: 0.4825340509414673\n",
      "Validation: Epoch [14], Batch [552/938], Loss: 0.6390731930732727\n",
      "Validation: Epoch [14], Batch [553/938], Loss: 0.8732390999794006\n",
      "Validation: Epoch [14], Batch [554/938], Loss: 0.8391838073730469\n",
      "Validation: Epoch [14], Batch [555/938], Loss: 0.9418212175369263\n",
      "Validation: Epoch [14], Batch [556/938], Loss: 0.7536149621009827\n",
      "Validation: Epoch [14], Batch [557/938], Loss: 0.821241021156311\n",
      "Validation: Epoch [14], Batch [558/938], Loss: 0.5330248475074768\n",
      "Validation: Epoch [14], Batch [559/938], Loss: 0.5658544301986694\n",
      "Validation: Epoch [14], Batch [560/938], Loss: 0.6899269819259644\n",
      "Validation: Epoch [14], Batch [561/938], Loss: 0.8116929531097412\n",
      "Validation: Epoch [14], Batch [562/938], Loss: 0.8943430185317993\n",
      "Validation: Epoch [14], Batch [563/938], Loss: 0.8411532640457153\n",
      "Validation: Epoch [14], Batch [564/938], Loss: 0.991706371307373\n",
      "Validation: Epoch [14], Batch [565/938], Loss: 1.274308443069458\n",
      "Validation: Epoch [14], Batch [566/938], Loss: 0.6784684658050537\n",
      "Validation: Epoch [14], Batch [567/938], Loss: 0.8756723999977112\n",
      "Validation: Epoch [14], Batch [568/938], Loss: 0.6906492114067078\n",
      "Validation: Epoch [14], Batch [569/938], Loss: 0.9704478979110718\n",
      "Validation: Epoch [14], Batch [570/938], Loss: 0.7269469499588013\n",
      "Validation: Epoch [14], Batch [571/938], Loss: 0.4230494499206543\n",
      "Validation: Epoch [14], Batch [572/938], Loss: 0.8180481195449829\n",
      "Validation: Epoch [14], Batch [573/938], Loss: 0.45461127161979675\n",
      "Validation: Epoch [14], Batch [574/938], Loss: 0.9179385900497437\n",
      "Validation: Epoch [14], Batch [575/938], Loss: 0.7934779524803162\n",
      "Validation: Epoch [14], Batch [576/938], Loss: 0.7830749750137329\n",
      "Validation: Epoch [14], Batch [577/938], Loss: 0.930462121963501\n",
      "Validation: Epoch [14], Batch [578/938], Loss: 0.5596624612808228\n",
      "Validation: Epoch [14], Batch [579/938], Loss: 0.7290216684341431\n",
      "Validation: Epoch [14], Batch [580/938], Loss: 0.7078493237495422\n",
      "Validation: Epoch [14], Batch [581/938], Loss: 0.6542564034461975\n",
      "Validation: Epoch [14], Batch [582/938], Loss: 0.8526378870010376\n",
      "Validation: Epoch [14], Batch [583/938], Loss: 0.9750336408615112\n",
      "Validation: Epoch [14], Batch [584/938], Loss: 0.6403546333312988\n",
      "Validation: Epoch [14], Batch [585/938], Loss: 0.5720348954200745\n",
      "Validation: Epoch [14], Batch [586/938], Loss: 0.646403968334198\n",
      "Validation: Epoch [14], Batch [587/938], Loss: 0.7759349346160889\n",
      "Validation: Epoch [14], Batch [588/938], Loss: 0.6406182050704956\n",
      "Validation: Epoch [14], Batch [589/938], Loss: 0.9368075132369995\n",
      "Validation: Epoch [14], Batch [590/938], Loss: 0.7751420736312866\n",
      "Validation: Epoch [14], Batch [591/938], Loss: 0.5837560296058655\n",
      "Validation: Epoch [14], Batch [592/938], Loss: 0.7813925743103027\n",
      "Validation: Epoch [14], Batch [593/938], Loss: 0.7342620491981506\n",
      "Validation: Epoch [14], Batch [594/938], Loss: 0.7044711112976074\n",
      "Validation: Epoch [14], Batch [595/938], Loss: 1.122412085533142\n",
      "Validation: Epoch [14], Batch [596/938], Loss: 0.6540848016738892\n",
      "Validation: Epoch [14], Batch [597/938], Loss: 0.9764575958251953\n",
      "Validation: Epoch [14], Batch [598/938], Loss: 0.801145613193512\n",
      "Validation: Epoch [14], Batch [599/938], Loss: 0.910273551940918\n",
      "Validation: Epoch [14], Batch [600/938], Loss: 0.7466528415679932\n",
      "Validation: Epoch [14], Batch [601/938], Loss: 0.8093025088310242\n",
      "Validation: Epoch [14], Batch [602/938], Loss: 0.8822758197784424\n",
      "Validation: Epoch [14], Batch [603/938], Loss: 0.8493543863296509\n",
      "Validation: Epoch [14], Batch [604/938], Loss: 0.703743040561676\n",
      "Validation: Epoch [14], Batch [605/938], Loss: 0.7117902040481567\n",
      "Validation: Epoch [14], Batch [606/938], Loss: 0.6620604395866394\n",
      "Validation: Epoch [14], Batch [607/938], Loss: 0.7094627022743225\n",
      "Validation: Epoch [14], Batch [608/938], Loss: 0.6536867618560791\n",
      "Validation: Epoch [14], Batch [609/938], Loss: 0.6612086892127991\n",
      "Validation: Epoch [14], Batch [610/938], Loss: 0.6952179074287415\n",
      "Validation: Epoch [14], Batch [611/938], Loss: 0.8039238452911377\n",
      "Validation: Epoch [14], Batch [612/938], Loss: 0.7072423100471497\n",
      "Validation: Epoch [14], Batch [613/938], Loss: 0.6680101752281189\n",
      "Validation: Epoch [14], Batch [614/938], Loss: 0.6246577501296997\n",
      "Validation: Epoch [14], Batch [615/938], Loss: 0.9458552598953247\n",
      "Validation: Epoch [14], Batch [616/938], Loss: 0.6066775321960449\n",
      "Validation: Epoch [14], Batch [617/938], Loss: 0.7626595497131348\n",
      "Validation: Epoch [14], Batch [618/938], Loss: 1.0209234952926636\n",
      "Validation: Epoch [14], Batch [619/938], Loss: 0.655713677406311\n",
      "Validation: Epoch [14], Batch [620/938], Loss: 0.8657799363136292\n",
      "Validation: Epoch [14], Batch [621/938], Loss: 0.8184621930122375\n",
      "Validation: Epoch [14], Batch [622/938], Loss: 0.612781822681427\n",
      "Validation: Epoch [14], Batch [623/938], Loss: 0.7596281170845032\n",
      "Validation: Epoch [14], Batch [624/938], Loss: 0.8454927802085876\n",
      "Validation: Epoch [14], Batch [625/938], Loss: 0.6251126527786255\n",
      "Validation: Epoch [14], Batch [626/938], Loss: 1.0867655277252197\n",
      "Validation: Epoch [14], Batch [627/938], Loss: 1.0134543180465698\n",
      "Validation: Epoch [14], Batch [628/938], Loss: 0.8148677945137024\n",
      "Validation: Epoch [14], Batch [629/938], Loss: 0.8928225040435791\n",
      "Validation: Epoch [14], Batch [630/938], Loss: 0.6314214468002319\n",
      "Validation: Epoch [14], Batch [631/938], Loss: 0.6836472749710083\n",
      "Validation: Epoch [14], Batch [632/938], Loss: 0.6600508689880371\n",
      "Validation: Epoch [14], Batch [633/938], Loss: 0.6587932109832764\n",
      "Validation: Epoch [14], Batch [634/938], Loss: 0.8313794732093811\n",
      "Validation: Epoch [14], Batch [635/938], Loss: 0.6481632590293884\n",
      "Validation: Epoch [14], Batch [636/938], Loss: 0.5399958491325378\n",
      "Validation: Epoch [14], Batch [637/938], Loss: 0.7906726598739624\n",
      "Validation: Epoch [14], Batch [638/938], Loss: 0.5041806697845459\n",
      "Validation: Epoch [14], Batch [639/938], Loss: 0.5279093980789185\n",
      "Validation: Epoch [14], Batch [640/938], Loss: 0.6137766242027283\n",
      "Validation: Epoch [14], Batch [641/938], Loss: 0.9252793192863464\n",
      "Validation: Epoch [14], Batch [642/938], Loss: 0.824404239654541\n",
      "Validation: Epoch [14], Batch [643/938], Loss: 0.7795077562332153\n",
      "Validation: Epoch [14], Batch [644/938], Loss: 0.732408881187439\n",
      "Validation: Epoch [14], Batch [645/938], Loss: 0.8689515590667725\n",
      "Validation: Epoch [14], Batch [646/938], Loss: 0.6697496175765991\n",
      "Validation: Epoch [14], Batch [647/938], Loss: 0.8627281785011292\n",
      "Validation: Epoch [14], Batch [648/938], Loss: 0.9159574508666992\n",
      "Validation: Epoch [14], Batch [649/938], Loss: 0.7065693736076355\n",
      "Validation: Epoch [14], Batch [650/938], Loss: 0.6869020462036133\n",
      "Validation: Epoch [14], Batch [651/938], Loss: 0.8015204668045044\n",
      "Validation: Epoch [14], Batch [652/938], Loss: 0.7656582593917847\n",
      "Validation: Epoch [14], Batch [653/938], Loss: 0.7832378149032593\n",
      "Validation: Epoch [14], Batch [654/938], Loss: 0.5103232264518738\n",
      "Validation: Epoch [14], Batch [655/938], Loss: 0.7383394241333008\n",
      "Validation: Epoch [14], Batch [656/938], Loss: 0.7665135860443115\n",
      "Validation: Epoch [14], Batch [657/938], Loss: 0.6864258646965027\n",
      "Validation: Epoch [14], Batch [658/938], Loss: 0.690173327922821\n",
      "Validation: Epoch [14], Batch [659/938], Loss: 0.7666354775428772\n",
      "Validation: Epoch [14], Batch [660/938], Loss: 0.7508330345153809\n",
      "Validation: Epoch [14], Batch [661/938], Loss: 0.7542744874954224\n",
      "Validation: Epoch [14], Batch [662/938], Loss: 0.8437225222587585\n",
      "Validation: Epoch [14], Batch [663/938], Loss: 0.7136226296424866\n",
      "Validation: Epoch [14], Batch [664/938], Loss: 0.8201937675476074\n",
      "Validation: Epoch [14], Batch [665/938], Loss: 0.644341230392456\n",
      "Validation: Epoch [14], Batch [666/938], Loss: 0.7711981534957886\n",
      "Validation: Epoch [14], Batch [667/938], Loss: 0.7481622695922852\n",
      "Validation: Epoch [14], Batch [668/938], Loss: 0.5482774972915649\n",
      "Validation: Epoch [14], Batch [669/938], Loss: 0.5502747297286987\n",
      "Validation: Epoch [14], Batch [670/938], Loss: 0.8401397466659546\n",
      "Validation: Epoch [14], Batch [671/938], Loss: 0.7683995366096497\n",
      "Validation: Epoch [14], Batch [672/938], Loss: 0.7572677731513977\n",
      "Validation: Epoch [14], Batch [673/938], Loss: 0.6539620757102966\n",
      "Validation: Epoch [14], Batch [674/938], Loss: 0.6506605744361877\n",
      "Validation: Epoch [14], Batch [675/938], Loss: 0.8586485981941223\n",
      "Validation: Epoch [14], Batch [676/938], Loss: 0.8769059777259827\n",
      "Validation: Epoch [14], Batch [677/938], Loss: 0.5231274962425232\n",
      "Validation: Epoch [14], Batch [678/938], Loss: 0.7460620403289795\n",
      "Validation: Epoch [14], Batch [679/938], Loss: 0.624871015548706\n",
      "Validation: Epoch [14], Batch [680/938], Loss: 0.35408738255500793\n",
      "Validation: Epoch [14], Batch [681/938], Loss: 0.7032288908958435\n",
      "Validation: Epoch [14], Batch [682/938], Loss: 0.5456458926200867\n",
      "Validation: Epoch [14], Batch [683/938], Loss: 0.6313341856002808\n",
      "Validation: Epoch [14], Batch [684/938], Loss: 0.6246986389160156\n",
      "Validation: Epoch [14], Batch [685/938], Loss: 0.7557913064956665\n",
      "Validation: Epoch [14], Batch [686/938], Loss: 0.7267592549324036\n",
      "Validation: Epoch [14], Batch [687/938], Loss: 0.48808005452156067\n",
      "Validation: Epoch [14], Batch [688/938], Loss: 0.7739013433456421\n",
      "Validation: Epoch [14], Batch [689/938], Loss: 0.5450425744056702\n",
      "Validation: Epoch [14], Batch [690/938], Loss: 0.8388607501983643\n",
      "Validation: Epoch [14], Batch [691/938], Loss: 0.7778029441833496\n",
      "Validation: Epoch [14], Batch [692/938], Loss: 0.7997036576271057\n",
      "Validation: Epoch [14], Batch [693/938], Loss: 0.7589566111564636\n",
      "Validation: Epoch [14], Batch [694/938], Loss: 1.0045380592346191\n",
      "Validation: Epoch [14], Batch [695/938], Loss: 0.8732544779777527\n",
      "Validation: Epoch [14], Batch [696/938], Loss: 0.9388639330863953\n",
      "Validation: Epoch [14], Batch [697/938], Loss: 0.7685258388519287\n",
      "Validation: Epoch [14], Batch [698/938], Loss: 0.6817339062690735\n",
      "Validation: Epoch [14], Batch [699/938], Loss: 0.6348686814308167\n",
      "Validation: Epoch [14], Batch [700/938], Loss: 0.673017680644989\n",
      "Validation: Epoch [14], Batch [701/938], Loss: 0.9151596426963806\n",
      "Validation: Epoch [14], Batch [702/938], Loss: 0.5945246815681458\n",
      "Validation: Epoch [14], Batch [703/938], Loss: 0.6647689342498779\n",
      "Validation: Epoch [14], Batch [704/938], Loss: 0.8255961537361145\n",
      "Validation: Epoch [14], Batch [705/938], Loss: 0.5945125222206116\n",
      "Validation: Epoch [14], Batch [706/938], Loss: 0.7540310025215149\n",
      "Validation: Epoch [14], Batch [707/938], Loss: 0.8366379737854004\n",
      "Validation: Epoch [14], Batch [708/938], Loss: 0.6292271018028259\n",
      "Validation: Epoch [14], Batch [709/938], Loss: 0.8605462312698364\n",
      "Validation: Epoch [14], Batch [710/938], Loss: 0.7116056680679321\n",
      "Validation: Epoch [14], Batch [711/938], Loss: 0.6385930776596069\n",
      "Validation: Epoch [14], Batch [712/938], Loss: 0.7690832018852234\n",
      "Validation: Epoch [14], Batch [713/938], Loss: 0.6607052683830261\n",
      "Validation: Epoch [14], Batch [714/938], Loss: 0.9774412512779236\n",
      "Validation: Epoch [14], Batch [715/938], Loss: 1.0121654272079468\n",
      "Validation: Epoch [14], Batch [716/938], Loss: 0.9908171892166138\n",
      "Validation: Epoch [14], Batch [717/938], Loss: 0.5672953128814697\n",
      "Validation: Epoch [14], Batch [718/938], Loss: 0.7707732319831848\n",
      "Validation: Epoch [14], Batch [719/938], Loss: 0.7383018732070923\n",
      "Validation: Epoch [14], Batch [720/938], Loss: 0.727323591709137\n",
      "Validation: Epoch [14], Batch [721/938], Loss: 0.4360599219799042\n",
      "Validation: Epoch [14], Batch [722/938], Loss: 0.863795816898346\n",
      "Validation: Epoch [14], Batch [723/938], Loss: 0.6386740803718567\n",
      "Validation: Epoch [14], Batch [724/938], Loss: 0.6143797039985657\n",
      "Validation: Epoch [14], Batch [725/938], Loss: 0.7057129144668579\n",
      "Validation: Epoch [14], Batch [726/938], Loss: 0.6693038940429688\n",
      "Validation: Epoch [14], Batch [727/938], Loss: 0.662678599357605\n",
      "Validation: Epoch [14], Batch [728/938], Loss: 0.7636435031890869\n",
      "Validation: Epoch [14], Batch [729/938], Loss: 0.6284922361373901\n",
      "Validation: Epoch [14], Batch [730/938], Loss: 0.6673849821090698\n",
      "Validation: Epoch [14], Batch [731/938], Loss: 0.8207002878189087\n",
      "Validation: Epoch [14], Batch [732/938], Loss: 0.6908067464828491\n",
      "Validation: Epoch [14], Batch [733/938], Loss: 0.7305656671524048\n",
      "Validation: Epoch [14], Batch [734/938], Loss: 0.4932582378387451\n",
      "Validation: Epoch [14], Batch [735/938], Loss: 0.7347521781921387\n",
      "Validation: Epoch [14], Batch [736/938], Loss: 0.7229809761047363\n",
      "Validation: Epoch [14], Batch [737/938], Loss: 0.720988929271698\n",
      "Validation: Epoch [14], Batch [738/938], Loss: 0.6676936149597168\n",
      "Validation: Epoch [14], Batch [739/938], Loss: 0.7022770643234253\n",
      "Validation: Epoch [14], Batch [740/938], Loss: 0.5978555679321289\n",
      "Validation: Epoch [14], Batch [741/938], Loss: 0.8901669979095459\n",
      "Validation: Epoch [14], Batch [742/938], Loss: 0.6945450901985168\n",
      "Validation: Epoch [14], Batch [743/938], Loss: 1.0283253192901611\n",
      "Validation: Epoch [14], Batch [744/938], Loss: 0.6002483367919922\n",
      "Validation: Epoch [14], Batch [745/938], Loss: 0.7400333881378174\n",
      "Validation: Epoch [14], Batch [746/938], Loss: 0.5594474077224731\n",
      "Validation: Epoch [14], Batch [747/938], Loss: 0.702573299407959\n",
      "Validation: Epoch [14], Batch [748/938], Loss: 0.5924794673919678\n",
      "Validation: Epoch [14], Batch [749/938], Loss: 0.7078733444213867\n",
      "Validation: Epoch [14], Batch [750/938], Loss: 0.7821011543273926\n",
      "Validation: Epoch [14], Batch [751/938], Loss: 0.7527651786804199\n",
      "Validation: Epoch [14], Batch [752/938], Loss: 0.7345495820045471\n",
      "Validation: Epoch [14], Batch [753/938], Loss: 0.5875504612922668\n",
      "Validation: Epoch [14], Batch [754/938], Loss: 0.5395585298538208\n",
      "Validation: Epoch [14], Batch [755/938], Loss: 0.8034061193466187\n",
      "Validation: Epoch [14], Batch [756/938], Loss: 0.5931292772293091\n",
      "Validation: Epoch [14], Batch [757/938], Loss: 0.5704641938209534\n",
      "Validation: Epoch [14], Batch [758/938], Loss: 0.7630528211593628\n",
      "Validation: Epoch [14], Batch [759/938], Loss: 0.769917905330658\n",
      "Validation: Epoch [14], Batch [760/938], Loss: 1.1061551570892334\n",
      "Validation: Epoch [14], Batch [761/938], Loss: 0.7355404496192932\n",
      "Validation: Epoch [14], Batch [762/938], Loss: 0.5608958601951599\n",
      "Validation: Epoch [14], Batch [763/938], Loss: 0.7246171236038208\n",
      "Validation: Epoch [14], Batch [764/938], Loss: 0.945971667766571\n",
      "Validation: Epoch [14], Batch [765/938], Loss: 0.7033513784408569\n",
      "Validation: Epoch [14], Batch [766/938], Loss: 0.736243486404419\n",
      "Validation: Epoch [14], Batch [767/938], Loss: 0.9669924974441528\n",
      "Validation: Epoch [14], Batch [768/938], Loss: 0.63489830493927\n",
      "Validation: Epoch [14], Batch [769/938], Loss: 0.6931621432304382\n",
      "Validation: Epoch [14], Batch [770/938], Loss: 0.6351321339607239\n",
      "Validation: Epoch [14], Batch [771/938], Loss: 0.6876775026321411\n",
      "Validation: Epoch [14], Batch [772/938], Loss: 0.9007722735404968\n",
      "Validation: Epoch [14], Batch [773/938], Loss: 0.8187021017074585\n",
      "Validation: Epoch [14], Batch [774/938], Loss: 0.7381158471107483\n",
      "Validation: Epoch [14], Batch [775/938], Loss: 0.6965776681900024\n",
      "Validation: Epoch [14], Batch [776/938], Loss: 0.7727364301681519\n",
      "Validation: Epoch [14], Batch [777/938], Loss: 0.9087978005409241\n",
      "Validation: Epoch [14], Batch [778/938], Loss: 0.896338701248169\n",
      "Validation: Epoch [14], Batch [779/938], Loss: 0.8439584374427795\n",
      "Validation: Epoch [14], Batch [780/938], Loss: 0.6244881749153137\n",
      "Validation: Epoch [14], Batch [781/938], Loss: 0.9068588018417358\n",
      "Validation: Epoch [14], Batch [782/938], Loss: 0.7245786190032959\n",
      "Validation: Epoch [14], Batch [783/938], Loss: 0.5874633193016052\n",
      "Validation: Epoch [14], Batch [784/938], Loss: 0.5454708337783813\n",
      "Validation: Epoch [14], Batch [785/938], Loss: 0.971649706363678\n",
      "Validation: Epoch [14], Batch [786/938], Loss: 0.7005254626274109\n",
      "Validation: Epoch [14], Batch [787/938], Loss: 0.6484243273735046\n",
      "Validation: Epoch [14], Batch [788/938], Loss: 0.610146164894104\n",
      "Validation: Epoch [14], Batch [789/938], Loss: 0.6292260885238647\n",
      "Validation: Epoch [14], Batch [790/938], Loss: 0.663906455039978\n",
      "Validation: Epoch [14], Batch [791/938], Loss: 0.8351408243179321\n",
      "Validation: Epoch [14], Batch [792/938], Loss: 0.8545773029327393\n",
      "Validation: Epoch [14], Batch [793/938], Loss: 0.7640025615692139\n",
      "Validation: Epoch [14], Batch [794/938], Loss: 0.7968508005142212\n",
      "Validation: Epoch [14], Batch [795/938], Loss: 0.5587124228477478\n",
      "Validation: Epoch [14], Batch [796/938], Loss: 0.8405559062957764\n",
      "Validation: Epoch [14], Batch [797/938], Loss: 0.7850655317306519\n",
      "Validation: Epoch [14], Batch [798/938], Loss: 0.7661767601966858\n",
      "Validation: Epoch [14], Batch [799/938], Loss: 0.6036798357963562\n",
      "Validation: Epoch [14], Batch [800/938], Loss: 0.7599315643310547\n",
      "Validation: Epoch [14], Batch [801/938], Loss: 0.9183053970336914\n",
      "Validation: Epoch [14], Batch [802/938], Loss: 0.7265695333480835\n",
      "Validation: Epoch [14], Batch [803/938], Loss: 0.7744978666305542\n",
      "Validation: Epoch [14], Batch [804/938], Loss: 0.5952705144882202\n",
      "Validation: Epoch [14], Batch [805/938], Loss: 0.8405242562294006\n",
      "Validation: Epoch [14], Batch [806/938], Loss: 0.730475902557373\n",
      "Validation: Epoch [14], Batch [807/938], Loss: 0.6197329163551331\n",
      "Validation: Epoch [14], Batch [808/938], Loss: 0.3383781313896179\n",
      "Validation: Epoch [14], Batch [809/938], Loss: 0.49598076939582825\n",
      "Validation: Epoch [14], Batch [810/938], Loss: 0.5452558994293213\n",
      "Validation: Epoch [14], Batch [811/938], Loss: 0.732114851474762\n",
      "Validation: Epoch [14], Batch [812/938], Loss: 0.8597471117973328\n",
      "Validation: Epoch [14], Batch [813/938], Loss: 0.7134396433830261\n",
      "Validation: Epoch [14], Batch [814/938], Loss: 0.8622955679893494\n",
      "Validation: Epoch [14], Batch [815/938], Loss: 0.9427197575569153\n",
      "Validation: Epoch [14], Batch [816/938], Loss: 1.0607237815856934\n",
      "Validation: Epoch [14], Batch [817/938], Loss: 0.6678447127342224\n",
      "Validation: Epoch [14], Batch [818/938], Loss: 0.593063473701477\n",
      "Validation: Epoch [14], Batch [819/938], Loss: 0.8937615156173706\n",
      "Validation: Epoch [14], Batch [820/938], Loss: 0.8091771006584167\n",
      "Validation: Epoch [14], Batch [821/938], Loss: 0.5350116491317749\n",
      "Validation: Epoch [14], Batch [822/938], Loss: 0.8085918426513672\n",
      "Validation: Epoch [14], Batch [823/938], Loss: 0.6388689279556274\n",
      "Validation: Epoch [14], Batch [824/938], Loss: 0.7534652948379517\n",
      "Validation: Epoch [14], Batch [825/938], Loss: 0.6197843551635742\n",
      "Validation: Epoch [14], Batch [826/938], Loss: 0.6921331882476807\n",
      "Validation: Epoch [14], Batch [827/938], Loss: 0.5493603944778442\n",
      "Validation: Epoch [14], Batch [828/938], Loss: 0.8039944171905518\n",
      "Validation: Epoch [14], Batch [829/938], Loss: 0.716110348701477\n",
      "Validation: Epoch [14], Batch [830/938], Loss: 0.7280570268630981\n",
      "Validation: Epoch [14], Batch [831/938], Loss: 0.9447738528251648\n",
      "Validation: Epoch [14], Batch [832/938], Loss: 0.5993092060089111\n",
      "Validation: Epoch [14], Batch [833/938], Loss: 0.6320133209228516\n",
      "Validation: Epoch [14], Batch [834/938], Loss: 0.8767397403717041\n",
      "Validation: Epoch [14], Batch [835/938], Loss: 0.7865516543388367\n",
      "Validation: Epoch [14], Batch [836/938], Loss: 0.5435143113136292\n",
      "Validation: Epoch [14], Batch [837/938], Loss: 0.6417999863624573\n",
      "Validation: Epoch [14], Batch [838/938], Loss: 0.849471390247345\n",
      "Validation: Epoch [14], Batch [839/938], Loss: 0.7833406329154968\n",
      "Validation: Epoch [14], Batch [840/938], Loss: 0.7022258043289185\n",
      "Validation: Epoch [14], Batch [841/938], Loss: 0.7175975441932678\n",
      "Validation: Epoch [14], Batch [842/938], Loss: 0.6788930296897888\n",
      "Validation: Epoch [14], Batch [843/938], Loss: 0.6534192562103271\n",
      "Validation: Epoch [14], Batch [844/938], Loss: 0.8363462686538696\n",
      "Validation: Epoch [14], Batch [845/938], Loss: 0.677381694316864\n",
      "Validation: Epoch [14], Batch [846/938], Loss: 0.8502956628799438\n",
      "Validation: Epoch [14], Batch [847/938], Loss: 0.6452754735946655\n",
      "Validation: Epoch [14], Batch [848/938], Loss: 0.8326926827430725\n",
      "Validation: Epoch [14], Batch [849/938], Loss: 0.7397037148475647\n",
      "Validation: Epoch [14], Batch [850/938], Loss: 0.6974137425422668\n",
      "Validation: Epoch [14], Batch [851/938], Loss: 0.8846464157104492\n",
      "Validation: Epoch [14], Batch [852/938], Loss: 0.485612154006958\n",
      "Validation: Epoch [14], Batch [853/938], Loss: 0.8246299624443054\n",
      "Validation: Epoch [14], Batch [854/938], Loss: 0.7105064392089844\n",
      "Validation: Epoch [14], Batch [855/938], Loss: 0.7041254043579102\n",
      "Validation: Epoch [14], Batch [856/938], Loss: 0.6023739576339722\n",
      "Validation: Epoch [14], Batch [857/938], Loss: 0.5694020390510559\n",
      "Validation: Epoch [14], Batch [858/938], Loss: 0.8352867364883423\n",
      "Validation: Epoch [14], Batch [859/938], Loss: 0.7870703339576721\n",
      "Validation: Epoch [14], Batch [860/938], Loss: 0.6901260018348694\n",
      "Validation: Epoch [14], Batch [861/938], Loss: 0.980236828327179\n",
      "Validation: Epoch [14], Batch [862/938], Loss: 0.7589356899261475\n",
      "Validation: Epoch [14], Batch [863/938], Loss: 1.0546255111694336\n",
      "Validation: Epoch [14], Batch [864/938], Loss: 0.8770630955696106\n",
      "Validation: Epoch [14], Batch [865/938], Loss: 0.903780460357666\n",
      "Validation: Epoch [14], Batch [866/938], Loss: 0.8275755643844604\n",
      "Validation: Epoch [14], Batch [867/938], Loss: 0.7932642698287964\n",
      "Validation: Epoch [14], Batch [868/938], Loss: 0.4576869606971741\n",
      "Validation: Epoch [14], Batch [869/938], Loss: 0.7639898657798767\n",
      "Validation: Epoch [14], Batch [870/938], Loss: 0.8062277436256409\n",
      "Validation: Epoch [14], Batch [871/938], Loss: 0.7125510573387146\n",
      "Validation: Epoch [14], Batch [872/938], Loss: 0.6916202306747437\n",
      "Validation: Epoch [14], Batch [873/938], Loss: 0.736247181892395\n",
      "Validation: Epoch [14], Batch [874/938], Loss: 0.6328944563865662\n",
      "Validation: Epoch [14], Batch [875/938], Loss: 1.0434517860412598\n",
      "Validation: Epoch [14], Batch [876/938], Loss: 0.5422133207321167\n",
      "Validation: Epoch [14], Batch [877/938], Loss: 0.7712705731391907\n",
      "Validation: Epoch [14], Batch [878/938], Loss: 0.6974397897720337\n",
      "Validation: Epoch [14], Batch [879/938], Loss: 0.7728760242462158\n",
      "Validation: Epoch [14], Batch [880/938], Loss: 0.7554470896720886\n",
      "Validation: Epoch [14], Batch [881/938], Loss: 0.4883177578449249\n",
      "Validation: Epoch [14], Batch [882/938], Loss: 0.6536359190940857\n",
      "Validation: Epoch [14], Batch [883/938], Loss: 0.881649374961853\n",
      "Validation: Epoch [14], Batch [884/938], Loss: 0.6928858160972595\n",
      "Validation: Epoch [14], Batch [885/938], Loss: 0.770031750202179\n",
      "Validation: Epoch [14], Batch [886/938], Loss: 0.7668665051460266\n",
      "Validation: Epoch [14], Batch [887/938], Loss: 0.7330625653266907\n",
      "Validation: Epoch [14], Batch [888/938], Loss: 0.6077882051467896\n",
      "Validation: Epoch [14], Batch [889/938], Loss: 0.9152702689170837\n",
      "Validation: Epoch [14], Batch [890/938], Loss: 0.6478830575942993\n",
      "Validation: Epoch [14], Batch [891/938], Loss: 0.6699749827384949\n",
      "Validation: Epoch [14], Batch [892/938], Loss: 0.7390059232711792\n",
      "Validation: Epoch [14], Batch [893/938], Loss: 0.5871182084083557\n",
      "Validation: Epoch [14], Batch [894/938], Loss: 0.6958249807357788\n",
      "Validation: Epoch [14], Batch [895/938], Loss: 0.6250547766685486\n",
      "Validation: Epoch [14], Batch [896/938], Loss: 0.7504069209098816\n",
      "Validation: Epoch [14], Batch [897/938], Loss: 0.8922649621963501\n",
      "Validation: Epoch [14], Batch [898/938], Loss: 0.6179661154747009\n",
      "Validation: Epoch [14], Batch [899/938], Loss: 0.6533661484718323\n",
      "Validation: Epoch [14], Batch [900/938], Loss: 0.8984214067459106\n",
      "Validation: Epoch [14], Batch [901/938], Loss: 0.7367736101150513\n",
      "Validation: Epoch [14], Batch [902/938], Loss: 0.7902022004127502\n",
      "Validation: Epoch [14], Batch [903/938], Loss: 0.9447034597396851\n",
      "Validation: Epoch [14], Batch [904/938], Loss: 0.7812691926956177\n",
      "Validation: Epoch [14], Batch [905/938], Loss: 0.5698837041854858\n",
      "Validation: Epoch [14], Batch [906/938], Loss: 0.9047330021858215\n",
      "Validation: Epoch [14], Batch [907/938], Loss: 0.683551013469696\n",
      "Validation: Epoch [14], Batch [908/938], Loss: 0.7872493863105774\n",
      "Validation: Epoch [14], Batch [909/938], Loss: 0.6199957132339478\n",
      "Validation: Epoch [14], Batch [910/938], Loss: 0.5938940048217773\n",
      "Validation: Epoch [14], Batch [911/938], Loss: 0.8890729546546936\n",
      "Validation: Epoch [14], Batch [912/938], Loss: 0.7131599187850952\n",
      "Validation: Epoch [14], Batch [913/938], Loss: 0.7026240229606628\n",
      "Validation: Epoch [14], Batch [914/938], Loss: 0.922157347202301\n",
      "Validation: Epoch [14], Batch [915/938], Loss: 0.9524408578872681\n",
      "Validation: Epoch [14], Batch [916/938], Loss: 0.9413375854492188\n",
      "Validation: Epoch [14], Batch [917/938], Loss: 0.6671013832092285\n",
      "Validation: Epoch [14], Batch [918/938], Loss: 0.749841570854187\n",
      "Validation: Epoch [14], Batch [919/938], Loss: 0.8762789964675903\n",
      "Validation: Epoch [14], Batch [920/938], Loss: 1.0597522258758545\n",
      "Validation: Epoch [14], Batch [921/938], Loss: 0.6995986104011536\n",
      "Validation: Epoch [14], Batch [922/938], Loss: 0.7748697996139526\n",
      "Validation: Epoch [14], Batch [923/938], Loss: 0.7465530633926392\n",
      "Validation: Epoch [14], Batch [924/938], Loss: 0.8894217014312744\n",
      "Validation: Epoch [14], Batch [925/938], Loss: 0.7006193995475769\n",
      "Validation: Epoch [14], Batch [926/938], Loss: 0.8164896368980408\n",
      "Validation: Epoch [14], Batch [927/938], Loss: 0.7488579154014587\n",
      "Validation: Epoch [14], Batch [928/938], Loss: 0.883424699306488\n",
      "Validation: Epoch [14], Batch [929/938], Loss: 0.7307066321372986\n",
      "Validation: Epoch [14], Batch [930/938], Loss: 0.7744156122207642\n",
      "Validation: Epoch [14], Batch [931/938], Loss: 0.6201565265655518\n",
      "Validation: Epoch [14], Batch [932/938], Loss: 0.7651849389076233\n",
      "Validation: Epoch [14], Batch [933/938], Loss: 0.9741232395172119\n",
      "Validation: Epoch [14], Batch [934/938], Loss: 0.7491035461425781\n",
      "Validation: Epoch [14], Batch [935/938], Loss: 0.7339732646942139\n",
      "Validation: Epoch [14], Batch [936/938], Loss: 0.6122211217880249\n",
      "Validation: Epoch [14], Batch [937/938], Loss: 0.5413019061088562\n",
      "Validation: Epoch [14], Batch [938/938], Loss: 0.6852339506149292\n",
      "Accuracy of test set: 0.7648333333333334\n",
      "Train: Epoch [15], Batch [1/938], Loss: 0.6518460512161255\n",
      "Train: Epoch [15], Batch [2/938], Loss: 0.6650427579879761\n",
      "Train: Epoch [15], Batch [3/938], Loss: 0.7517754435539246\n",
      "Train: Epoch [15], Batch [4/938], Loss: 0.8152242302894592\n",
      "Train: Epoch [15], Batch [5/938], Loss: 0.6078540086746216\n",
      "Train: Epoch [15], Batch [6/938], Loss: 0.8371780514717102\n",
      "Train: Epoch [15], Batch [7/938], Loss: 0.4291161596775055\n",
      "Train: Epoch [15], Batch [8/938], Loss: 0.7348907589912415\n",
      "Train: Epoch [15], Batch [9/938], Loss: 0.6662973165512085\n",
      "Train: Epoch [15], Batch [10/938], Loss: 0.8840064406394958\n",
      "Train: Epoch [15], Batch [11/938], Loss: 0.79451584815979\n",
      "Train: Epoch [15], Batch [12/938], Loss: 0.5818509459495544\n",
      "Train: Epoch [15], Batch [13/938], Loss: 0.7816605567932129\n",
      "Train: Epoch [15], Batch [14/938], Loss: 0.48022696375846863\n",
      "Train: Epoch [15], Batch [15/938], Loss: 0.6852465867996216\n",
      "Train: Epoch [15], Batch [16/938], Loss: 0.6896730065345764\n",
      "Train: Epoch [15], Batch [17/938], Loss: 0.9275428652763367\n",
      "Train: Epoch [15], Batch [18/938], Loss: 0.7492498159408569\n",
      "Train: Epoch [15], Batch [19/938], Loss: 0.7300488352775574\n",
      "Train: Epoch [15], Batch [20/938], Loss: 0.9995040893554688\n",
      "Train: Epoch [15], Batch [21/938], Loss: 0.5590266585350037\n",
      "Train: Epoch [15], Batch [22/938], Loss: 0.7390978336334229\n",
      "Train: Epoch [15], Batch [23/938], Loss: 0.7379552721977234\n",
      "Train: Epoch [15], Batch [24/938], Loss: 0.650885820388794\n",
      "Train: Epoch [15], Batch [25/938], Loss: 0.7163073420524597\n",
      "Train: Epoch [15], Batch [26/938], Loss: 0.6712650656700134\n",
      "Train: Epoch [15], Batch [27/938], Loss: 0.6833906173706055\n",
      "Train: Epoch [15], Batch [28/938], Loss: 0.6295165419578552\n",
      "Train: Epoch [15], Batch [29/938], Loss: 0.9298797249794006\n",
      "Train: Epoch [15], Batch [30/938], Loss: 0.7902959585189819\n",
      "Train: Epoch [15], Batch [31/938], Loss: 0.5559073686599731\n",
      "Train: Epoch [15], Batch [32/938], Loss: 0.8438847661018372\n",
      "Train: Epoch [15], Batch [33/938], Loss: 0.5910378098487854\n",
      "Train: Epoch [15], Batch [34/938], Loss: 0.676988422870636\n",
      "Train: Epoch [15], Batch [35/938], Loss: 0.9796998500823975\n",
      "Train: Epoch [15], Batch [36/938], Loss: 0.6908802390098572\n",
      "Train: Epoch [15], Batch [37/938], Loss: 0.5542540550231934\n",
      "Train: Epoch [15], Batch [38/938], Loss: 0.6778571009635925\n",
      "Train: Epoch [15], Batch [39/938], Loss: 0.8587348461151123\n",
      "Train: Epoch [15], Batch [40/938], Loss: 0.35789117217063904\n",
      "Train: Epoch [15], Batch [41/938], Loss: 0.6441460251808167\n",
      "Train: Epoch [15], Batch [42/938], Loss: 0.8965425491333008\n",
      "Train: Epoch [15], Batch [43/938], Loss: 0.6142497658729553\n",
      "Train: Epoch [15], Batch [44/938], Loss: 0.743592381477356\n",
      "Train: Epoch [15], Batch [45/938], Loss: 0.8559064269065857\n",
      "Train: Epoch [15], Batch [46/938], Loss: 0.6805259585380554\n",
      "Train: Epoch [15], Batch [47/938], Loss: 0.9522348642349243\n",
      "Train: Epoch [15], Batch [48/938], Loss: 0.8763434886932373\n",
      "Train: Epoch [15], Batch [49/938], Loss: 0.5413949489593506\n",
      "Train: Epoch [15], Batch [50/938], Loss: 0.6798532009124756\n",
      "Train: Epoch [15], Batch [51/938], Loss: 0.6406928896903992\n",
      "Train: Epoch [15], Batch [52/938], Loss: 0.6860998868942261\n",
      "Train: Epoch [15], Batch [53/938], Loss: 0.7302902936935425\n",
      "Train: Epoch [15], Batch [54/938], Loss: 0.5342843532562256\n",
      "Train: Epoch [15], Batch [55/938], Loss: 0.9067102670669556\n",
      "Train: Epoch [15], Batch [56/938], Loss: 0.6648213863372803\n",
      "Train: Epoch [15], Batch [57/938], Loss: 0.6686971187591553\n",
      "Train: Epoch [15], Batch [58/938], Loss: 0.7221049666404724\n",
      "Train: Epoch [15], Batch [59/938], Loss: 0.8091751933097839\n",
      "Train: Epoch [15], Batch [60/938], Loss: 0.6342775225639343\n",
      "Train: Epoch [15], Batch [61/938], Loss: 0.6775603890419006\n",
      "Train: Epoch [15], Batch [62/938], Loss: 0.8330551385879517\n",
      "Train: Epoch [15], Batch [63/938], Loss: 0.8408770561218262\n",
      "Train: Epoch [15], Batch [64/938], Loss: 0.5926900506019592\n",
      "Train: Epoch [15], Batch [65/938], Loss: 0.9080297946929932\n",
      "Train: Epoch [15], Batch [66/938], Loss: 0.7088264226913452\n",
      "Train: Epoch [15], Batch [67/938], Loss: 0.521754264831543\n",
      "Train: Epoch [15], Batch [68/938], Loss: 0.7890325784683228\n",
      "Train: Epoch [15], Batch [69/938], Loss: 0.6923983097076416\n",
      "Train: Epoch [15], Batch [70/938], Loss: 0.6419749855995178\n",
      "Train: Epoch [15], Batch [71/938], Loss: 0.5904143452644348\n",
      "Train: Epoch [15], Batch [72/938], Loss: 0.44961389899253845\n",
      "Train: Epoch [15], Batch [73/938], Loss: 0.7849254012107849\n",
      "Train: Epoch [15], Batch [74/938], Loss: 0.5791563987731934\n",
      "Train: Epoch [15], Batch [75/938], Loss: 0.5760707259178162\n",
      "Train: Epoch [15], Batch [76/938], Loss: 0.7437022924423218\n",
      "Train: Epoch [15], Batch [77/938], Loss: 0.6579661965370178\n",
      "Train: Epoch [15], Batch [78/938], Loss: 0.8113088607788086\n",
      "Train: Epoch [15], Batch [79/938], Loss: 0.5876337885856628\n",
      "Train: Epoch [15], Batch [80/938], Loss: 0.4607570171356201\n",
      "Train: Epoch [15], Batch [81/938], Loss: 0.7306615710258484\n",
      "Train: Epoch [15], Batch [82/938], Loss: 0.7433567643165588\n",
      "Train: Epoch [15], Batch [83/938], Loss: 0.589620053768158\n",
      "Train: Epoch [15], Batch [84/938], Loss: 0.7258197069168091\n",
      "Train: Epoch [15], Batch [85/938], Loss: 0.6472711563110352\n",
      "Train: Epoch [15], Batch [86/938], Loss: 0.85325026512146\n",
      "Train: Epoch [15], Batch [87/938], Loss: 0.7286967635154724\n",
      "Train: Epoch [15], Batch [88/938], Loss: 0.8373429179191589\n",
      "Train: Epoch [15], Batch [89/938], Loss: 0.6666317582130432\n",
      "Train: Epoch [15], Batch [90/938], Loss: 0.549897313117981\n",
      "Train: Epoch [15], Batch [91/938], Loss: 0.9799050092697144\n",
      "Train: Epoch [15], Batch [92/938], Loss: 0.6878142356872559\n",
      "Train: Epoch [15], Batch [93/938], Loss: 0.7953729033470154\n",
      "Train: Epoch [15], Batch [94/938], Loss: 0.6472979784011841\n",
      "Train: Epoch [15], Batch [95/938], Loss: 0.7614867687225342\n",
      "Train: Epoch [15], Batch [96/938], Loss: 0.7300933599472046\n",
      "Train: Epoch [15], Batch [97/938], Loss: 0.7976555228233337\n",
      "Train: Epoch [15], Batch [98/938], Loss: 0.8853145241737366\n",
      "Train: Epoch [15], Batch [99/938], Loss: 0.5910707116127014\n",
      "Train: Epoch [15], Batch [100/938], Loss: 0.763969361782074\n",
      "Train: Epoch [15], Batch [101/938], Loss: 0.6465749144554138\n",
      "Train: Epoch [15], Batch [102/938], Loss: 0.8145192861557007\n",
      "Train: Epoch [15], Batch [103/938], Loss: 0.5918317437171936\n",
      "Train: Epoch [15], Batch [104/938], Loss: 1.0330138206481934\n",
      "Train: Epoch [15], Batch [105/938], Loss: 0.8665213584899902\n",
      "Train: Epoch [15], Batch [106/938], Loss: 0.520087718963623\n",
      "Train: Epoch [15], Batch [107/938], Loss: 0.7358754873275757\n",
      "Train: Epoch [15], Batch [108/938], Loss: 0.6133245825767517\n",
      "Train: Epoch [15], Batch [109/938], Loss: 0.531190812587738\n",
      "Train: Epoch [15], Batch [110/938], Loss: 0.7581758499145508\n",
      "Train: Epoch [15], Batch [111/938], Loss: 0.8118941187858582\n",
      "Train: Epoch [15], Batch [112/938], Loss: 0.7512757778167725\n",
      "Train: Epoch [15], Batch [113/938], Loss: 0.5850904583930969\n",
      "Train: Epoch [15], Batch [114/938], Loss: 0.697372555732727\n",
      "Train: Epoch [15], Batch [115/938], Loss: 0.7001959681510925\n",
      "Train: Epoch [15], Batch [116/938], Loss: 0.9104116559028625\n",
      "Train: Epoch [15], Batch [117/938], Loss: 0.6669960618019104\n",
      "Train: Epoch [15], Batch [118/938], Loss: 0.5066901445388794\n",
      "Train: Epoch [15], Batch [119/938], Loss: 0.49974238872528076\n",
      "Train: Epoch [15], Batch [120/938], Loss: 0.6479084491729736\n",
      "Train: Epoch [15], Batch [121/938], Loss: 0.784824013710022\n",
      "Train: Epoch [15], Batch [122/938], Loss: 0.6175341010093689\n",
      "Train: Epoch [15], Batch [123/938], Loss: 1.139878511428833\n",
      "Train: Epoch [15], Batch [124/938], Loss: 0.5727537870407104\n",
      "Train: Epoch [15], Batch [125/938], Loss: 0.9428069591522217\n",
      "Train: Epoch [15], Batch [126/938], Loss: 0.6260054111480713\n",
      "Train: Epoch [15], Batch [127/938], Loss: 0.6772513389587402\n",
      "Train: Epoch [15], Batch [128/938], Loss: 0.6797359585762024\n",
      "Train: Epoch [15], Batch [129/938], Loss: 0.641913890838623\n",
      "Train: Epoch [15], Batch [130/938], Loss: 0.7780375480651855\n",
      "Train: Epoch [15], Batch [131/938], Loss: 0.6030752658843994\n",
      "Train: Epoch [15], Batch [132/938], Loss: 0.7635320425033569\n",
      "Train: Epoch [15], Batch [133/938], Loss: 0.8894639015197754\n",
      "Train: Epoch [15], Batch [134/938], Loss: 0.6400943994522095\n",
      "Train: Epoch [15], Batch [135/938], Loss: 0.8068278431892395\n",
      "Train: Epoch [15], Batch [136/938], Loss: 0.7092609405517578\n",
      "Train: Epoch [15], Batch [137/938], Loss: 0.5555866360664368\n",
      "Train: Epoch [15], Batch [138/938], Loss: 0.7533157467842102\n",
      "Train: Epoch [15], Batch [139/938], Loss: 0.7326725125312805\n",
      "Train: Epoch [15], Batch [140/938], Loss: 0.4983633756637573\n",
      "Train: Epoch [15], Batch [141/938], Loss: 0.7790740132331848\n",
      "Train: Epoch [15], Batch [142/938], Loss: 0.7080109715461731\n",
      "Train: Epoch [15], Batch [143/938], Loss: 0.7796040177345276\n",
      "Train: Epoch [15], Batch [144/938], Loss: 0.6445956230163574\n",
      "Train: Epoch [15], Batch [145/938], Loss: 0.8710770010948181\n",
      "Train: Epoch [15], Batch [146/938], Loss: 0.6932562589645386\n",
      "Train: Epoch [15], Batch [147/938], Loss: 0.4944104552268982\n",
      "Train: Epoch [15], Batch [148/938], Loss: 0.7638952136039734\n",
      "Train: Epoch [15], Batch [149/938], Loss: 0.6450855135917664\n",
      "Train: Epoch [15], Batch [150/938], Loss: 0.6031360626220703\n",
      "Train: Epoch [15], Batch [151/938], Loss: 0.5848456025123596\n",
      "Train: Epoch [15], Batch [152/938], Loss: 0.6778846979141235\n",
      "Train: Epoch [15], Batch [153/938], Loss: 0.5976696014404297\n",
      "Train: Epoch [15], Batch [154/938], Loss: 0.6708931922912598\n",
      "Train: Epoch [15], Batch [155/938], Loss: 0.9018718600273132\n",
      "Train: Epoch [15], Batch [156/938], Loss: 0.5894664525985718\n",
      "Train: Epoch [15], Batch [157/938], Loss: 0.9371461868286133\n",
      "Train: Epoch [15], Batch [158/938], Loss: 0.48401278257369995\n",
      "Train: Epoch [15], Batch [159/938], Loss: 0.7188354730606079\n",
      "Train: Epoch [15], Batch [160/938], Loss: 0.4633215069770813\n",
      "Train: Epoch [15], Batch [161/938], Loss: 0.6076628565788269\n",
      "Train: Epoch [15], Batch [162/938], Loss: 0.7786346673965454\n",
      "Train: Epoch [15], Batch [163/938], Loss: 0.8577724695205688\n",
      "Train: Epoch [15], Batch [164/938], Loss: 0.6767263412475586\n",
      "Train: Epoch [15], Batch [165/938], Loss: 0.8388090133666992\n",
      "Train: Epoch [15], Batch [166/938], Loss: 0.5935803055763245\n",
      "Train: Epoch [15], Batch [167/938], Loss: 0.6745224595069885\n",
      "Train: Epoch [15], Batch [168/938], Loss: 0.5109283924102783\n",
      "Train: Epoch [15], Batch [169/938], Loss: 0.6691379547119141\n",
      "Train: Epoch [15], Batch [170/938], Loss: 0.9394052028656006\n",
      "Train: Epoch [15], Batch [171/938], Loss: 0.805019199848175\n",
      "Train: Epoch [15], Batch [172/938], Loss: 0.9282041788101196\n",
      "Train: Epoch [15], Batch [173/938], Loss: 0.5674229264259338\n",
      "Train: Epoch [15], Batch [174/938], Loss: 0.8324328660964966\n",
      "Train: Epoch [15], Batch [175/938], Loss: 0.7254877090454102\n",
      "Train: Epoch [15], Batch [176/938], Loss: 0.7165790796279907\n",
      "Train: Epoch [15], Batch [177/938], Loss: 0.9970992803573608\n",
      "Train: Epoch [15], Batch [178/938], Loss: 0.9706173539161682\n",
      "Train: Epoch [15], Batch [179/938], Loss: 0.9262086749076843\n",
      "Train: Epoch [15], Batch [180/938], Loss: 0.9057597517967224\n",
      "Train: Epoch [15], Batch [181/938], Loss: 0.8863558769226074\n",
      "Train: Epoch [15], Batch [182/938], Loss: 0.5604931116104126\n",
      "Train: Epoch [15], Batch [183/938], Loss: 0.8149822354316711\n",
      "Train: Epoch [15], Batch [184/938], Loss: 0.716600239276886\n",
      "Train: Epoch [15], Batch [185/938], Loss: 0.8877555727958679\n",
      "Train: Epoch [15], Batch [186/938], Loss: 0.6501885652542114\n",
      "Train: Epoch [15], Batch [187/938], Loss: 0.7787075042724609\n",
      "Train: Epoch [15], Batch [188/938], Loss: 0.5035653710365295\n",
      "Train: Epoch [15], Batch [189/938], Loss: 0.6597898602485657\n",
      "Train: Epoch [15], Batch [190/938], Loss: 0.7213806509971619\n",
      "Train: Epoch [15], Batch [191/938], Loss: 0.5833799839019775\n",
      "Train: Epoch [15], Batch [192/938], Loss: 0.5058134198188782\n",
      "Train: Epoch [15], Batch [193/938], Loss: 0.6337111592292786\n",
      "Train: Epoch [15], Batch [194/938], Loss: 0.5746336579322815\n",
      "Train: Epoch [15], Batch [195/938], Loss: 0.656791090965271\n",
      "Train: Epoch [15], Batch [196/938], Loss: 0.8246775269508362\n",
      "Train: Epoch [15], Batch [197/938], Loss: 0.5553810596466064\n",
      "Train: Epoch [15], Batch [198/938], Loss: 0.6854731440544128\n",
      "Train: Epoch [15], Batch [199/938], Loss: 0.7510145902633667\n",
      "Train: Epoch [15], Batch [200/938], Loss: 0.647577702999115\n",
      "Train: Epoch [15], Batch [201/938], Loss: 0.7518517374992371\n",
      "Train: Epoch [15], Batch [202/938], Loss: 0.8150448799133301\n",
      "Train: Epoch [15], Batch [203/938], Loss: 0.6155421733856201\n",
      "Train: Epoch [15], Batch [204/938], Loss: 0.6693922877311707\n",
      "Train: Epoch [15], Batch [205/938], Loss: 0.5526646971702576\n",
      "Train: Epoch [15], Batch [206/938], Loss: 0.9328033328056335\n",
      "Train: Epoch [15], Batch [207/938], Loss: 0.5451294779777527\n",
      "Train: Epoch [15], Batch [208/938], Loss: 0.6697506308555603\n",
      "Train: Epoch [15], Batch [209/938], Loss: 0.6298179626464844\n",
      "Train: Epoch [15], Batch [210/938], Loss: 0.6897198557853699\n",
      "Train: Epoch [15], Batch [211/938], Loss: 0.8409796953201294\n",
      "Train: Epoch [15], Batch [212/938], Loss: 0.5221987366676331\n",
      "Train: Epoch [15], Batch [213/938], Loss: 0.6614201068878174\n",
      "Train: Epoch [15], Batch [214/938], Loss: 0.4947589039802551\n",
      "Train: Epoch [15], Batch [215/938], Loss: 0.7033437490463257\n",
      "Train: Epoch [15], Batch [216/938], Loss: 0.6357585787773132\n",
      "Train: Epoch [15], Batch [217/938], Loss: 0.6728021502494812\n",
      "Train: Epoch [15], Batch [218/938], Loss: 0.9600837230682373\n",
      "Train: Epoch [15], Batch [219/938], Loss: 0.5836011171340942\n",
      "Train: Epoch [15], Batch [220/938], Loss: 0.5977866053581238\n",
      "Train: Epoch [15], Batch [221/938], Loss: 0.7747830152511597\n",
      "Train: Epoch [15], Batch [222/938], Loss: 0.5654016733169556\n",
      "Train: Epoch [15], Batch [223/938], Loss: 0.5745347738265991\n",
      "Train: Epoch [15], Batch [224/938], Loss: 0.6114619374275208\n",
      "Train: Epoch [15], Batch [225/938], Loss: 0.6712844371795654\n",
      "Train: Epoch [15], Batch [226/938], Loss: 0.7686706781387329\n",
      "Train: Epoch [15], Batch [227/938], Loss: 0.7467842698097229\n",
      "Train: Epoch [15], Batch [228/938], Loss: 0.8150017857551575\n",
      "Train: Epoch [15], Batch [229/938], Loss: 0.8021519780158997\n",
      "Train: Epoch [15], Batch [230/938], Loss: 0.7592202425003052\n",
      "Train: Epoch [15], Batch [231/938], Loss: 0.605131208896637\n",
      "Train: Epoch [15], Batch [232/938], Loss: 0.8926447629928589\n",
      "Train: Epoch [15], Batch [233/938], Loss: 0.8244602084159851\n",
      "Train: Epoch [15], Batch [234/938], Loss: 0.5972036123275757\n",
      "Train: Epoch [15], Batch [235/938], Loss: 0.5555508732795715\n",
      "Train: Epoch [15], Batch [236/938], Loss: 0.8413941264152527\n",
      "Train: Epoch [15], Batch [237/938], Loss: 0.6409048438072205\n",
      "Train: Epoch [15], Batch [238/938], Loss: 0.6978185176849365\n",
      "Train: Epoch [15], Batch [239/938], Loss: 0.8862206935882568\n",
      "Train: Epoch [15], Batch [240/938], Loss: 0.5728917121887207\n",
      "Train: Epoch [15], Batch [241/938], Loss: 0.6907779574394226\n",
      "Train: Epoch [15], Batch [242/938], Loss: 0.6669899225234985\n",
      "Train: Epoch [15], Batch [243/938], Loss: 0.7660437822341919\n",
      "Train: Epoch [15], Batch [244/938], Loss: 0.6107485890388489\n",
      "Train: Epoch [15], Batch [245/938], Loss: 0.5664194822311401\n",
      "Train: Epoch [15], Batch [246/938], Loss: 0.6573895215988159\n",
      "Train: Epoch [15], Batch [247/938], Loss: 0.741283655166626\n",
      "Train: Epoch [15], Batch [248/938], Loss: 0.544588565826416\n",
      "Train: Epoch [15], Batch [249/938], Loss: 0.7779289484024048\n",
      "Train: Epoch [15], Batch [250/938], Loss: 0.5696089863777161\n",
      "Train: Epoch [15], Batch [251/938], Loss: 0.5887427926063538\n",
      "Train: Epoch [15], Batch [252/938], Loss: 0.674283504486084\n",
      "Train: Epoch [15], Batch [253/938], Loss: 0.6437956094741821\n",
      "Train: Epoch [15], Batch [254/938], Loss: 0.5502126216888428\n",
      "Train: Epoch [15], Batch [255/938], Loss: 0.7259685397148132\n",
      "Train: Epoch [15], Batch [256/938], Loss: 0.6650497317314148\n",
      "Train: Epoch [15], Batch [257/938], Loss: 0.6148176193237305\n",
      "Train: Epoch [15], Batch [258/938], Loss: 0.6216779947280884\n",
      "Train: Epoch [15], Batch [259/938], Loss: 0.7764865756034851\n",
      "Train: Epoch [15], Batch [260/938], Loss: 0.8017813563346863\n",
      "Train: Epoch [15], Batch [261/938], Loss: 0.7057731747627258\n",
      "Train: Epoch [15], Batch [262/938], Loss: 0.6772854328155518\n",
      "Train: Epoch [15], Batch [263/938], Loss: 0.6794482469558716\n",
      "Train: Epoch [15], Batch [264/938], Loss: 0.7808529138565063\n",
      "Train: Epoch [15], Batch [265/938], Loss: 0.5994119644165039\n",
      "Train: Epoch [15], Batch [266/938], Loss: 0.6716247200965881\n",
      "Train: Epoch [15], Batch [267/938], Loss: 0.7201576232910156\n",
      "Train: Epoch [15], Batch [268/938], Loss: 0.5849925875663757\n",
      "Train: Epoch [15], Batch [269/938], Loss: 0.9980318546295166\n",
      "Train: Epoch [15], Batch [270/938], Loss: 0.6589997410774231\n",
      "Train: Epoch [15], Batch [271/938], Loss: 0.6312081813812256\n",
      "Train: Epoch [15], Batch [272/938], Loss: 0.6038582921028137\n",
      "Train: Epoch [15], Batch [273/938], Loss: 0.7006816864013672\n",
      "Train: Epoch [15], Batch [274/938], Loss: 0.563758134841919\n",
      "Train: Epoch [15], Batch [275/938], Loss: 0.9528353810310364\n",
      "Train: Epoch [15], Batch [276/938], Loss: 0.753098726272583\n",
      "Train: Epoch [15], Batch [277/938], Loss: 0.5289738178253174\n",
      "Train: Epoch [15], Batch [278/938], Loss: 0.6103569269180298\n",
      "Train: Epoch [15], Batch [279/938], Loss: 0.6613185405731201\n",
      "Train: Epoch [15], Batch [280/938], Loss: 0.6189649105072021\n",
      "Train: Epoch [15], Batch [281/938], Loss: 0.6383318901062012\n",
      "Train: Epoch [15], Batch [282/938], Loss: 0.770064651966095\n",
      "Train: Epoch [15], Batch [283/938], Loss: 0.7794581651687622\n",
      "Train: Epoch [15], Batch [284/938], Loss: 0.6460391283035278\n",
      "Train: Epoch [15], Batch [285/938], Loss: 0.641265869140625\n",
      "Train: Epoch [15], Batch [286/938], Loss: 0.6341944336891174\n",
      "Train: Epoch [15], Batch [287/938], Loss: 0.6728861331939697\n",
      "Train: Epoch [15], Batch [288/938], Loss: 0.894336462020874\n",
      "Train: Epoch [15], Batch [289/938], Loss: 0.599280834197998\n",
      "Train: Epoch [15], Batch [290/938], Loss: 0.40366876125335693\n",
      "Train: Epoch [15], Batch [291/938], Loss: 0.8775693774223328\n",
      "Train: Epoch [15], Batch [292/938], Loss: 0.8545128107070923\n",
      "Train: Epoch [15], Batch [293/938], Loss: 0.6456670761108398\n",
      "Train: Epoch [15], Batch [294/938], Loss: 0.739250123500824\n",
      "Train: Epoch [15], Batch [295/938], Loss: 0.818610668182373\n",
      "Train: Epoch [15], Batch [296/938], Loss: 0.7575879693031311\n",
      "Train: Epoch [15], Batch [297/938], Loss: 0.6410382986068726\n",
      "Train: Epoch [15], Batch [298/938], Loss: 0.7782824635505676\n",
      "Train: Epoch [15], Batch [299/938], Loss: 0.6048698425292969\n",
      "Train: Epoch [15], Batch [300/938], Loss: 0.6958193778991699\n",
      "Train: Epoch [15], Batch [301/938], Loss: 0.6636314392089844\n",
      "Train: Epoch [15], Batch [302/938], Loss: 0.5027852058410645\n",
      "Train: Epoch [15], Batch [303/938], Loss: 0.5717337131500244\n",
      "Train: Epoch [15], Batch [304/938], Loss: 0.7058433890342712\n",
      "Train: Epoch [15], Batch [305/938], Loss: 0.6592124104499817\n",
      "Train: Epoch [15], Batch [306/938], Loss: 0.805126428604126\n",
      "Train: Epoch [15], Batch [307/938], Loss: 0.5787989497184753\n",
      "Train: Epoch [15], Batch [308/938], Loss: 0.6255608201026917\n",
      "Train: Epoch [15], Batch [309/938], Loss: 0.7790687680244446\n",
      "Train: Epoch [15], Batch [310/938], Loss: 0.7236776947975159\n",
      "Train: Epoch [15], Batch [311/938], Loss: 0.765529215335846\n",
      "Train: Epoch [15], Batch [312/938], Loss: 0.8244920969009399\n",
      "Train: Epoch [15], Batch [313/938], Loss: 1.0382804870605469\n",
      "Train: Epoch [15], Batch [314/938], Loss: 0.6132934093475342\n",
      "Train: Epoch [15], Batch [315/938], Loss: 0.47759324312210083\n",
      "Train: Epoch [15], Batch [316/938], Loss: 0.58146733045578\n",
      "Train: Epoch [15], Batch [317/938], Loss: 0.7555838823318481\n",
      "Train: Epoch [15], Batch [318/938], Loss: 0.788189172744751\n",
      "Train: Epoch [15], Batch [319/938], Loss: 0.7844700813293457\n",
      "Train: Epoch [15], Batch [320/938], Loss: 0.7541791796684265\n",
      "Train: Epoch [15], Batch [321/938], Loss: 0.6615638732910156\n",
      "Train: Epoch [15], Batch [322/938], Loss: 0.7453809380531311\n",
      "Train: Epoch [15], Batch [323/938], Loss: 0.8054748773574829\n",
      "Train: Epoch [15], Batch [324/938], Loss: 0.8009997010231018\n",
      "Train: Epoch [15], Batch [325/938], Loss: 0.593036413192749\n",
      "Train: Epoch [15], Batch [326/938], Loss: 1.0021388530731201\n",
      "Train: Epoch [15], Batch [327/938], Loss: 0.6943165063858032\n",
      "Train: Epoch [15], Batch [328/938], Loss: 0.6883803606033325\n",
      "Train: Epoch [15], Batch [329/938], Loss: 0.7372543215751648\n",
      "Train: Epoch [15], Batch [330/938], Loss: 0.621479868888855\n",
      "Train: Epoch [15], Batch [331/938], Loss: 0.7761942148208618\n",
      "Train: Epoch [15], Batch [332/938], Loss: 0.7709541320800781\n",
      "Train: Epoch [15], Batch [333/938], Loss: 0.6376938819885254\n",
      "Train: Epoch [15], Batch [334/938], Loss: 0.7249733209609985\n",
      "Train: Epoch [15], Batch [335/938], Loss: 0.4304181933403015\n",
      "Train: Epoch [15], Batch [336/938], Loss: 0.7639415860176086\n",
      "Train: Epoch [15], Batch [337/938], Loss: 0.5822159051895142\n",
      "Train: Epoch [15], Batch [338/938], Loss: 0.7222765684127808\n",
      "Train: Epoch [15], Batch [339/938], Loss: 0.6537586450576782\n",
      "Train: Epoch [15], Batch [340/938], Loss: 0.8405764102935791\n",
      "Train: Epoch [15], Batch [341/938], Loss: 0.6679540276527405\n",
      "Train: Epoch [15], Batch [342/938], Loss: 0.6500666737556458\n",
      "Train: Epoch [15], Batch [343/938], Loss: 0.5812216401100159\n",
      "Train: Epoch [15], Batch [344/938], Loss: 0.7798427939414978\n",
      "Train: Epoch [15], Batch [345/938], Loss: 0.754045844078064\n",
      "Train: Epoch [15], Batch [346/938], Loss: 0.7643041610717773\n",
      "Train: Epoch [15], Batch [347/938], Loss: 0.7738487720489502\n",
      "Train: Epoch [15], Batch [348/938], Loss: 0.7554639577865601\n",
      "Train: Epoch [15], Batch [349/938], Loss: 0.5614709258079529\n",
      "Train: Epoch [15], Batch [350/938], Loss: 0.9354947209358215\n",
      "Train: Epoch [15], Batch [351/938], Loss: 0.711464524269104\n",
      "Train: Epoch [15], Batch [352/938], Loss: 0.6793227195739746\n",
      "Train: Epoch [15], Batch [353/938], Loss: 1.0158060789108276\n",
      "Train: Epoch [15], Batch [354/938], Loss: 0.6302870512008667\n",
      "Train: Epoch [15], Batch [355/938], Loss: 0.6181316375732422\n",
      "Train: Epoch [15], Batch [356/938], Loss: 0.7288621068000793\n",
      "Train: Epoch [15], Batch [357/938], Loss: 0.643455982208252\n",
      "Train: Epoch [15], Batch [358/938], Loss: 0.7906643152236938\n",
      "Train: Epoch [15], Batch [359/938], Loss: 0.7769685387611389\n",
      "Train: Epoch [15], Batch [360/938], Loss: 0.6649166345596313\n",
      "Train: Epoch [15], Batch [361/938], Loss: 0.9308227300643921\n",
      "Train: Epoch [15], Batch [362/938], Loss: 0.6727887392044067\n",
      "Train: Epoch [15], Batch [363/938], Loss: 0.7962089776992798\n",
      "Train: Epoch [15], Batch [364/938], Loss: 0.6178520917892456\n",
      "Train: Epoch [15], Batch [365/938], Loss: 0.6943472623825073\n",
      "Train: Epoch [15], Batch [366/938], Loss: 0.6552364826202393\n",
      "Train: Epoch [15], Batch [367/938], Loss: 0.43507787585258484\n",
      "Train: Epoch [15], Batch [368/938], Loss: 0.6123201847076416\n",
      "Train: Epoch [15], Batch [369/938], Loss: 0.5581628680229187\n",
      "Train: Epoch [15], Batch [370/938], Loss: 0.7130939364433289\n",
      "Train: Epoch [15], Batch [371/938], Loss: 0.7158622145652771\n",
      "Train: Epoch [15], Batch [372/938], Loss: 0.4808466136455536\n",
      "Train: Epoch [15], Batch [373/938], Loss: 0.46413642168045044\n",
      "Train: Epoch [15], Batch [374/938], Loss: 0.6206279397010803\n",
      "Train: Epoch [15], Batch [375/938], Loss: 0.6762095093727112\n",
      "Train: Epoch [15], Batch [376/938], Loss: 0.6102765798568726\n",
      "Train: Epoch [15], Batch [377/938], Loss: 0.5128877758979797\n",
      "Train: Epoch [15], Batch [378/938], Loss: 0.9514452219009399\n",
      "Train: Epoch [15], Batch [379/938], Loss: 0.6441851258277893\n",
      "Train: Epoch [15], Batch [380/938], Loss: 0.5842019319534302\n",
      "Train: Epoch [15], Batch [381/938], Loss: 0.6712402105331421\n",
      "Train: Epoch [15], Batch [382/938], Loss: 0.9015681147575378\n",
      "Train: Epoch [15], Batch [383/938], Loss: 0.6355467438697815\n",
      "Train: Epoch [15], Batch [384/938], Loss: 0.453393816947937\n",
      "Train: Epoch [15], Batch [385/938], Loss: 0.8661546111106873\n",
      "Train: Epoch [15], Batch [386/938], Loss: 0.7594116926193237\n",
      "Train: Epoch [15], Batch [387/938], Loss: 0.8014037609100342\n",
      "Train: Epoch [15], Batch [388/938], Loss: 0.5646689534187317\n",
      "Train: Epoch [15], Batch [389/938], Loss: 0.6312453150749207\n",
      "Train: Epoch [15], Batch [390/938], Loss: 0.6849072575569153\n",
      "Train: Epoch [15], Batch [391/938], Loss: 0.6689507365226746\n",
      "Train: Epoch [15], Batch [392/938], Loss: 0.40508222579956055\n",
      "Train: Epoch [15], Batch [393/938], Loss: 1.069345235824585\n",
      "Train: Epoch [15], Batch [394/938], Loss: 0.8028701543807983\n",
      "Train: Epoch [15], Batch [395/938], Loss: 0.6805384159088135\n",
      "Train: Epoch [15], Batch [396/938], Loss: 1.0022233724594116\n",
      "Train: Epoch [15], Batch [397/938], Loss: 0.8601611256599426\n",
      "Train: Epoch [15], Batch [398/938], Loss: 0.6176205277442932\n",
      "Train: Epoch [15], Batch [399/938], Loss: 0.8107331395149231\n",
      "Train: Epoch [15], Batch [400/938], Loss: 0.6164144277572632\n",
      "Train: Epoch [15], Batch [401/938], Loss: 0.7031322717666626\n",
      "Train: Epoch [15], Batch [402/938], Loss: 0.5307255983352661\n",
      "Train: Epoch [15], Batch [403/938], Loss: 0.8884022235870361\n",
      "Train: Epoch [15], Batch [404/938], Loss: 0.8642271757125854\n",
      "Train: Epoch [15], Batch [405/938], Loss: 0.7837255597114563\n",
      "Train: Epoch [15], Batch [406/938], Loss: 0.9543408155441284\n",
      "Train: Epoch [15], Batch [407/938], Loss: 0.4271971881389618\n",
      "Train: Epoch [15], Batch [408/938], Loss: 0.7319667935371399\n",
      "Train: Epoch [15], Batch [409/938], Loss: 0.714611828327179\n",
      "Train: Epoch [15], Batch [410/938], Loss: 0.8085195422172546\n",
      "Train: Epoch [15], Batch [411/938], Loss: 0.7128280997276306\n",
      "Train: Epoch [15], Batch [412/938], Loss: 1.0446840524673462\n",
      "Train: Epoch [15], Batch [413/938], Loss: 0.6935229897499084\n",
      "Train: Epoch [15], Batch [414/938], Loss: 0.6643526554107666\n",
      "Train: Epoch [15], Batch [415/938], Loss: 0.8228511810302734\n",
      "Train: Epoch [15], Batch [416/938], Loss: 0.6803377866744995\n",
      "Train: Epoch [15], Batch [417/938], Loss: 0.8623880743980408\n",
      "Train: Epoch [15], Batch [418/938], Loss: 0.9490748643875122\n",
      "Train: Epoch [15], Batch [419/938], Loss: 0.5014926195144653\n",
      "Train: Epoch [15], Batch [420/938], Loss: 0.6766324639320374\n",
      "Train: Epoch [15], Batch [421/938], Loss: 0.9360076785087585\n",
      "Train: Epoch [15], Batch [422/938], Loss: 0.6761165857315063\n",
      "Train: Epoch [15], Batch [423/938], Loss: 0.6495677828788757\n",
      "Train: Epoch [15], Batch [424/938], Loss: 0.6139615178108215\n",
      "Train: Epoch [15], Batch [425/938], Loss: 0.7554948925971985\n",
      "Train: Epoch [15], Batch [426/938], Loss: 0.7014620900154114\n",
      "Train: Epoch [15], Batch [427/938], Loss: 0.7847821712493896\n",
      "Train: Epoch [15], Batch [428/938], Loss: 0.7403899431228638\n",
      "Train: Epoch [15], Batch [429/938], Loss: 0.5072048306465149\n",
      "Train: Epoch [15], Batch [430/938], Loss: 0.6872792840003967\n",
      "Train: Epoch [15], Batch [431/938], Loss: 0.4271257519721985\n",
      "Train: Epoch [15], Batch [432/938], Loss: 0.8476200103759766\n",
      "Train: Epoch [15], Batch [433/938], Loss: 0.5917993783950806\n",
      "Train: Epoch [15], Batch [434/938], Loss: 0.8559547066688538\n",
      "Train: Epoch [15], Batch [435/938], Loss: 0.923172116279602\n",
      "Train: Epoch [15], Batch [436/938], Loss: 0.45289456844329834\n",
      "Train: Epoch [15], Batch [437/938], Loss: 0.565772533416748\n",
      "Train: Epoch [15], Batch [438/938], Loss: 0.4501297175884247\n",
      "Train: Epoch [15], Batch [439/938], Loss: 0.525946319103241\n",
      "Train: Epoch [15], Batch [440/938], Loss: 0.8684834837913513\n",
      "Train: Epoch [15], Batch [441/938], Loss: 0.6797851920127869\n",
      "Train: Epoch [15], Batch [442/938], Loss: 0.3776950538158417\n",
      "Train: Epoch [15], Batch [443/938], Loss: 0.5757909417152405\n",
      "Train: Epoch [15], Batch [444/938], Loss: 0.7282708287239075\n",
      "Train: Epoch [15], Batch [445/938], Loss: 0.6422851085662842\n",
      "Train: Epoch [15], Batch [446/938], Loss: 0.9726418852806091\n",
      "Train: Epoch [15], Batch [447/938], Loss: 0.7326139807701111\n",
      "Train: Epoch [15], Batch [448/938], Loss: 0.6187185049057007\n",
      "Train: Epoch [15], Batch [449/938], Loss: 0.5789298415184021\n",
      "Train: Epoch [15], Batch [450/938], Loss: 0.7993074655532837\n",
      "Train: Epoch [15], Batch [451/938], Loss: 0.8650546669960022\n",
      "Train: Epoch [15], Batch [452/938], Loss: 0.5902730226516724\n",
      "Train: Epoch [15], Batch [453/938], Loss: 0.692459762096405\n",
      "Train: Epoch [15], Batch [454/938], Loss: 0.6252574920654297\n",
      "Train: Epoch [15], Batch [455/938], Loss: 0.6809696555137634\n",
      "Train: Epoch [15], Batch [456/938], Loss: 0.6971793174743652\n",
      "Train: Epoch [15], Batch [457/938], Loss: 0.7009732127189636\n",
      "Train: Epoch [15], Batch [458/938], Loss: 0.7555086612701416\n",
      "Train: Epoch [15], Batch [459/938], Loss: 0.7431018352508545\n",
      "Train: Epoch [15], Batch [460/938], Loss: 0.6778477430343628\n",
      "Train: Epoch [15], Batch [461/938], Loss: 0.7773646116256714\n",
      "Train: Epoch [15], Batch [462/938], Loss: 0.4962051510810852\n",
      "Train: Epoch [15], Batch [463/938], Loss: 0.5955159068107605\n",
      "Train: Epoch [15], Batch [464/938], Loss: 0.7523072361946106\n",
      "Train: Epoch [15], Batch [465/938], Loss: 0.8673363924026489\n",
      "Train: Epoch [15], Batch [466/938], Loss: 0.6498864889144897\n",
      "Train: Epoch [15], Batch [467/938], Loss: 0.6986258625984192\n",
      "Train: Epoch [15], Batch [468/938], Loss: 0.5976201891899109\n",
      "Train: Epoch [15], Batch [469/938], Loss: 0.7287529706954956\n",
      "Train: Epoch [15], Batch [470/938], Loss: 0.7695772647857666\n",
      "Train: Epoch [15], Batch [471/938], Loss: 0.7585067749023438\n",
      "Train: Epoch [15], Batch [472/938], Loss: 0.6185665726661682\n",
      "Train: Epoch [15], Batch [473/938], Loss: 0.7353910207748413\n",
      "Train: Epoch [15], Batch [474/938], Loss: 0.6655927896499634\n",
      "Train: Epoch [15], Batch [475/938], Loss: 1.051378607749939\n",
      "Train: Epoch [15], Batch [476/938], Loss: 0.6725647449493408\n",
      "Train: Epoch [15], Batch [477/938], Loss: 0.6957123875617981\n",
      "Train: Epoch [15], Batch [478/938], Loss: 0.7486945986747742\n",
      "Train: Epoch [15], Batch [479/938], Loss: 0.6486491560935974\n",
      "Train: Epoch [15], Batch [480/938], Loss: 0.7716689705848694\n",
      "Train: Epoch [15], Batch [481/938], Loss: 0.6693055629730225\n",
      "Train: Epoch [15], Batch [482/938], Loss: 0.9591134786605835\n",
      "Train: Epoch [15], Batch [483/938], Loss: 0.7420456409454346\n",
      "Train: Epoch [15], Batch [484/938], Loss: 0.8380311727523804\n",
      "Train: Epoch [15], Batch [485/938], Loss: 0.9163262248039246\n",
      "Train: Epoch [15], Batch [486/938], Loss: 0.8306559324264526\n",
      "Train: Epoch [15], Batch [487/938], Loss: 1.0172419548034668\n",
      "Train: Epoch [15], Batch [488/938], Loss: 0.5954709053039551\n",
      "Train: Epoch [15], Batch [489/938], Loss: 0.8975214958190918\n",
      "Train: Epoch [15], Batch [490/938], Loss: 0.7931822538375854\n",
      "Train: Epoch [15], Batch [491/938], Loss: 0.5360344052314758\n",
      "Train: Epoch [15], Batch [492/938], Loss: 0.8040404319763184\n",
      "Train: Epoch [15], Batch [493/938], Loss: 0.7238355875015259\n",
      "Train: Epoch [15], Batch [494/938], Loss: 0.765170693397522\n",
      "Train: Epoch [15], Batch [495/938], Loss: 0.8311542272567749\n",
      "Train: Epoch [15], Batch [496/938], Loss: 0.7335938811302185\n",
      "Train: Epoch [15], Batch [497/938], Loss: 1.0395058393478394\n",
      "Train: Epoch [15], Batch [498/938], Loss: 0.6045891642570496\n",
      "Train: Epoch [15], Batch [499/938], Loss: 0.7993695735931396\n",
      "Train: Epoch [15], Batch [500/938], Loss: 0.7062236666679382\n",
      "Train: Epoch [15], Batch [501/938], Loss: 0.6406810283660889\n",
      "Train: Epoch [15], Batch [502/938], Loss: 0.8085647821426392\n",
      "Train: Epoch [15], Batch [503/938], Loss: 0.6756691932678223\n",
      "Train: Epoch [15], Batch [504/938], Loss: 0.5347505807876587\n",
      "Train: Epoch [15], Batch [505/938], Loss: 0.9295001029968262\n",
      "Train: Epoch [15], Batch [506/938], Loss: 0.8256620168685913\n",
      "Train: Epoch [15], Batch [507/938], Loss: 0.5840786695480347\n",
      "Train: Epoch [15], Batch [508/938], Loss: 0.6908230781555176\n",
      "Train: Epoch [15], Batch [509/938], Loss: 0.7010703086853027\n",
      "Train: Epoch [15], Batch [510/938], Loss: 0.795861005783081\n",
      "Train: Epoch [15], Batch [511/938], Loss: 0.6416732668876648\n",
      "Train: Epoch [15], Batch [512/938], Loss: 0.64900803565979\n",
      "Train: Epoch [15], Batch [513/938], Loss: 0.8166217803955078\n",
      "Train: Epoch [15], Batch [514/938], Loss: 0.6691577434539795\n",
      "Train: Epoch [15], Batch [515/938], Loss: 0.6953228712081909\n",
      "Train: Epoch [15], Batch [516/938], Loss: 0.8040030002593994\n",
      "Train: Epoch [15], Batch [517/938], Loss: 0.7889329791069031\n",
      "Train: Epoch [15], Batch [518/938], Loss: 0.6918175220489502\n",
      "Train: Epoch [15], Batch [519/938], Loss: 0.6360357999801636\n",
      "Train: Epoch [15], Batch [520/938], Loss: 0.767918586730957\n",
      "Train: Epoch [15], Batch [521/938], Loss: 0.9211499094963074\n",
      "Train: Epoch [15], Batch [522/938], Loss: 0.7249182462692261\n",
      "Train: Epoch [15], Batch [523/938], Loss: 0.6034792065620422\n",
      "Train: Epoch [15], Batch [524/938], Loss: 0.6741740703582764\n",
      "Train: Epoch [15], Batch [525/938], Loss: 0.6490932703018188\n",
      "Train: Epoch [15], Batch [526/938], Loss: 0.9217231273651123\n",
      "Train: Epoch [15], Batch [527/938], Loss: 0.626507043838501\n",
      "Train: Epoch [15], Batch [528/938], Loss: 0.5906205773353577\n",
      "Train: Epoch [15], Batch [529/938], Loss: 0.7022286653518677\n",
      "Train: Epoch [15], Batch [530/938], Loss: 0.890762209892273\n",
      "Train: Epoch [15], Batch [531/938], Loss: 0.8674623370170593\n",
      "Train: Epoch [15], Batch [532/938], Loss: 0.8250959515571594\n",
      "Train: Epoch [15], Batch [533/938], Loss: 0.6871612071990967\n",
      "Train: Epoch [15], Batch [534/938], Loss: 0.5683073997497559\n",
      "Train: Epoch [15], Batch [535/938], Loss: 0.6845506429672241\n",
      "Train: Epoch [15], Batch [536/938], Loss: 0.49110615253448486\n",
      "Train: Epoch [15], Batch [537/938], Loss: 0.4792514443397522\n",
      "Train: Epoch [15], Batch [538/938], Loss: 0.5550116896629333\n",
      "Train: Epoch [15], Batch [539/938], Loss: 0.8871798515319824\n",
      "Train: Epoch [15], Batch [540/938], Loss: 0.6832970976829529\n",
      "Train: Epoch [15], Batch [541/938], Loss: 0.6055490970611572\n",
      "Train: Epoch [15], Batch [542/938], Loss: 0.7198284864425659\n",
      "Train: Epoch [15], Batch [543/938], Loss: 0.6680355072021484\n",
      "Train: Epoch [15], Batch [544/938], Loss: 0.6257660388946533\n",
      "Train: Epoch [15], Batch [545/938], Loss: 0.7768876552581787\n",
      "Train: Epoch [15], Batch [546/938], Loss: 0.5570189952850342\n",
      "Train: Epoch [15], Batch [547/938], Loss: 0.50888592004776\n",
      "Train: Epoch [15], Batch [548/938], Loss: 0.8087146282196045\n",
      "Train: Epoch [15], Batch [549/938], Loss: 0.37386736273765564\n",
      "Train: Epoch [15], Batch [550/938], Loss: 0.6580427885055542\n",
      "Train: Epoch [15], Batch [551/938], Loss: 0.8388129472732544\n",
      "Train: Epoch [15], Batch [552/938], Loss: 0.5097708702087402\n",
      "Train: Epoch [15], Batch [553/938], Loss: 0.6404746174812317\n",
      "Train: Epoch [15], Batch [554/938], Loss: 0.8547102808952332\n",
      "Train: Epoch [15], Batch [555/938], Loss: 0.47147536277770996\n",
      "Train: Epoch [15], Batch [556/938], Loss: 0.986929714679718\n",
      "Train: Epoch [15], Batch [557/938], Loss: 0.5542649030685425\n",
      "Train: Epoch [15], Batch [558/938], Loss: 0.8056336045265198\n",
      "Train: Epoch [15], Batch [559/938], Loss: 0.5826608538627625\n",
      "Train: Epoch [15], Batch [560/938], Loss: 0.6054916381835938\n",
      "Train: Epoch [15], Batch [561/938], Loss: 0.5793541073799133\n",
      "Train: Epoch [15], Batch [562/938], Loss: 0.6475483179092407\n",
      "Train: Epoch [15], Batch [563/938], Loss: 0.6710605025291443\n",
      "Train: Epoch [15], Batch [564/938], Loss: 0.6409903764724731\n",
      "Train: Epoch [15], Batch [565/938], Loss: 0.8500301241874695\n",
      "Train: Epoch [15], Batch [566/938], Loss: 0.6229295134544373\n",
      "Train: Epoch [15], Batch [567/938], Loss: 1.0019901990890503\n",
      "Train: Epoch [15], Batch [568/938], Loss: 0.8382747173309326\n",
      "Train: Epoch [15], Batch [569/938], Loss: 0.7522060871124268\n",
      "Train: Epoch [15], Batch [570/938], Loss: 0.7106448411941528\n",
      "Train: Epoch [15], Batch [571/938], Loss: 0.8994196057319641\n",
      "Train: Epoch [15], Batch [572/938], Loss: 0.5365536212921143\n",
      "Train: Epoch [15], Batch [573/938], Loss: 0.7395406365394592\n",
      "Train: Epoch [15], Batch [574/938], Loss: 0.8102291226387024\n",
      "Train: Epoch [15], Batch [575/938], Loss: 0.687125027179718\n",
      "Train: Epoch [15], Batch [576/938], Loss: 0.9034397006034851\n",
      "Train: Epoch [15], Batch [577/938], Loss: 0.4471467435359955\n",
      "Train: Epoch [15], Batch [578/938], Loss: 0.9337756633758545\n",
      "Train: Epoch [15], Batch [579/938], Loss: 0.5988039374351501\n",
      "Train: Epoch [15], Batch [580/938], Loss: 0.7661803960800171\n",
      "Train: Epoch [15], Batch [581/938], Loss: 0.635138988494873\n",
      "Train: Epoch [15], Batch [582/938], Loss: 0.6662836074829102\n",
      "Train: Epoch [15], Batch [583/938], Loss: 0.5390872955322266\n",
      "Train: Epoch [15], Batch [584/938], Loss: 0.758014976978302\n",
      "Train: Epoch [15], Batch [585/938], Loss: 0.8525084257125854\n",
      "Train: Epoch [15], Batch [586/938], Loss: 0.5781956911087036\n",
      "Train: Epoch [15], Batch [587/938], Loss: 0.7472572922706604\n",
      "Train: Epoch [15], Batch [588/938], Loss: 0.7331569790840149\n",
      "Train: Epoch [15], Batch [589/938], Loss: 0.6317119002342224\n",
      "Train: Epoch [15], Batch [590/938], Loss: 0.506450355052948\n",
      "Train: Epoch [15], Batch [591/938], Loss: 0.6648973226547241\n",
      "Train: Epoch [15], Batch [592/938], Loss: 0.7009944915771484\n",
      "Train: Epoch [15], Batch [593/938], Loss: 0.5280486941337585\n",
      "Train: Epoch [15], Batch [594/938], Loss: 0.5924866795539856\n",
      "Train: Epoch [15], Batch [595/938], Loss: 0.8150023221969604\n",
      "Train: Epoch [15], Batch [596/938], Loss: 0.5354689359664917\n",
      "Train: Epoch [15], Batch [597/938], Loss: 0.652821958065033\n",
      "Train: Epoch [15], Batch [598/938], Loss: 0.7906091809272766\n",
      "Train: Epoch [15], Batch [599/938], Loss: 0.6611776947975159\n",
      "Train: Epoch [15], Batch [600/938], Loss: 0.8751494884490967\n",
      "Train: Epoch [15], Batch [601/938], Loss: 0.611746609210968\n",
      "Train: Epoch [15], Batch [602/938], Loss: 0.7479195594787598\n",
      "Train: Epoch [15], Batch [603/938], Loss: 0.9812796115875244\n",
      "Train: Epoch [15], Batch [604/938], Loss: 0.4587673544883728\n",
      "Train: Epoch [15], Batch [605/938], Loss: 0.7528419494628906\n",
      "Train: Epoch [15], Batch [606/938], Loss: 0.5527180433273315\n",
      "Train: Epoch [15], Batch [607/938], Loss: 0.8465349674224854\n",
      "Train: Epoch [15], Batch [608/938], Loss: 0.6406576037406921\n",
      "Train: Epoch [15], Batch [609/938], Loss: 0.551780641078949\n",
      "Train: Epoch [15], Batch [610/938], Loss: 1.000472903251648\n",
      "Train: Epoch [15], Batch [611/938], Loss: 0.7301737070083618\n",
      "Train: Epoch [15], Batch [612/938], Loss: 0.8590150475502014\n",
      "Train: Epoch [15], Batch [613/938], Loss: 0.5387014150619507\n",
      "Train: Epoch [15], Batch [614/938], Loss: 0.7127029895782471\n",
      "Train: Epoch [15], Batch [615/938], Loss: 0.9817492961883545\n",
      "Train: Epoch [15], Batch [616/938], Loss: 0.7264982461929321\n",
      "Train: Epoch [15], Batch [617/938], Loss: 0.6626131534576416\n",
      "Train: Epoch [15], Batch [618/938], Loss: 0.718590497970581\n",
      "Train: Epoch [15], Batch [619/938], Loss: 0.781885027885437\n",
      "Train: Epoch [15], Batch [620/938], Loss: 0.8712355494499207\n",
      "Train: Epoch [15], Batch [621/938], Loss: 0.6125501394271851\n",
      "Train: Epoch [15], Batch [622/938], Loss: 0.39448630809783936\n",
      "Train: Epoch [15], Batch [623/938], Loss: 0.6015796661376953\n",
      "Train: Epoch [15], Batch [624/938], Loss: 0.6600440740585327\n",
      "Train: Epoch [15], Batch [625/938], Loss: 0.6100674271583557\n",
      "Train: Epoch [15], Batch [626/938], Loss: 0.5524290204048157\n",
      "Train: Epoch [15], Batch [627/938], Loss: 0.636442244052887\n",
      "Train: Epoch [15], Batch [628/938], Loss: 0.7983296513557434\n",
      "Train: Epoch [15], Batch [629/938], Loss: 0.7695146203041077\n",
      "Train: Epoch [15], Batch [630/938], Loss: 0.7137617468833923\n",
      "Train: Epoch [15], Batch [631/938], Loss: 0.6615562438964844\n",
      "Train: Epoch [15], Batch [632/938], Loss: 0.7639505863189697\n",
      "Train: Epoch [15], Batch [633/938], Loss: 0.9292546510696411\n",
      "Train: Epoch [15], Batch [634/938], Loss: 0.9656223654747009\n",
      "Train: Epoch [15], Batch [635/938], Loss: 0.7004484534263611\n",
      "Train: Epoch [15], Batch [636/938], Loss: 0.6891911625862122\n",
      "Train: Epoch [15], Batch [637/938], Loss: 0.7157419919967651\n",
      "Train: Epoch [15], Batch [638/938], Loss: 0.7592092156410217\n",
      "Train: Epoch [15], Batch [639/938], Loss: 0.7471355199813843\n",
      "Train: Epoch [15], Batch [640/938], Loss: 0.9531260132789612\n",
      "Train: Epoch [15], Batch [641/938], Loss: 0.6123901009559631\n",
      "Train: Epoch [15], Batch [642/938], Loss: 0.5920631289482117\n",
      "Train: Epoch [15], Batch [643/938], Loss: 0.9423098564147949\n",
      "Train: Epoch [15], Batch [644/938], Loss: 0.48133477568626404\n",
      "Train: Epoch [15], Batch [645/938], Loss: 0.8558703064918518\n",
      "Train: Epoch [15], Batch [646/938], Loss: 0.6730770468711853\n",
      "Train: Epoch [15], Batch [647/938], Loss: 0.6349259614944458\n",
      "Train: Epoch [15], Batch [648/938], Loss: 0.6966612935066223\n",
      "Train: Epoch [15], Batch [649/938], Loss: 0.859874427318573\n",
      "Train: Epoch [15], Batch [650/938], Loss: 0.7929350733757019\n",
      "Train: Epoch [15], Batch [651/938], Loss: 0.6831483840942383\n",
      "Train: Epoch [15], Batch [652/938], Loss: 0.6483469605445862\n",
      "Train: Epoch [15], Batch [653/938], Loss: 0.6639664769172668\n",
      "Train: Epoch [15], Batch [654/938], Loss: 0.7479896545410156\n",
      "Train: Epoch [15], Batch [655/938], Loss: 0.6819165349006653\n",
      "Train: Epoch [15], Batch [656/938], Loss: 0.8951232433319092\n",
      "Train: Epoch [15], Batch [657/938], Loss: 0.5236207842826843\n",
      "Train: Epoch [15], Batch [658/938], Loss: 0.8356001377105713\n",
      "Train: Epoch [15], Batch [659/938], Loss: 0.4643056392669678\n",
      "Train: Epoch [15], Batch [660/938], Loss: 0.6415989995002747\n",
      "Train: Epoch [15], Batch [661/938], Loss: 0.7346874475479126\n",
      "Train: Epoch [15], Batch [662/938], Loss: 0.9353983402252197\n",
      "Train: Epoch [15], Batch [663/938], Loss: 0.7074893116950989\n",
      "Train: Epoch [15], Batch [664/938], Loss: 0.8691560626029968\n",
      "Train: Epoch [15], Batch [665/938], Loss: 0.4055141806602478\n",
      "Train: Epoch [15], Batch [666/938], Loss: 0.598047137260437\n",
      "Train: Epoch [15], Batch [667/938], Loss: 0.5842487812042236\n",
      "Train: Epoch [15], Batch [668/938], Loss: 0.7781827449798584\n",
      "Train: Epoch [15], Batch [669/938], Loss: 0.6796112060546875\n",
      "Train: Epoch [15], Batch [670/938], Loss: 0.7552170157432556\n",
      "Train: Epoch [15], Batch [671/938], Loss: 0.994951069355011\n",
      "Train: Epoch [15], Batch [672/938], Loss: 0.5260418653488159\n",
      "Train: Epoch [15], Batch [673/938], Loss: 0.8056119084358215\n",
      "Train: Epoch [15], Batch [674/938], Loss: 0.6631642580032349\n",
      "Train: Epoch [15], Batch [675/938], Loss: 0.8562173247337341\n",
      "Train: Epoch [15], Batch [676/938], Loss: 0.668211042881012\n",
      "Train: Epoch [15], Batch [677/938], Loss: 0.6327797770500183\n",
      "Train: Epoch [15], Batch [678/938], Loss: 0.5998501777648926\n",
      "Train: Epoch [15], Batch [679/938], Loss: 0.6569688320159912\n",
      "Train: Epoch [15], Batch [680/938], Loss: 0.5858006477355957\n",
      "Train: Epoch [15], Batch [681/938], Loss: 0.5734014511108398\n",
      "Train: Epoch [15], Batch [682/938], Loss: 0.8272778391838074\n",
      "Train: Epoch [15], Batch [683/938], Loss: 0.8771041631698608\n",
      "Train: Epoch [15], Batch [684/938], Loss: 0.5133097767829895\n",
      "Train: Epoch [15], Batch [685/938], Loss: 0.8572162389755249\n",
      "Train: Epoch [15], Batch [686/938], Loss: 0.6968396306037903\n",
      "Train: Epoch [15], Batch [687/938], Loss: 0.6155223846435547\n",
      "Train: Epoch [15], Batch [688/938], Loss: 0.4720461070537567\n",
      "Train: Epoch [15], Batch [689/938], Loss: 0.628364086151123\n",
      "Train: Epoch [15], Batch [690/938], Loss: 0.7869627475738525\n",
      "Train: Epoch [15], Batch [691/938], Loss: 0.7464990615844727\n",
      "Train: Epoch [15], Batch [692/938], Loss: 0.4793950617313385\n",
      "Train: Epoch [15], Batch [693/938], Loss: 0.641715407371521\n",
      "Train: Epoch [15], Batch [694/938], Loss: 0.5880277752876282\n",
      "Train: Epoch [15], Batch [695/938], Loss: 0.8184549808502197\n",
      "Train: Epoch [15], Batch [696/938], Loss: 0.5500615835189819\n",
      "Train: Epoch [15], Batch [697/938], Loss: 0.8300034403800964\n",
      "Train: Epoch [15], Batch [698/938], Loss: 0.5698582530021667\n",
      "Train: Epoch [15], Batch [699/938], Loss: 0.8894087672233582\n",
      "Train: Epoch [15], Batch [700/938], Loss: 0.7167873382568359\n",
      "Train: Epoch [15], Batch [701/938], Loss: 0.6886348724365234\n",
      "Train: Epoch [15], Batch [702/938], Loss: 0.8092630505561829\n",
      "Train: Epoch [15], Batch [703/938], Loss: 0.5846622586250305\n",
      "Train: Epoch [15], Batch [704/938], Loss: 0.8575959205627441\n",
      "Train: Epoch [15], Batch [705/938], Loss: 0.7177168130874634\n",
      "Train: Epoch [15], Batch [706/938], Loss: 0.6998623609542847\n",
      "Train: Epoch [15], Batch [707/938], Loss: 0.5799900889396667\n",
      "Train: Epoch [15], Batch [708/938], Loss: 1.332147479057312\n",
      "Train: Epoch [15], Batch [709/938], Loss: 1.0315139293670654\n",
      "Train: Epoch [15], Batch [710/938], Loss: 0.6634241938591003\n",
      "Train: Epoch [15], Batch [711/938], Loss: 0.7552779912948608\n",
      "Train: Epoch [15], Batch [712/938], Loss: 0.5943590402603149\n",
      "Train: Epoch [15], Batch [713/938], Loss: 0.4116899371147156\n",
      "Train: Epoch [15], Batch [714/938], Loss: 0.5845193266868591\n",
      "Train: Epoch [15], Batch [715/938], Loss: 0.6896482110023499\n",
      "Train: Epoch [15], Batch [716/938], Loss: 0.5589178204536438\n",
      "Train: Epoch [15], Batch [717/938], Loss: 0.5478547215461731\n",
      "Train: Epoch [15], Batch [718/938], Loss: 0.6693541407585144\n",
      "Train: Epoch [15], Batch [719/938], Loss: 0.8204386234283447\n",
      "Train: Epoch [15], Batch [720/938], Loss: 0.780770480632782\n",
      "Train: Epoch [15], Batch [721/938], Loss: 0.4647082984447479\n",
      "Train: Epoch [15], Batch [722/938], Loss: 0.7793363332748413\n",
      "Train: Epoch [15], Batch [723/938], Loss: 0.6187652945518494\n",
      "Train: Epoch [15], Batch [724/938], Loss: 0.5756402611732483\n",
      "Train: Epoch [15], Batch [725/938], Loss: 0.990189790725708\n",
      "Train: Epoch [15], Batch [726/938], Loss: 0.6749176979064941\n",
      "Train: Epoch [15], Batch [727/938], Loss: 0.7893352508544922\n",
      "Train: Epoch [15], Batch [728/938], Loss: 0.5823352336883545\n",
      "Train: Epoch [15], Batch [729/938], Loss: 0.5075933337211609\n",
      "Train: Epoch [15], Batch [730/938], Loss: 0.7337925434112549\n",
      "Train: Epoch [15], Batch [731/938], Loss: 0.535514235496521\n",
      "Train: Epoch [15], Batch [732/938], Loss: 0.546046793460846\n",
      "Train: Epoch [15], Batch [733/938], Loss: 0.6289540529251099\n",
      "Train: Epoch [15], Batch [734/938], Loss: 0.6064480543136597\n",
      "Train: Epoch [15], Batch [735/938], Loss: 0.5370106101036072\n",
      "Train: Epoch [15], Batch [736/938], Loss: 0.42513594031333923\n",
      "Train: Epoch [15], Batch [737/938], Loss: 0.6884948015213013\n",
      "Train: Epoch [15], Batch [738/938], Loss: 1.0041040182113647\n",
      "Train: Epoch [15], Batch [739/938], Loss: 0.9242001175880432\n",
      "Train: Epoch [15], Batch [740/938], Loss: 0.6489043235778809\n",
      "Train: Epoch [15], Batch [741/938], Loss: 0.4977658987045288\n",
      "Train: Epoch [15], Batch [742/938], Loss: 0.6503801345825195\n",
      "Train: Epoch [15], Batch [743/938], Loss: 0.9566870331764221\n",
      "Train: Epoch [15], Batch [744/938], Loss: 0.6779884099960327\n",
      "Train: Epoch [15], Batch [745/938], Loss: 0.6781184673309326\n",
      "Train: Epoch [15], Batch [746/938], Loss: 0.6172300577163696\n",
      "Train: Epoch [15], Batch [747/938], Loss: 0.7018322348594666\n",
      "Train: Epoch [15], Batch [748/938], Loss: 0.7327316403388977\n",
      "Train: Epoch [15], Batch [749/938], Loss: 0.9047922492027283\n",
      "Train: Epoch [15], Batch [750/938], Loss: 0.7122336626052856\n",
      "Train: Epoch [15], Batch [751/938], Loss: 0.571422278881073\n",
      "Train: Epoch [15], Batch [752/938], Loss: 0.8064920902252197\n",
      "Train: Epoch [15], Batch [753/938], Loss: 0.7381397485733032\n",
      "Train: Epoch [15], Batch [754/938], Loss: 0.8738635182380676\n",
      "Train: Epoch [15], Batch [755/938], Loss: 0.7155200839042664\n",
      "Train: Epoch [15], Batch [756/938], Loss: 0.873923659324646\n",
      "Train: Epoch [15], Batch [757/938], Loss: 0.6844543814659119\n",
      "Train: Epoch [15], Batch [758/938], Loss: 0.7408074140548706\n",
      "Train: Epoch [15], Batch [759/938], Loss: 0.8618486523628235\n",
      "Train: Epoch [15], Batch [760/938], Loss: 0.8902347087860107\n",
      "Train: Epoch [15], Batch [761/938], Loss: 0.4876649081707001\n",
      "Train: Epoch [15], Batch [762/938], Loss: 0.48377588391304016\n",
      "Train: Epoch [15], Batch [763/938], Loss: 0.49008509516716003\n",
      "Train: Epoch [15], Batch [764/938], Loss: 0.5195291042327881\n",
      "Train: Epoch [15], Batch [765/938], Loss: 0.6009323000907898\n",
      "Train: Epoch [15], Batch [766/938], Loss: 0.8384381532669067\n",
      "Train: Epoch [15], Batch [767/938], Loss: 0.8770036697387695\n",
      "Train: Epoch [15], Batch [768/938], Loss: 1.1924936771392822\n",
      "Train: Epoch [15], Batch [769/938], Loss: 1.0212103128433228\n",
      "Train: Epoch [15], Batch [770/938], Loss: 0.8583846092224121\n",
      "Train: Epoch [15], Batch [771/938], Loss: 0.9180500507354736\n",
      "Train: Epoch [15], Batch [772/938], Loss: 0.923584520816803\n",
      "Train: Epoch [15], Batch [773/938], Loss: 0.5856350660324097\n",
      "Train: Epoch [15], Batch [774/938], Loss: 0.7622044086456299\n",
      "Train: Epoch [15], Batch [775/938], Loss: 0.5575359463691711\n",
      "Train: Epoch [15], Batch [776/938], Loss: 0.6024662852287292\n",
      "Train: Epoch [15], Batch [777/938], Loss: 0.7411478757858276\n",
      "Train: Epoch [15], Batch [778/938], Loss: 0.755012035369873\n",
      "Train: Epoch [15], Batch [779/938], Loss: 0.7754191756248474\n",
      "Train: Epoch [15], Batch [780/938], Loss: 0.6460299491882324\n",
      "Train: Epoch [15], Batch [781/938], Loss: 0.6266285181045532\n",
      "Train: Epoch [15], Batch [782/938], Loss: 0.7045224905014038\n",
      "Train: Epoch [15], Batch [783/938], Loss: 0.6691334247589111\n",
      "Train: Epoch [15], Batch [784/938], Loss: 0.8253371715545654\n",
      "Train: Epoch [15], Batch [785/938], Loss: 0.616024911403656\n",
      "Train: Epoch [15], Batch [786/938], Loss: 0.5713871717453003\n",
      "Train: Epoch [15], Batch [787/938], Loss: 0.6015623807907104\n",
      "Train: Epoch [15], Batch [788/938], Loss: 0.8046106100082397\n",
      "Train: Epoch [15], Batch [789/938], Loss: 0.49873942136764526\n",
      "Train: Epoch [15], Batch [790/938], Loss: 0.6362171769142151\n",
      "Train: Epoch [15], Batch [791/938], Loss: 0.8847509026527405\n",
      "Train: Epoch [15], Batch [792/938], Loss: 0.6985335350036621\n",
      "Train: Epoch [15], Batch [793/938], Loss: 0.8602219820022583\n",
      "Train: Epoch [15], Batch [794/938], Loss: 0.6880537271499634\n",
      "Train: Epoch [15], Batch [795/938], Loss: 0.590928316116333\n",
      "Train: Epoch [15], Batch [796/938], Loss: 1.1671479940414429\n",
      "Train: Epoch [15], Batch [797/938], Loss: 0.8358005285263062\n",
      "Train: Epoch [15], Batch [798/938], Loss: 0.6268414855003357\n",
      "Train: Epoch [15], Batch [799/938], Loss: 0.617135763168335\n",
      "Train: Epoch [15], Batch [800/938], Loss: 0.42527613043785095\n",
      "Train: Epoch [15], Batch [801/938], Loss: 0.5562421083450317\n",
      "Train: Epoch [15], Batch [802/938], Loss: 0.6977462768554688\n",
      "Train: Epoch [15], Batch [803/938], Loss: 0.6957986354827881\n",
      "Train: Epoch [15], Batch [804/938], Loss: 0.7974068522453308\n",
      "Train: Epoch [15], Batch [805/938], Loss: 0.6791466474533081\n",
      "Train: Epoch [15], Batch [806/938], Loss: 0.7132209539413452\n",
      "Train: Epoch [15], Batch [807/938], Loss: 0.5073570013046265\n",
      "Train: Epoch [15], Batch [808/938], Loss: 0.7531203031539917\n",
      "Train: Epoch [15], Batch [809/938], Loss: 0.5204904675483704\n",
      "Train: Epoch [15], Batch [810/938], Loss: 1.053160309791565\n",
      "Train: Epoch [15], Batch [811/938], Loss: 0.6627396941184998\n",
      "Train: Epoch [15], Batch [812/938], Loss: 0.390394389629364\n",
      "Train: Epoch [15], Batch [813/938], Loss: 0.7671715021133423\n",
      "Train: Epoch [15], Batch [814/938], Loss: 0.8560957312583923\n",
      "Train: Epoch [15], Batch [815/938], Loss: 0.872417688369751\n",
      "Train: Epoch [15], Batch [816/938], Loss: 0.74021315574646\n",
      "Train: Epoch [15], Batch [817/938], Loss: 0.5960862040519714\n",
      "Train: Epoch [15], Batch [818/938], Loss: 0.3981820344924927\n",
      "Train: Epoch [15], Batch [819/938], Loss: 0.6174754500389099\n",
      "Train: Epoch [15], Batch [820/938], Loss: 0.4871482253074646\n",
      "Train: Epoch [15], Batch [821/938], Loss: 0.6812705993652344\n",
      "Train: Epoch [15], Batch [822/938], Loss: 0.8014402985572815\n",
      "Train: Epoch [15], Batch [823/938], Loss: 0.6263273358345032\n",
      "Train: Epoch [15], Batch [824/938], Loss: 0.6007645726203918\n",
      "Train: Epoch [15], Batch [825/938], Loss: 0.6180393099784851\n",
      "Train: Epoch [15], Batch [826/938], Loss: 0.8362761735916138\n",
      "Train: Epoch [15], Batch [827/938], Loss: 0.47495514154434204\n",
      "Train: Epoch [15], Batch [828/938], Loss: 0.8900905251502991\n",
      "Train: Epoch [15], Batch [829/938], Loss: 0.6899499893188477\n",
      "Train: Epoch [15], Batch [830/938], Loss: 0.8281970024108887\n",
      "Train: Epoch [15], Batch [831/938], Loss: 0.7011734843254089\n",
      "Train: Epoch [15], Batch [832/938], Loss: 0.7185533046722412\n",
      "Train: Epoch [15], Batch [833/938], Loss: 0.870672881603241\n",
      "Train: Epoch [15], Batch [834/938], Loss: 0.9067279100418091\n",
      "Train: Epoch [15], Batch [835/938], Loss: 0.4877685606479645\n",
      "Train: Epoch [15], Batch [836/938], Loss: 0.8578453063964844\n",
      "Train: Epoch [15], Batch [837/938], Loss: 0.7740922570228577\n",
      "Train: Epoch [15], Batch [838/938], Loss: 0.6980252861976624\n",
      "Train: Epoch [15], Batch [839/938], Loss: 0.5228893160820007\n",
      "Train: Epoch [15], Batch [840/938], Loss: 0.7409399747848511\n",
      "Train: Epoch [15], Batch [841/938], Loss: 0.5855414271354675\n",
      "Train: Epoch [15], Batch [842/938], Loss: 0.7284281849861145\n",
      "Train: Epoch [15], Batch [843/938], Loss: 0.8293159008026123\n",
      "Train: Epoch [15], Batch [844/938], Loss: 0.6285852789878845\n",
      "Train: Epoch [15], Batch [845/938], Loss: 0.6608856916427612\n",
      "Train: Epoch [15], Batch [846/938], Loss: 0.6408548951148987\n",
      "Train: Epoch [15], Batch [847/938], Loss: 0.7487587928771973\n",
      "Train: Epoch [15], Batch [848/938], Loss: 0.601858377456665\n",
      "Train: Epoch [15], Batch [849/938], Loss: 0.7435475587844849\n",
      "Train: Epoch [15], Batch [850/938], Loss: 0.604482114315033\n",
      "Train: Epoch [15], Batch [851/938], Loss: 0.6141039133071899\n",
      "Train: Epoch [15], Batch [852/938], Loss: 0.7656958103179932\n",
      "Train: Epoch [15], Batch [853/938], Loss: 0.5393062233924866\n",
      "Train: Epoch [15], Batch [854/938], Loss: 0.7036681175231934\n",
      "Train: Epoch [15], Batch [855/938], Loss: 0.8731675148010254\n",
      "Train: Epoch [15], Batch [856/938], Loss: 0.5089966058731079\n",
      "Train: Epoch [15], Batch [857/938], Loss: 0.7949252724647522\n",
      "Train: Epoch [15], Batch [858/938], Loss: 0.7666553854942322\n",
      "Train: Epoch [15], Batch [859/938], Loss: 0.7182285785675049\n",
      "Train: Epoch [15], Batch [860/938], Loss: 0.8272176384925842\n",
      "Train: Epoch [15], Batch [861/938], Loss: 0.47405606508255005\n",
      "Train: Epoch [15], Batch [862/938], Loss: 1.0260515213012695\n",
      "Train: Epoch [15], Batch [863/938], Loss: 0.5263323783874512\n",
      "Train: Epoch [15], Batch [864/938], Loss: 0.5615735650062561\n",
      "Train: Epoch [15], Batch [865/938], Loss: 0.5082263350486755\n",
      "Train: Epoch [15], Batch [866/938], Loss: 0.7660692930221558\n",
      "Train: Epoch [15], Batch [867/938], Loss: 0.44194352626800537\n",
      "Train: Epoch [15], Batch [868/938], Loss: 0.48194757103919983\n",
      "Train: Epoch [15], Batch [869/938], Loss: 0.6747837662696838\n",
      "Train: Epoch [15], Batch [870/938], Loss: 0.6477054357528687\n",
      "Train: Epoch [15], Batch [871/938], Loss: 1.1060608625411987\n",
      "Train: Epoch [15], Batch [872/938], Loss: 0.6708580255508423\n",
      "Train: Epoch [15], Batch [873/938], Loss: 0.8249327540397644\n",
      "Train: Epoch [15], Batch [874/938], Loss: 0.7520260214805603\n",
      "Train: Epoch [15], Batch [875/938], Loss: 0.5910328030586243\n",
      "Train: Epoch [15], Batch [876/938], Loss: 0.732717752456665\n",
      "Train: Epoch [15], Batch [877/938], Loss: 0.6666248440742493\n",
      "Train: Epoch [15], Batch [878/938], Loss: 0.7645673155784607\n",
      "Train: Epoch [15], Batch [879/938], Loss: 0.7521469593048096\n",
      "Train: Epoch [15], Batch [880/938], Loss: 0.7338917255401611\n",
      "Train: Epoch [15], Batch [881/938], Loss: 0.7460991144180298\n",
      "Train: Epoch [15], Batch [882/938], Loss: 0.7228518724441528\n",
      "Train: Epoch [15], Batch [883/938], Loss: 0.7865148782730103\n",
      "Train: Epoch [15], Batch [884/938], Loss: 0.8320841789245605\n",
      "Train: Epoch [15], Batch [885/938], Loss: 0.8599624633789062\n",
      "Train: Epoch [15], Batch [886/938], Loss: 0.9850946068763733\n",
      "Train: Epoch [15], Batch [887/938], Loss: 0.6795565485954285\n",
      "Train: Epoch [15], Batch [888/938], Loss: 0.5606245994567871\n",
      "Train: Epoch [15], Batch [889/938], Loss: 0.7793774008750916\n",
      "Train: Epoch [15], Batch [890/938], Loss: 0.8053473830223083\n",
      "Train: Epoch [15], Batch [891/938], Loss: 0.5626958012580872\n",
      "Train: Epoch [15], Batch [892/938], Loss: 0.5972695350646973\n",
      "Train: Epoch [15], Batch [893/938], Loss: 0.6215804219245911\n",
      "Train: Epoch [15], Batch [894/938], Loss: 0.6223376989364624\n",
      "Train: Epoch [15], Batch [895/938], Loss: 0.7870082855224609\n",
      "Train: Epoch [15], Batch [896/938], Loss: 0.7557671070098877\n",
      "Train: Epoch [15], Batch [897/938], Loss: 0.7617926001548767\n",
      "Train: Epoch [15], Batch [898/938], Loss: 0.688982367515564\n",
      "Train: Epoch [15], Batch [899/938], Loss: 0.5856978893280029\n",
      "Train: Epoch [15], Batch [900/938], Loss: 0.5958439111709595\n",
      "Train: Epoch [15], Batch [901/938], Loss: 0.8822901248931885\n",
      "Train: Epoch [15], Batch [902/938], Loss: 0.760269284248352\n",
      "Train: Epoch [15], Batch [903/938], Loss: 0.8096243739128113\n",
      "Train: Epoch [15], Batch [904/938], Loss: 0.8254122138023376\n",
      "Train: Epoch [15], Batch [905/938], Loss: 0.8226798176765442\n",
      "Train: Epoch [15], Batch [906/938], Loss: 0.5835795998573303\n",
      "Train: Epoch [15], Batch [907/938], Loss: 0.8842794299125671\n",
      "Train: Epoch [15], Batch [908/938], Loss: 0.6118195652961731\n",
      "Train: Epoch [15], Batch [909/938], Loss: 0.6267904043197632\n",
      "Train: Epoch [15], Batch [910/938], Loss: 0.6378991603851318\n",
      "Train: Epoch [15], Batch [911/938], Loss: 0.8171985149383545\n",
      "Train: Epoch [15], Batch [912/938], Loss: 0.7902252674102783\n",
      "Train: Epoch [15], Batch [913/938], Loss: 0.7913459539413452\n",
      "Train: Epoch [15], Batch [914/938], Loss: 0.7534612417221069\n",
      "Train: Epoch [15], Batch [915/938], Loss: 0.7379454374313354\n",
      "Train: Epoch [15], Batch [916/938], Loss: 0.8605208992958069\n",
      "Train: Epoch [15], Batch [917/938], Loss: 0.6864926218986511\n",
      "Train: Epoch [15], Batch [918/938], Loss: 0.7143038511276245\n",
      "Train: Epoch [15], Batch [919/938], Loss: 0.7157068848609924\n",
      "Train: Epoch [15], Batch [920/938], Loss: 0.528413712978363\n",
      "Train: Epoch [15], Batch [921/938], Loss: 0.7751139998435974\n",
      "Train: Epoch [15], Batch [922/938], Loss: 0.6551055312156677\n",
      "Train: Epoch [15], Batch [923/938], Loss: 0.3931841254234314\n",
      "Train: Epoch [15], Batch [924/938], Loss: 0.619960367679596\n",
      "Train: Epoch [15], Batch [925/938], Loss: 0.5365639925003052\n",
      "Train: Epoch [15], Batch [926/938], Loss: 0.5266206860542297\n",
      "Train: Epoch [15], Batch [927/938], Loss: 0.8403711318969727\n",
      "Train: Epoch [15], Batch [928/938], Loss: 0.7851486206054688\n",
      "Train: Epoch [15], Batch [929/938], Loss: 0.7739061117172241\n",
      "Train: Epoch [15], Batch [930/938], Loss: 0.6826177835464478\n",
      "Train: Epoch [15], Batch [931/938], Loss: 0.7250553369522095\n",
      "Train: Epoch [15], Batch [932/938], Loss: 0.500095546245575\n",
      "Train: Epoch [15], Batch [933/938], Loss: 0.7320755124092102\n",
      "Train: Epoch [15], Batch [934/938], Loss: 0.8258422017097473\n",
      "Train: Epoch [15], Batch [935/938], Loss: 0.7682724595069885\n",
      "Train: Epoch [15], Batch [936/938], Loss: 0.6408966779708862\n",
      "Train: Epoch [15], Batch [937/938], Loss: 0.7902732491493225\n",
      "Train: Epoch [15], Batch [938/938], Loss: 0.789524495601654\n",
      "Accuracy of train set: 0.78755\n",
      "Validation: Epoch [15], Batch [1/938], Loss: 0.7179737091064453\n",
      "Validation: Epoch [15], Batch [2/938], Loss: 0.8535323739051819\n",
      "Validation: Epoch [15], Batch [3/938], Loss: 0.46992456912994385\n",
      "Validation: Epoch [15], Batch [4/938], Loss: 0.624814510345459\n",
      "Validation: Epoch [15], Batch [5/938], Loss: 0.7196953296661377\n",
      "Validation: Epoch [15], Batch [6/938], Loss: 0.8581526279449463\n",
      "Validation: Epoch [15], Batch [7/938], Loss: 0.8625227212905884\n",
      "Validation: Epoch [15], Batch [8/938], Loss: 0.5946641564369202\n",
      "Validation: Epoch [15], Batch [9/938], Loss: 0.635461688041687\n",
      "Validation: Epoch [15], Batch [10/938], Loss: 0.689433217048645\n",
      "Validation: Epoch [15], Batch [11/938], Loss: 0.7021958827972412\n",
      "Validation: Epoch [15], Batch [12/938], Loss: 0.7666828632354736\n",
      "Validation: Epoch [15], Batch [13/938], Loss: 0.6816648244857788\n",
      "Validation: Epoch [15], Batch [14/938], Loss: 0.4922065734863281\n",
      "Validation: Epoch [15], Batch [15/938], Loss: 0.6122250556945801\n",
      "Validation: Epoch [15], Batch [16/938], Loss: 0.6547683477401733\n",
      "Validation: Epoch [15], Batch [17/938], Loss: 0.7498502135276794\n",
      "Validation: Epoch [15], Batch [18/938], Loss: 0.6403997540473938\n",
      "Validation: Epoch [15], Batch [19/938], Loss: 0.8391759395599365\n",
      "Validation: Epoch [15], Batch [20/938], Loss: 0.9166854619979858\n",
      "Validation: Epoch [15], Batch [21/938], Loss: 0.698645830154419\n",
      "Validation: Epoch [15], Batch [22/938], Loss: 0.6138028502464294\n",
      "Validation: Epoch [15], Batch [23/938], Loss: 0.5649221539497375\n",
      "Validation: Epoch [15], Batch [24/938], Loss: 0.5472193360328674\n",
      "Validation: Epoch [15], Batch [25/938], Loss: 0.7285094857215881\n",
      "Validation: Epoch [15], Batch [26/938], Loss: 0.7953634262084961\n",
      "Validation: Epoch [15], Batch [27/938], Loss: 0.6443292498588562\n",
      "Validation: Epoch [15], Batch [28/938], Loss: 0.8881276845932007\n",
      "Validation: Epoch [15], Batch [29/938], Loss: 0.6466310024261475\n",
      "Validation: Epoch [15], Batch [30/938], Loss: 0.6139435172080994\n",
      "Validation: Epoch [15], Batch [31/938], Loss: 0.7886519432067871\n",
      "Validation: Epoch [15], Batch [32/938], Loss: 0.5512137413024902\n",
      "Validation: Epoch [15], Batch [33/938], Loss: 0.72133469581604\n",
      "Validation: Epoch [15], Batch [34/938], Loss: 0.44904112815856934\n",
      "Validation: Epoch [15], Batch [35/938], Loss: 0.6895368099212646\n",
      "Validation: Epoch [15], Batch [36/938], Loss: 0.6974091529846191\n",
      "Validation: Epoch [15], Batch [37/938], Loss: 0.8725760579109192\n",
      "Validation: Epoch [15], Batch [38/938], Loss: 0.8771654367446899\n",
      "Validation: Epoch [15], Batch [39/938], Loss: 0.7125996947288513\n",
      "Validation: Epoch [15], Batch [40/938], Loss: 0.7405315637588501\n",
      "Validation: Epoch [15], Batch [41/938], Loss: 0.6917662620544434\n",
      "Validation: Epoch [15], Batch [42/938], Loss: 0.7379989624023438\n",
      "Validation: Epoch [15], Batch [43/938], Loss: 0.8465104103088379\n",
      "Validation: Epoch [15], Batch [44/938], Loss: 0.8457707166671753\n",
      "Validation: Epoch [15], Batch [45/938], Loss: 0.6712129712104797\n",
      "Validation: Epoch [15], Batch [46/938], Loss: 0.5146040320396423\n",
      "Validation: Epoch [15], Batch [47/938], Loss: 0.5459641218185425\n",
      "Validation: Epoch [15], Batch [48/938], Loss: 0.7364059686660767\n",
      "Validation: Epoch [15], Batch [49/938], Loss: 0.7173406481742859\n",
      "Validation: Epoch [15], Batch [50/938], Loss: 0.5827372074127197\n",
      "Validation: Epoch [15], Batch [51/938], Loss: 0.6471903920173645\n",
      "Validation: Epoch [15], Batch [52/938], Loss: 0.6470271944999695\n",
      "Validation: Epoch [15], Batch [53/938], Loss: 0.7630075812339783\n",
      "Validation: Epoch [15], Batch [54/938], Loss: 0.5514097809791565\n",
      "Validation: Epoch [15], Batch [55/938], Loss: 0.7474050521850586\n",
      "Validation: Epoch [15], Batch [56/938], Loss: 0.8106276988983154\n",
      "Validation: Epoch [15], Batch [57/938], Loss: 0.7353925704956055\n",
      "Validation: Epoch [15], Batch [58/938], Loss: 0.5036517381668091\n",
      "Validation: Epoch [15], Batch [59/938], Loss: 0.46592432260513306\n",
      "Validation: Epoch [15], Batch [60/938], Loss: 0.6443703174591064\n",
      "Validation: Epoch [15], Batch [61/938], Loss: 0.7732563018798828\n",
      "Validation: Epoch [15], Batch [62/938], Loss: 0.6491289734840393\n",
      "Validation: Epoch [15], Batch [63/938], Loss: 0.7106296420097351\n",
      "Validation: Epoch [15], Batch [64/938], Loss: 0.8842389583587646\n",
      "Validation: Epoch [15], Batch [65/938], Loss: 0.657446026802063\n",
      "Validation: Epoch [15], Batch [66/938], Loss: 0.5564616918563843\n",
      "Validation: Epoch [15], Batch [67/938], Loss: 0.607341468334198\n",
      "Validation: Epoch [15], Batch [68/938], Loss: 0.8901825547218323\n",
      "Validation: Epoch [15], Batch [69/938], Loss: 0.6228891015052795\n",
      "Validation: Epoch [15], Batch [70/938], Loss: 0.7923979759216309\n",
      "Validation: Epoch [15], Batch [71/938], Loss: 0.6199326515197754\n",
      "Validation: Epoch [15], Batch [72/938], Loss: 0.6819021105766296\n",
      "Validation: Epoch [15], Batch [73/938], Loss: 0.5418517589569092\n",
      "Validation: Epoch [15], Batch [74/938], Loss: 0.6445711851119995\n",
      "Validation: Epoch [15], Batch [75/938], Loss: 0.7505374550819397\n",
      "Validation: Epoch [15], Batch [76/938], Loss: 0.9011919498443604\n",
      "Validation: Epoch [15], Batch [77/938], Loss: 0.656889021396637\n",
      "Validation: Epoch [15], Batch [78/938], Loss: 0.6504901051521301\n",
      "Validation: Epoch [15], Batch [79/938], Loss: 0.740115225315094\n",
      "Validation: Epoch [15], Batch [80/938], Loss: 0.795961856842041\n",
      "Validation: Epoch [15], Batch [81/938], Loss: 0.6983180046081543\n",
      "Validation: Epoch [15], Batch [82/938], Loss: 0.7926909327507019\n",
      "Validation: Epoch [15], Batch [83/938], Loss: 0.4846416413784027\n",
      "Validation: Epoch [15], Batch [84/938], Loss: 0.5343887805938721\n",
      "Validation: Epoch [15], Batch [85/938], Loss: 0.4795856475830078\n",
      "Validation: Epoch [15], Batch [86/938], Loss: 0.7957849502563477\n",
      "Validation: Epoch [15], Batch [87/938], Loss: 0.6064313650131226\n",
      "Validation: Epoch [15], Batch [88/938], Loss: 0.5608025193214417\n",
      "Validation: Epoch [15], Batch [89/938], Loss: 0.6735440492630005\n",
      "Validation: Epoch [15], Batch [90/938], Loss: 0.6468257308006287\n",
      "Validation: Epoch [15], Batch [91/938], Loss: 0.7221131324768066\n",
      "Validation: Epoch [15], Batch [92/938], Loss: 0.6844974756240845\n",
      "Validation: Epoch [15], Batch [93/938], Loss: 0.5859764218330383\n",
      "Validation: Epoch [15], Batch [94/938], Loss: 0.6535598039627075\n",
      "Validation: Epoch [15], Batch [95/938], Loss: 0.6030148267745972\n",
      "Validation: Epoch [15], Batch [96/938], Loss: 0.8281723260879517\n",
      "Validation: Epoch [15], Batch [97/938], Loss: 0.7630478143692017\n",
      "Validation: Epoch [15], Batch [98/938], Loss: 0.9190195798873901\n",
      "Validation: Epoch [15], Batch [99/938], Loss: 0.7986645698547363\n",
      "Validation: Epoch [15], Batch [100/938], Loss: 0.5683616399765015\n",
      "Validation: Epoch [15], Batch [101/938], Loss: 0.6251223087310791\n",
      "Validation: Epoch [15], Batch [102/938], Loss: 0.6938971281051636\n",
      "Validation: Epoch [15], Batch [103/938], Loss: 0.9664793610572815\n",
      "Validation: Epoch [15], Batch [104/938], Loss: 0.5959613919258118\n",
      "Validation: Epoch [15], Batch [105/938], Loss: 0.35030364990234375\n",
      "Validation: Epoch [15], Batch [106/938], Loss: 0.6912742257118225\n",
      "Validation: Epoch [15], Batch [107/938], Loss: 0.9018886685371399\n",
      "Validation: Epoch [15], Batch [108/938], Loss: 0.8710530996322632\n",
      "Validation: Epoch [15], Batch [109/938], Loss: 0.4591200351715088\n",
      "Validation: Epoch [15], Batch [110/938], Loss: 0.879887580871582\n",
      "Validation: Epoch [15], Batch [111/938], Loss: 0.6314042806625366\n",
      "Validation: Epoch [15], Batch [112/938], Loss: 0.6008340716362\n",
      "Validation: Epoch [15], Batch [113/938], Loss: 0.7588506937026978\n",
      "Validation: Epoch [15], Batch [114/938], Loss: 0.6484659314155579\n",
      "Validation: Epoch [15], Batch [115/938], Loss: 0.9869371652603149\n",
      "Validation: Epoch [15], Batch [116/938], Loss: 0.7554527521133423\n",
      "Validation: Epoch [15], Batch [117/938], Loss: 0.6074343919754028\n",
      "Validation: Epoch [15], Batch [118/938], Loss: 0.6966635584831238\n",
      "Validation: Epoch [15], Batch [119/938], Loss: 0.5807512998580933\n",
      "Validation: Epoch [15], Batch [120/938], Loss: 0.7820258140563965\n",
      "Validation: Epoch [15], Batch [121/938], Loss: 0.6427608728408813\n",
      "Validation: Epoch [15], Batch [122/938], Loss: 0.530151903629303\n",
      "Validation: Epoch [15], Batch [123/938], Loss: 0.6985236406326294\n",
      "Validation: Epoch [15], Batch [124/938], Loss: 0.6942997574806213\n",
      "Validation: Epoch [15], Batch [125/938], Loss: 0.544952392578125\n",
      "Validation: Epoch [15], Batch [126/938], Loss: 0.6495831608772278\n",
      "Validation: Epoch [15], Batch [127/938], Loss: 0.4935292601585388\n",
      "Validation: Epoch [15], Batch [128/938], Loss: 0.5200139880180359\n",
      "Validation: Epoch [15], Batch [129/938], Loss: 0.5953191518783569\n",
      "Validation: Epoch [15], Batch [130/938], Loss: 0.6540920734405518\n",
      "Validation: Epoch [15], Batch [131/938], Loss: 0.8120301365852356\n",
      "Validation: Epoch [15], Batch [132/938], Loss: 0.7808243036270142\n",
      "Validation: Epoch [15], Batch [133/938], Loss: 0.6883449554443359\n",
      "Validation: Epoch [15], Batch [134/938], Loss: 1.0277806520462036\n",
      "Validation: Epoch [15], Batch [135/938], Loss: 0.7388355731964111\n",
      "Validation: Epoch [15], Batch [136/938], Loss: 0.5677773952484131\n",
      "Validation: Epoch [15], Batch [137/938], Loss: 0.9437758326530457\n",
      "Validation: Epoch [15], Batch [138/938], Loss: 0.5418491363525391\n",
      "Validation: Epoch [15], Batch [139/938], Loss: 0.649810791015625\n",
      "Validation: Epoch [15], Batch [140/938], Loss: 0.5646557807922363\n",
      "Validation: Epoch [15], Batch [141/938], Loss: 0.5703281760215759\n",
      "Validation: Epoch [15], Batch [142/938], Loss: 0.6297836899757385\n",
      "Validation: Epoch [15], Batch [143/938], Loss: 0.6822894811630249\n",
      "Validation: Epoch [15], Batch [144/938], Loss: 0.5819442272186279\n",
      "Validation: Epoch [15], Batch [145/938], Loss: 0.9602563381195068\n",
      "Validation: Epoch [15], Batch [146/938], Loss: 0.770539402961731\n",
      "Validation: Epoch [15], Batch [147/938], Loss: 0.8291285037994385\n",
      "Validation: Epoch [15], Batch [148/938], Loss: 0.5491815805435181\n",
      "Validation: Epoch [15], Batch [149/938], Loss: 0.6400066018104553\n",
      "Validation: Epoch [15], Batch [150/938], Loss: 0.5516425371170044\n",
      "Validation: Epoch [15], Batch [151/938], Loss: 0.8509679436683655\n",
      "Validation: Epoch [15], Batch [152/938], Loss: 0.5483292937278748\n",
      "Validation: Epoch [15], Batch [153/938], Loss: 0.9964641332626343\n",
      "Validation: Epoch [15], Batch [154/938], Loss: 0.6759011149406433\n",
      "Validation: Epoch [15], Batch [155/938], Loss: 0.6437860131263733\n",
      "Validation: Epoch [15], Batch [156/938], Loss: 1.024614691734314\n",
      "Validation: Epoch [15], Batch [157/938], Loss: 0.7583212852478027\n",
      "Validation: Epoch [15], Batch [158/938], Loss: 0.5986541509628296\n",
      "Validation: Epoch [15], Batch [159/938], Loss: 0.6947317719459534\n",
      "Validation: Epoch [15], Batch [160/938], Loss: 0.8007975220680237\n",
      "Validation: Epoch [15], Batch [161/938], Loss: 0.7469564080238342\n",
      "Validation: Epoch [15], Batch [162/938], Loss: 0.6696455478668213\n",
      "Validation: Epoch [15], Batch [163/938], Loss: 0.7138515710830688\n",
      "Validation: Epoch [15], Batch [164/938], Loss: 0.4677102565765381\n",
      "Validation: Epoch [15], Batch [165/938], Loss: 0.8753300905227661\n",
      "Validation: Epoch [15], Batch [166/938], Loss: 0.6007052659988403\n",
      "Validation: Epoch [15], Batch [167/938], Loss: 0.5876404047012329\n",
      "Validation: Epoch [15], Batch [168/938], Loss: 0.8910210728645325\n",
      "Validation: Epoch [15], Batch [169/938], Loss: 0.7787169814109802\n",
      "Validation: Epoch [15], Batch [170/938], Loss: 0.5773127675056458\n",
      "Validation: Epoch [15], Batch [171/938], Loss: 0.6192178130149841\n",
      "Validation: Epoch [15], Batch [172/938], Loss: 0.7054538726806641\n",
      "Validation: Epoch [15], Batch [173/938], Loss: 0.5290684103965759\n",
      "Validation: Epoch [15], Batch [174/938], Loss: 0.6031031608581543\n",
      "Validation: Epoch [15], Batch [175/938], Loss: 0.7903741002082825\n",
      "Validation: Epoch [15], Batch [176/938], Loss: 0.7222130298614502\n",
      "Validation: Epoch [15], Batch [177/938], Loss: 0.6105812788009644\n",
      "Validation: Epoch [15], Batch [178/938], Loss: 0.6022344827651978\n",
      "Validation: Epoch [15], Batch [179/938], Loss: 0.7695918679237366\n",
      "Validation: Epoch [15], Batch [180/938], Loss: 0.5153130292892456\n",
      "Validation: Epoch [15], Batch [181/938], Loss: 0.7371780872344971\n",
      "Validation: Epoch [15], Batch [182/938], Loss: 0.7257730960845947\n",
      "Validation: Epoch [15], Batch [183/938], Loss: 0.9062674045562744\n",
      "Validation: Epoch [15], Batch [184/938], Loss: 0.5021982789039612\n",
      "Validation: Epoch [15], Batch [185/938], Loss: 0.7051023840904236\n",
      "Validation: Epoch [15], Batch [186/938], Loss: 0.5723199844360352\n",
      "Validation: Epoch [15], Batch [187/938], Loss: 0.5557401180267334\n",
      "Validation: Epoch [15], Batch [188/938], Loss: 0.5493764877319336\n",
      "Validation: Epoch [15], Batch [189/938], Loss: 0.7100422382354736\n",
      "Validation: Epoch [15], Batch [190/938], Loss: 0.673269510269165\n",
      "Validation: Epoch [15], Batch [191/938], Loss: 0.7266123294830322\n",
      "Validation: Epoch [15], Batch [192/938], Loss: 0.828444242477417\n",
      "Validation: Epoch [15], Batch [193/938], Loss: 0.5109838843345642\n",
      "Validation: Epoch [15], Batch [194/938], Loss: 0.8107762336730957\n",
      "Validation: Epoch [15], Batch [195/938], Loss: 0.6477203965187073\n",
      "Validation: Epoch [15], Batch [196/938], Loss: 0.5734383463859558\n",
      "Validation: Epoch [15], Batch [197/938], Loss: 0.6916593909263611\n",
      "Validation: Epoch [15], Batch [198/938], Loss: 0.7242248058319092\n",
      "Validation: Epoch [15], Batch [199/938], Loss: 0.6801175475120544\n",
      "Validation: Epoch [15], Batch [200/938], Loss: 0.774533212184906\n",
      "Validation: Epoch [15], Batch [201/938], Loss: 0.5798285603523254\n",
      "Validation: Epoch [15], Batch [202/938], Loss: 0.7515550255775452\n",
      "Validation: Epoch [15], Batch [203/938], Loss: 0.6700832843780518\n",
      "Validation: Epoch [15], Batch [204/938], Loss: 0.658947229385376\n",
      "Validation: Epoch [15], Batch [205/938], Loss: 0.5326191186904907\n",
      "Validation: Epoch [15], Batch [206/938], Loss: 0.8841853141784668\n",
      "Validation: Epoch [15], Batch [207/938], Loss: 0.6094220876693726\n",
      "Validation: Epoch [15], Batch [208/938], Loss: 0.5486454367637634\n",
      "Validation: Epoch [15], Batch [209/938], Loss: 0.7417870759963989\n",
      "Validation: Epoch [15], Batch [210/938], Loss: 0.9561928510665894\n",
      "Validation: Epoch [15], Batch [211/938], Loss: 0.7022601366043091\n",
      "Validation: Epoch [15], Batch [212/938], Loss: 0.5434229373931885\n",
      "Validation: Epoch [15], Batch [213/938], Loss: 0.6516698002815247\n",
      "Validation: Epoch [15], Batch [214/938], Loss: 0.6376715302467346\n",
      "Validation: Epoch [15], Batch [215/938], Loss: 0.6029456257820129\n",
      "Validation: Epoch [15], Batch [216/938], Loss: 0.9232282638549805\n",
      "Validation: Epoch [15], Batch [217/938], Loss: 0.7241932153701782\n",
      "Validation: Epoch [15], Batch [218/938], Loss: 0.6543993353843689\n",
      "Validation: Epoch [15], Batch [219/938], Loss: 0.6444880366325378\n",
      "Validation: Epoch [15], Batch [220/938], Loss: 0.6552949547767639\n",
      "Validation: Epoch [15], Batch [221/938], Loss: 0.5866721868515015\n",
      "Validation: Epoch [15], Batch [222/938], Loss: 0.7441388368606567\n",
      "Validation: Epoch [15], Batch [223/938], Loss: 0.6118676662445068\n",
      "Validation: Epoch [15], Batch [224/938], Loss: 0.7671866416931152\n",
      "Validation: Epoch [15], Batch [225/938], Loss: 0.5004180073738098\n",
      "Validation: Epoch [15], Batch [226/938], Loss: 0.7380576133728027\n",
      "Validation: Epoch [15], Batch [227/938], Loss: 0.713133692741394\n",
      "Validation: Epoch [15], Batch [228/938], Loss: 0.954035758972168\n",
      "Validation: Epoch [15], Batch [229/938], Loss: 0.6497300863265991\n",
      "Validation: Epoch [15], Batch [230/938], Loss: 0.7971692085266113\n",
      "Validation: Epoch [15], Batch [231/938], Loss: 0.7525887489318848\n",
      "Validation: Epoch [15], Batch [232/938], Loss: 1.0620628595352173\n",
      "Validation: Epoch [15], Batch [233/938], Loss: 0.5673335194587708\n",
      "Validation: Epoch [15], Batch [234/938], Loss: 0.5793280601501465\n",
      "Validation: Epoch [15], Batch [235/938], Loss: 0.8055062294006348\n",
      "Validation: Epoch [15], Batch [236/938], Loss: 0.7422103881835938\n",
      "Validation: Epoch [15], Batch [237/938], Loss: 0.6310974955558777\n",
      "Validation: Epoch [15], Batch [238/938], Loss: 0.9221364259719849\n",
      "Validation: Epoch [15], Batch [239/938], Loss: 0.4772045612335205\n",
      "Validation: Epoch [15], Batch [240/938], Loss: 0.7304487228393555\n",
      "Validation: Epoch [15], Batch [241/938], Loss: 0.793608546257019\n",
      "Validation: Epoch [15], Batch [242/938], Loss: 0.7023113369941711\n",
      "Validation: Epoch [15], Batch [243/938], Loss: 0.6132798194885254\n",
      "Validation: Epoch [15], Batch [244/938], Loss: 0.6241540908813477\n",
      "Validation: Epoch [15], Batch [245/938], Loss: 0.8520376086235046\n",
      "Validation: Epoch [15], Batch [246/938], Loss: 0.6766490936279297\n",
      "Validation: Epoch [15], Batch [247/938], Loss: 0.7321745157241821\n",
      "Validation: Epoch [15], Batch [248/938], Loss: 0.8082229495048523\n",
      "Validation: Epoch [15], Batch [249/938], Loss: 0.6364333629608154\n",
      "Validation: Epoch [15], Batch [250/938], Loss: 0.7250366806983948\n",
      "Validation: Epoch [15], Batch [251/938], Loss: 0.7655022740364075\n",
      "Validation: Epoch [15], Batch [252/938], Loss: 0.6216602921485901\n",
      "Validation: Epoch [15], Batch [253/938], Loss: 0.7539095878601074\n",
      "Validation: Epoch [15], Batch [254/938], Loss: 0.6026082038879395\n",
      "Validation: Epoch [15], Batch [255/938], Loss: 0.7481199502944946\n",
      "Validation: Epoch [15], Batch [256/938], Loss: 0.7161908149719238\n",
      "Validation: Epoch [15], Batch [257/938], Loss: 0.7530177235603333\n",
      "Validation: Epoch [15], Batch [258/938], Loss: 0.7618293762207031\n",
      "Validation: Epoch [15], Batch [259/938], Loss: 0.5889831781387329\n",
      "Validation: Epoch [15], Batch [260/938], Loss: 0.6917355060577393\n",
      "Validation: Epoch [15], Batch [261/938], Loss: 0.828196108341217\n",
      "Validation: Epoch [15], Batch [262/938], Loss: 0.6875622868537903\n",
      "Validation: Epoch [15], Batch [263/938], Loss: 0.7216402292251587\n",
      "Validation: Epoch [15], Batch [264/938], Loss: 0.8737806081771851\n",
      "Validation: Epoch [15], Batch [265/938], Loss: 0.6817240715026855\n",
      "Validation: Epoch [15], Batch [266/938], Loss: 0.8184329867362976\n",
      "Validation: Epoch [15], Batch [267/938], Loss: 0.8492996096611023\n",
      "Validation: Epoch [15], Batch [268/938], Loss: 0.642486035823822\n",
      "Validation: Epoch [15], Batch [269/938], Loss: 0.608641505241394\n",
      "Validation: Epoch [15], Batch [270/938], Loss: 0.6893165111541748\n",
      "Validation: Epoch [15], Batch [271/938], Loss: 0.9370794892311096\n",
      "Validation: Epoch [15], Batch [272/938], Loss: 1.0133819580078125\n",
      "Validation: Epoch [15], Batch [273/938], Loss: 0.8314484357833862\n",
      "Validation: Epoch [15], Batch [274/938], Loss: 0.5218304395675659\n",
      "Validation: Epoch [15], Batch [275/938], Loss: 0.4659998416900635\n",
      "Validation: Epoch [15], Batch [276/938], Loss: 0.8578462600708008\n",
      "Validation: Epoch [15], Batch [277/938], Loss: 0.7726247906684875\n",
      "Validation: Epoch [15], Batch [278/938], Loss: 0.7510530948638916\n",
      "Validation: Epoch [15], Batch [279/938], Loss: 0.5623317360877991\n",
      "Validation: Epoch [15], Batch [280/938], Loss: 0.6539641618728638\n",
      "Validation: Epoch [15], Batch [281/938], Loss: 0.6216226816177368\n",
      "Validation: Epoch [15], Batch [282/938], Loss: 0.7293630242347717\n",
      "Validation: Epoch [15], Batch [283/938], Loss: 0.5982787609100342\n",
      "Validation: Epoch [15], Batch [284/938], Loss: 0.618864119052887\n",
      "Validation: Epoch [15], Batch [285/938], Loss: 0.7174703478813171\n",
      "Validation: Epoch [15], Batch [286/938], Loss: 0.659048855304718\n",
      "Validation: Epoch [15], Batch [287/938], Loss: 0.7074395418167114\n",
      "Validation: Epoch [15], Batch [288/938], Loss: 0.859603226184845\n",
      "Validation: Epoch [15], Batch [289/938], Loss: 0.4799197316169739\n",
      "Validation: Epoch [15], Batch [290/938], Loss: 1.0004327297210693\n",
      "Validation: Epoch [15], Batch [291/938], Loss: 0.965074360370636\n",
      "Validation: Epoch [15], Batch [292/938], Loss: 0.7076259851455688\n",
      "Validation: Epoch [15], Batch [293/938], Loss: 0.6103416681289673\n",
      "Validation: Epoch [15], Batch [294/938], Loss: 0.5845524072647095\n",
      "Validation: Epoch [15], Batch [295/938], Loss: 0.47174450755119324\n",
      "Validation: Epoch [15], Batch [296/938], Loss: 0.629553496837616\n",
      "Validation: Epoch [15], Batch [297/938], Loss: 0.7173871994018555\n",
      "Validation: Epoch [15], Batch [298/938], Loss: 0.8166958093643188\n",
      "Validation: Epoch [15], Batch [299/938], Loss: 0.46295392513275146\n",
      "Validation: Epoch [15], Batch [300/938], Loss: 0.6600769758224487\n",
      "Validation: Epoch [15], Batch [301/938], Loss: 0.7381817102432251\n",
      "Validation: Epoch [15], Batch [302/938], Loss: 0.81148362159729\n",
      "Validation: Epoch [15], Batch [303/938], Loss: 0.7213578820228577\n",
      "Validation: Epoch [15], Batch [304/938], Loss: 0.6025812029838562\n",
      "Validation: Epoch [15], Batch [305/938], Loss: 0.6975803375244141\n",
      "Validation: Epoch [15], Batch [306/938], Loss: 0.6942952871322632\n",
      "Validation: Epoch [15], Batch [307/938], Loss: 0.7455779314041138\n",
      "Validation: Epoch [15], Batch [308/938], Loss: 0.5293915271759033\n",
      "Validation: Epoch [15], Batch [309/938], Loss: 0.6137757897377014\n",
      "Validation: Epoch [15], Batch [310/938], Loss: 0.8371544480323792\n",
      "Validation: Epoch [15], Batch [311/938], Loss: 0.735028862953186\n",
      "Validation: Epoch [15], Batch [312/938], Loss: 0.6238176226615906\n",
      "Validation: Epoch [15], Batch [313/938], Loss: 0.8936469554901123\n",
      "Validation: Epoch [15], Batch [314/938], Loss: 0.5485441088676453\n",
      "Validation: Epoch [15], Batch [315/938], Loss: 0.5733152031898499\n",
      "Validation: Epoch [15], Batch [316/938], Loss: 0.6553696393966675\n",
      "Validation: Epoch [15], Batch [317/938], Loss: 0.7751926779747009\n",
      "Validation: Epoch [15], Batch [318/938], Loss: 0.4215947687625885\n",
      "Validation: Epoch [15], Batch [319/938], Loss: 0.5949599742889404\n",
      "Validation: Epoch [15], Batch [320/938], Loss: 0.8190648555755615\n",
      "Validation: Epoch [15], Batch [321/938], Loss: 0.8612480163574219\n",
      "Validation: Epoch [15], Batch [322/938], Loss: 0.6177809834480286\n",
      "Validation: Epoch [15], Batch [323/938], Loss: 0.703741192817688\n",
      "Validation: Epoch [15], Batch [324/938], Loss: 0.5567481517791748\n",
      "Validation: Epoch [15], Batch [325/938], Loss: 0.6188225150108337\n",
      "Validation: Epoch [15], Batch [326/938], Loss: 0.5446370840072632\n",
      "Validation: Epoch [15], Batch [327/938], Loss: 0.7764550447463989\n",
      "Validation: Epoch [15], Batch [328/938], Loss: 0.8322634696960449\n",
      "Validation: Epoch [15], Batch [329/938], Loss: 0.7226982116699219\n",
      "Validation: Epoch [15], Batch [330/938], Loss: 0.634535014629364\n",
      "Validation: Epoch [15], Batch [331/938], Loss: 0.7347214221954346\n",
      "Validation: Epoch [15], Batch [332/938], Loss: 0.6972463130950928\n",
      "Validation: Epoch [15], Batch [333/938], Loss: 0.5798826217651367\n",
      "Validation: Epoch [15], Batch [334/938], Loss: 0.6940217614173889\n",
      "Validation: Epoch [15], Batch [335/938], Loss: 0.6250311136245728\n",
      "Validation: Epoch [15], Batch [336/938], Loss: 0.638346791267395\n",
      "Validation: Epoch [15], Batch [337/938], Loss: 0.6394065618515015\n",
      "Validation: Epoch [15], Batch [338/938], Loss: 0.5227642059326172\n",
      "Validation: Epoch [15], Batch [339/938], Loss: 0.6771988868713379\n",
      "Validation: Epoch [15], Batch [340/938], Loss: 0.644349217414856\n",
      "Validation: Epoch [15], Batch [341/938], Loss: 0.9175772666931152\n",
      "Validation: Epoch [15], Batch [342/938], Loss: 0.520606517791748\n",
      "Validation: Epoch [15], Batch [343/938], Loss: 0.6261138319969177\n",
      "Validation: Epoch [15], Batch [344/938], Loss: 0.7283848524093628\n",
      "Validation: Epoch [15], Batch [345/938], Loss: 0.6690040826797485\n",
      "Validation: Epoch [15], Batch [346/938], Loss: 0.6125915050506592\n",
      "Validation: Epoch [15], Batch [347/938], Loss: 0.7203964591026306\n",
      "Validation: Epoch [15], Batch [348/938], Loss: 0.7631402015686035\n",
      "Validation: Epoch [15], Batch [349/938], Loss: 0.5136739015579224\n",
      "Validation: Epoch [15], Batch [350/938], Loss: 0.5594675540924072\n",
      "Validation: Epoch [15], Batch [351/938], Loss: 0.7174707651138306\n",
      "Validation: Epoch [15], Batch [352/938], Loss: 0.637418270111084\n",
      "Validation: Epoch [15], Batch [353/938], Loss: 0.8390852808952332\n",
      "Validation: Epoch [15], Batch [354/938], Loss: 0.7298349738121033\n",
      "Validation: Epoch [15], Batch [355/938], Loss: 0.8561856746673584\n",
      "Validation: Epoch [15], Batch [356/938], Loss: 0.7907147407531738\n",
      "Validation: Epoch [15], Batch [357/938], Loss: 0.7765862941741943\n",
      "Validation: Epoch [15], Batch [358/938], Loss: 0.7314043641090393\n",
      "Validation: Epoch [15], Batch [359/938], Loss: 0.6795274615287781\n",
      "Validation: Epoch [15], Batch [360/938], Loss: 0.8664856553077698\n",
      "Validation: Epoch [15], Batch [361/938], Loss: 0.6463268399238586\n",
      "Validation: Epoch [15], Batch [362/938], Loss: 0.7698465585708618\n",
      "Validation: Epoch [15], Batch [363/938], Loss: 0.6771823763847351\n",
      "Validation: Epoch [15], Batch [364/938], Loss: 0.6777706146240234\n",
      "Validation: Epoch [15], Batch [365/938], Loss: 0.6399658918380737\n",
      "Validation: Epoch [15], Batch [366/938], Loss: 0.5031173825263977\n",
      "Validation: Epoch [15], Batch [367/938], Loss: 0.5388075113296509\n",
      "Validation: Epoch [15], Batch [368/938], Loss: 0.6846354603767395\n",
      "Validation: Epoch [15], Batch [369/938], Loss: 0.6057062745094299\n",
      "Validation: Epoch [15], Batch [370/938], Loss: 0.5678422451019287\n",
      "Validation: Epoch [15], Batch [371/938], Loss: 0.7480436563491821\n",
      "Validation: Epoch [15], Batch [372/938], Loss: 0.7367937564849854\n",
      "Validation: Epoch [15], Batch [373/938], Loss: 0.39118027687072754\n",
      "Validation: Epoch [15], Batch [374/938], Loss: 0.659859836101532\n",
      "Validation: Epoch [15], Batch [375/938], Loss: 0.5391209125518799\n",
      "Validation: Epoch [15], Batch [376/938], Loss: 0.6022129654884338\n",
      "Validation: Epoch [15], Batch [377/938], Loss: 0.544635534286499\n",
      "Validation: Epoch [15], Batch [378/938], Loss: 0.7702677249908447\n",
      "Validation: Epoch [15], Batch [379/938], Loss: 0.7737526297569275\n",
      "Validation: Epoch [15], Batch [380/938], Loss: 0.715221107006073\n",
      "Validation: Epoch [15], Batch [381/938], Loss: 0.7068954110145569\n",
      "Validation: Epoch [15], Batch [382/938], Loss: 0.5022108554840088\n",
      "Validation: Epoch [15], Batch [383/938], Loss: 0.43950238823890686\n",
      "Validation: Epoch [15], Batch [384/938], Loss: 0.64931321144104\n",
      "Validation: Epoch [15], Batch [385/938], Loss: 0.7128984928131104\n",
      "Validation: Epoch [15], Batch [386/938], Loss: 0.5941113233566284\n",
      "Validation: Epoch [15], Batch [387/938], Loss: 0.5854610800743103\n",
      "Validation: Epoch [15], Batch [388/938], Loss: 0.635716438293457\n",
      "Validation: Epoch [15], Batch [389/938], Loss: 0.6002289056777954\n",
      "Validation: Epoch [15], Batch [390/938], Loss: 0.8630551099777222\n",
      "Validation: Epoch [15], Batch [391/938], Loss: 0.876460075378418\n",
      "Validation: Epoch [15], Batch [392/938], Loss: 0.5522399544715881\n",
      "Validation: Epoch [15], Batch [393/938], Loss: 0.8345314264297485\n",
      "Validation: Epoch [15], Batch [394/938], Loss: 0.9013826847076416\n",
      "Validation: Epoch [15], Batch [395/938], Loss: 0.7784643173217773\n",
      "Validation: Epoch [15], Batch [396/938], Loss: 0.6996932029724121\n",
      "Validation: Epoch [15], Batch [397/938], Loss: 0.6859256029129028\n",
      "Validation: Epoch [15], Batch [398/938], Loss: 0.7679463624954224\n",
      "Validation: Epoch [15], Batch [399/938], Loss: 0.660222589969635\n",
      "Validation: Epoch [15], Batch [400/938], Loss: 0.6177015900611877\n",
      "Validation: Epoch [15], Batch [401/938], Loss: 0.5570035576820374\n",
      "Validation: Epoch [15], Batch [402/938], Loss: 0.8509994149208069\n",
      "Validation: Epoch [15], Batch [403/938], Loss: 0.6754504442214966\n",
      "Validation: Epoch [15], Batch [404/938], Loss: 0.8876304030418396\n",
      "Validation: Epoch [15], Batch [405/938], Loss: 0.7983646392822266\n",
      "Validation: Epoch [15], Batch [406/938], Loss: 0.6429908275604248\n",
      "Validation: Epoch [15], Batch [407/938], Loss: 0.6685985326766968\n",
      "Validation: Epoch [15], Batch [408/938], Loss: 0.5132214426994324\n",
      "Validation: Epoch [15], Batch [409/938], Loss: 0.6483979821205139\n",
      "Validation: Epoch [15], Batch [410/938], Loss: 0.6621720790863037\n",
      "Validation: Epoch [15], Batch [411/938], Loss: 0.48900938034057617\n",
      "Validation: Epoch [15], Batch [412/938], Loss: 0.7651581764221191\n",
      "Validation: Epoch [15], Batch [413/938], Loss: 0.6083295345306396\n",
      "Validation: Epoch [15], Batch [414/938], Loss: 0.7718473076820374\n",
      "Validation: Epoch [15], Batch [415/938], Loss: 0.9400053024291992\n",
      "Validation: Epoch [15], Batch [416/938], Loss: 0.4344039559364319\n",
      "Validation: Epoch [15], Batch [417/938], Loss: 0.6636810302734375\n",
      "Validation: Epoch [15], Batch [418/938], Loss: 0.6559554934501648\n",
      "Validation: Epoch [15], Batch [419/938], Loss: 0.5866940021514893\n",
      "Validation: Epoch [15], Batch [420/938], Loss: 0.7746356129646301\n",
      "Validation: Epoch [15], Batch [421/938], Loss: 0.6884636878967285\n",
      "Validation: Epoch [15], Batch [422/938], Loss: 0.7368274331092834\n",
      "Validation: Epoch [15], Batch [423/938], Loss: 0.7709035277366638\n",
      "Validation: Epoch [15], Batch [424/938], Loss: 0.9549551010131836\n",
      "Validation: Epoch [15], Batch [425/938], Loss: 0.6868010759353638\n",
      "Validation: Epoch [15], Batch [426/938], Loss: 0.8054976463317871\n",
      "Validation: Epoch [15], Batch [427/938], Loss: 0.6254509687423706\n",
      "Validation: Epoch [15], Batch [428/938], Loss: 0.7402948141098022\n",
      "Validation: Epoch [15], Batch [429/938], Loss: 0.4129413068294525\n",
      "Validation: Epoch [15], Batch [430/938], Loss: 0.6258379220962524\n",
      "Validation: Epoch [15], Batch [431/938], Loss: 0.6539948582649231\n",
      "Validation: Epoch [15], Batch [432/938], Loss: 0.9213882684707642\n",
      "Validation: Epoch [15], Batch [433/938], Loss: 0.5018559694290161\n",
      "Validation: Epoch [15], Batch [434/938], Loss: 0.6748263239860535\n",
      "Validation: Epoch [15], Batch [435/938], Loss: 0.7643062472343445\n",
      "Validation: Epoch [15], Batch [436/938], Loss: 0.763127326965332\n",
      "Validation: Epoch [15], Batch [437/938], Loss: 0.48387905955314636\n",
      "Validation: Epoch [15], Batch [438/938], Loss: 0.6580029129981995\n",
      "Validation: Epoch [15], Batch [439/938], Loss: 0.7308000326156616\n",
      "Validation: Epoch [15], Batch [440/938], Loss: 0.5979448556900024\n",
      "Validation: Epoch [15], Batch [441/938], Loss: 0.7404382228851318\n",
      "Validation: Epoch [15], Batch [442/938], Loss: 0.7283679246902466\n",
      "Validation: Epoch [15], Batch [443/938], Loss: 0.7723367214202881\n",
      "Validation: Epoch [15], Batch [444/938], Loss: 0.6680667400360107\n",
      "Validation: Epoch [15], Batch [445/938], Loss: 0.5680121183395386\n",
      "Validation: Epoch [15], Batch [446/938], Loss: 0.654932975769043\n",
      "Validation: Epoch [15], Batch [447/938], Loss: 0.5469567775726318\n",
      "Validation: Epoch [15], Batch [448/938], Loss: 0.6570500731468201\n",
      "Validation: Epoch [15], Batch [449/938], Loss: 0.4643290936946869\n",
      "Validation: Epoch [15], Batch [450/938], Loss: 1.1117080450057983\n",
      "Validation: Epoch [15], Batch [451/938], Loss: 0.8524194955825806\n",
      "Validation: Epoch [15], Batch [452/938], Loss: 0.5961841344833374\n",
      "Validation: Epoch [15], Batch [453/938], Loss: 0.7133808135986328\n",
      "Validation: Epoch [15], Batch [454/938], Loss: 0.7920774221420288\n",
      "Validation: Epoch [15], Batch [455/938], Loss: 0.6085718870162964\n",
      "Validation: Epoch [15], Batch [456/938], Loss: 0.7380983829498291\n",
      "Validation: Epoch [15], Batch [457/938], Loss: 0.5867095589637756\n",
      "Validation: Epoch [15], Batch [458/938], Loss: 1.112880825996399\n",
      "Validation: Epoch [15], Batch [459/938], Loss: 0.5051031112670898\n",
      "Validation: Epoch [15], Batch [460/938], Loss: 0.7986289262771606\n",
      "Validation: Epoch [15], Batch [461/938], Loss: 0.5291435122489929\n",
      "Validation: Epoch [15], Batch [462/938], Loss: 0.5442878007888794\n",
      "Validation: Epoch [15], Batch [463/938], Loss: 0.8902795314788818\n",
      "Validation: Epoch [15], Batch [464/938], Loss: 0.4921480119228363\n",
      "Validation: Epoch [15], Batch [465/938], Loss: 0.5904070138931274\n",
      "Validation: Epoch [15], Batch [466/938], Loss: 0.5189188122749329\n",
      "Validation: Epoch [15], Batch [467/938], Loss: 0.7310474514961243\n",
      "Validation: Epoch [15], Batch [468/938], Loss: 0.777316689491272\n",
      "Validation: Epoch [15], Batch [469/938], Loss: 0.7169990539550781\n",
      "Validation: Epoch [15], Batch [470/938], Loss: 0.7929243445396423\n",
      "Validation: Epoch [15], Batch [471/938], Loss: 0.6433259844779968\n",
      "Validation: Epoch [15], Batch [472/938], Loss: 0.5792881846427917\n",
      "Validation: Epoch [15], Batch [473/938], Loss: 0.6858642101287842\n",
      "Validation: Epoch [15], Batch [474/938], Loss: 0.7211477160453796\n",
      "Validation: Epoch [15], Batch [475/938], Loss: 0.6910481452941895\n",
      "Validation: Epoch [15], Batch [476/938], Loss: 0.6706463694572449\n",
      "Validation: Epoch [15], Batch [477/938], Loss: 0.7994094491004944\n",
      "Validation: Epoch [15], Batch [478/938], Loss: 0.49356240034103394\n",
      "Validation: Epoch [15], Batch [479/938], Loss: 0.3948202431201935\n",
      "Validation: Epoch [15], Batch [480/938], Loss: 0.8357991576194763\n",
      "Validation: Epoch [15], Batch [481/938], Loss: 0.5839674472808838\n",
      "Validation: Epoch [15], Batch [482/938], Loss: 0.46960482001304626\n",
      "Validation: Epoch [15], Batch [483/938], Loss: 0.5392244458198547\n",
      "Validation: Epoch [15], Batch [484/938], Loss: 0.9212324619293213\n",
      "Validation: Epoch [15], Batch [485/938], Loss: 0.7801685333251953\n",
      "Validation: Epoch [15], Batch [486/938], Loss: 1.2656755447387695\n",
      "Validation: Epoch [15], Batch [487/938], Loss: 0.6941288709640503\n",
      "Validation: Epoch [15], Batch [488/938], Loss: 0.8123799562454224\n",
      "Validation: Epoch [15], Batch [489/938], Loss: 0.889845073223114\n",
      "Validation: Epoch [15], Batch [490/938], Loss: 0.7077214121818542\n",
      "Validation: Epoch [15], Batch [491/938], Loss: 0.8838818669319153\n",
      "Validation: Epoch [15], Batch [492/938], Loss: 0.875967800617218\n",
      "Validation: Epoch [15], Batch [493/938], Loss: 0.7247085571289062\n",
      "Validation: Epoch [15], Batch [494/938], Loss: 0.7183992266654968\n",
      "Validation: Epoch [15], Batch [495/938], Loss: 0.7023159861564636\n",
      "Validation: Epoch [15], Batch [496/938], Loss: 0.757947564125061\n",
      "Validation: Epoch [15], Batch [497/938], Loss: 0.7088391780853271\n",
      "Validation: Epoch [15], Batch [498/938], Loss: 0.9478567838668823\n",
      "Validation: Epoch [15], Batch [499/938], Loss: 0.6761130094528198\n",
      "Validation: Epoch [15], Batch [500/938], Loss: 0.8503683805465698\n",
      "Validation: Epoch [15], Batch [501/938], Loss: 0.6748380064964294\n",
      "Validation: Epoch [15], Batch [502/938], Loss: 0.9076089859008789\n",
      "Validation: Epoch [15], Batch [503/938], Loss: 0.5239203572273254\n",
      "Validation: Epoch [15], Batch [504/938], Loss: 0.519023597240448\n",
      "Validation: Epoch [15], Batch [505/938], Loss: 0.6136713027954102\n",
      "Validation: Epoch [15], Batch [506/938], Loss: 0.6225920915603638\n",
      "Validation: Epoch [15], Batch [507/938], Loss: 0.6564928889274597\n",
      "Validation: Epoch [15], Batch [508/938], Loss: 0.6652655005455017\n",
      "Validation: Epoch [15], Batch [509/938], Loss: 0.7293204069137573\n",
      "Validation: Epoch [15], Batch [510/938], Loss: 0.7296353578567505\n",
      "Validation: Epoch [15], Batch [511/938], Loss: 0.6555952429771423\n",
      "Validation: Epoch [15], Batch [512/938], Loss: 0.6775012016296387\n",
      "Validation: Epoch [15], Batch [513/938], Loss: 0.6802940368652344\n",
      "Validation: Epoch [15], Batch [514/938], Loss: 0.5446459054946899\n",
      "Validation: Epoch [15], Batch [515/938], Loss: 0.7415270805358887\n",
      "Validation: Epoch [15], Batch [516/938], Loss: 0.7307320237159729\n",
      "Validation: Epoch [15], Batch [517/938], Loss: 0.6065317988395691\n",
      "Validation: Epoch [15], Batch [518/938], Loss: 0.8566233515739441\n",
      "Validation: Epoch [15], Batch [519/938], Loss: 0.43395334482192993\n",
      "Validation: Epoch [15], Batch [520/938], Loss: 0.5166535377502441\n",
      "Validation: Epoch [15], Batch [521/938], Loss: 0.6061694622039795\n",
      "Validation: Epoch [15], Batch [522/938], Loss: 0.6606777310371399\n",
      "Validation: Epoch [15], Batch [523/938], Loss: 0.47449684143066406\n",
      "Validation: Epoch [15], Batch [524/938], Loss: 0.6473115086555481\n",
      "Validation: Epoch [15], Batch [525/938], Loss: 0.5774085521697998\n",
      "Validation: Epoch [15], Batch [526/938], Loss: 0.4792574346065521\n",
      "Validation: Epoch [15], Batch [527/938], Loss: 0.8581205010414124\n",
      "Validation: Epoch [15], Batch [528/938], Loss: 0.6471331119537354\n",
      "Validation: Epoch [15], Batch [529/938], Loss: 0.6629409790039062\n",
      "Validation: Epoch [15], Batch [530/938], Loss: 0.6589765548706055\n",
      "Validation: Epoch [15], Batch [531/938], Loss: 0.6918152570724487\n",
      "Validation: Epoch [15], Batch [532/938], Loss: 0.47433027625083923\n",
      "Validation: Epoch [15], Batch [533/938], Loss: 0.5227103233337402\n",
      "Validation: Epoch [15], Batch [534/938], Loss: 0.7393023371696472\n",
      "Validation: Epoch [15], Batch [535/938], Loss: 0.7618751525878906\n",
      "Validation: Epoch [15], Batch [536/938], Loss: 0.8206855654716492\n",
      "Validation: Epoch [15], Batch [537/938], Loss: 0.6089527010917664\n",
      "Validation: Epoch [15], Batch [538/938], Loss: 0.6527020931243896\n",
      "Validation: Epoch [15], Batch [539/938], Loss: 0.6942484974861145\n",
      "Validation: Epoch [15], Batch [540/938], Loss: 0.5614340305328369\n",
      "Validation: Epoch [15], Batch [541/938], Loss: 0.5121632814407349\n",
      "Validation: Epoch [15], Batch [542/938], Loss: 0.8605383634567261\n",
      "Validation: Epoch [15], Batch [543/938], Loss: 0.7062119245529175\n",
      "Validation: Epoch [15], Batch [544/938], Loss: 0.8434033393859863\n",
      "Validation: Epoch [15], Batch [545/938], Loss: 0.6368898153305054\n",
      "Validation: Epoch [15], Batch [546/938], Loss: 0.6499652862548828\n",
      "Validation: Epoch [15], Batch [547/938], Loss: 0.47681325674057007\n",
      "Validation: Epoch [15], Batch [548/938], Loss: 0.6635338664054871\n",
      "Validation: Epoch [15], Batch [549/938], Loss: 0.6577998995780945\n",
      "Validation: Epoch [15], Batch [550/938], Loss: 0.6109766364097595\n",
      "Validation: Epoch [15], Batch [551/938], Loss: 0.6852849125862122\n",
      "Validation: Epoch [15], Batch [552/938], Loss: 0.7844718098640442\n",
      "Validation: Epoch [15], Batch [553/938], Loss: 0.6850985288619995\n",
      "Validation: Epoch [15], Batch [554/938], Loss: 0.7501468062400818\n",
      "Validation: Epoch [15], Batch [555/938], Loss: 0.7468050718307495\n",
      "Validation: Epoch [15], Batch [556/938], Loss: 0.5284554362297058\n",
      "Validation: Epoch [15], Batch [557/938], Loss: 0.8320910930633545\n",
      "Validation: Epoch [15], Batch [558/938], Loss: 0.8041669726371765\n",
      "Validation: Epoch [15], Batch [559/938], Loss: 0.5725782513618469\n",
      "Validation: Epoch [15], Batch [560/938], Loss: 1.0851258039474487\n",
      "Validation: Epoch [15], Batch [561/938], Loss: 0.8164461255073547\n",
      "Validation: Epoch [15], Batch [562/938], Loss: 0.8422579765319824\n",
      "Validation: Epoch [15], Batch [563/938], Loss: 0.7548843622207642\n",
      "Validation: Epoch [15], Batch [564/938], Loss: 0.7003992795944214\n",
      "Validation: Epoch [15], Batch [565/938], Loss: 0.47820791602134705\n",
      "Validation: Epoch [15], Batch [566/938], Loss: 0.6075383424758911\n",
      "Validation: Epoch [15], Batch [567/938], Loss: 0.49662330746650696\n",
      "Validation: Epoch [15], Batch [568/938], Loss: 0.6142166256904602\n",
      "Validation: Epoch [15], Batch [569/938], Loss: 0.6982473731040955\n",
      "Validation: Epoch [15], Batch [570/938], Loss: 0.6847814321517944\n",
      "Validation: Epoch [15], Batch [571/938], Loss: 0.6144319772720337\n",
      "Validation: Epoch [15], Batch [572/938], Loss: 0.54422527551651\n",
      "Validation: Epoch [15], Batch [573/938], Loss: 0.8206039071083069\n",
      "Validation: Epoch [15], Batch [574/938], Loss: 0.8510436415672302\n",
      "Validation: Epoch [15], Batch [575/938], Loss: 0.5701628923416138\n",
      "Validation: Epoch [15], Batch [576/938], Loss: 0.5688108801841736\n",
      "Validation: Epoch [15], Batch [577/938], Loss: 0.770819902420044\n",
      "Validation: Epoch [15], Batch [578/938], Loss: 0.9391340613365173\n",
      "Validation: Epoch [15], Batch [579/938], Loss: 0.7610711455345154\n",
      "Validation: Epoch [15], Batch [580/938], Loss: 0.5994477868080139\n",
      "Validation: Epoch [15], Batch [581/938], Loss: 0.7005397081375122\n",
      "Validation: Epoch [15], Batch [582/938], Loss: 0.8331435322761536\n",
      "Validation: Epoch [15], Batch [583/938], Loss: 0.7328009605407715\n",
      "Validation: Epoch [15], Batch [584/938], Loss: 0.6748239994049072\n",
      "Validation: Epoch [15], Batch [585/938], Loss: 0.5685280561447144\n",
      "Validation: Epoch [15], Batch [586/938], Loss: 0.7859045267105103\n",
      "Validation: Epoch [15], Batch [587/938], Loss: 0.7005318403244019\n",
      "Validation: Epoch [15], Batch [588/938], Loss: 0.9850518107414246\n",
      "Validation: Epoch [15], Batch [589/938], Loss: 0.8600382208824158\n",
      "Validation: Epoch [15], Batch [590/938], Loss: 0.8024144768714905\n",
      "Validation: Epoch [15], Batch [591/938], Loss: 0.6345959305763245\n",
      "Validation: Epoch [15], Batch [592/938], Loss: 0.6798977851867676\n",
      "Validation: Epoch [15], Batch [593/938], Loss: 0.7788011431694031\n",
      "Validation: Epoch [15], Batch [594/938], Loss: 0.44836562871932983\n",
      "Validation: Epoch [15], Batch [595/938], Loss: 0.9651321768760681\n",
      "Validation: Epoch [15], Batch [596/938], Loss: 0.6544893980026245\n",
      "Validation: Epoch [15], Batch [597/938], Loss: 0.5278916954994202\n",
      "Validation: Epoch [15], Batch [598/938], Loss: 0.8570222854614258\n",
      "Validation: Epoch [15], Batch [599/938], Loss: 0.7650523781776428\n",
      "Validation: Epoch [15], Batch [600/938], Loss: 0.7968966960906982\n",
      "Validation: Epoch [15], Batch [601/938], Loss: 0.5613542199134827\n",
      "Validation: Epoch [15], Batch [602/938], Loss: 0.7016916275024414\n",
      "Validation: Epoch [15], Batch [603/938], Loss: 0.7840989232063293\n",
      "Validation: Epoch [15], Batch [604/938], Loss: 1.0371692180633545\n",
      "Validation: Epoch [15], Batch [605/938], Loss: 0.5533255934715271\n",
      "Validation: Epoch [15], Batch [606/938], Loss: 0.8112991452217102\n",
      "Validation: Epoch [15], Batch [607/938], Loss: 0.6100634932518005\n",
      "Validation: Epoch [15], Batch [608/938], Loss: 0.6851125955581665\n",
      "Validation: Epoch [15], Batch [609/938], Loss: 0.8674349784851074\n",
      "Validation: Epoch [15], Batch [610/938], Loss: 0.7322713732719421\n",
      "Validation: Epoch [15], Batch [611/938], Loss: 0.6536427736282349\n",
      "Validation: Epoch [15], Batch [612/938], Loss: 0.5107941031455994\n",
      "Validation: Epoch [15], Batch [613/938], Loss: 0.7662220001220703\n",
      "Validation: Epoch [15], Batch [614/938], Loss: 0.7882997393608093\n",
      "Validation: Epoch [15], Batch [615/938], Loss: 0.4633249044418335\n",
      "Validation: Epoch [15], Batch [616/938], Loss: 0.428949236869812\n",
      "Validation: Epoch [15], Batch [617/938], Loss: 0.5007839202880859\n",
      "Validation: Epoch [15], Batch [618/938], Loss: 0.4146091639995575\n",
      "Validation: Epoch [15], Batch [619/938], Loss: 0.5078411102294922\n",
      "Validation: Epoch [15], Batch [620/938], Loss: 0.5051106810569763\n",
      "Validation: Epoch [15], Batch [621/938], Loss: 0.8409643173217773\n",
      "Validation: Epoch [15], Batch [622/938], Loss: 0.6770821213722229\n",
      "Validation: Epoch [15], Batch [623/938], Loss: 0.7074159383773804\n",
      "Validation: Epoch [15], Batch [624/938], Loss: 0.6405401229858398\n",
      "Validation: Epoch [15], Batch [625/938], Loss: 0.6198546886444092\n",
      "Validation: Epoch [15], Batch [626/938], Loss: 0.7131252884864807\n",
      "Validation: Epoch [15], Batch [627/938], Loss: 0.9719055891036987\n",
      "Validation: Epoch [15], Batch [628/938], Loss: 0.5593706965446472\n",
      "Validation: Epoch [15], Batch [629/938], Loss: 0.7907834649085999\n",
      "Validation: Epoch [15], Batch [630/938], Loss: 0.6801631450653076\n",
      "Validation: Epoch [15], Batch [631/938], Loss: 0.7055474519729614\n",
      "Validation: Epoch [15], Batch [632/938], Loss: 0.6361183524131775\n",
      "Validation: Epoch [15], Batch [633/938], Loss: 0.651130735874176\n",
      "Validation: Epoch [15], Batch [634/938], Loss: 0.8750892877578735\n",
      "Validation: Epoch [15], Batch [635/938], Loss: 0.8953597545623779\n",
      "Validation: Epoch [15], Batch [636/938], Loss: 1.1084390878677368\n",
      "Validation: Epoch [15], Batch [637/938], Loss: 0.6943727731704712\n",
      "Validation: Epoch [15], Batch [638/938], Loss: 0.7951468229293823\n",
      "Validation: Epoch [15], Batch [639/938], Loss: 0.7024782299995422\n",
      "Validation: Epoch [15], Batch [640/938], Loss: 0.8126932978630066\n",
      "Validation: Epoch [15], Batch [641/938], Loss: 0.5848039984703064\n",
      "Validation: Epoch [15], Batch [642/938], Loss: 0.6825762987136841\n",
      "Validation: Epoch [15], Batch [643/938], Loss: 0.718408465385437\n",
      "Validation: Epoch [15], Batch [644/938], Loss: 0.8359099626541138\n",
      "Validation: Epoch [15], Batch [645/938], Loss: 0.7902314066886902\n",
      "Validation: Epoch [15], Batch [646/938], Loss: 0.7414042353630066\n",
      "Validation: Epoch [15], Batch [647/938], Loss: 0.6022235155105591\n",
      "Validation: Epoch [15], Batch [648/938], Loss: 0.7496504187583923\n",
      "Validation: Epoch [15], Batch [649/938], Loss: 0.6767145395278931\n",
      "Validation: Epoch [15], Batch [650/938], Loss: 0.6684963703155518\n",
      "Validation: Epoch [15], Batch [651/938], Loss: 0.8216652274131775\n",
      "Validation: Epoch [15], Batch [652/938], Loss: 0.7344474196434021\n",
      "Validation: Epoch [15], Batch [653/938], Loss: 0.4377448856830597\n",
      "Validation: Epoch [15], Batch [654/938], Loss: 0.5729122757911682\n",
      "Validation: Epoch [15], Batch [655/938], Loss: 0.9477008581161499\n",
      "Validation: Epoch [15], Batch [656/938], Loss: 0.7039772272109985\n",
      "Validation: Epoch [15], Batch [657/938], Loss: 0.6098661422729492\n",
      "Validation: Epoch [15], Batch [658/938], Loss: 0.7921766638755798\n",
      "Validation: Epoch [15], Batch [659/938], Loss: 0.6790376901626587\n",
      "Validation: Epoch [15], Batch [660/938], Loss: 0.5928817987442017\n",
      "Validation: Epoch [15], Batch [661/938], Loss: 0.7304134368896484\n",
      "Validation: Epoch [15], Batch [662/938], Loss: 0.4655487835407257\n",
      "Validation: Epoch [15], Batch [663/938], Loss: 0.6792562007904053\n",
      "Validation: Epoch [15], Batch [664/938], Loss: 0.8592817187309265\n",
      "Validation: Epoch [15], Batch [665/938], Loss: 0.4687262773513794\n",
      "Validation: Epoch [15], Batch [666/938], Loss: 0.5616404414176941\n",
      "Validation: Epoch [15], Batch [667/938], Loss: 0.7305572032928467\n",
      "Validation: Epoch [15], Batch [668/938], Loss: 0.766798734664917\n",
      "Validation: Epoch [15], Batch [669/938], Loss: 0.6135545969009399\n",
      "Validation: Epoch [15], Batch [670/938], Loss: 0.6985989809036255\n",
      "Validation: Epoch [15], Batch [671/938], Loss: 0.500351071357727\n",
      "Validation: Epoch [15], Batch [672/938], Loss: 0.4991569519042969\n",
      "Validation: Epoch [15], Batch [673/938], Loss: 0.511020302772522\n",
      "Validation: Epoch [15], Batch [674/938], Loss: 0.8704448938369751\n",
      "Validation: Epoch [15], Batch [675/938], Loss: 0.5757339596748352\n",
      "Validation: Epoch [15], Batch [676/938], Loss: 0.8157676458358765\n",
      "Validation: Epoch [15], Batch [677/938], Loss: 0.609358549118042\n",
      "Validation: Epoch [15], Batch [678/938], Loss: 0.6459822654724121\n",
      "Validation: Epoch [15], Batch [679/938], Loss: 0.5490543246269226\n",
      "Validation: Epoch [15], Batch [680/938], Loss: 0.5212790966033936\n",
      "Validation: Epoch [15], Batch [681/938], Loss: 0.6117714047431946\n",
      "Validation: Epoch [15], Batch [682/938], Loss: 0.6872202157974243\n",
      "Validation: Epoch [15], Batch [683/938], Loss: 0.8545641899108887\n",
      "Validation: Epoch [15], Batch [684/938], Loss: 0.6284547448158264\n",
      "Validation: Epoch [15], Batch [685/938], Loss: 0.6886674761772156\n",
      "Validation: Epoch [15], Batch [686/938], Loss: 0.560323178768158\n",
      "Validation: Epoch [15], Batch [687/938], Loss: 0.7430500984191895\n",
      "Validation: Epoch [15], Batch [688/938], Loss: 0.6609948873519897\n",
      "Validation: Epoch [15], Batch [689/938], Loss: 0.7274202704429626\n",
      "Validation: Epoch [15], Batch [690/938], Loss: 0.6367669701576233\n",
      "Validation: Epoch [15], Batch [691/938], Loss: 0.7468887567520142\n",
      "Validation: Epoch [15], Batch [692/938], Loss: 0.7154295444488525\n",
      "Validation: Epoch [15], Batch [693/938], Loss: 0.7348438501358032\n",
      "Validation: Epoch [15], Batch [694/938], Loss: 0.624931812286377\n",
      "Validation: Epoch [15], Batch [695/938], Loss: 0.8635013103485107\n",
      "Validation: Epoch [15], Batch [696/938], Loss: 0.7104421257972717\n",
      "Validation: Epoch [15], Batch [697/938], Loss: 0.6734907031059265\n",
      "Validation: Epoch [15], Batch [698/938], Loss: 0.5806270241737366\n",
      "Validation: Epoch [15], Batch [699/938], Loss: 0.7770291566848755\n",
      "Validation: Epoch [15], Batch [700/938], Loss: 0.48924919962882996\n",
      "Validation: Epoch [15], Batch [701/938], Loss: 0.8628115653991699\n",
      "Validation: Epoch [15], Batch [702/938], Loss: 0.716488778591156\n",
      "Validation: Epoch [15], Batch [703/938], Loss: 0.8666311502456665\n",
      "Validation: Epoch [15], Batch [704/938], Loss: 0.7943916320800781\n",
      "Validation: Epoch [15], Batch [705/938], Loss: 0.5153287649154663\n",
      "Validation: Epoch [15], Batch [706/938], Loss: 0.5349332094192505\n",
      "Validation: Epoch [15], Batch [707/938], Loss: 0.6315845251083374\n",
      "Validation: Epoch [15], Batch [708/938], Loss: 0.4620635211467743\n",
      "Validation: Epoch [15], Batch [709/938], Loss: 0.622380256652832\n",
      "Validation: Epoch [15], Batch [710/938], Loss: 0.6483591198921204\n",
      "Validation: Epoch [15], Batch [711/938], Loss: 0.5580240488052368\n",
      "Validation: Epoch [15], Batch [712/938], Loss: 0.6327850222587585\n",
      "Validation: Epoch [15], Batch [713/938], Loss: 0.9491481781005859\n",
      "Validation: Epoch [15], Batch [714/938], Loss: 0.8322138786315918\n",
      "Validation: Epoch [15], Batch [715/938], Loss: 0.539373517036438\n",
      "Validation: Epoch [15], Batch [716/938], Loss: 0.8926249146461487\n",
      "Validation: Epoch [15], Batch [717/938], Loss: 0.6413337588310242\n",
      "Validation: Epoch [15], Batch [718/938], Loss: 0.6847297549247742\n",
      "Validation: Epoch [15], Batch [719/938], Loss: 0.5483958721160889\n",
      "Validation: Epoch [15], Batch [720/938], Loss: 0.6048283576965332\n",
      "Validation: Epoch [15], Batch [721/938], Loss: 0.7338991761207581\n",
      "Validation: Epoch [15], Batch [722/938], Loss: 0.5964338779449463\n",
      "Validation: Epoch [15], Batch [723/938], Loss: 0.72150719165802\n",
      "Validation: Epoch [15], Batch [724/938], Loss: 0.6131768226623535\n",
      "Validation: Epoch [15], Batch [725/938], Loss: 0.9036482572555542\n",
      "Validation: Epoch [15], Batch [726/938], Loss: 0.7289244532585144\n",
      "Validation: Epoch [15], Batch [727/938], Loss: 0.8144176006317139\n",
      "Validation: Epoch [15], Batch [728/938], Loss: 0.5573994517326355\n",
      "Validation: Epoch [15], Batch [729/938], Loss: 0.7993992567062378\n",
      "Validation: Epoch [15], Batch [730/938], Loss: 0.5137038826942444\n",
      "Validation: Epoch [15], Batch [731/938], Loss: 0.5723921656608582\n",
      "Validation: Epoch [15], Batch [732/938], Loss: 0.727404773235321\n",
      "Validation: Epoch [15], Batch [733/938], Loss: 0.8909309506416321\n",
      "Validation: Epoch [15], Batch [734/938], Loss: 0.6917147040367126\n",
      "Validation: Epoch [15], Batch [735/938], Loss: 0.8040940761566162\n",
      "Validation: Epoch [15], Batch [736/938], Loss: 0.618614912033081\n",
      "Validation: Epoch [15], Batch [737/938], Loss: 0.7588093876838684\n",
      "Validation: Epoch [15], Batch [738/938], Loss: 0.7505488395690918\n",
      "Validation: Epoch [15], Batch [739/938], Loss: 0.5332359075546265\n",
      "Validation: Epoch [15], Batch [740/938], Loss: 0.7527567148208618\n",
      "Validation: Epoch [15], Batch [741/938], Loss: 0.6963497996330261\n",
      "Validation: Epoch [15], Batch [742/938], Loss: 0.759417712688446\n",
      "Validation: Epoch [15], Batch [743/938], Loss: 0.6125982999801636\n",
      "Validation: Epoch [15], Batch [744/938], Loss: 0.7042437791824341\n",
      "Validation: Epoch [15], Batch [745/938], Loss: 0.6106353998184204\n",
      "Validation: Epoch [15], Batch [746/938], Loss: 0.6379966735839844\n",
      "Validation: Epoch [15], Batch [747/938], Loss: 0.5716065168380737\n",
      "Validation: Epoch [15], Batch [748/938], Loss: 0.6604854464530945\n",
      "Validation: Epoch [15], Batch [749/938], Loss: 0.7131601572036743\n",
      "Validation: Epoch [15], Batch [750/938], Loss: 0.48529335856437683\n",
      "Validation: Epoch [15], Batch [751/938], Loss: 0.6643819808959961\n",
      "Validation: Epoch [15], Batch [752/938], Loss: 0.5392060279846191\n",
      "Validation: Epoch [15], Batch [753/938], Loss: 0.5811495780944824\n",
      "Validation: Epoch [15], Batch [754/938], Loss: 0.6781269311904907\n",
      "Validation: Epoch [15], Batch [755/938], Loss: 0.6101430058479309\n",
      "Validation: Epoch [15], Batch [756/938], Loss: 0.775066077709198\n",
      "Validation: Epoch [15], Batch [757/938], Loss: 0.5391967296600342\n",
      "Validation: Epoch [15], Batch [758/938], Loss: 0.7166720628738403\n",
      "Validation: Epoch [15], Batch [759/938], Loss: 0.7230538129806519\n",
      "Validation: Epoch [15], Batch [760/938], Loss: 0.5933614373207092\n",
      "Validation: Epoch [15], Batch [761/938], Loss: 0.8534786701202393\n",
      "Validation: Epoch [15], Batch [762/938], Loss: 0.6008659601211548\n",
      "Validation: Epoch [15], Batch [763/938], Loss: 0.5495532751083374\n",
      "Validation: Epoch [15], Batch [764/938], Loss: 0.8653990030288696\n",
      "Validation: Epoch [15], Batch [765/938], Loss: 0.5481440424919128\n",
      "Validation: Epoch [15], Batch [766/938], Loss: 0.41282156109809875\n",
      "Validation: Epoch [15], Batch [767/938], Loss: 0.7613391876220703\n",
      "Validation: Epoch [15], Batch [768/938], Loss: 0.7720215916633606\n",
      "Validation: Epoch [15], Batch [769/938], Loss: 0.645312488079071\n",
      "Validation: Epoch [15], Batch [770/938], Loss: 1.0314568281173706\n",
      "Validation: Epoch [15], Batch [771/938], Loss: 0.9056887626647949\n",
      "Validation: Epoch [15], Batch [772/938], Loss: 0.6826232671737671\n",
      "Validation: Epoch [15], Batch [773/938], Loss: 0.7766993641853333\n",
      "Validation: Epoch [15], Batch [774/938], Loss: 0.7521249651908875\n",
      "Validation: Epoch [15], Batch [775/938], Loss: 0.7340878248214722\n",
      "Validation: Epoch [15], Batch [776/938], Loss: 0.6841140985488892\n",
      "Validation: Epoch [15], Batch [777/938], Loss: 0.8324249982833862\n",
      "Validation: Epoch [15], Batch [778/938], Loss: 0.6261190176010132\n",
      "Validation: Epoch [15], Batch [779/938], Loss: 0.6831966638565063\n",
      "Validation: Epoch [15], Batch [780/938], Loss: 0.7084610462188721\n",
      "Validation: Epoch [15], Batch [781/938], Loss: 0.8517928123474121\n",
      "Validation: Epoch [15], Batch [782/938], Loss: 0.8859920501708984\n",
      "Validation: Epoch [15], Batch [783/938], Loss: 0.8101975917816162\n",
      "Validation: Epoch [15], Batch [784/938], Loss: 0.6374799609184265\n",
      "Validation: Epoch [15], Batch [785/938], Loss: 0.7330659031867981\n",
      "Validation: Epoch [15], Batch [786/938], Loss: 0.47763362526893616\n",
      "Validation: Epoch [15], Batch [787/938], Loss: 0.6703368425369263\n",
      "Validation: Epoch [15], Batch [788/938], Loss: 0.6078354716300964\n",
      "Validation: Epoch [15], Batch [789/938], Loss: 0.6928243637084961\n",
      "Validation: Epoch [15], Batch [790/938], Loss: 0.8459845781326294\n",
      "Validation: Epoch [15], Batch [791/938], Loss: 0.6631483435630798\n",
      "Validation: Epoch [15], Batch [792/938], Loss: 0.8917529582977295\n",
      "Validation: Epoch [15], Batch [793/938], Loss: 0.46248796582221985\n",
      "Validation: Epoch [15], Batch [794/938], Loss: 0.6285200119018555\n",
      "Validation: Epoch [15], Batch [795/938], Loss: 0.7431356310844421\n",
      "Validation: Epoch [15], Batch [796/938], Loss: 1.0875604152679443\n",
      "Validation: Epoch [15], Batch [797/938], Loss: 0.832690954208374\n",
      "Validation: Epoch [15], Batch [798/938], Loss: 0.7164514064788818\n",
      "Validation: Epoch [15], Batch [799/938], Loss: 0.7062423229217529\n",
      "Validation: Epoch [15], Batch [800/938], Loss: 0.3866308629512787\n",
      "Validation: Epoch [15], Batch [801/938], Loss: 0.7256077527999878\n",
      "Validation: Epoch [15], Batch [802/938], Loss: 0.4884759187698364\n",
      "Validation: Epoch [15], Batch [803/938], Loss: 0.6465224027633667\n",
      "Validation: Epoch [15], Batch [804/938], Loss: 0.6873370409011841\n",
      "Validation: Epoch [15], Batch [805/938], Loss: 0.45736968517303467\n",
      "Validation: Epoch [15], Batch [806/938], Loss: 0.7120790481567383\n",
      "Validation: Epoch [15], Batch [807/938], Loss: 0.587335467338562\n",
      "Validation: Epoch [15], Batch [808/938], Loss: 0.7329823970794678\n",
      "Validation: Epoch [15], Batch [809/938], Loss: 0.7459899187088013\n",
      "Validation: Epoch [15], Batch [810/938], Loss: 0.9005557894706726\n",
      "Validation: Epoch [15], Batch [811/938], Loss: 0.626366376876831\n",
      "Validation: Epoch [15], Batch [812/938], Loss: 0.8146998882293701\n",
      "Validation: Epoch [15], Batch [813/938], Loss: 0.48880016803741455\n",
      "Validation: Epoch [15], Batch [814/938], Loss: 0.5025867819786072\n",
      "Validation: Epoch [15], Batch [815/938], Loss: 0.661916971206665\n",
      "Validation: Epoch [15], Batch [816/938], Loss: 0.8015804290771484\n",
      "Validation: Epoch [15], Batch [817/938], Loss: 0.8192773461341858\n",
      "Validation: Epoch [15], Batch [818/938], Loss: 0.7494747638702393\n",
      "Validation: Epoch [15], Batch [819/938], Loss: 0.6256682276725769\n",
      "Validation: Epoch [15], Batch [820/938], Loss: 0.655948281288147\n",
      "Validation: Epoch [15], Batch [821/938], Loss: 0.7690039277076721\n",
      "Validation: Epoch [15], Batch [822/938], Loss: 0.7079042792320251\n",
      "Validation: Epoch [15], Batch [823/938], Loss: 0.853010356426239\n",
      "Validation: Epoch [15], Batch [824/938], Loss: 0.7116245627403259\n",
      "Validation: Epoch [15], Batch [825/938], Loss: 0.74488365650177\n",
      "Validation: Epoch [15], Batch [826/938], Loss: 0.7598254680633545\n",
      "Validation: Epoch [15], Batch [827/938], Loss: 0.5030723214149475\n",
      "Validation: Epoch [15], Batch [828/938], Loss: 0.7755263447761536\n",
      "Validation: Epoch [15], Batch [829/938], Loss: 0.8356797695159912\n",
      "Validation: Epoch [15], Batch [830/938], Loss: 0.8166024088859558\n",
      "Validation: Epoch [15], Batch [831/938], Loss: 0.7546515464782715\n",
      "Validation: Epoch [15], Batch [832/938], Loss: 0.7534219026565552\n",
      "Validation: Epoch [15], Batch [833/938], Loss: 0.5969000458717346\n",
      "Validation: Epoch [15], Batch [834/938], Loss: 0.6641559600830078\n",
      "Validation: Epoch [15], Batch [835/938], Loss: 0.5398941040039062\n",
      "Validation: Epoch [15], Batch [836/938], Loss: 0.5817769169807434\n",
      "Validation: Epoch [15], Batch [837/938], Loss: 0.5002723932266235\n",
      "Validation: Epoch [15], Batch [838/938], Loss: 0.9151710867881775\n",
      "Validation: Epoch [15], Batch [839/938], Loss: 0.8564755916595459\n",
      "Validation: Epoch [15], Batch [840/938], Loss: 0.6262307167053223\n",
      "Validation: Epoch [15], Batch [841/938], Loss: 0.7338134050369263\n",
      "Validation: Epoch [15], Batch [842/938], Loss: 0.7714567184448242\n",
      "Validation: Epoch [15], Batch [843/938], Loss: 0.635389506816864\n",
      "Validation: Epoch [15], Batch [844/938], Loss: 0.6230836510658264\n",
      "Validation: Epoch [15], Batch [845/938], Loss: 0.7060991525650024\n",
      "Validation: Epoch [15], Batch [846/938], Loss: 0.8340778946876526\n",
      "Validation: Epoch [15], Batch [847/938], Loss: 0.5244826078414917\n",
      "Validation: Epoch [15], Batch [848/938], Loss: 0.7681356072425842\n",
      "Validation: Epoch [15], Batch [849/938], Loss: 0.7146109342575073\n",
      "Validation: Epoch [15], Batch [850/938], Loss: 0.896597683429718\n",
      "Validation: Epoch [15], Batch [851/938], Loss: 0.7556499242782593\n",
      "Validation: Epoch [15], Batch [852/938], Loss: 0.5190821886062622\n",
      "Validation: Epoch [15], Batch [853/938], Loss: 0.7603196501731873\n",
      "Validation: Epoch [15], Batch [854/938], Loss: 0.5943197011947632\n",
      "Validation: Epoch [15], Batch [855/938], Loss: 0.8739026784896851\n",
      "Validation: Epoch [15], Batch [856/938], Loss: 0.49436643719673157\n",
      "Validation: Epoch [15], Batch [857/938], Loss: 0.5861552357673645\n",
      "Validation: Epoch [15], Batch [858/938], Loss: 0.4658627510070801\n",
      "Validation: Epoch [15], Batch [859/938], Loss: 0.7306414246559143\n",
      "Validation: Epoch [15], Batch [860/938], Loss: 0.6182688474655151\n",
      "Validation: Epoch [15], Batch [861/938], Loss: 0.7728424072265625\n",
      "Validation: Epoch [15], Batch [862/938], Loss: 1.0051453113555908\n",
      "Validation: Epoch [15], Batch [863/938], Loss: 0.7514904141426086\n",
      "Validation: Epoch [15], Batch [864/938], Loss: 0.5431101322174072\n",
      "Validation: Epoch [15], Batch [865/938], Loss: 0.7150923609733582\n",
      "Validation: Epoch [15], Batch [866/938], Loss: 0.7082814574241638\n",
      "Validation: Epoch [15], Batch [867/938], Loss: 0.745391845703125\n",
      "Validation: Epoch [15], Batch [868/938], Loss: 0.6036021113395691\n",
      "Validation: Epoch [15], Batch [869/938], Loss: 0.8488452434539795\n",
      "Validation: Epoch [15], Batch [870/938], Loss: 0.6522872447967529\n",
      "Validation: Epoch [15], Batch [871/938], Loss: 0.5828429460525513\n",
      "Validation: Epoch [15], Batch [872/938], Loss: 0.7050790190696716\n",
      "Validation: Epoch [15], Batch [873/938], Loss: 0.7610764503479004\n",
      "Validation: Epoch [15], Batch [874/938], Loss: 0.5993579030036926\n",
      "Validation: Epoch [15], Batch [875/938], Loss: 0.5572562217712402\n",
      "Validation: Epoch [15], Batch [876/938], Loss: 0.6338920593261719\n",
      "Validation: Epoch [15], Batch [877/938], Loss: 0.8476034998893738\n",
      "Validation: Epoch [15], Batch [878/938], Loss: 0.6112492084503174\n",
      "Validation: Epoch [15], Batch [879/938], Loss: 0.6825616955757141\n",
      "Validation: Epoch [15], Batch [880/938], Loss: 0.7113245725631714\n",
      "Validation: Epoch [15], Batch [881/938], Loss: 0.933671772480011\n",
      "Validation: Epoch [15], Batch [882/938], Loss: 0.5961973667144775\n",
      "Validation: Epoch [15], Batch [883/938], Loss: 0.7533875703811646\n",
      "Validation: Epoch [15], Batch [884/938], Loss: 0.681361198425293\n",
      "Validation: Epoch [15], Batch [885/938], Loss: 0.7114490270614624\n",
      "Validation: Epoch [15], Batch [886/938], Loss: 0.7473463416099548\n",
      "Validation: Epoch [15], Batch [887/938], Loss: 0.9689331650733948\n",
      "Validation: Epoch [15], Batch [888/938], Loss: 0.7937233448028564\n",
      "Validation: Epoch [15], Batch [889/938], Loss: 0.7378640174865723\n",
      "Validation: Epoch [15], Batch [890/938], Loss: 0.8496274352073669\n",
      "Validation: Epoch [15], Batch [891/938], Loss: 0.7667816877365112\n",
      "Validation: Epoch [15], Batch [892/938], Loss: 0.6730982661247253\n",
      "Validation: Epoch [15], Batch [893/938], Loss: 0.8402398824691772\n",
      "Validation: Epoch [15], Batch [894/938], Loss: 0.6367062926292419\n",
      "Validation: Epoch [15], Batch [895/938], Loss: 0.574232816696167\n",
      "Validation: Epoch [15], Batch [896/938], Loss: 0.7168959379196167\n",
      "Validation: Epoch [15], Batch [897/938], Loss: 0.8408856391906738\n",
      "Validation: Epoch [15], Batch [898/938], Loss: 0.7434608936309814\n",
      "Validation: Epoch [15], Batch [899/938], Loss: 0.8601775765419006\n",
      "Validation: Epoch [15], Batch [900/938], Loss: 0.43517008423805237\n",
      "Validation: Epoch [15], Batch [901/938], Loss: 0.6782277226448059\n",
      "Validation: Epoch [15], Batch [902/938], Loss: 0.42349809408187866\n",
      "Validation: Epoch [15], Batch [903/938], Loss: 0.7664604783058167\n",
      "Validation: Epoch [15], Batch [904/938], Loss: 0.6148542165756226\n",
      "Validation: Epoch [15], Batch [905/938], Loss: 0.6776167154312134\n",
      "Validation: Epoch [15], Batch [906/938], Loss: 0.7499797940254211\n",
      "Validation: Epoch [15], Batch [907/938], Loss: 0.6356898546218872\n",
      "Validation: Epoch [15], Batch [908/938], Loss: 0.5545390844345093\n",
      "Validation: Epoch [15], Batch [909/938], Loss: 0.7514433264732361\n",
      "Validation: Epoch [15], Batch [910/938], Loss: 0.73471599817276\n",
      "Validation: Epoch [15], Batch [911/938], Loss: 0.7623331546783447\n",
      "Validation: Epoch [15], Batch [912/938], Loss: 0.6970402002334595\n",
      "Validation: Epoch [15], Batch [913/938], Loss: 0.801110565662384\n",
      "Validation: Epoch [15], Batch [914/938], Loss: 0.6127621531486511\n",
      "Validation: Epoch [15], Batch [915/938], Loss: 0.6147136688232422\n",
      "Validation: Epoch [15], Batch [916/938], Loss: 1.0232652425765991\n",
      "Validation: Epoch [15], Batch [917/938], Loss: 0.9284674525260925\n",
      "Validation: Epoch [15], Batch [918/938], Loss: 0.6949034929275513\n",
      "Validation: Epoch [15], Batch [919/938], Loss: 0.7547180652618408\n",
      "Validation: Epoch [15], Batch [920/938], Loss: 0.40509992837905884\n",
      "Validation: Epoch [15], Batch [921/938], Loss: 0.7482672333717346\n",
      "Validation: Epoch [15], Batch [922/938], Loss: 0.6762951612472534\n",
      "Validation: Epoch [15], Batch [923/938], Loss: 0.7616800665855408\n",
      "Validation: Epoch [15], Batch [924/938], Loss: 0.510858952999115\n",
      "Validation: Epoch [15], Batch [925/938], Loss: 0.9018428325653076\n",
      "Validation: Epoch [15], Batch [926/938], Loss: 0.6885371804237366\n",
      "Validation: Epoch [15], Batch [927/938], Loss: 0.5669714212417603\n",
      "Validation: Epoch [15], Batch [928/938], Loss: 0.8691983819007874\n",
      "Validation: Epoch [15], Batch [929/938], Loss: 0.5304704308509827\n",
      "Validation: Epoch [15], Batch [930/938], Loss: 0.638187050819397\n",
      "Validation: Epoch [15], Batch [931/938], Loss: 0.7614731788635254\n",
      "Validation: Epoch [15], Batch [932/938], Loss: 0.7495933771133423\n",
      "Validation: Epoch [15], Batch [933/938], Loss: 0.5695496797561646\n",
      "Validation: Epoch [15], Batch [934/938], Loss: 0.6932759881019592\n",
      "Validation: Epoch [15], Batch [935/938], Loss: 0.7319685220718384\n",
      "Validation: Epoch [15], Batch [936/938], Loss: 0.48767757415771484\n",
      "Validation: Epoch [15], Batch [937/938], Loss: 0.5518923401832581\n",
      "Validation: Epoch [15], Batch [938/938], Loss: 0.6703360080718994\n",
      "Accuracy of test set: 0.7999\n",
      "Train: Epoch [16], Batch [1/938], Loss: 0.7170276045799255\n",
      "Train: Epoch [16], Batch [2/938], Loss: 0.6403030753135681\n",
      "Train: Epoch [16], Batch [3/938], Loss: 0.6499760150909424\n",
      "Train: Epoch [16], Batch [4/938], Loss: 0.8515094518661499\n",
      "Train: Epoch [16], Batch [5/938], Loss: 0.8715939521789551\n",
      "Train: Epoch [16], Batch [6/938], Loss: 0.7483980655670166\n",
      "Train: Epoch [16], Batch [7/938], Loss: 0.5997456312179565\n",
      "Train: Epoch [16], Batch [8/938], Loss: 0.6335107684135437\n",
      "Train: Epoch [16], Batch [9/938], Loss: 0.8141402006149292\n",
      "Train: Epoch [16], Batch [10/938], Loss: 0.6533429026603699\n",
      "Train: Epoch [16], Batch [11/938], Loss: 0.5246396660804749\n",
      "Train: Epoch [16], Batch [12/938], Loss: 0.7661514282226562\n",
      "Train: Epoch [16], Batch [13/938], Loss: 0.4720160961151123\n",
      "Train: Epoch [16], Batch [14/938], Loss: 0.8876996040344238\n",
      "Train: Epoch [16], Batch [15/938], Loss: 0.7028258442878723\n",
      "Train: Epoch [16], Batch [16/938], Loss: 0.4959212839603424\n",
      "Train: Epoch [16], Batch [17/938], Loss: 0.8033345937728882\n",
      "Train: Epoch [16], Batch [18/938], Loss: 0.7313938140869141\n",
      "Train: Epoch [16], Batch [19/938], Loss: 0.798245906829834\n",
      "Train: Epoch [16], Batch [20/938], Loss: 0.5150146484375\n",
      "Train: Epoch [16], Batch [21/938], Loss: 0.6531469225883484\n",
      "Train: Epoch [16], Batch [22/938], Loss: 0.7093011140823364\n",
      "Train: Epoch [16], Batch [23/938], Loss: 1.0759150981903076\n",
      "Train: Epoch [16], Batch [24/938], Loss: 0.5688349604606628\n",
      "Train: Epoch [16], Batch [25/938], Loss: 0.7202125191688538\n",
      "Train: Epoch [16], Batch [26/938], Loss: 0.5890003442764282\n",
      "Train: Epoch [16], Batch [27/938], Loss: 0.8112351894378662\n",
      "Train: Epoch [16], Batch [28/938], Loss: 0.5389640927314758\n",
      "Train: Epoch [16], Batch [29/938], Loss: 0.6128845810890198\n",
      "Train: Epoch [16], Batch [30/938], Loss: 0.5860005617141724\n",
      "Train: Epoch [16], Batch [31/938], Loss: 0.7960878014564514\n",
      "Train: Epoch [16], Batch [32/938], Loss: 0.6263494491577148\n",
      "Train: Epoch [16], Batch [33/938], Loss: 0.7007456421852112\n",
      "Train: Epoch [16], Batch [34/938], Loss: 0.6391412615776062\n",
      "Train: Epoch [16], Batch [35/938], Loss: 0.664772629737854\n",
      "Train: Epoch [16], Batch [36/938], Loss: 0.727856457233429\n",
      "Train: Epoch [16], Batch [37/938], Loss: 0.6150919795036316\n",
      "Train: Epoch [16], Batch [38/938], Loss: 0.5590778589248657\n",
      "Train: Epoch [16], Batch [39/938], Loss: 0.6791262626647949\n",
      "Train: Epoch [16], Batch [40/938], Loss: 0.8429805636405945\n",
      "Train: Epoch [16], Batch [41/938], Loss: 0.8391466736793518\n",
      "Train: Epoch [16], Batch [42/938], Loss: 0.5831058025360107\n",
      "Train: Epoch [16], Batch [43/938], Loss: 0.6724263429641724\n",
      "Train: Epoch [16], Batch [44/938], Loss: 0.6281875967979431\n",
      "Train: Epoch [16], Batch [45/938], Loss: 0.7826951742172241\n",
      "Train: Epoch [16], Batch [46/938], Loss: 0.6855045557022095\n",
      "Train: Epoch [16], Batch [47/938], Loss: 0.7412737011909485\n",
      "Train: Epoch [16], Batch [48/938], Loss: 0.7007442116737366\n",
      "Train: Epoch [16], Batch [49/938], Loss: 0.9286984205245972\n",
      "Train: Epoch [16], Batch [50/938], Loss: 0.6654484272003174\n",
      "Train: Epoch [16], Batch [51/938], Loss: 0.7284197807312012\n",
      "Train: Epoch [16], Batch [52/938], Loss: 0.6519113183021545\n",
      "Train: Epoch [16], Batch [53/938], Loss: 0.6850385665893555\n",
      "Train: Epoch [16], Batch [54/938], Loss: 0.8936231732368469\n",
      "Train: Epoch [16], Batch [55/938], Loss: 0.6510248780250549\n",
      "Train: Epoch [16], Batch [56/938], Loss: 0.6919872164726257\n",
      "Train: Epoch [16], Batch [57/938], Loss: 0.6130633354187012\n",
      "Train: Epoch [16], Batch [58/938], Loss: 0.9020752906799316\n",
      "Train: Epoch [16], Batch [59/938], Loss: 0.6594863533973694\n",
      "Train: Epoch [16], Batch [60/938], Loss: 0.6853909492492676\n",
      "Train: Epoch [16], Batch [61/938], Loss: 0.45059680938720703\n",
      "Train: Epoch [16], Batch [62/938], Loss: 0.8702729940414429\n",
      "Train: Epoch [16], Batch [63/938], Loss: 0.7021741271018982\n",
      "Train: Epoch [16], Batch [64/938], Loss: 0.4836615324020386\n",
      "Train: Epoch [16], Batch [65/938], Loss: 0.8715350031852722\n",
      "Train: Epoch [16], Batch [66/938], Loss: 0.714773952960968\n",
      "Train: Epoch [16], Batch [67/938], Loss: 0.5628657341003418\n",
      "Train: Epoch [16], Batch [68/938], Loss: 1.1131408214569092\n",
      "Train: Epoch [16], Batch [69/938], Loss: 0.7336834669113159\n",
      "Train: Epoch [16], Batch [70/938], Loss: 0.667982816696167\n",
      "Train: Epoch [16], Batch [71/938], Loss: 0.545726478099823\n",
      "Train: Epoch [16], Batch [72/938], Loss: 0.5552083849906921\n",
      "Train: Epoch [16], Batch [73/938], Loss: 0.9221914410591125\n",
      "Train: Epoch [16], Batch [74/938], Loss: 0.8101014494895935\n",
      "Train: Epoch [16], Batch [75/938], Loss: 0.7314843535423279\n",
      "Train: Epoch [16], Batch [76/938], Loss: 0.8663230538368225\n",
      "Train: Epoch [16], Batch [77/938], Loss: 0.6778032183647156\n",
      "Train: Epoch [16], Batch [78/938], Loss: 0.6493563652038574\n",
      "Train: Epoch [16], Batch [79/938], Loss: 0.6071062684059143\n",
      "Train: Epoch [16], Batch [80/938], Loss: 0.6050020456314087\n",
      "Train: Epoch [16], Batch [81/938], Loss: 0.5581192970275879\n",
      "Train: Epoch [16], Batch [82/938], Loss: 0.720221996307373\n",
      "Train: Epoch [16], Batch [83/938], Loss: 0.5460249185562134\n",
      "Train: Epoch [16], Batch [84/938], Loss: 0.6128944158554077\n",
      "Train: Epoch [16], Batch [85/938], Loss: 0.7259045839309692\n",
      "Train: Epoch [16], Batch [86/938], Loss: 0.5838261842727661\n",
      "Train: Epoch [16], Batch [87/938], Loss: 0.777941107749939\n",
      "Train: Epoch [16], Batch [88/938], Loss: 0.6296303868293762\n",
      "Train: Epoch [16], Batch [89/938], Loss: 0.790163516998291\n",
      "Train: Epoch [16], Batch [90/938], Loss: 0.6132190823554993\n",
      "Train: Epoch [16], Batch [91/938], Loss: 0.3939911127090454\n",
      "Train: Epoch [16], Batch [92/938], Loss: 0.5124934315681458\n",
      "Train: Epoch [16], Batch [93/938], Loss: 0.6909551620483398\n",
      "Train: Epoch [16], Batch [94/938], Loss: 0.7156580090522766\n",
      "Train: Epoch [16], Batch [95/938], Loss: 0.683239221572876\n",
      "Train: Epoch [16], Batch [96/938], Loss: 0.6769673824310303\n",
      "Train: Epoch [16], Batch [97/938], Loss: 0.7574225664138794\n",
      "Train: Epoch [16], Batch [98/938], Loss: 0.5512627363204956\n",
      "Train: Epoch [16], Batch [99/938], Loss: 0.718130886554718\n",
      "Train: Epoch [16], Batch [100/938], Loss: 0.6542391180992126\n",
      "Train: Epoch [16], Batch [101/938], Loss: 0.7345355749130249\n",
      "Train: Epoch [16], Batch [102/938], Loss: 0.48809266090393066\n",
      "Train: Epoch [16], Batch [103/938], Loss: 0.5080761313438416\n",
      "Train: Epoch [16], Batch [104/938], Loss: 0.9715427160263062\n",
      "Train: Epoch [16], Batch [105/938], Loss: 0.7037909626960754\n",
      "Train: Epoch [16], Batch [106/938], Loss: 0.7050601243972778\n",
      "Train: Epoch [16], Batch [107/938], Loss: 0.7374922037124634\n",
      "Train: Epoch [16], Batch [108/938], Loss: 0.5761104822158813\n",
      "Train: Epoch [16], Batch [109/938], Loss: 0.8469358682632446\n",
      "Train: Epoch [16], Batch [110/938], Loss: 0.4558895528316498\n",
      "Train: Epoch [16], Batch [111/938], Loss: 0.6841323971748352\n",
      "Train: Epoch [16], Batch [112/938], Loss: 0.535737156867981\n",
      "Train: Epoch [16], Batch [113/938], Loss: 0.48073166608810425\n",
      "Train: Epoch [16], Batch [114/938], Loss: 0.40351927280426025\n",
      "Train: Epoch [16], Batch [115/938], Loss: 0.8079836964607239\n",
      "Train: Epoch [16], Batch [116/938], Loss: 0.5179873108863831\n",
      "Train: Epoch [16], Batch [117/938], Loss: 0.934147834777832\n",
      "Train: Epoch [16], Batch [118/938], Loss: 0.8874893188476562\n",
      "Train: Epoch [16], Batch [119/938], Loss: 0.6283842325210571\n",
      "Train: Epoch [16], Batch [120/938], Loss: 0.47141194343566895\n",
      "Train: Epoch [16], Batch [121/938], Loss: 0.8053795695304871\n",
      "Train: Epoch [16], Batch [122/938], Loss: 0.6341695785522461\n",
      "Train: Epoch [16], Batch [123/938], Loss: 0.7241605520248413\n",
      "Train: Epoch [16], Batch [124/938], Loss: 0.9146276712417603\n",
      "Train: Epoch [16], Batch [125/938], Loss: 0.9006774425506592\n",
      "Train: Epoch [16], Batch [126/938], Loss: 0.5572905540466309\n",
      "Train: Epoch [16], Batch [127/938], Loss: 0.5476117134094238\n",
      "Train: Epoch [16], Batch [128/938], Loss: 0.457282692193985\n",
      "Train: Epoch [16], Batch [129/938], Loss: 0.767839252948761\n",
      "Train: Epoch [16], Batch [130/938], Loss: 0.7368634343147278\n",
      "Train: Epoch [16], Batch [131/938], Loss: 0.7080292701721191\n",
      "Train: Epoch [16], Batch [132/938], Loss: 0.425815224647522\n",
      "Train: Epoch [16], Batch [133/938], Loss: 0.6147536635398865\n",
      "Train: Epoch [16], Batch [134/938], Loss: 0.6496803760528564\n",
      "Train: Epoch [16], Batch [135/938], Loss: 0.6369564533233643\n",
      "Train: Epoch [16], Batch [136/938], Loss: 0.6956689357757568\n",
      "Train: Epoch [16], Batch [137/938], Loss: 0.9559369087219238\n",
      "Train: Epoch [16], Batch [138/938], Loss: 0.7166767716407776\n",
      "Train: Epoch [16], Batch [139/938], Loss: 0.589718759059906\n",
      "Train: Epoch [16], Batch [140/938], Loss: 0.8337818384170532\n",
      "Train: Epoch [16], Batch [141/938], Loss: 0.34637218713760376\n",
      "Train: Epoch [16], Batch [142/938], Loss: 0.501232385635376\n",
      "Train: Epoch [16], Batch [143/938], Loss: 0.5928295850753784\n",
      "Train: Epoch [16], Batch [144/938], Loss: 0.845745861530304\n",
      "Train: Epoch [16], Batch [145/938], Loss: 0.8493130207061768\n",
      "Train: Epoch [16], Batch [146/938], Loss: 0.6969874501228333\n",
      "Train: Epoch [16], Batch [147/938], Loss: 0.7241370677947998\n",
      "Train: Epoch [16], Batch [148/938], Loss: 0.6851937770843506\n",
      "Train: Epoch [16], Batch [149/938], Loss: 0.5853639841079712\n",
      "Train: Epoch [16], Batch [150/938], Loss: 0.8920297026634216\n",
      "Train: Epoch [16], Batch [151/938], Loss: 0.6745291948318481\n",
      "Train: Epoch [16], Batch [152/938], Loss: 0.6075555682182312\n",
      "Train: Epoch [16], Batch [153/938], Loss: 0.4663442075252533\n",
      "Train: Epoch [16], Batch [154/938], Loss: 0.5882601141929626\n",
      "Train: Epoch [16], Batch [155/938], Loss: 0.7312492728233337\n",
      "Train: Epoch [16], Batch [156/938], Loss: 0.7742682099342346\n",
      "Train: Epoch [16], Batch [157/938], Loss: 0.8608980178833008\n",
      "Train: Epoch [16], Batch [158/938], Loss: 0.7883175015449524\n",
      "Train: Epoch [16], Batch [159/938], Loss: 0.7985703945159912\n",
      "Train: Epoch [16], Batch [160/938], Loss: 0.7163827419281006\n",
      "Train: Epoch [16], Batch [161/938], Loss: 0.7283653020858765\n",
      "Train: Epoch [16], Batch [162/938], Loss: 0.8293044567108154\n",
      "Train: Epoch [16], Batch [163/938], Loss: 0.48437485098838806\n",
      "Train: Epoch [16], Batch [164/938], Loss: 0.8765944242477417\n",
      "Train: Epoch [16], Batch [165/938], Loss: 0.6023224592208862\n",
      "Train: Epoch [16], Batch [166/938], Loss: 0.6320135593414307\n",
      "Train: Epoch [16], Batch [167/938], Loss: 0.667881190776825\n",
      "Train: Epoch [16], Batch [168/938], Loss: 0.8159795999526978\n",
      "Train: Epoch [16], Batch [169/938], Loss: 0.6150042414665222\n",
      "Train: Epoch [16], Batch [170/938], Loss: 0.5946974754333496\n",
      "Train: Epoch [16], Batch [171/938], Loss: 0.6948516368865967\n",
      "Train: Epoch [16], Batch [172/938], Loss: 0.7955086827278137\n",
      "Train: Epoch [16], Batch [173/938], Loss: 0.7777634859085083\n",
      "Train: Epoch [16], Batch [174/938], Loss: 0.6728227734565735\n",
      "Train: Epoch [16], Batch [175/938], Loss: 0.6780581474304199\n",
      "Train: Epoch [16], Batch [176/938], Loss: 0.7762669324874878\n",
      "Train: Epoch [16], Batch [177/938], Loss: 0.7303064465522766\n",
      "Train: Epoch [16], Batch [178/938], Loss: 0.6548112034797668\n",
      "Train: Epoch [16], Batch [179/938], Loss: 0.40028634667396545\n",
      "Train: Epoch [16], Batch [180/938], Loss: 0.7456415891647339\n",
      "Train: Epoch [16], Batch [181/938], Loss: 0.8048112392425537\n",
      "Train: Epoch [16], Batch [182/938], Loss: 0.7113229036331177\n",
      "Train: Epoch [16], Batch [183/938], Loss: 0.9081560969352722\n",
      "Train: Epoch [16], Batch [184/938], Loss: 0.6541417837142944\n",
      "Train: Epoch [16], Batch [185/938], Loss: 0.5504871606826782\n",
      "Train: Epoch [16], Batch [186/938], Loss: 0.8260076642036438\n",
      "Train: Epoch [16], Batch [187/938], Loss: 0.9373733997344971\n",
      "Train: Epoch [16], Batch [188/938], Loss: 0.8037682771682739\n",
      "Train: Epoch [16], Batch [189/938], Loss: 0.6908937692642212\n",
      "Train: Epoch [16], Batch [190/938], Loss: 0.5623796582221985\n",
      "Train: Epoch [16], Batch [191/938], Loss: 0.7866421341896057\n",
      "Train: Epoch [16], Batch [192/938], Loss: 0.8179733753204346\n",
      "Train: Epoch [16], Batch [193/938], Loss: 0.7717631459236145\n",
      "Train: Epoch [16], Batch [194/938], Loss: 0.8993464112281799\n",
      "Train: Epoch [16], Batch [195/938], Loss: 0.736831545829773\n",
      "Train: Epoch [16], Batch [196/938], Loss: 0.6386132836341858\n",
      "Train: Epoch [16], Batch [197/938], Loss: 0.6401041150093079\n",
      "Train: Epoch [16], Batch [198/938], Loss: 0.6242099404335022\n",
      "Train: Epoch [16], Batch [199/938], Loss: 0.7262190580368042\n",
      "Train: Epoch [16], Batch [200/938], Loss: 0.7371508479118347\n",
      "Train: Epoch [16], Batch [201/938], Loss: 0.6344159841537476\n",
      "Train: Epoch [16], Batch [202/938], Loss: 0.753595769405365\n",
      "Train: Epoch [16], Batch [203/938], Loss: 0.6931062936782837\n",
      "Train: Epoch [16], Batch [204/938], Loss: 0.7512041330337524\n",
      "Train: Epoch [16], Batch [205/938], Loss: 0.6249258518218994\n",
      "Train: Epoch [16], Batch [206/938], Loss: 0.7698670029640198\n",
      "Train: Epoch [16], Batch [207/938], Loss: 0.5615850687026978\n",
      "Train: Epoch [16], Batch [208/938], Loss: 0.7115907669067383\n",
      "Train: Epoch [16], Batch [209/938], Loss: 0.7087276577949524\n",
      "Train: Epoch [16], Batch [210/938], Loss: 0.7259573936462402\n",
      "Train: Epoch [16], Batch [211/938], Loss: 0.5296482443809509\n",
      "Train: Epoch [16], Batch [212/938], Loss: 0.5181822180747986\n",
      "Train: Epoch [16], Batch [213/938], Loss: 0.39570415019989014\n",
      "Train: Epoch [16], Batch [214/938], Loss: 0.6159377098083496\n",
      "Train: Epoch [16], Batch [215/938], Loss: 0.6464816331863403\n",
      "Train: Epoch [16], Batch [216/938], Loss: 0.8679560422897339\n",
      "Train: Epoch [16], Batch [217/938], Loss: 1.012791633605957\n",
      "Train: Epoch [16], Batch [218/938], Loss: 0.9182873964309692\n",
      "Train: Epoch [16], Batch [219/938], Loss: 0.6283700466156006\n",
      "Train: Epoch [16], Batch [220/938], Loss: 0.8918020129203796\n",
      "Train: Epoch [16], Batch [221/938], Loss: 0.6935119032859802\n",
      "Train: Epoch [16], Batch [222/938], Loss: 0.8437143564224243\n",
      "Train: Epoch [16], Batch [223/938], Loss: 0.8331189155578613\n",
      "Train: Epoch [16], Batch [224/938], Loss: 0.6257719993591309\n",
      "Train: Epoch [16], Batch [225/938], Loss: 0.9345765113830566\n",
      "Train: Epoch [16], Batch [226/938], Loss: 0.5844708681106567\n",
      "Train: Epoch [16], Batch [227/938], Loss: 0.5657715797424316\n",
      "Train: Epoch [16], Batch [228/938], Loss: 0.5068346261978149\n",
      "Train: Epoch [16], Batch [229/938], Loss: 0.7074328660964966\n",
      "Train: Epoch [16], Batch [230/938], Loss: 0.5508353114128113\n",
      "Train: Epoch [16], Batch [231/938], Loss: 0.5946910381317139\n",
      "Train: Epoch [16], Batch [232/938], Loss: 0.659549355506897\n",
      "Train: Epoch [16], Batch [233/938], Loss: 0.8292742371559143\n",
      "Train: Epoch [16], Batch [234/938], Loss: 0.6426476836204529\n",
      "Train: Epoch [16], Batch [235/938], Loss: 0.7275240421295166\n",
      "Train: Epoch [16], Batch [236/938], Loss: 0.6851707696914673\n",
      "Train: Epoch [16], Batch [237/938], Loss: 0.9534220695495605\n",
      "Train: Epoch [16], Batch [238/938], Loss: 0.6993154287338257\n",
      "Train: Epoch [16], Batch [239/938], Loss: 0.6594635844230652\n",
      "Train: Epoch [16], Batch [240/938], Loss: 0.6745567321777344\n",
      "Train: Epoch [16], Batch [241/938], Loss: 0.8348408341407776\n",
      "Train: Epoch [16], Batch [242/938], Loss: 0.7687263488769531\n",
      "Train: Epoch [16], Batch [243/938], Loss: 0.8177475333213806\n",
      "Train: Epoch [16], Batch [244/938], Loss: 0.8022676110267639\n",
      "Train: Epoch [16], Batch [245/938], Loss: 0.5374573469161987\n",
      "Train: Epoch [16], Batch [246/938], Loss: 0.6643985509872437\n",
      "Train: Epoch [16], Batch [247/938], Loss: 0.8359203338623047\n",
      "Train: Epoch [16], Batch [248/938], Loss: 0.7058621048927307\n",
      "Train: Epoch [16], Batch [249/938], Loss: 0.7544410824775696\n",
      "Train: Epoch [16], Batch [250/938], Loss: 0.5696839094161987\n",
      "Train: Epoch [16], Batch [251/938], Loss: 0.503046452999115\n",
      "Train: Epoch [16], Batch [252/938], Loss: 0.7530813813209534\n",
      "Train: Epoch [16], Batch [253/938], Loss: 0.772964358329773\n",
      "Train: Epoch [16], Batch [254/938], Loss: 0.5202218294143677\n",
      "Train: Epoch [16], Batch [255/938], Loss: 0.902604341506958\n",
      "Train: Epoch [16], Batch [256/938], Loss: 1.029873251914978\n",
      "Train: Epoch [16], Batch [257/938], Loss: 0.6447787284851074\n",
      "Train: Epoch [16], Batch [258/938], Loss: 0.6509032845497131\n",
      "Train: Epoch [16], Batch [259/938], Loss: 0.7639663219451904\n",
      "Train: Epoch [16], Batch [260/938], Loss: 0.9066348671913147\n",
      "Train: Epoch [16], Batch [261/938], Loss: 0.860616147518158\n",
      "Train: Epoch [16], Batch [262/938], Loss: 0.6213273406028748\n",
      "Train: Epoch [16], Batch [263/938], Loss: 0.7253850102424622\n",
      "Train: Epoch [16], Batch [264/938], Loss: 0.6442165374755859\n",
      "Train: Epoch [16], Batch [265/938], Loss: 0.767927348613739\n",
      "Train: Epoch [16], Batch [266/938], Loss: 0.6994732618331909\n",
      "Train: Epoch [16], Batch [267/938], Loss: 0.6621361970901489\n",
      "Train: Epoch [16], Batch [268/938], Loss: 0.9052680134773254\n",
      "Train: Epoch [16], Batch [269/938], Loss: 0.5643109083175659\n",
      "Train: Epoch [16], Batch [270/938], Loss: 0.7911891341209412\n",
      "Train: Epoch [16], Batch [271/938], Loss: 0.6840866804122925\n",
      "Train: Epoch [16], Batch [272/938], Loss: 0.7185304164886475\n",
      "Train: Epoch [16], Batch [273/938], Loss: 1.0397406816482544\n",
      "Train: Epoch [16], Batch [274/938], Loss: 0.6520482301712036\n",
      "Train: Epoch [16], Batch [275/938], Loss: 0.7387428879737854\n",
      "Train: Epoch [16], Batch [276/938], Loss: 0.6775593757629395\n",
      "Train: Epoch [16], Batch [277/938], Loss: 0.740631639957428\n",
      "Train: Epoch [16], Batch [278/938], Loss: 0.5770001411437988\n",
      "Train: Epoch [16], Batch [279/938], Loss: 0.8575589656829834\n",
      "Train: Epoch [16], Batch [280/938], Loss: 0.5601646900177002\n",
      "Train: Epoch [16], Batch [281/938], Loss: 0.8801476955413818\n",
      "Train: Epoch [16], Batch [282/938], Loss: 0.7713819146156311\n",
      "Train: Epoch [16], Batch [283/938], Loss: 0.6870768070220947\n",
      "Train: Epoch [16], Batch [284/938], Loss: 0.7980716824531555\n",
      "Train: Epoch [16], Batch [285/938], Loss: 0.5035805702209473\n",
      "Train: Epoch [16], Batch [286/938], Loss: 0.57330322265625\n",
      "Train: Epoch [16], Batch [287/938], Loss: 0.7603755593299866\n",
      "Train: Epoch [16], Batch [288/938], Loss: 0.6951931715011597\n",
      "Train: Epoch [16], Batch [289/938], Loss: 0.504435658454895\n",
      "Train: Epoch [16], Batch [290/938], Loss: 0.6630621552467346\n",
      "Train: Epoch [16], Batch [291/938], Loss: 0.8042334914207458\n",
      "Train: Epoch [16], Batch [292/938], Loss: 0.6299319267272949\n",
      "Train: Epoch [16], Batch [293/938], Loss: 0.525898277759552\n",
      "Train: Epoch [16], Batch [294/938], Loss: 0.5713189840316772\n",
      "Train: Epoch [16], Batch [295/938], Loss: 0.5697585344314575\n",
      "Train: Epoch [16], Batch [296/938], Loss: 0.6869415640830994\n",
      "Train: Epoch [16], Batch [297/938], Loss: 0.7646806240081787\n",
      "Train: Epoch [16], Batch [298/938], Loss: 0.6036747097969055\n",
      "Train: Epoch [16], Batch [299/938], Loss: 0.6144168376922607\n",
      "Train: Epoch [16], Batch [300/938], Loss: 0.7034875154495239\n",
      "Train: Epoch [16], Batch [301/938], Loss: 0.8569880723953247\n",
      "Train: Epoch [16], Batch [302/938], Loss: 0.5719029903411865\n",
      "Train: Epoch [16], Batch [303/938], Loss: 0.6281843185424805\n",
      "Train: Epoch [16], Batch [304/938], Loss: 0.5061655044555664\n",
      "Train: Epoch [16], Batch [305/938], Loss: 0.5368998646736145\n",
      "Train: Epoch [16], Batch [306/938], Loss: 0.7227694988250732\n",
      "Train: Epoch [16], Batch [307/938], Loss: 0.6406888365745544\n",
      "Train: Epoch [16], Batch [308/938], Loss: 0.6915087699890137\n",
      "Train: Epoch [16], Batch [309/938], Loss: 0.8533753752708435\n",
      "Train: Epoch [16], Batch [310/938], Loss: 0.670242190361023\n",
      "Train: Epoch [16], Batch [311/938], Loss: 0.8033291101455688\n",
      "Train: Epoch [16], Batch [312/938], Loss: 0.9825676679611206\n",
      "Train: Epoch [16], Batch [313/938], Loss: 0.7475810050964355\n",
      "Train: Epoch [16], Batch [314/938], Loss: 0.547113299369812\n",
      "Train: Epoch [16], Batch [315/938], Loss: 0.6224249005317688\n",
      "Train: Epoch [16], Batch [316/938], Loss: 0.5378913879394531\n",
      "Train: Epoch [16], Batch [317/938], Loss: 0.7417663335800171\n",
      "Train: Epoch [16], Batch [318/938], Loss: 0.8897466063499451\n",
      "Train: Epoch [16], Batch [319/938], Loss: 0.6661732792854309\n",
      "Train: Epoch [16], Batch [320/938], Loss: 0.5390781760215759\n",
      "Train: Epoch [16], Batch [321/938], Loss: 0.694524884223938\n",
      "Train: Epoch [16], Batch [322/938], Loss: 0.7313153743743896\n",
      "Train: Epoch [16], Batch [323/938], Loss: 0.6823997497558594\n",
      "Train: Epoch [16], Batch [324/938], Loss: 0.6730601787567139\n",
      "Train: Epoch [16], Batch [325/938], Loss: 0.8805222511291504\n",
      "Train: Epoch [16], Batch [326/938], Loss: 0.8455469608306885\n",
      "Train: Epoch [16], Batch [327/938], Loss: 0.5968661308288574\n",
      "Train: Epoch [16], Batch [328/938], Loss: 0.6471070051193237\n",
      "Train: Epoch [16], Batch [329/938], Loss: 0.5303528904914856\n",
      "Train: Epoch [16], Batch [330/938], Loss: 0.643773078918457\n",
      "Train: Epoch [16], Batch [331/938], Loss: 0.8592342138290405\n",
      "Train: Epoch [16], Batch [332/938], Loss: 0.7069936990737915\n",
      "Train: Epoch [16], Batch [333/938], Loss: 0.6543212532997131\n",
      "Train: Epoch [16], Batch [334/938], Loss: 0.5378475189208984\n",
      "Train: Epoch [16], Batch [335/938], Loss: 0.5604785680770874\n",
      "Train: Epoch [16], Batch [336/938], Loss: 0.7584768533706665\n",
      "Train: Epoch [16], Batch [337/938], Loss: 0.5775647163391113\n",
      "Train: Epoch [16], Batch [338/938], Loss: 0.8355444669723511\n",
      "Train: Epoch [16], Batch [339/938], Loss: 0.6351742744445801\n",
      "Train: Epoch [16], Batch [340/938], Loss: 0.6707633137702942\n",
      "Train: Epoch [16], Batch [341/938], Loss: 0.6213836073875427\n",
      "Train: Epoch [16], Batch [342/938], Loss: 0.5615761280059814\n",
      "Train: Epoch [16], Batch [343/938], Loss: 0.7657263278961182\n",
      "Train: Epoch [16], Batch [344/938], Loss: 0.7282193303108215\n",
      "Train: Epoch [16], Batch [345/938], Loss: 0.5778306126594543\n",
      "Train: Epoch [16], Batch [346/938], Loss: 0.900765597820282\n",
      "Train: Epoch [16], Batch [347/938], Loss: 0.7282575368881226\n",
      "Train: Epoch [16], Batch [348/938], Loss: 0.5266708731651306\n",
      "Train: Epoch [16], Batch [349/938], Loss: 0.6936400532722473\n",
      "Train: Epoch [16], Batch [350/938], Loss: 0.6917497515678406\n",
      "Train: Epoch [16], Batch [351/938], Loss: 0.5051978230476379\n",
      "Train: Epoch [16], Batch [352/938], Loss: 0.6681755781173706\n",
      "Train: Epoch [16], Batch [353/938], Loss: 0.6451008319854736\n",
      "Train: Epoch [16], Batch [354/938], Loss: 0.6359880566596985\n",
      "Train: Epoch [16], Batch [355/938], Loss: 0.7219483852386475\n",
      "Train: Epoch [16], Batch [356/938], Loss: 0.5642735362052917\n",
      "Train: Epoch [16], Batch [357/938], Loss: 0.6920158863067627\n",
      "Train: Epoch [16], Batch [358/938], Loss: 0.771632194519043\n",
      "Train: Epoch [16], Batch [359/938], Loss: 0.8639785051345825\n",
      "Train: Epoch [16], Batch [360/938], Loss: 0.6706897020339966\n",
      "Train: Epoch [16], Batch [361/938], Loss: 0.9756101369857788\n",
      "Train: Epoch [16], Batch [362/938], Loss: 0.4967387020587921\n",
      "Train: Epoch [16], Batch [363/938], Loss: 0.5754760503768921\n",
      "Train: Epoch [16], Batch [364/938], Loss: 0.6622008681297302\n",
      "Train: Epoch [16], Batch [365/938], Loss: 0.8122106790542603\n",
      "Train: Epoch [16], Batch [366/938], Loss: 0.5630220174789429\n",
      "Train: Epoch [16], Batch [367/938], Loss: 0.6332380771636963\n",
      "Train: Epoch [16], Batch [368/938], Loss: 0.5018731355667114\n",
      "Train: Epoch [16], Batch [369/938], Loss: 0.7213447093963623\n",
      "Train: Epoch [16], Batch [370/938], Loss: 0.6095811724662781\n",
      "Train: Epoch [16], Batch [371/938], Loss: 0.9094552993774414\n",
      "Train: Epoch [16], Batch [372/938], Loss: 0.7962299585342407\n",
      "Train: Epoch [16], Batch [373/938], Loss: 0.8204967975616455\n",
      "Train: Epoch [16], Batch [374/938], Loss: 0.7407181859016418\n",
      "Train: Epoch [16], Batch [375/938], Loss: 0.688880980014801\n",
      "Train: Epoch [16], Batch [376/938], Loss: 0.7824073433876038\n",
      "Train: Epoch [16], Batch [377/938], Loss: 0.5092298984527588\n",
      "Train: Epoch [16], Batch [378/938], Loss: 0.5981281399726868\n",
      "Train: Epoch [16], Batch [379/938], Loss: 0.7980301380157471\n",
      "Train: Epoch [16], Batch [380/938], Loss: 0.8763706684112549\n",
      "Train: Epoch [16], Batch [381/938], Loss: 0.6255578398704529\n",
      "Train: Epoch [16], Batch [382/938], Loss: 0.6160860061645508\n",
      "Train: Epoch [16], Batch [383/938], Loss: 0.651962399482727\n",
      "Train: Epoch [16], Batch [384/938], Loss: 0.6664615273475647\n",
      "Train: Epoch [16], Batch [385/938], Loss: 0.8493378162384033\n",
      "Train: Epoch [16], Batch [386/938], Loss: 0.6663081049919128\n",
      "Train: Epoch [16], Batch [387/938], Loss: 0.8028730154037476\n",
      "Train: Epoch [16], Batch [388/938], Loss: 0.7810408473014832\n",
      "Train: Epoch [16], Batch [389/938], Loss: 0.8163726329803467\n",
      "Train: Epoch [16], Batch [390/938], Loss: 0.7496326565742493\n",
      "Train: Epoch [16], Batch [391/938], Loss: 0.9290422201156616\n",
      "Train: Epoch [16], Batch [392/938], Loss: 0.7561291456222534\n",
      "Train: Epoch [16], Batch [393/938], Loss: 0.8036810159683228\n",
      "Train: Epoch [16], Batch [394/938], Loss: 0.5375009775161743\n",
      "Train: Epoch [16], Batch [395/938], Loss: 0.728890061378479\n",
      "Train: Epoch [16], Batch [396/938], Loss: 0.5910001993179321\n",
      "Train: Epoch [16], Batch [397/938], Loss: 0.5712854266166687\n",
      "Train: Epoch [16], Batch [398/938], Loss: 0.6971379518508911\n",
      "Train: Epoch [16], Batch [399/938], Loss: 0.7635510563850403\n",
      "Train: Epoch [16], Batch [400/938], Loss: 0.640874445438385\n",
      "Train: Epoch [16], Batch [401/938], Loss: 0.8149531483650208\n",
      "Train: Epoch [16], Batch [402/938], Loss: 0.6390122771263123\n",
      "Train: Epoch [16], Batch [403/938], Loss: 0.5589292049407959\n",
      "Train: Epoch [16], Batch [404/938], Loss: 0.8053663372993469\n",
      "Train: Epoch [16], Batch [405/938], Loss: 0.6651861667633057\n",
      "Train: Epoch [16], Batch [406/938], Loss: 0.7455272078514099\n",
      "Train: Epoch [16], Batch [407/938], Loss: 0.8339345455169678\n",
      "Train: Epoch [16], Batch [408/938], Loss: 0.6024465560913086\n",
      "Train: Epoch [16], Batch [409/938], Loss: 1.0974980592727661\n",
      "Train: Epoch [16], Batch [410/938], Loss: 0.8133326768875122\n",
      "Train: Epoch [16], Batch [411/938], Loss: 0.5078872442245483\n",
      "Train: Epoch [16], Batch [412/938], Loss: 0.4973216652870178\n",
      "Train: Epoch [16], Batch [413/938], Loss: 0.7085437774658203\n",
      "Train: Epoch [16], Batch [414/938], Loss: 0.4706358313560486\n",
      "Train: Epoch [16], Batch [415/938], Loss: 0.6794112324714661\n",
      "Train: Epoch [16], Batch [416/938], Loss: 0.8218151926994324\n",
      "Train: Epoch [16], Batch [417/938], Loss: 0.8054801821708679\n",
      "Train: Epoch [16], Batch [418/938], Loss: 0.6782046556472778\n",
      "Train: Epoch [16], Batch [419/938], Loss: 0.5936532616615295\n",
      "Train: Epoch [16], Batch [420/938], Loss: 0.6499649286270142\n",
      "Train: Epoch [16], Batch [421/938], Loss: 0.3166569471359253\n",
      "Train: Epoch [16], Batch [422/938], Loss: 0.6290196776390076\n",
      "Train: Epoch [16], Batch [423/938], Loss: 0.6637136936187744\n",
      "Train: Epoch [16], Batch [424/938], Loss: 0.9578976035118103\n",
      "Train: Epoch [16], Batch [425/938], Loss: 0.491593599319458\n",
      "Train: Epoch [16], Batch [426/938], Loss: 0.5107531547546387\n",
      "Train: Epoch [16], Batch [427/938], Loss: 0.7613852620124817\n",
      "Train: Epoch [16], Batch [428/938], Loss: 0.7436579465866089\n",
      "Train: Epoch [16], Batch [429/938], Loss: 0.73736172914505\n",
      "Train: Epoch [16], Batch [430/938], Loss: 0.5270960330963135\n",
      "Train: Epoch [16], Batch [431/938], Loss: 0.6815884113311768\n",
      "Train: Epoch [16], Batch [432/938], Loss: 0.8380225896835327\n",
      "Train: Epoch [16], Batch [433/938], Loss: 0.7255166172981262\n",
      "Train: Epoch [16], Batch [434/938], Loss: 0.8265336751937866\n",
      "Train: Epoch [16], Batch [435/938], Loss: 0.554078996181488\n",
      "Train: Epoch [16], Batch [436/938], Loss: 0.7011145353317261\n",
      "Train: Epoch [16], Batch [437/938], Loss: 0.4250437021255493\n",
      "Train: Epoch [16], Batch [438/938], Loss: 0.9173678159713745\n",
      "Train: Epoch [16], Batch [439/938], Loss: 0.6129100918769836\n",
      "Train: Epoch [16], Batch [440/938], Loss: 0.6063555479049683\n",
      "Train: Epoch [16], Batch [441/938], Loss: 0.80498868227005\n",
      "Train: Epoch [16], Batch [442/938], Loss: 0.6830641031265259\n",
      "Train: Epoch [16], Batch [443/938], Loss: 0.7233555316925049\n",
      "Train: Epoch [16], Batch [444/938], Loss: 0.6214228868484497\n",
      "Train: Epoch [16], Batch [445/938], Loss: 0.48570993542671204\n",
      "Train: Epoch [16], Batch [446/938], Loss: 0.7818710803985596\n",
      "Train: Epoch [16], Batch [447/938], Loss: 0.5453425645828247\n",
      "Train: Epoch [16], Batch [448/938], Loss: 0.395975798368454\n",
      "Train: Epoch [16], Batch [449/938], Loss: 0.6393646001815796\n",
      "Train: Epoch [16], Batch [450/938], Loss: 0.3791310787200928\n",
      "Train: Epoch [16], Batch [451/938], Loss: 1.1270142793655396\n",
      "Train: Epoch [16], Batch [452/938], Loss: 0.6897772550582886\n",
      "Train: Epoch [16], Batch [453/938], Loss: 0.5040115714073181\n",
      "Train: Epoch [16], Batch [454/938], Loss: 0.608109712600708\n",
      "Train: Epoch [16], Batch [455/938], Loss: 0.6006713509559631\n",
      "Train: Epoch [16], Batch [456/938], Loss: 0.7128472328186035\n",
      "Train: Epoch [16], Batch [457/938], Loss: 0.5553241968154907\n",
      "Train: Epoch [16], Batch [458/938], Loss: 0.6824363470077515\n",
      "Train: Epoch [16], Batch [459/938], Loss: 0.5800485014915466\n",
      "Train: Epoch [16], Batch [460/938], Loss: 0.7249908447265625\n",
      "Train: Epoch [16], Batch [461/938], Loss: 0.8242849111557007\n",
      "Train: Epoch [16], Batch [462/938], Loss: 0.8351046442985535\n",
      "Train: Epoch [16], Batch [463/938], Loss: 0.6810733675956726\n",
      "Train: Epoch [16], Batch [464/938], Loss: 0.7649614810943604\n",
      "Train: Epoch [16], Batch [465/938], Loss: 0.7399932742118835\n",
      "Train: Epoch [16], Batch [466/938], Loss: 0.8414513468742371\n",
      "Train: Epoch [16], Batch [467/938], Loss: 0.7977694869041443\n",
      "Train: Epoch [16], Batch [468/938], Loss: 0.822001576423645\n",
      "Train: Epoch [16], Batch [469/938], Loss: 0.6322131156921387\n",
      "Train: Epoch [16], Batch [470/938], Loss: 0.5824699401855469\n",
      "Train: Epoch [16], Batch [471/938], Loss: 0.6444215774536133\n",
      "Train: Epoch [16], Batch [472/938], Loss: 0.6804671287536621\n",
      "Train: Epoch [16], Batch [473/938], Loss: 0.5979529619216919\n",
      "Train: Epoch [16], Batch [474/938], Loss: 0.6261990070343018\n",
      "Train: Epoch [16], Batch [475/938], Loss: 0.8608635663986206\n",
      "Train: Epoch [16], Batch [476/938], Loss: 0.6300165057182312\n",
      "Train: Epoch [16], Batch [477/938], Loss: 0.6942955851554871\n",
      "Train: Epoch [16], Batch [478/938], Loss: 0.8498075008392334\n",
      "Train: Epoch [16], Batch [479/938], Loss: 0.860954225063324\n",
      "Train: Epoch [16], Batch [480/938], Loss: 0.8590692281723022\n",
      "Train: Epoch [16], Batch [481/938], Loss: 0.7242175340652466\n",
      "Train: Epoch [16], Batch [482/938], Loss: 0.5540353059768677\n",
      "Train: Epoch [16], Batch [483/938], Loss: 0.7003461122512817\n",
      "Train: Epoch [16], Batch [484/938], Loss: 0.6478067636489868\n",
      "Train: Epoch [16], Batch [485/938], Loss: 0.8023653626441956\n",
      "Train: Epoch [16], Batch [486/938], Loss: 0.7912273406982422\n",
      "Train: Epoch [16], Batch [487/938], Loss: 0.6466484069824219\n",
      "Train: Epoch [16], Batch [488/938], Loss: 0.7671875953674316\n",
      "Train: Epoch [16], Batch [489/938], Loss: 0.7140369415283203\n",
      "Train: Epoch [16], Batch [490/938], Loss: 0.5416290760040283\n",
      "Train: Epoch [16], Batch [491/938], Loss: 0.6321091055870056\n",
      "Train: Epoch [16], Batch [492/938], Loss: 0.8324720859527588\n",
      "Train: Epoch [16], Batch [493/938], Loss: 0.6367757320404053\n",
      "Train: Epoch [16], Batch [494/938], Loss: 0.7230830192565918\n",
      "Train: Epoch [16], Batch [495/938], Loss: 1.0452568531036377\n",
      "Train: Epoch [16], Batch [496/938], Loss: 0.784640908241272\n",
      "Train: Epoch [16], Batch [497/938], Loss: 0.737038254737854\n",
      "Train: Epoch [16], Batch [498/938], Loss: 0.5159576535224915\n",
      "Train: Epoch [16], Batch [499/938], Loss: 0.7331447005271912\n",
      "Train: Epoch [16], Batch [500/938], Loss: 0.7809723615646362\n",
      "Train: Epoch [16], Batch [501/938], Loss: 0.5190490484237671\n",
      "Train: Epoch [16], Batch [502/938], Loss: 0.7547300457954407\n",
      "Train: Epoch [16], Batch [503/938], Loss: 0.7148123979568481\n",
      "Train: Epoch [16], Batch [504/938], Loss: 0.7165437936782837\n",
      "Train: Epoch [16], Batch [505/938], Loss: 0.6926320195198059\n",
      "Train: Epoch [16], Batch [506/938], Loss: 0.6539048552513123\n",
      "Train: Epoch [16], Batch [507/938], Loss: 0.6990030407905579\n",
      "Train: Epoch [16], Batch [508/938], Loss: 0.5275981426239014\n",
      "Train: Epoch [16], Batch [509/938], Loss: 0.5552194118499756\n",
      "Train: Epoch [16], Batch [510/938], Loss: 0.9256443977355957\n",
      "Train: Epoch [16], Batch [511/938], Loss: 0.7189733386039734\n",
      "Train: Epoch [16], Batch [512/938], Loss: 0.7324146628379822\n",
      "Train: Epoch [16], Batch [513/938], Loss: 0.6929355263710022\n",
      "Train: Epoch [16], Batch [514/938], Loss: 0.708148717880249\n",
      "Train: Epoch [16], Batch [515/938], Loss: 0.7318171858787537\n",
      "Train: Epoch [16], Batch [516/938], Loss: 0.6532148718833923\n",
      "Train: Epoch [16], Batch [517/938], Loss: 0.5702029466629028\n",
      "Train: Epoch [16], Batch [518/938], Loss: 0.5169639587402344\n",
      "Train: Epoch [16], Batch [519/938], Loss: 0.6776049137115479\n",
      "Train: Epoch [16], Batch [520/938], Loss: 0.8486438989639282\n",
      "Train: Epoch [16], Batch [521/938], Loss: 0.6091362237930298\n",
      "Train: Epoch [16], Batch [522/938], Loss: 0.8889903426170349\n",
      "Train: Epoch [16], Batch [523/938], Loss: 0.8006515502929688\n",
      "Train: Epoch [16], Batch [524/938], Loss: 0.6685026288032532\n",
      "Train: Epoch [16], Batch [525/938], Loss: 0.6274960041046143\n",
      "Train: Epoch [16], Batch [526/938], Loss: 0.5057567358016968\n",
      "Train: Epoch [16], Batch [527/938], Loss: 0.7856309413909912\n",
      "Train: Epoch [16], Batch [528/938], Loss: 0.844181478023529\n",
      "Train: Epoch [16], Batch [529/938], Loss: 0.6044089794158936\n",
      "Train: Epoch [16], Batch [530/938], Loss: 0.8600080013275146\n",
      "Train: Epoch [16], Batch [531/938], Loss: 0.8259978294372559\n",
      "Train: Epoch [16], Batch [532/938], Loss: 0.5826667547225952\n",
      "Train: Epoch [16], Batch [533/938], Loss: 0.4996338486671448\n",
      "Train: Epoch [16], Batch [534/938], Loss: 0.6663773655891418\n",
      "Train: Epoch [16], Batch [535/938], Loss: 0.6377919912338257\n",
      "Train: Epoch [16], Batch [536/938], Loss: 0.7126976251602173\n",
      "Train: Epoch [16], Batch [537/938], Loss: 0.5536098480224609\n",
      "Train: Epoch [16], Batch [538/938], Loss: 0.7233690619468689\n",
      "Train: Epoch [16], Batch [539/938], Loss: 0.6937931776046753\n",
      "Train: Epoch [16], Batch [540/938], Loss: 0.6387993693351746\n",
      "Train: Epoch [16], Batch [541/938], Loss: 0.7588263154029846\n",
      "Train: Epoch [16], Batch [542/938], Loss: 0.6445955038070679\n",
      "Train: Epoch [16], Batch [543/938], Loss: 0.8576847910881042\n",
      "Train: Epoch [16], Batch [544/938], Loss: 0.7395826578140259\n",
      "Train: Epoch [16], Batch [545/938], Loss: 0.6259361505508423\n",
      "Train: Epoch [16], Batch [546/938], Loss: 0.5914084911346436\n",
      "Train: Epoch [16], Batch [547/938], Loss: 0.7017838358879089\n",
      "Train: Epoch [16], Batch [548/938], Loss: 0.6053257584571838\n",
      "Train: Epoch [16], Batch [549/938], Loss: 0.6296336054801941\n",
      "Train: Epoch [16], Batch [550/938], Loss: 0.6652905344963074\n",
      "Train: Epoch [16], Batch [551/938], Loss: 0.6432619690895081\n",
      "Train: Epoch [16], Batch [552/938], Loss: 0.44612550735473633\n",
      "Train: Epoch [16], Batch [553/938], Loss: 0.5148550868034363\n",
      "Train: Epoch [16], Batch [554/938], Loss: 0.5997212529182434\n",
      "Train: Epoch [16], Batch [555/938], Loss: 0.5129342675209045\n",
      "Train: Epoch [16], Batch [556/938], Loss: 0.7094643712043762\n",
      "Train: Epoch [16], Batch [557/938], Loss: 0.5240956544876099\n",
      "Train: Epoch [16], Batch [558/938], Loss: 0.7612922787666321\n",
      "Train: Epoch [16], Batch [559/938], Loss: 0.5842112302780151\n",
      "Train: Epoch [16], Batch [560/938], Loss: 0.45438697934150696\n",
      "Train: Epoch [16], Batch [561/938], Loss: 0.8614648580551147\n",
      "Train: Epoch [16], Batch [562/938], Loss: 0.9084276556968689\n",
      "Train: Epoch [16], Batch [563/938], Loss: 0.8122975826263428\n",
      "Train: Epoch [16], Batch [564/938], Loss: 0.7532626390457153\n",
      "Train: Epoch [16], Batch [565/938], Loss: 0.595998227596283\n",
      "Train: Epoch [16], Batch [566/938], Loss: 0.6113202571868896\n",
      "Train: Epoch [16], Batch [567/938], Loss: 0.6629980206489563\n",
      "Train: Epoch [16], Batch [568/938], Loss: 0.6469757556915283\n",
      "Train: Epoch [16], Batch [569/938], Loss: 0.45328792929649353\n",
      "Train: Epoch [16], Batch [570/938], Loss: 0.5235748291015625\n",
      "Train: Epoch [16], Batch [571/938], Loss: 0.779660165309906\n",
      "Train: Epoch [16], Batch [572/938], Loss: 0.541292667388916\n",
      "Train: Epoch [16], Batch [573/938], Loss: 0.48144012689590454\n",
      "Train: Epoch [16], Batch [574/938], Loss: 0.8341836333274841\n",
      "Train: Epoch [16], Batch [575/938], Loss: 0.4607320725917816\n",
      "Train: Epoch [16], Batch [576/938], Loss: 0.6689715385437012\n",
      "Train: Epoch [16], Batch [577/938], Loss: 0.5274954438209534\n",
      "Train: Epoch [16], Batch [578/938], Loss: 0.762911319732666\n",
      "Train: Epoch [16], Batch [579/938], Loss: 0.5736609697341919\n",
      "Train: Epoch [16], Batch [580/938], Loss: 0.7009736895561218\n",
      "Train: Epoch [16], Batch [581/938], Loss: 0.7608815431594849\n",
      "Train: Epoch [16], Batch [582/938], Loss: 0.7365745306015015\n",
      "Train: Epoch [16], Batch [583/938], Loss: 0.7185266613960266\n",
      "Train: Epoch [16], Batch [584/938], Loss: 0.5675826072692871\n",
      "Train: Epoch [16], Batch [585/938], Loss: 0.755120038986206\n",
      "Train: Epoch [16], Batch [586/938], Loss: 0.6894737482070923\n",
      "Train: Epoch [16], Batch [587/938], Loss: 0.6548241376876831\n",
      "Train: Epoch [16], Batch [588/938], Loss: 0.7500253915786743\n",
      "Train: Epoch [16], Batch [589/938], Loss: 0.8540502190589905\n",
      "Train: Epoch [16], Batch [590/938], Loss: 0.5915865302085876\n",
      "Train: Epoch [16], Batch [591/938], Loss: 0.7937190532684326\n",
      "Train: Epoch [16], Batch [592/938], Loss: 0.5843875408172607\n",
      "Train: Epoch [16], Batch [593/938], Loss: 0.6117892861366272\n",
      "Train: Epoch [16], Batch [594/938], Loss: 0.4232589900493622\n",
      "Train: Epoch [16], Batch [595/938], Loss: 0.6011828780174255\n",
      "Train: Epoch [16], Batch [596/938], Loss: 0.9264732003211975\n",
      "Train: Epoch [16], Batch [597/938], Loss: 0.8106289505958557\n",
      "Train: Epoch [16], Batch [598/938], Loss: 0.7091121077537537\n",
      "Train: Epoch [16], Batch [599/938], Loss: 0.8376151323318481\n",
      "Train: Epoch [16], Batch [600/938], Loss: 0.758171558380127\n",
      "Train: Epoch [16], Batch [601/938], Loss: 0.5649760365486145\n",
      "Train: Epoch [16], Batch [602/938], Loss: 0.7298197150230408\n",
      "Train: Epoch [16], Batch [603/938], Loss: 0.8513742685317993\n",
      "Train: Epoch [16], Batch [604/938], Loss: 0.639559805393219\n",
      "Train: Epoch [16], Batch [605/938], Loss: 0.5860657095909119\n",
      "Train: Epoch [16], Batch [606/938], Loss: 0.7082669734954834\n",
      "Train: Epoch [16], Batch [607/938], Loss: 0.6339062452316284\n",
      "Train: Epoch [16], Batch [608/938], Loss: 0.7168183922767639\n",
      "Train: Epoch [16], Batch [609/938], Loss: 0.7731903791427612\n",
      "Train: Epoch [16], Batch [610/938], Loss: 0.7670862674713135\n",
      "Train: Epoch [16], Batch [611/938], Loss: 0.7336603403091431\n",
      "Train: Epoch [16], Batch [612/938], Loss: 0.8866003155708313\n",
      "Train: Epoch [16], Batch [613/938], Loss: 0.5395219922065735\n",
      "Train: Epoch [16], Batch [614/938], Loss: 0.9350399971008301\n",
      "Train: Epoch [16], Batch [615/938], Loss: 0.7947534322738647\n",
      "Train: Epoch [16], Batch [616/938], Loss: 0.6857041120529175\n",
      "Train: Epoch [16], Batch [617/938], Loss: 0.7552134394645691\n",
      "Train: Epoch [16], Batch [618/938], Loss: 0.9507569670677185\n",
      "Train: Epoch [16], Batch [619/938], Loss: 0.7220298647880554\n",
      "Train: Epoch [16], Batch [620/938], Loss: 0.8192384243011475\n",
      "Train: Epoch [16], Batch [621/938], Loss: 0.555616557598114\n",
      "Train: Epoch [16], Batch [622/938], Loss: 0.6871797442436218\n",
      "Train: Epoch [16], Batch [623/938], Loss: 0.766242265701294\n",
      "Train: Epoch [16], Batch [624/938], Loss: 0.6342241764068604\n",
      "Train: Epoch [16], Batch [625/938], Loss: 0.7247132062911987\n",
      "Train: Epoch [16], Batch [626/938], Loss: 0.8151327967643738\n",
      "Train: Epoch [16], Batch [627/938], Loss: 0.5789518356323242\n",
      "Train: Epoch [16], Batch [628/938], Loss: 0.7703418135643005\n",
      "Train: Epoch [16], Batch [629/938], Loss: 0.5339856743812561\n",
      "Train: Epoch [16], Batch [630/938], Loss: 0.7537822127342224\n",
      "Train: Epoch [16], Batch [631/938], Loss: 0.5800043344497681\n",
      "Train: Epoch [16], Batch [632/938], Loss: 0.505339503288269\n",
      "Train: Epoch [16], Batch [633/938], Loss: 0.8134993314743042\n",
      "Train: Epoch [16], Batch [634/938], Loss: 0.7160449624061584\n",
      "Train: Epoch [16], Batch [635/938], Loss: 0.5414912104606628\n",
      "Train: Epoch [16], Batch [636/938], Loss: 0.5725415349006653\n",
      "Train: Epoch [16], Batch [637/938], Loss: 0.7309582233428955\n",
      "Train: Epoch [16], Batch [638/938], Loss: 0.5559324026107788\n",
      "Train: Epoch [16], Batch [639/938], Loss: 0.6774795055389404\n",
      "Train: Epoch [16], Batch [640/938], Loss: 0.6624665856361389\n",
      "Train: Epoch [16], Batch [641/938], Loss: 0.6696512699127197\n",
      "Train: Epoch [16], Batch [642/938], Loss: 0.8610224723815918\n",
      "Train: Epoch [16], Batch [643/938], Loss: 0.8051115274429321\n",
      "Train: Epoch [16], Batch [644/938], Loss: 0.7888048887252808\n",
      "Train: Epoch [16], Batch [645/938], Loss: 0.4188905656337738\n",
      "Train: Epoch [16], Batch [646/938], Loss: 0.7018347978591919\n",
      "Train: Epoch [16], Batch [647/938], Loss: 0.6736600399017334\n",
      "Train: Epoch [16], Batch [648/938], Loss: 0.5839296579360962\n",
      "Train: Epoch [16], Batch [649/938], Loss: 0.5957266092300415\n",
      "Train: Epoch [16], Batch [650/938], Loss: 0.6751924753189087\n",
      "Train: Epoch [16], Batch [651/938], Loss: 0.5248164534568787\n",
      "Train: Epoch [16], Batch [652/938], Loss: 0.8865892291069031\n",
      "Train: Epoch [16], Batch [653/938], Loss: 0.663785994052887\n",
      "Train: Epoch [16], Batch [654/938], Loss: 0.5259535312652588\n",
      "Train: Epoch [16], Batch [655/938], Loss: 0.569689154624939\n",
      "Train: Epoch [16], Batch [656/938], Loss: 0.8760544657707214\n",
      "Train: Epoch [16], Batch [657/938], Loss: 0.7482050657272339\n",
      "Train: Epoch [16], Batch [658/938], Loss: 0.6739963293075562\n",
      "Train: Epoch [16], Batch [659/938], Loss: 0.6915873289108276\n",
      "Train: Epoch [16], Batch [660/938], Loss: 0.8718740940093994\n",
      "Train: Epoch [16], Batch [661/938], Loss: 0.5723816752433777\n",
      "Train: Epoch [16], Batch [662/938], Loss: 0.9808071255683899\n",
      "Train: Epoch [16], Batch [663/938], Loss: 0.7978973388671875\n",
      "Train: Epoch [16], Batch [664/938], Loss: 0.43313056230545044\n",
      "Train: Epoch [16], Batch [665/938], Loss: 0.835789144039154\n",
      "Train: Epoch [16], Batch [666/938], Loss: 0.60364830493927\n",
      "Train: Epoch [16], Batch [667/938], Loss: 0.7679215669631958\n",
      "Train: Epoch [16], Batch [668/938], Loss: 0.4916653633117676\n",
      "Train: Epoch [16], Batch [669/938], Loss: 0.676798403263092\n",
      "Train: Epoch [16], Batch [670/938], Loss: 0.7283819913864136\n",
      "Train: Epoch [16], Batch [671/938], Loss: 0.7891356945037842\n",
      "Train: Epoch [16], Batch [672/938], Loss: 0.650298535823822\n",
      "Train: Epoch [16], Batch [673/938], Loss: 0.7464724183082581\n",
      "Train: Epoch [16], Batch [674/938], Loss: 0.5494037866592407\n",
      "Train: Epoch [16], Batch [675/938], Loss: 0.8756887316703796\n",
      "Train: Epoch [16], Batch [676/938], Loss: 0.8074927926063538\n",
      "Train: Epoch [16], Batch [677/938], Loss: 0.5698467493057251\n",
      "Train: Epoch [16], Batch [678/938], Loss: 0.7896856069564819\n",
      "Train: Epoch [16], Batch [679/938], Loss: 0.6307988166809082\n",
      "Train: Epoch [16], Batch [680/938], Loss: 0.5384979248046875\n",
      "Train: Epoch [16], Batch [681/938], Loss: 0.8026167154312134\n",
      "Train: Epoch [16], Batch [682/938], Loss: 0.581548273563385\n",
      "Train: Epoch [16], Batch [683/938], Loss: 0.7123426795005798\n",
      "Train: Epoch [16], Batch [684/938], Loss: 0.8160534501075745\n",
      "Train: Epoch [16], Batch [685/938], Loss: 0.7932889461517334\n",
      "Train: Epoch [16], Batch [686/938], Loss: 0.7600044012069702\n",
      "Train: Epoch [16], Batch [687/938], Loss: 0.6997491717338562\n",
      "Train: Epoch [16], Batch [688/938], Loss: 0.727304220199585\n",
      "Train: Epoch [16], Batch [689/938], Loss: 0.5997563600540161\n",
      "Train: Epoch [16], Batch [690/938], Loss: 0.5411957502365112\n",
      "Train: Epoch [16], Batch [691/938], Loss: 0.8635956048965454\n",
      "Train: Epoch [16], Batch [692/938], Loss: 0.5947330594062805\n",
      "Train: Epoch [16], Batch [693/938], Loss: 1.3132085800170898\n",
      "Train: Epoch [16], Batch [694/938], Loss: 0.719404935836792\n",
      "Train: Epoch [16], Batch [695/938], Loss: 0.5987621545791626\n",
      "Train: Epoch [16], Batch [696/938], Loss: 0.7711750268936157\n",
      "Train: Epoch [16], Batch [697/938], Loss: 0.726743221282959\n",
      "Train: Epoch [16], Batch [698/938], Loss: 0.8591416478157043\n",
      "Train: Epoch [16], Batch [699/938], Loss: 0.8028568625450134\n",
      "Train: Epoch [16], Batch [700/938], Loss: 0.8885077238082886\n",
      "Train: Epoch [16], Batch [701/938], Loss: 0.63532555103302\n",
      "Train: Epoch [16], Batch [702/938], Loss: 0.7823219299316406\n",
      "Train: Epoch [16], Batch [703/938], Loss: 0.9679414629936218\n",
      "Train: Epoch [16], Batch [704/938], Loss: 0.6670002937316895\n",
      "Train: Epoch [16], Batch [705/938], Loss: 0.6602787971496582\n",
      "Train: Epoch [16], Batch [706/938], Loss: 0.5224105715751648\n",
      "Train: Epoch [16], Batch [707/938], Loss: 0.8565622568130493\n",
      "Train: Epoch [16], Batch [708/938], Loss: 0.7983808517456055\n",
      "Train: Epoch [16], Batch [709/938], Loss: 0.6084564328193665\n",
      "Train: Epoch [16], Batch [710/938], Loss: 0.8104007244110107\n",
      "Train: Epoch [16], Batch [711/938], Loss: 0.8730734586715698\n",
      "Train: Epoch [16], Batch [712/938], Loss: 0.587333619594574\n",
      "Train: Epoch [16], Batch [713/938], Loss: 0.8313078284263611\n",
      "Train: Epoch [16], Batch [714/938], Loss: 0.5448518395423889\n",
      "Train: Epoch [16], Batch [715/938], Loss: 0.7553576231002808\n",
      "Train: Epoch [16], Batch [716/938], Loss: 0.5315074324607849\n",
      "Train: Epoch [16], Batch [717/938], Loss: 0.6435303092002869\n",
      "Train: Epoch [16], Batch [718/938], Loss: 0.5102095007896423\n",
      "Train: Epoch [16], Batch [719/938], Loss: 0.7303628325462341\n",
      "Train: Epoch [16], Batch [720/938], Loss: 0.44896745681762695\n",
      "Train: Epoch [16], Batch [721/938], Loss: 0.9080158472061157\n",
      "Train: Epoch [16], Batch [722/938], Loss: 0.7439401745796204\n",
      "Train: Epoch [16], Batch [723/938], Loss: 0.7045324444770813\n",
      "Train: Epoch [16], Batch [724/938], Loss: 0.5135490298271179\n",
      "Train: Epoch [16], Batch [725/938], Loss: 0.6711961627006531\n",
      "Train: Epoch [16], Batch [726/938], Loss: 0.541492223739624\n",
      "Train: Epoch [16], Batch [727/938], Loss: 0.585934579372406\n",
      "Train: Epoch [16], Batch [728/938], Loss: 0.6333308815956116\n",
      "Train: Epoch [16], Batch [729/938], Loss: 0.745969295501709\n",
      "Train: Epoch [16], Batch [730/938], Loss: 0.8125618100166321\n",
      "Train: Epoch [16], Batch [731/938], Loss: 0.7020431160926819\n",
      "Train: Epoch [16], Batch [732/938], Loss: 0.6243710517883301\n",
      "Train: Epoch [16], Batch [733/938], Loss: 0.39358392357826233\n",
      "Train: Epoch [16], Batch [734/938], Loss: 0.6669273376464844\n",
      "Train: Epoch [16], Batch [735/938], Loss: 0.6846215128898621\n",
      "Train: Epoch [16], Batch [736/938], Loss: 0.5381569862365723\n",
      "Train: Epoch [16], Batch [737/938], Loss: 0.4316447675228119\n",
      "Train: Epoch [16], Batch [738/938], Loss: 0.6845471858978271\n",
      "Train: Epoch [16], Batch [739/938], Loss: 0.5781795978546143\n",
      "Train: Epoch [16], Batch [740/938], Loss: 0.6349545121192932\n",
      "Train: Epoch [16], Batch [741/938], Loss: 0.8707695603370667\n",
      "Train: Epoch [16], Batch [742/938], Loss: 0.5767551064491272\n",
      "Train: Epoch [16], Batch [743/938], Loss: 0.644087016582489\n",
      "Train: Epoch [16], Batch [744/938], Loss: 0.5641946196556091\n",
      "Train: Epoch [16], Batch [745/938], Loss: 0.7287319302558899\n",
      "Train: Epoch [16], Batch [746/938], Loss: 0.9813182950019836\n",
      "Train: Epoch [16], Batch [747/938], Loss: 0.6985039114952087\n",
      "Train: Epoch [16], Batch [748/938], Loss: 0.4916892647743225\n",
      "Train: Epoch [16], Batch [749/938], Loss: 0.5458465814590454\n",
      "Train: Epoch [16], Batch [750/938], Loss: 0.7099453806877136\n",
      "Train: Epoch [16], Batch [751/938], Loss: 0.8992393612861633\n",
      "Train: Epoch [16], Batch [752/938], Loss: 0.8108620643615723\n",
      "Train: Epoch [16], Batch [753/938], Loss: 0.6134827136993408\n",
      "Train: Epoch [16], Batch [754/938], Loss: 0.6873104572296143\n",
      "Train: Epoch [16], Batch [755/938], Loss: 1.0575876235961914\n",
      "Train: Epoch [16], Batch [756/938], Loss: 0.601344883441925\n",
      "Train: Epoch [16], Batch [757/938], Loss: 0.7661905288696289\n",
      "Train: Epoch [16], Batch [758/938], Loss: 0.615693211555481\n",
      "Train: Epoch [16], Batch [759/938], Loss: 0.7787617444992065\n",
      "Train: Epoch [16], Batch [760/938], Loss: 0.6607582569122314\n",
      "Train: Epoch [16], Batch [761/938], Loss: 0.5484199523925781\n",
      "Train: Epoch [16], Batch [762/938], Loss: 0.7746747732162476\n",
      "Train: Epoch [16], Batch [763/938], Loss: 0.7146025896072388\n",
      "Train: Epoch [16], Batch [764/938], Loss: 0.5464553236961365\n",
      "Train: Epoch [16], Batch [765/938], Loss: 0.7581367492675781\n",
      "Train: Epoch [16], Batch [766/938], Loss: 0.6149099469184875\n",
      "Train: Epoch [16], Batch [767/938], Loss: 0.8880355358123779\n",
      "Train: Epoch [16], Batch [768/938], Loss: 0.7076621651649475\n",
      "Train: Epoch [16], Batch [769/938], Loss: 0.5532751083374023\n",
      "Train: Epoch [16], Batch [770/938], Loss: 0.7214171886444092\n",
      "Train: Epoch [16], Batch [771/938], Loss: 0.521859347820282\n",
      "Train: Epoch [16], Batch [772/938], Loss: 0.8056309819221497\n",
      "Train: Epoch [16], Batch [773/938], Loss: 0.6696123480796814\n",
      "Train: Epoch [16], Batch [774/938], Loss: 0.5308756232261658\n",
      "Train: Epoch [16], Batch [775/938], Loss: 0.7667735815048218\n",
      "Train: Epoch [16], Batch [776/938], Loss: 0.6462644338607788\n",
      "Train: Epoch [16], Batch [777/938], Loss: 0.7183712720870972\n",
      "Train: Epoch [16], Batch [778/938], Loss: 0.6688587665557861\n",
      "Train: Epoch [16], Batch [779/938], Loss: 0.8677334785461426\n",
      "Train: Epoch [16], Batch [780/938], Loss: 0.7035316228866577\n",
      "Train: Epoch [16], Batch [781/938], Loss: 0.6723264455795288\n",
      "Train: Epoch [16], Batch [782/938], Loss: 0.8420698642730713\n",
      "Train: Epoch [16], Batch [783/938], Loss: 0.8342723250389099\n",
      "Train: Epoch [16], Batch [784/938], Loss: 0.7685730457305908\n",
      "Train: Epoch [16], Batch [785/938], Loss: 0.6444082856178284\n",
      "Train: Epoch [16], Batch [786/938], Loss: 0.6795884966850281\n",
      "Train: Epoch [16], Batch [787/938], Loss: 0.5799717307090759\n",
      "Train: Epoch [16], Batch [788/938], Loss: 0.8109487891197205\n",
      "Train: Epoch [16], Batch [789/938], Loss: 0.7910345792770386\n",
      "Train: Epoch [16], Batch [790/938], Loss: 0.5751910209655762\n",
      "Train: Epoch [16], Batch [791/938], Loss: 0.8253905177116394\n",
      "Train: Epoch [16], Batch [792/938], Loss: 0.6107730865478516\n",
      "Train: Epoch [16], Batch [793/938], Loss: 0.685362696647644\n",
      "Train: Epoch [16], Batch [794/938], Loss: 0.6390363574028015\n",
      "Train: Epoch [16], Batch [795/938], Loss: 0.4861382246017456\n",
      "Train: Epoch [16], Batch [796/938], Loss: 0.8799444437026978\n",
      "Train: Epoch [16], Batch [797/938], Loss: 0.5945648550987244\n",
      "Train: Epoch [16], Batch [798/938], Loss: 0.5665622353553772\n",
      "Train: Epoch [16], Batch [799/938], Loss: 0.5397161841392517\n",
      "Train: Epoch [16], Batch [800/938], Loss: 0.7558943033218384\n",
      "Train: Epoch [16], Batch [801/938], Loss: 0.7125158309936523\n",
      "Train: Epoch [16], Batch [802/938], Loss: 0.6383596062660217\n",
      "Train: Epoch [16], Batch [803/938], Loss: 0.7214789390563965\n",
      "Train: Epoch [16], Batch [804/938], Loss: 0.7308762669563293\n",
      "Train: Epoch [16], Batch [805/938], Loss: 0.8167709112167358\n",
      "Train: Epoch [16], Batch [806/938], Loss: 0.7229719161987305\n",
      "Train: Epoch [16], Batch [807/938], Loss: 0.4446890950202942\n",
      "Train: Epoch [16], Batch [808/938], Loss: 0.7372674942016602\n",
      "Train: Epoch [16], Batch [809/938], Loss: 0.7763396501541138\n",
      "Train: Epoch [16], Batch [810/938], Loss: 0.5415530204772949\n",
      "Train: Epoch [16], Batch [811/938], Loss: 1.0261142253875732\n",
      "Train: Epoch [16], Batch [812/938], Loss: 0.9126739501953125\n",
      "Train: Epoch [16], Batch [813/938], Loss: 0.633188784122467\n",
      "Train: Epoch [16], Batch [814/938], Loss: 0.7678411602973938\n",
      "Train: Epoch [16], Batch [815/938], Loss: 0.5312831997871399\n",
      "Train: Epoch [16], Batch [816/938], Loss: 0.7454430460929871\n",
      "Train: Epoch [16], Batch [817/938], Loss: 0.8395985960960388\n",
      "Train: Epoch [16], Batch [818/938], Loss: 0.85506272315979\n",
      "Train: Epoch [16], Batch [819/938], Loss: 0.6226221323013306\n",
      "Train: Epoch [16], Batch [820/938], Loss: 0.5676692128181458\n",
      "Train: Epoch [16], Batch [821/938], Loss: 0.6127614378929138\n",
      "Train: Epoch [16], Batch [822/938], Loss: 0.5587369799613953\n",
      "Train: Epoch [16], Batch [823/938], Loss: 0.719074547290802\n",
      "Train: Epoch [16], Batch [824/938], Loss: 0.7079381942749023\n",
      "Train: Epoch [16], Batch [825/938], Loss: 0.47742921113967896\n",
      "Train: Epoch [16], Batch [826/938], Loss: 0.7162771821022034\n",
      "Train: Epoch [16], Batch [827/938], Loss: 0.6621178984642029\n",
      "Train: Epoch [16], Batch [828/938], Loss: 0.7918907403945923\n",
      "Train: Epoch [16], Batch [829/938], Loss: 0.6835494637489319\n",
      "Train: Epoch [16], Batch [830/938], Loss: 0.6810939908027649\n",
      "Train: Epoch [16], Batch [831/938], Loss: 0.8453359007835388\n",
      "Train: Epoch [16], Batch [832/938], Loss: 0.542847216129303\n",
      "Train: Epoch [16], Batch [833/938], Loss: 0.6351641416549683\n",
      "Train: Epoch [16], Batch [834/938], Loss: 0.8006846308708191\n",
      "Train: Epoch [16], Batch [835/938], Loss: 0.6124111413955688\n",
      "Train: Epoch [16], Batch [836/938], Loss: 0.6290194988250732\n",
      "Train: Epoch [16], Batch [837/938], Loss: 0.6271507143974304\n",
      "Train: Epoch [16], Batch [838/938], Loss: 0.972740888595581\n",
      "Train: Epoch [16], Batch [839/938], Loss: 0.6366573572158813\n",
      "Train: Epoch [16], Batch [840/938], Loss: 0.9357172250747681\n",
      "Train: Epoch [16], Batch [841/938], Loss: 0.5444283485412598\n",
      "Train: Epoch [16], Batch [842/938], Loss: 0.7481715679168701\n",
      "Train: Epoch [16], Batch [843/938], Loss: 0.5149415135383606\n",
      "Train: Epoch [16], Batch [844/938], Loss: 0.570961594581604\n",
      "Train: Epoch [16], Batch [845/938], Loss: 0.8905719518661499\n",
      "Train: Epoch [16], Batch [846/938], Loss: 0.8224682807922363\n",
      "Train: Epoch [16], Batch [847/938], Loss: 0.635771632194519\n",
      "Train: Epoch [16], Batch [848/938], Loss: 0.5726007223129272\n",
      "Train: Epoch [16], Batch [849/938], Loss: 0.5577914714813232\n",
      "Train: Epoch [16], Batch [850/938], Loss: 0.6783890724182129\n",
      "Train: Epoch [16], Batch [851/938], Loss: 0.5944963693618774\n",
      "Train: Epoch [16], Batch [852/938], Loss: 0.7013093829154968\n",
      "Train: Epoch [16], Batch [853/938], Loss: 0.877272367477417\n",
      "Train: Epoch [16], Batch [854/938], Loss: 0.4265274405479431\n",
      "Train: Epoch [16], Batch [855/938], Loss: 0.896816074848175\n",
      "Train: Epoch [16], Batch [856/938], Loss: 0.6479787826538086\n",
      "Train: Epoch [16], Batch [857/938], Loss: 0.5574800372123718\n",
      "Train: Epoch [16], Batch [858/938], Loss: 0.7570714950561523\n",
      "Train: Epoch [16], Batch [859/938], Loss: 0.7263389825820923\n",
      "Train: Epoch [16], Batch [860/938], Loss: 0.677055835723877\n",
      "Train: Epoch [16], Batch [861/938], Loss: 0.7058588862419128\n",
      "Train: Epoch [16], Batch [862/938], Loss: 0.7118963003158569\n",
      "Train: Epoch [16], Batch [863/938], Loss: 0.6672504544258118\n",
      "Train: Epoch [16], Batch [864/938], Loss: 0.8151506185531616\n",
      "Train: Epoch [16], Batch [865/938], Loss: 0.8750643134117126\n",
      "Train: Epoch [16], Batch [866/938], Loss: 0.7597611546516418\n",
      "Train: Epoch [16], Batch [867/938], Loss: 0.7080152034759521\n",
      "Train: Epoch [16], Batch [868/938], Loss: 0.8276664614677429\n",
      "Train: Epoch [16], Batch [869/938], Loss: 0.44828855991363525\n",
      "Train: Epoch [16], Batch [870/938], Loss: 0.6073500514030457\n",
      "Train: Epoch [16], Batch [871/938], Loss: 0.7723764181137085\n",
      "Train: Epoch [16], Batch [872/938], Loss: 0.778701663017273\n",
      "Train: Epoch [16], Batch [873/938], Loss: 0.6749832034111023\n",
      "Train: Epoch [16], Batch [874/938], Loss: 0.8988386988639832\n",
      "Train: Epoch [16], Batch [875/938], Loss: 0.9507544040679932\n",
      "Train: Epoch [16], Batch [876/938], Loss: 0.7479791641235352\n",
      "Train: Epoch [16], Batch [877/938], Loss: 0.6242737770080566\n",
      "Train: Epoch [16], Batch [878/938], Loss: 0.7179121375083923\n",
      "Train: Epoch [16], Batch [879/938], Loss: 0.4210341274738312\n",
      "Train: Epoch [16], Batch [880/938], Loss: 0.49096548557281494\n",
      "Train: Epoch [16], Batch [881/938], Loss: 0.7419541478157043\n",
      "Train: Epoch [16], Batch [882/938], Loss: 0.6182928681373596\n",
      "Train: Epoch [16], Batch [883/938], Loss: 0.691353440284729\n",
      "Train: Epoch [16], Batch [884/938], Loss: 0.5550383925437927\n",
      "Train: Epoch [16], Batch [885/938], Loss: 0.7878514528274536\n",
      "Train: Epoch [16], Batch [886/938], Loss: 0.509726881980896\n",
      "Train: Epoch [16], Batch [887/938], Loss: 0.5306214690208435\n",
      "Train: Epoch [16], Batch [888/938], Loss: 0.7432212829589844\n",
      "Train: Epoch [16], Batch [889/938], Loss: 0.4716321527957916\n",
      "Train: Epoch [16], Batch [890/938], Loss: 0.7254027724266052\n",
      "Train: Epoch [16], Batch [891/938], Loss: 0.6521812081336975\n",
      "Train: Epoch [16], Batch [892/938], Loss: 0.4775824546813965\n",
      "Train: Epoch [16], Batch [893/938], Loss: 0.8300343155860901\n",
      "Train: Epoch [16], Batch [894/938], Loss: 0.6769892573356628\n",
      "Train: Epoch [16], Batch [895/938], Loss: 0.890908420085907\n",
      "Train: Epoch [16], Batch [896/938], Loss: 0.5731717348098755\n",
      "Train: Epoch [16], Batch [897/938], Loss: 0.4613190293312073\n",
      "Train: Epoch [16], Batch [898/938], Loss: 0.6554437279701233\n",
      "Train: Epoch [16], Batch [899/938], Loss: 0.8780160546302795\n",
      "Train: Epoch [16], Batch [900/938], Loss: 0.6912182569503784\n",
      "Train: Epoch [16], Batch [901/938], Loss: 0.7946853637695312\n",
      "Train: Epoch [16], Batch [902/938], Loss: 0.6871554255485535\n",
      "Train: Epoch [16], Batch [903/938], Loss: 0.6630034446716309\n",
      "Train: Epoch [16], Batch [904/938], Loss: 0.5250722169876099\n",
      "Train: Epoch [16], Batch [905/938], Loss: 0.4296860098838806\n",
      "Train: Epoch [16], Batch [906/938], Loss: 0.7600982785224915\n",
      "Train: Epoch [16], Batch [907/938], Loss: 0.5509932041168213\n",
      "Train: Epoch [16], Batch [908/938], Loss: 0.7419943809509277\n",
      "Train: Epoch [16], Batch [909/938], Loss: 1.0807017087936401\n",
      "Train: Epoch [16], Batch [910/938], Loss: 0.823373556137085\n",
      "Train: Epoch [16], Batch [911/938], Loss: 0.7192190289497375\n",
      "Train: Epoch [16], Batch [912/938], Loss: 0.9105982780456543\n",
      "Train: Epoch [16], Batch [913/938], Loss: 0.5390728712081909\n",
      "Train: Epoch [16], Batch [914/938], Loss: 0.9028780460357666\n",
      "Train: Epoch [16], Batch [915/938], Loss: 0.6920253038406372\n",
      "Train: Epoch [16], Batch [916/938], Loss: 0.7513521313667297\n",
      "Train: Epoch [16], Batch [917/938], Loss: 0.5262674689292908\n",
      "Train: Epoch [16], Batch [918/938], Loss: 0.594660758972168\n",
      "Train: Epoch [16], Batch [919/938], Loss: 0.5724651217460632\n",
      "Train: Epoch [16], Batch [920/938], Loss: 0.5684174299240112\n",
      "Train: Epoch [16], Batch [921/938], Loss: 0.7165254354476929\n",
      "Train: Epoch [16], Batch [922/938], Loss: 0.952126681804657\n",
      "Train: Epoch [16], Batch [923/938], Loss: 0.6752082109451294\n",
      "Train: Epoch [16], Batch [924/938], Loss: 0.7566795349121094\n",
      "Train: Epoch [16], Batch [925/938], Loss: 1.0461387634277344\n",
      "Train: Epoch [16], Batch [926/938], Loss: 0.5948637127876282\n",
      "Train: Epoch [16], Batch [927/938], Loss: 0.8192880749702454\n",
      "Train: Epoch [16], Batch [928/938], Loss: 0.5801783204078674\n",
      "Train: Epoch [16], Batch [929/938], Loss: 0.7205228805541992\n",
      "Train: Epoch [16], Batch [930/938], Loss: 0.9294284582138062\n",
      "Train: Epoch [16], Batch [931/938], Loss: 0.6464218497276306\n",
      "Train: Epoch [16], Batch [932/938], Loss: 0.9742507934570312\n",
      "Train: Epoch [16], Batch [933/938], Loss: 0.8326174020767212\n",
      "Train: Epoch [16], Batch [934/938], Loss: 0.8803492188453674\n",
      "Train: Epoch [16], Batch [935/938], Loss: 0.647517740726471\n",
      "Train: Epoch [16], Batch [936/938], Loss: 0.9191237688064575\n",
      "Train: Epoch [16], Batch [937/938], Loss: 0.7978161573410034\n",
      "Train: Epoch [16], Batch [938/938], Loss: 0.9558490514755249\n",
      "Accuracy of train set: 0.7905666666666666\n",
      "Validation: Epoch [16], Batch [1/938], Loss: 0.7112007141113281\n",
      "Validation: Epoch [16], Batch [2/938], Loss: 0.5842873454093933\n",
      "Validation: Epoch [16], Batch [3/938], Loss: 0.639531135559082\n",
      "Validation: Epoch [16], Batch [4/938], Loss: 0.46086394786834717\n",
      "Validation: Epoch [16], Batch [5/938], Loss: 0.7761721014976501\n",
      "Validation: Epoch [16], Batch [6/938], Loss: 0.9368659257888794\n",
      "Validation: Epoch [16], Batch [7/938], Loss: 0.3830520510673523\n",
      "Validation: Epoch [16], Batch [8/938], Loss: 0.8654347062110901\n",
      "Validation: Epoch [16], Batch [9/938], Loss: 0.5649427771568298\n",
      "Validation: Epoch [16], Batch [10/938], Loss: 0.6356008052825928\n",
      "Validation: Epoch [16], Batch [11/938], Loss: 0.6954705119132996\n",
      "Validation: Epoch [16], Batch [12/938], Loss: 0.5871421694755554\n",
      "Validation: Epoch [16], Batch [13/938], Loss: 0.8051638007164001\n",
      "Validation: Epoch [16], Batch [14/938], Loss: 0.36745283007621765\n",
      "Validation: Epoch [16], Batch [15/938], Loss: 0.5859609842300415\n",
      "Validation: Epoch [16], Batch [16/938], Loss: 0.7411176562309265\n",
      "Validation: Epoch [16], Batch [17/938], Loss: 0.6430984139442444\n",
      "Validation: Epoch [16], Batch [18/938], Loss: 1.0341105461120605\n",
      "Validation: Epoch [16], Batch [19/938], Loss: 0.7283384799957275\n",
      "Validation: Epoch [16], Batch [20/938], Loss: 0.6183876395225525\n",
      "Validation: Epoch [16], Batch [21/938], Loss: 0.52602618932724\n",
      "Validation: Epoch [16], Batch [22/938], Loss: 0.6454296708106995\n",
      "Validation: Epoch [16], Batch [23/938], Loss: 0.7199124097824097\n",
      "Validation: Epoch [16], Batch [24/938], Loss: 0.7820194959640503\n",
      "Validation: Epoch [16], Batch [25/938], Loss: 0.7335016131401062\n",
      "Validation: Epoch [16], Batch [26/938], Loss: 0.5285029411315918\n",
      "Validation: Epoch [16], Batch [27/938], Loss: 0.6274781823158264\n",
      "Validation: Epoch [16], Batch [28/938], Loss: 0.8256518244743347\n",
      "Validation: Epoch [16], Batch [29/938], Loss: 0.9416419267654419\n",
      "Validation: Epoch [16], Batch [30/938], Loss: 0.769248902797699\n",
      "Validation: Epoch [16], Batch [31/938], Loss: 0.694093644618988\n",
      "Validation: Epoch [16], Batch [32/938], Loss: 0.5667603611946106\n",
      "Validation: Epoch [16], Batch [33/938], Loss: 0.733048677444458\n",
      "Validation: Epoch [16], Batch [34/938], Loss: 0.6454398036003113\n",
      "Validation: Epoch [16], Batch [35/938], Loss: 0.5632889270782471\n",
      "Validation: Epoch [16], Batch [36/938], Loss: 0.7005725502967834\n",
      "Validation: Epoch [16], Batch [37/938], Loss: 0.6956672072410583\n",
      "Validation: Epoch [16], Batch [38/938], Loss: 0.76173335313797\n",
      "Validation: Epoch [16], Batch [39/938], Loss: 0.7396517992019653\n",
      "Validation: Epoch [16], Batch [40/938], Loss: 0.7476962208747864\n",
      "Validation: Epoch [16], Batch [41/938], Loss: 0.7117865085601807\n",
      "Validation: Epoch [16], Batch [42/938], Loss: 0.4522117078304291\n",
      "Validation: Epoch [16], Batch [43/938], Loss: 0.7326721549034119\n",
      "Validation: Epoch [16], Batch [44/938], Loss: 0.7807260155677795\n",
      "Validation: Epoch [16], Batch [45/938], Loss: 0.5555891990661621\n",
      "Validation: Epoch [16], Batch [46/938], Loss: 0.6710495948791504\n",
      "Validation: Epoch [16], Batch [47/938], Loss: 0.6615886688232422\n",
      "Validation: Epoch [16], Batch [48/938], Loss: 0.5353453755378723\n",
      "Validation: Epoch [16], Batch [49/938], Loss: 0.6351910829544067\n",
      "Validation: Epoch [16], Batch [50/938], Loss: 0.6044779419898987\n",
      "Validation: Epoch [16], Batch [51/938], Loss: 0.7487825155258179\n",
      "Validation: Epoch [16], Batch [52/938], Loss: 0.7161970138549805\n",
      "Validation: Epoch [16], Batch [53/938], Loss: 0.7045371532440186\n",
      "Validation: Epoch [16], Batch [54/938], Loss: 0.7960027456283569\n",
      "Validation: Epoch [16], Batch [55/938], Loss: 0.6587229371070862\n",
      "Validation: Epoch [16], Batch [56/938], Loss: 0.5217742919921875\n",
      "Validation: Epoch [16], Batch [57/938], Loss: 0.5501086711883545\n",
      "Validation: Epoch [16], Batch [58/938], Loss: 0.6351562738418579\n",
      "Validation: Epoch [16], Batch [59/938], Loss: 0.6428268551826477\n",
      "Validation: Epoch [16], Batch [60/938], Loss: 0.5611791610717773\n",
      "Validation: Epoch [16], Batch [61/938], Loss: 0.6371065378189087\n",
      "Validation: Epoch [16], Batch [62/938], Loss: 0.42436277866363525\n",
      "Validation: Epoch [16], Batch [63/938], Loss: 0.7445486187934875\n",
      "Validation: Epoch [16], Batch [64/938], Loss: 0.6793636083602905\n",
      "Validation: Epoch [16], Batch [65/938], Loss: 0.7526202201843262\n",
      "Validation: Epoch [16], Batch [66/938], Loss: 0.7036328315734863\n",
      "Validation: Epoch [16], Batch [67/938], Loss: 0.9855016469955444\n",
      "Validation: Epoch [16], Batch [68/938], Loss: 0.6768485307693481\n",
      "Validation: Epoch [16], Batch [69/938], Loss: 0.6213515996932983\n",
      "Validation: Epoch [16], Batch [70/938], Loss: 0.6950647830963135\n",
      "Validation: Epoch [16], Batch [71/938], Loss: 0.7563641667366028\n",
      "Validation: Epoch [16], Batch [72/938], Loss: 0.9348649978637695\n",
      "Validation: Epoch [16], Batch [73/938], Loss: 0.6374917030334473\n",
      "Validation: Epoch [16], Batch [74/938], Loss: 0.8347510099411011\n",
      "Validation: Epoch [16], Batch [75/938], Loss: 0.7099131941795349\n",
      "Validation: Epoch [16], Batch [76/938], Loss: 0.9186726808547974\n",
      "Validation: Epoch [16], Batch [77/938], Loss: 0.7716977596282959\n",
      "Validation: Epoch [16], Batch [78/938], Loss: 0.6745160818099976\n",
      "Validation: Epoch [16], Batch [79/938], Loss: 0.49929386377334595\n",
      "Validation: Epoch [16], Batch [80/938], Loss: 0.4304649829864502\n",
      "Validation: Epoch [16], Batch [81/938], Loss: 0.804070770740509\n",
      "Validation: Epoch [16], Batch [82/938], Loss: 0.5382192134857178\n",
      "Validation: Epoch [16], Batch [83/938], Loss: 0.7942813634872437\n",
      "Validation: Epoch [16], Batch [84/938], Loss: 0.8462787866592407\n",
      "Validation: Epoch [16], Batch [85/938], Loss: 0.8177945613861084\n",
      "Validation: Epoch [16], Batch [86/938], Loss: 0.8561109900474548\n",
      "Validation: Epoch [16], Batch [87/938], Loss: 0.5414292216300964\n",
      "Validation: Epoch [16], Batch [88/938], Loss: 0.6652060151100159\n",
      "Validation: Epoch [16], Batch [89/938], Loss: 0.6374703645706177\n",
      "Validation: Epoch [16], Batch [90/938], Loss: 0.5779320001602173\n",
      "Validation: Epoch [16], Batch [91/938], Loss: 0.7469596266746521\n",
      "Validation: Epoch [16], Batch [92/938], Loss: 0.6275701522827148\n",
      "Validation: Epoch [16], Batch [93/938], Loss: 0.7880424857139587\n",
      "Validation: Epoch [16], Batch [94/938], Loss: 0.5369058847427368\n",
      "Validation: Epoch [16], Batch [95/938], Loss: 0.6608179807662964\n",
      "Validation: Epoch [16], Batch [96/938], Loss: 0.5753150582313538\n",
      "Validation: Epoch [16], Batch [97/938], Loss: 0.5619170069694519\n",
      "Validation: Epoch [16], Batch [98/938], Loss: 0.7828115224838257\n",
      "Validation: Epoch [16], Batch [99/938], Loss: 0.6080371141433716\n",
      "Validation: Epoch [16], Batch [100/938], Loss: 0.6790698170661926\n",
      "Validation: Epoch [16], Batch [101/938], Loss: 0.6629613041877747\n",
      "Validation: Epoch [16], Batch [102/938], Loss: 0.627565860748291\n",
      "Validation: Epoch [16], Batch [103/938], Loss: 0.6614513397216797\n",
      "Validation: Epoch [16], Batch [104/938], Loss: 0.393868625164032\n",
      "Validation: Epoch [16], Batch [105/938], Loss: 0.5405927896499634\n",
      "Validation: Epoch [16], Batch [106/938], Loss: 0.6895337700843811\n",
      "Validation: Epoch [16], Batch [107/938], Loss: 0.8387378454208374\n",
      "Validation: Epoch [16], Batch [108/938], Loss: 0.6859121322631836\n",
      "Validation: Epoch [16], Batch [109/938], Loss: 0.7462058067321777\n",
      "Validation: Epoch [16], Batch [110/938], Loss: 0.6481683850288391\n",
      "Validation: Epoch [16], Batch [111/938], Loss: 0.728857159614563\n",
      "Validation: Epoch [16], Batch [112/938], Loss: 0.4562098979949951\n",
      "Validation: Epoch [16], Batch [113/938], Loss: 0.7409398555755615\n",
      "Validation: Epoch [16], Batch [114/938], Loss: 0.6550003886222839\n",
      "Validation: Epoch [16], Batch [115/938], Loss: 0.6988769769668579\n",
      "Validation: Epoch [16], Batch [116/938], Loss: 0.7663710117340088\n",
      "Validation: Epoch [16], Batch [117/938], Loss: 0.6930141448974609\n",
      "Validation: Epoch [16], Batch [118/938], Loss: 0.9544849395751953\n",
      "Validation: Epoch [16], Batch [119/938], Loss: 0.8804476261138916\n",
      "Validation: Epoch [16], Batch [120/938], Loss: 0.5624014139175415\n",
      "Validation: Epoch [16], Batch [121/938], Loss: 0.6556782722473145\n",
      "Validation: Epoch [16], Batch [122/938], Loss: 0.48266756534576416\n",
      "Validation: Epoch [16], Batch [123/938], Loss: 0.7216047048568726\n",
      "Validation: Epoch [16], Batch [124/938], Loss: 0.5126287341117859\n",
      "Validation: Epoch [16], Batch [125/938], Loss: 0.6329978108406067\n",
      "Validation: Epoch [16], Batch [126/938], Loss: 0.7663817405700684\n",
      "Validation: Epoch [16], Batch [127/938], Loss: 0.7524638175964355\n",
      "Validation: Epoch [16], Batch [128/938], Loss: 0.4984370768070221\n",
      "Validation: Epoch [16], Batch [129/938], Loss: 0.8081805109977722\n",
      "Validation: Epoch [16], Batch [130/938], Loss: 0.4711036682128906\n",
      "Validation: Epoch [16], Batch [131/938], Loss: 0.5826361179351807\n",
      "Validation: Epoch [16], Batch [132/938], Loss: 0.5865237712860107\n",
      "Validation: Epoch [16], Batch [133/938], Loss: 0.6498132348060608\n",
      "Validation: Epoch [16], Batch [134/938], Loss: 0.7889823913574219\n",
      "Validation: Epoch [16], Batch [135/938], Loss: 0.5186707377433777\n",
      "Validation: Epoch [16], Batch [136/938], Loss: 0.7110788822174072\n",
      "Validation: Epoch [16], Batch [137/938], Loss: 0.5205393433570862\n",
      "Validation: Epoch [16], Batch [138/938], Loss: 0.7738264799118042\n",
      "Validation: Epoch [16], Batch [139/938], Loss: 0.6294977068901062\n",
      "Validation: Epoch [16], Batch [140/938], Loss: 0.7483072280883789\n",
      "Validation: Epoch [16], Batch [141/938], Loss: 0.5195019245147705\n",
      "Validation: Epoch [16], Batch [142/938], Loss: 0.7228726744651794\n",
      "Validation: Epoch [16], Batch [143/938], Loss: 0.5982332229614258\n",
      "Validation: Epoch [16], Batch [144/938], Loss: 0.6786252856254578\n",
      "Validation: Epoch [16], Batch [145/938], Loss: 0.8687217235565186\n",
      "Validation: Epoch [16], Batch [146/938], Loss: 0.7584710121154785\n",
      "Validation: Epoch [16], Batch [147/938], Loss: 0.5926861763000488\n",
      "Validation: Epoch [16], Batch [148/938], Loss: 0.6603488326072693\n",
      "Validation: Epoch [16], Batch [149/938], Loss: 0.5289928913116455\n",
      "Validation: Epoch [16], Batch [150/938], Loss: 0.7588697671890259\n",
      "Validation: Epoch [16], Batch [151/938], Loss: 0.8052658438682556\n",
      "Validation: Epoch [16], Batch [152/938], Loss: 0.7128642797470093\n",
      "Validation: Epoch [16], Batch [153/938], Loss: 0.8776592016220093\n",
      "Validation: Epoch [16], Batch [154/938], Loss: 0.5863597989082336\n",
      "Validation: Epoch [16], Batch [155/938], Loss: 0.49779677391052246\n",
      "Validation: Epoch [16], Batch [156/938], Loss: 0.6969114542007446\n",
      "Validation: Epoch [16], Batch [157/938], Loss: 0.39298713207244873\n",
      "Validation: Epoch [16], Batch [158/938], Loss: 0.5051127076148987\n",
      "Validation: Epoch [16], Batch [159/938], Loss: 0.636131763458252\n",
      "Validation: Epoch [16], Batch [160/938], Loss: 0.6222004890441895\n",
      "Validation: Epoch [16], Batch [161/938], Loss: 1.0003383159637451\n",
      "Validation: Epoch [16], Batch [162/938], Loss: 0.6810239553451538\n",
      "Validation: Epoch [16], Batch [163/938], Loss: 0.8467725515365601\n",
      "Validation: Epoch [16], Batch [164/938], Loss: 0.5762807130813599\n",
      "Validation: Epoch [16], Batch [165/938], Loss: 0.6769545674324036\n",
      "Validation: Epoch [16], Batch [166/938], Loss: 0.8210989236831665\n",
      "Validation: Epoch [16], Batch [167/938], Loss: 0.7220218181610107\n",
      "Validation: Epoch [16], Batch [168/938], Loss: 0.8588114380836487\n",
      "Validation: Epoch [16], Batch [169/938], Loss: 0.8989787101745605\n",
      "Validation: Epoch [16], Batch [170/938], Loss: 0.7970892786979675\n",
      "Validation: Epoch [16], Batch [171/938], Loss: 0.41542500257492065\n",
      "Validation: Epoch [16], Batch [172/938], Loss: 0.4461899399757385\n",
      "Validation: Epoch [16], Batch [173/938], Loss: 0.7563025951385498\n",
      "Validation: Epoch [16], Batch [174/938], Loss: 0.5032436847686768\n",
      "Validation: Epoch [16], Batch [175/938], Loss: 0.6154107451438904\n",
      "Validation: Epoch [16], Batch [176/938], Loss: 1.02695631980896\n",
      "Validation: Epoch [16], Batch [177/938], Loss: 0.8324561715126038\n",
      "Validation: Epoch [16], Batch [178/938], Loss: 0.4423550069332123\n",
      "Validation: Epoch [16], Batch [179/938], Loss: 0.8044275045394897\n",
      "Validation: Epoch [16], Batch [180/938], Loss: 0.5773362517356873\n",
      "Validation: Epoch [16], Batch [181/938], Loss: 0.6837302446365356\n",
      "Validation: Epoch [16], Batch [182/938], Loss: 0.5901325345039368\n",
      "Validation: Epoch [16], Batch [183/938], Loss: 0.5768580436706543\n",
      "Validation: Epoch [16], Batch [184/938], Loss: 0.7332590818405151\n",
      "Validation: Epoch [16], Batch [185/938], Loss: 0.5289802551269531\n",
      "Validation: Epoch [16], Batch [186/938], Loss: 0.5452091097831726\n",
      "Validation: Epoch [16], Batch [187/938], Loss: 0.6974583864212036\n",
      "Validation: Epoch [16], Batch [188/938], Loss: 0.5999724864959717\n",
      "Validation: Epoch [16], Batch [189/938], Loss: 0.5868005752563477\n",
      "Validation: Epoch [16], Batch [190/938], Loss: 0.8166473507881165\n",
      "Validation: Epoch [16], Batch [191/938], Loss: 0.5843324661254883\n",
      "Validation: Epoch [16], Batch [192/938], Loss: 0.7334983944892883\n",
      "Validation: Epoch [16], Batch [193/938], Loss: 0.4869134724140167\n",
      "Validation: Epoch [16], Batch [194/938], Loss: 0.713715136051178\n",
      "Validation: Epoch [16], Batch [195/938], Loss: 0.6936830282211304\n",
      "Validation: Epoch [16], Batch [196/938], Loss: 0.7458208203315735\n",
      "Validation: Epoch [16], Batch [197/938], Loss: 0.4724332094192505\n",
      "Validation: Epoch [16], Batch [198/938], Loss: 0.7103049755096436\n",
      "Validation: Epoch [16], Batch [199/938], Loss: 0.5427860021591187\n",
      "Validation: Epoch [16], Batch [200/938], Loss: 0.6198819279670715\n",
      "Validation: Epoch [16], Batch [201/938], Loss: 0.7180342674255371\n",
      "Validation: Epoch [16], Batch [202/938], Loss: 0.6701207160949707\n",
      "Validation: Epoch [16], Batch [203/938], Loss: 0.6511459946632385\n",
      "Validation: Epoch [16], Batch [204/938], Loss: 0.7585434913635254\n",
      "Validation: Epoch [16], Batch [205/938], Loss: 0.6700767278671265\n",
      "Validation: Epoch [16], Batch [206/938], Loss: 0.9546046257019043\n",
      "Validation: Epoch [16], Batch [207/938], Loss: 0.9511806964874268\n",
      "Validation: Epoch [16], Batch [208/938], Loss: 0.49894845485687256\n",
      "Validation: Epoch [16], Batch [209/938], Loss: 0.7123419046401978\n",
      "Validation: Epoch [16], Batch [210/938], Loss: 0.5332942008972168\n",
      "Validation: Epoch [16], Batch [211/938], Loss: 0.5408793091773987\n",
      "Validation: Epoch [16], Batch [212/938], Loss: 0.4901607930660248\n",
      "Validation: Epoch [16], Batch [213/938], Loss: 0.6403591632843018\n",
      "Validation: Epoch [16], Batch [214/938], Loss: 0.4977176785469055\n",
      "Validation: Epoch [16], Batch [215/938], Loss: 0.7734770774841309\n",
      "Validation: Epoch [16], Batch [216/938], Loss: 0.8886343836784363\n",
      "Validation: Epoch [16], Batch [217/938], Loss: 0.6473293900489807\n",
      "Validation: Epoch [16], Batch [218/938], Loss: 0.5657197833061218\n",
      "Validation: Epoch [16], Batch [219/938], Loss: 0.7506853938102722\n",
      "Validation: Epoch [16], Batch [220/938], Loss: 0.6207611560821533\n",
      "Validation: Epoch [16], Batch [221/938], Loss: 0.7518201470375061\n",
      "Validation: Epoch [16], Batch [222/938], Loss: 0.7316529750823975\n",
      "Validation: Epoch [16], Batch [223/938], Loss: 0.5912015438079834\n",
      "Validation: Epoch [16], Batch [224/938], Loss: 0.7494271993637085\n",
      "Validation: Epoch [16], Batch [225/938], Loss: 0.7482799887657166\n",
      "Validation: Epoch [16], Batch [226/938], Loss: 0.7006403207778931\n",
      "Validation: Epoch [16], Batch [227/938], Loss: 0.536383867263794\n",
      "Validation: Epoch [16], Batch [228/938], Loss: 0.8507393598556519\n",
      "Validation: Epoch [16], Batch [229/938], Loss: 0.7764585018157959\n",
      "Validation: Epoch [16], Batch [230/938], Loss: 0.7807447910308838\n",
      "Validation: Epoch [16], Batch [231/938], Loss: 0.7449066638946533\n",
      "Validation: Epoch [16], Batch [232/938], Loss: 0.7807705402374268\n",
      "Validation: Epoch [16], Batch [233/938], Loss: 0.6829381585121155\n",
      "Validation: Epoch [16], Batch [234/938], Loss: 0.7232226729393005\n",
      "Validation: Epoch [16], Batch [235/938], Loss: 0.7715564966201782\n",
      "Validation: Epoch [16], Batch [236/938], Loss: 0.9590848088264465\n",
      "Validation: Epoch [16], Batch [237/938], Loss: 0.7475456595420837\n",
      "Validation: Epoch [16], Batch [238/938], Loss: 0.6075698733329773\n",
      "Validation: Epoch [16], Batch [239/938], Loss: 0.6800388693809509\n",
      "Validation: Epoch [16], Batch [240/938], Loss: 0.6068670749664307\n",
      "Validation: Epoch [16], Batch [241/938], Loss: 0.6058164238929749\n",
      "Validation: Epoch [16], Batch [242/938], Loss: 0.5367971062660217\n",
      "Validation: Epoch [16], Batch [243/938], Loss: 0.6347548961639404\n",
      "Validation: Epoch [16], Batch [244/938], Loss: 0.7218896150588989\n",
      "Validation: Epoch [16], Batch [245/938], Loss: 0.6727264523506165\n",
      "Validation: Epoch [16], Batch [246/938], Loss: 0.6045517921447754\n",
      "Validation: Epoch [16], Batch [247/938], Loss: 0.385997474193573\n",
      "Validation: Epoch [16], Batch [248/938], Loss: 0.7915287613868713\n",
      "Validation: Epoch [16], Batch [249/938], Loss: 0.7325809001922607\n",
      "Validation: Epoch [16], Batch [250/938], Loss: 0.6448953151702881\n",
      "Validation: Epoch [16], Batch [251/938], Loss: 0.626419186592102\n",
      "Validation: Epoch [16], Batch [252/938], Loss: 0.8197321891784668\n",
      "Validation: Epoch [16], Batch [253/938], Loss: 0.7460887432098389\n",
      "Validation: Epoch [16], Batch [254/938], Loss: 0.5758846402168274\n",
      "Validation: Epoch [16], Batch [255/938], Loss: 0.9231982231140137\n",
      "Validation: Epoch [16], Batch [256/938], Loss: 0.5595317482948303\n",
      "Validation: Epoch [16], Batch [257/938], Loss: 0.5919153094291687\n",
      "Validation: Epoch [16], Batch [258/938], Loss: 0.6567862033843994\n",
      "Validation: Epoch [16], Batch [259/938], Loss: 0.5546490550041199\n",
      "Validation: Epoch [16], Batch [260/938], Loss: 0.8928201198577881\n",
      "Validation: Epoch [16], Batch [261/938], Loss: 0.8373674750328064\n",
      "Validation: Epoch [16], Batch [262/938], Loss: 0.6924389600753784\n",
      "Validation: Epoch [16], Batch [263/938], Loss: 0.7453343272209167\n",
      "Validation: Epoch [16], Batch [264/938], Loss: 0.7072795033454895\n",
      "Validation: Epoch [16], Batch [265/938], Loss: 0.7636730074882507\n",
      "Validation: Epoch [16], Batch [266/938], Loss: 0.7142386436462402\n",
      "Validation: Epoch [16], Batch [267/938], Loss: 0.5907459259033203\n",
      "Validation: Epoch [16], Batch [268/938], Loss: 0.7925965785980225\n",
      "Validation: Epoch [16], Batch [269/938], Loss: 0.7430664896965027\n",
      "Validation: Epoch [16], Batch [270/938], Loss: 0.5883232355117798\n",
      "Validation: Epoch [16], Batch [271/938], Loss: 0.661542534828186\n",
      "Validation: Epoch [16], Batch [272/938], Loss: 0.761426568031311\n",
      "Validation: Epoch [16], Batch [273/938], Loss: 0.49902018904685974\n",
      "Validation: Epoch [16], Batch [274/938], Loss: 0.7720230221748352\n",
      "Validation: Epoch [16], Batch [275/938], Loss: 0.8504869937896729\n",
      "Validation: Epoch [16], Batch [276/938], Loss: 0.6210947036743164\n",
      "Validation: Epoch [16], Batch [277/938], Loss: 0.7062252759933472\n",
      "Validation: Epoch [16], Batch [278/938], Loss: 0.6677918434143066\n",
      "Validation: Epoch [16], Batch [279/938], Loss: 0.7246705889701843\n",
      "Validation: Epoch [16], Batch [280/938], Loss: 0.9344907402992249\n",
      "Validation: Epoch [16], Batch [281/938], Loss: 0.7195261716842651\n",
      "Validation: Epoch [16], Batch [282/938], Loss: 0.7130584716796875\n",
      "Validation: Epoch [16], Batch [283/938], Loss: 0.5016105771064758\n",
      "Validation: Epoch [16], Batch [284/938], Loss: 0.6482765078544617\n",
      "Validation: Epoch [16], Batch [285/938], Loss: 0.6912378072738647\n",
      "Validation: Epoch [16], Batch [286/938], Loss: 0.6868373155593872\n",
      "Validation: Epoch [16], Batch [287/938], Loss: 1.020011305809021\n",
      "Validation: Epoch [16], Batch [288/938], Loss: 0.7735260725021362\n",
      "Validation: Epoch [16], Batch [289/938], Loss: 0.6061393618583679\n",
      "Validation: Epoch [16], Batch [290/938], Loss: 0.5909475088119507\n",
      "Validation: Epoch [16], Batch [291/938], Loss: 0.3292100131511688\n",
      "Validation: Epoch [16], Batch [292/938], Loss: 0.7160291075706482\n",
      "Validation: Epoch [16], Batch [293/938], Loss: 0.6136832237243652\n",
      "Validation: Epoch [16], Batch [294/938], Loss: 0.66541588306427\n",
      "Validation: Epoch [16], Batch [295/938], Loss: 0.7556147575378418\n",
      "Validation: Epoch [16], Batch [296/938], Loss: 0.8976053595542908\n",
      "Validation: Epoch [16], Batch [297/938], Loss: 0.5261496305465698\n",
      "Validation: Epoch [16], Batch [298/938], Loss: 0.8026875853538513\n",
      "Validation: Epoch [16], Batch [299/938], Loss: 0.7185183763504028\n",
      "Validation: Epoch [16], Batch [300/938], Loss: 0.6228060722351074\n",
      "Validation: Epoch [16], Batch [301/938], Loss: 0.7630723714828491\n",
      "Validation: Epoch [16], Batch [302/938], Loss: 0.5436309576034546\n",
      "Validation: Epoch [16], Batch [303/938], Loss: 0.6173784136772156\n",
      "Validation: Epoch [16], Batch [304/938], Loss: 0.8090305328369141\n",
      "Validation: Epoch [16], Batch [305/938], Loss: 0.7615994215011597\n",
      "Validation: Epoch [16], Batch [306/938], Loss: 0.6354458928108215\n",
      "Validation: Epoch [16], Batch [307/938], Loss: 0.7695239186286926\n",
      "Validation: Epoch [16], Batch [308/938], Loss: 0.8390461802482605\n",
      "Validation: Epoch [16], Batch [309/938], Loss: 0.5563293695449829\n",
      "Validation: Epoch [16], Batch [310/938], Loss: 0.6312576532363892\n",
      "Validation: Epoch [16], Batch [311/938], Loss: 0.5490175485610962\n",
      "Validation: Epoch [16], Batch [312/938], Loss: 0.5338789820671082\n",
      "Validation: Epoch [16], Batch [313/938], Loss: 0.830046534538269\n",
      "Validation: Epoch [16], Batch [314/938], Loss: 0.5577969551086426\n",
      "Validation: Epoch [16], Batch [315/938], Loss: 0.7013814449310303\n",
      "Validation: Epoch [16], Batch [316/938], Loss: 0.8114826679229736\n",
      "Validation: Epoch [16], Batch [317/938], Loss: 0.6327167749404907\n",
      "Validation: Epoch [16], Batch [318/938], Loss: 0.8252195119857788\n",
      "Validation: Epoch [16], Batch [319/938], Loss: 0.7216604948043823\n",
      "Validation: Epoch [16], Batch [320/938], Loss: 0.8169049024581909\n",
      "Validation: Epoch [16], Batch [321/938], Loss: 0.718948245048523\n",
      "Validation: Epoch [16], Batch [322/938], Loss: 0.7573641538619995\n",
      "Validation: Epoch [16], Batch [323/938], Loss: 0.7291973829269409\n",
      "Validation: Epoch [16], Batch [324/938], Loss: 0.5997307896614075\n",
      "Validation: Epoch [16], Batch [325/938], Loss: 0.6692678928375244\n",
      "Validation: Epoch [16], Batch [326/938], Loss: 0.5997830629348755\n",
      "Validation: Epoch [16], Batch [327/938], Loss: 0.7413669228553772\n",
      "Validation: Epoch [16], Batch [328/938], Loss: 0.8231773972511292\n",
      "Validation: Epoch [16], Batch [329/938], Loss: 1.0002391338348389\n",
      "Validation: Epoch [16], Batch [330/938], Loss: 0.7286303639411926\n",
      "Validation: Epoch [16], Batch [331/938], Loss: 0.5903844237327576\n",
      "Validation: Epoch [16], Batch [332/938], Loss: 0.5440906286239624\n",
      "Validation: Epoch [16], Batch [333/938], Loss: 0.8036276698112488\n",
      "Validation: Epoch [16], Batch [334/938], Loss: 0.808078408241272\n",
      "Validation: Epoch [16], Batch [335/938], Loss: 0.7651745676994324\n",
      "Validation: Epoch [16], Batch [336/938], Loss: 0.5785502791404724\n",
      "Validation: Epoch [16], Batch [337/938], Loss: 0.6433113813400269\n",
      "Validation: Epoch [16], Batch [338/938], Loss: 0.6563568115234375\n",
      "Validation: Epoch [16], Batch [339/938], Loss: 0.7345305681228638\n",
      "Validation: Epoch [16], Batch [340/938], Loss: 0.6302693486213684\n",
      "Validation: Epoch [16], Batch [341/938], Loss: 0.5155524611473083\n",
      "Validation: Epoch [16], Batch [342/938], Loss: 0.8360943794250488\n",
      "Validation: Epoch [16], Batch [343/938], Loss: 0.8674608469009399\n",
      "Validation: Epoch [16], Batch [344/938], Loss: 0.6593148708343506\n",
      "Validation: Epoch [16], Batch [345/938], Loss: 0.5837867856025696\n",
      "Validation: Epoch [16], Batch [346/938], Loss: 0.7901899814605713\n",
      "Validation: Epoch [16], Batch [347/938], Loss: 0.7474130988121033\n",
      "Validation: Epoch [16], Batch [348/938], Loss: 0.6080352067947388\n",
      "Validation: Epoch [16], Batch [349/938], Loss: 0.90129554271698\n",
      "Validation: Epoch [16], Batch [350/938], Loss: 0.5751933455467224\n",
      "Validation: Epoch [16], Batch [351/938], Loss: 0.6773486733436584\n",
      "Validation: Epoch [16], Batch [352/938], Loss: 0.8290049433708191\n",
      "Validation: Epoch [16], Batch [353/938], Loss: 0.6264410018920898\n",
      "Validation: Epoch [16], Batch [354/938], Loss: 0.5774683356285095\n",
      "Validation: Epoch [16], Batch [355/938], Loss: 0.5710499286651611\n",
      "Validation: Epoch [16], Batch [356/938], Loss: 0.5900846719741821\n",
      "Validation: Epoch [16], Batch [357/938], Loss: 0.674881100654602\n",
      "Validation: Epoch [16], Batch [358/938], Loss: 0.7873162627220154\n",
      "Validation: Epoch [16], Batch [359/938], Loss: 0.8473770022392273\n",
      "Validation: Epoch [16], Batch [360/938], Loss: 0.691832959651947\n",
      "Validation: Epoch [16], Batch [361/938], Loss: 0.9209031462669373\n",
      "Validation: Epoch [16], Batch [362/938], Loss: 0.720225989818573\n",
      "Validation: Epoch [16], Batch [363/938], Loss: 0.5422462224960327\n",
      "Validation: Epoch [16], Batch [364/938], Loss: 0.6537961363792419\n",
      "Validation: Epoch [16], Batch [365/938], Loss: 0.7238566279411316\n",
      "Validation: Epoch [16], Batch [366/938], Loss: 0.8596544861793518\n",
      "Validation: Epoch [16], Batch [367/938], Loss: 0.8617210984230042\n",
      "Validation: Epoch [16], Batch [368/938], Loss: 0.7319084405899048\n",
      "Validation: Epoch [16], Batch [369/938], Loss: 1.0976693630218506\n",
      "Validation: Epoch [16], Batch [370/938], Loss: 0.8810319900512695\n",
      "Validation: Epoch [16], Batch [371/938], Loss: 0.3488907217979431\n",
      "Validation: Epoch [16], Batch [372/938], Loss: 0.7066872715950012\n",
      "Validation: Epoch [16], Batch [373/938], Loss: 0.531078040599823\n",
      "Validation: Epoch [16], Batch [374/938], Loss: 0.6617619395256042\n",
      "Validation: Epoch [16], Batch [375/938], Loss: 0.7965712547302246\n",
      "Validation: Epoch [16], Batch [376/938], Loss: 0.7483590841293335\n",
      "Validation: Epoch [16], Batch [377/938], Loss: 0.7962191700935364\n",
      "Validation: Epoch [16], Batch [378/938], Loss: 0.7474318742752075\n",
      "Validation: Epoch [16], Batch [379/938], Loss: 0.7143188714981079\n",
      "Validation: Epoch [16], Batch [380/938], Loss: 0.8355119228363037\n",
      "Validation: Epoch [16], Batch [381/938], Loss: 0.7067432999610901\n",
      "Validation: Epoch [16], Batch [382/938], Loss: 0.6256241798400879\n",
      "Validation: Epoch [16], Batch [383/938], Loss: 0.5947126150131226\n",
      "Validation: Epoch [16], Batch [384/938], Loss: 0.6624711751937866\n",
      "Validation: Epoch [16], Batch [385/938], Loss: 0.8795380592346191\n",
      "Validation: Epoch [16], Batch [386/938], Loss: 0.702807605266571\n",
      "Validation: Epoch [16], Batch [387/938], Loss: 0.6837349534034729\n",
      "Validation: Epoch [16], Batch [388/938], Loss: 0.574719250202179\n",
      "Validation: Epoch [16], Batch [389/938], Loss: 0.7313889861106873\n",
      "Validation: Epoch [16], Batch [390/938], Loss: 0.6055788397789001\n",
      "Validation: Epoch [16], Batch [391/938], Loss: 0.6206541657447815\n",
      "Validation: Epoch [16], Batch [392/938], Loss: 0.7393715381622314\n",
      "Validation: Epoch [16], Batch [393/938], Loss: 0.4887387752532959\n",
      "Validation: Epoch [16], Batch [394/938], Loss: 0.623985230922699\n",
      "Validation: Epoch [16], Batch [395/938], Loss: 0.5554697513580322\n",
      "Validation: Epoch [16], Batch [396/938], Loss: 0.6981304287910461\n",
      "Validation: Epoch [16], Batch [397/938], Loss: 0.5368294715881348\n",
      "Validation: Epoch [16], Batch [398/938], Loss: 0.5067565441131592\n",
      "Validation: Epoch [16], Batch [399/938], Loss: 0.6456059217453003\n",
      "Validation: Epoch [16], Batch [400/938], Loss: 0.5627997517585754\n",
      "Validation: Epoch [16], Batch [401/938], Loss: 0.5873167514801025\n",
      "Validation: Epoch [16], Batch [402/938], Loss: 0.6108980178833008\n",
      "Validation: Epoch [16], Batch [403/938], Loss: 0.8000917434692383\n",
      "Validation: Epoch [16], Batch [404/938], Loss: 0.8016805052757263\n",
      "Validation: Epoch [16], Batch [405/938], Loss: 0.5826892852783203\n",
      "Validation: Epoch [16], Batch [406/938], Loss: 0.931820273399353\n",
      "Validation: Epoch [16], Batch [407/938], Loss: 0.5877929925918579\n",
      "Validation: Epoch [16], Batch [408/938], Loss: 0.8245012760162354\n",
      "Validation: Epoch [16], Batch [409/938], Loss: 0.9230987429618835\n",
      "Validation: Epoch [16], Batch [410/938], Loss: 0.7781166434288025\n",
      "Validation: Epoch [16], Batch [411/938], Loss: 0.5756599307060242\n",
      "Validation: Epoch [16], Batch [412/938], Loss: 0.41730257868766785\n",
      "Validation: Epoch [16], Batch [413/938], Loss: 0.6227062344551086\n",
      "Validation: Epoch [16], Batch [414/938], Loss: 0.8723785281181335\n",
      "Validation: Epoch [16], Batch [415/938], Loss: 0.8839553594589233\n",
      "Validation: Epoch [16], Batch [416/938], Loss: 0.6211643218994141\n",
      "Validation: Epoch [16], Batch [417/938], Loss: 0.6714417934417725\n",
      "Validation: Epoch [16], Batch [418/938], Loss: 0.6722251772880554\n",
      "Validation: Epoch [16], Batch [419/938], Loss: 0.5505802035331726\n",
      "Validation: Epoch [16], Batch [420/938], Loss: 0.797838032245636\n",
      "Validation: Epoch [16], Batch [421/938], Loss: 0.6789856553077698\n",
      "Validation: Epoch [16], Batch [422/938], Loss: 0.6228429675102234\n",
      "Validation: Epoch [16], Batch [423/938], Loss: 0.572891116142273\n",
      "Validation: Epoch [16], Batch [424/938], Loss: 0.6667672395706177\n",
      "Validation: Epoch [16], Batch [425/938], Loss: 0.6719869375228882\n",
      "Validation: Epoch [16], Batch [426/938], Loss: 0.6614221334457397\n",
      "Validation: Epoch [16], Batch [427/938], Loss: 0.43710288405418396\n",
      "Validation: Epoch [16], Batch [428/938], Loss: 0.6600469350814819\n",
      "Validation: Epoch [16], Batch [429/938], Loss: 0.546962559223175\n",
      "Validation: Epoch [16], Batch [430/938], Loss: 0.7637884616851807\n",
      "Validation: Epoch [16], Batch [431/938], Loss: 0.5539199113845825\n",
      "Validation: Epoch [16], Batch [432/938], Loss: 0.45028603076934814\n",
      "Validation: Epoch [16], Batch [433/938], Loss: 0.6668643355369568\n",
      "Validation: Epoch [16], Batch [434/938], Loss: 0.7121131420135498\n",
      "Validation: Epoch [16], Batch [435/938], Loss: 0.6291742324829102\n",
      "Validation: Epoch [16], Batch [436/938], Loss: 0.5893148183822632\n",
      "Validation: Epoch [16], Batch [437/938], Loss: 0.7821645736694336\n",
      "Validation: Epoch [16], Batch [438/938], Loss: 0.7024815082550049\n",
      "Validation: Epoch [16], Batch [439/938], Loss: 0.761012852191925\n",
      "Validation: Epoch [16], Batch [440/938], Loss: 0.5162613391876221\n",
      "Validation: Epoch [16], Batch [441/938], Loss: 0.7128328680992126\n",
      "Validation: Epoch [16], Batch [442/938], Loss: 0.6379862427711487\n",
      "Validation: Epoch [16], Batch [443/938], Loss: 0.8574972152709961\n",
      "Validation: Epoch [16], Batch [444/938], Loss: 1.0030643939971924\n",
      "Validation: Epoch [16], Batch [445/938], Loss: 0.6401170492172241\n",
      "Validation: Epoch [16], Batch [446/938], Loss: 0.7510330677032471\n",
      "Validation: Epoch [16], Batch [447/938], Loss: 0.496509313583374\n",
      "Validation: Epoch [16], Batch [448/938], Loss: 0.7805122137069702\n",
      "Validation: Epoch [16], Batch [449/938], Loss: 0.8148738741874695\n",
      "Validation: Epoch [16], Batch [450/938], Loss: 0.7775281667709351\n",
      "Validation: Epoch [16], Batch [451/938], Loss: 0.5960906744003296\n",
      "Validation: Epoch [16], Batch [452/938], Loss: 0.558677077293396\n",
      "Validation: Epoch [16], Batch [453/938], Loss: 0.829393744468689\n",
      "Validation: Epoch [16], Batch [454/938], Loss: 0.8448601961135864\n",
      "Validation: Epoch [16], Batch [455/938], Loss: 0.6498536467552185\n",
      "Validation: Epoch [16], Batch [456/938], Loss: 0.4963599145412445\n",
      "Validation: Epoch [16], Batch [457/938], Loss: 0.7452218532562256\n",
      "Validation: Epoch [16], Batch [458/938], Loss: 0.8046057224273682\n",
      "Validation: Epoch [16], Batch [459/938], Loss: 0.5300543308258057\n",
      "Validation: Epoch [16], Batch [460/938], Loss: 0.57830810546875\n",
      "Validation: Epoch [16], Batch [461/938], Loss: 0.7847005724906921\n",
      "Validation: Epoch [16], Batch [462/938], Loss: 0.6483736038208008\n",
      "Validation: Epoch [16], Batch [463/938], Loss: 0.7270062565803528\n",
      "Validation: Epoch [16], Batch [464/938], Loss: 0.8660281896591187\n",
      "Validation: Epoch [16], Batch [465/938], Loss: 0.6361997127532959\n",
      "Validation: Epoch [16], Batch [466/938], Loss: 0.7085692286491394\n",
      "Validation: Epoch [16], Batch [467/938], Loss: 0.6396324038505554\n",
      "Validation: Epoch [16], Batch [468/938], Loss: 0.7668379545211792\n",
      "Validation: Epoch [16], Batch [469/938], Loss: 0.7725681662559509\n",
      "Validation: Epoch [16], Batch [470/938], Loss: 0.6753928661346436\n",
      "Validation: Epoch [16], Batch [471/938], Loss: 0.7607972025871277\n",
      "Validation: Epoch [16], Batch [472/938], Loss: 0.7661254405975342\n",
      "Validation: Epoch [16], Batch [473/938], Loss: 0.5874179005622864\n",
      "Validation: Epoch [16], Batch [474/938], Loss: 0.6910406947135925\n",
      "Validation: Epoch [16], Batch [475/938], Loss: 0.4878174364566803\n",
      "Validation: Epoch [16], Batch [476/938], Loss: 0.7694554328918457\n",
      "Validation: Epoch [16], Batch [477/938], Loss: 0.6950596570968628\n",
      "Validation: Epoch [16], Batch [478/938], Loss: 0.6351997256278992\n",
      "Validation: Epoch [16], Batch [479/938], Loss: 0.5471668839454651\n",
      "Validation: Epoch [16], Batch [480/938], Loss: 0.5603880882263184\n",
      "Validation: Epoch [16], Batch [481/938], Loss: 0.8452615737915039\n",
      "Validation: Epoch [16], Batch [482/938], Loss: 0.47391799092292786\n",
      "Validation: Epoch [16], Batch [483/938], Loss: 0.8510884642601013\n",
      "Validation: Epoch [16], Batch [484/938], Loss: 0.5679326057434082\n",
      "Validation: Epoch [16], Batch [485/938], Loss: 0.8834707140922546\n",
      "Validation: Epoch [16], Batch [486/938], Loss: 0.7046490907669067\n",
      "Validation: Epoch [16], Batch [487/938], Loss: 0.5943739414215088\n",
      "Validation: Epoch [16], Batch [488/938], Loss: 0.7278708815574646\n",
      "Validation: Epoch [16], Batch [489/938], Loss: 0.7824825644493103\n",
      "Validation: Epoch [16], Batch [490/938], Loss: 0.6383429169654846\n",
      "Validation: Epoch [16], Batch [491/938], Loss: 0.7555625438690186\n",
      "Validation: Epoch [16], Batch [492/938], Loss: 0.5411726236343384\n",
      "Validation: Epoch [16], Batch [493/938], Loss: 0.7786771655082703\n",
      "Validation: Epoch [16], Batch [494/938], Loss: 0.9408693909645081\n",
      "Validation: Epoch [16], Batch [495/938], Loss: 0.6906458139419556\n",
      "Validation: Epoch [16], Batch [496/938], Loss: 0.5937395691871643\n",
      "Validation: Epoch [16], Batch [497/938], Loss: 0.5626938939094543\n",
      "Validation: Epoch [16], Batch [498/938], Loss: 0.726041853427887\n",
      "Validation: Epoch [16], Batch [499/938], Loss: 0.6197291612625122\n",
      "Validation: Epoch [16], Batch [500/938], Loss: 0.701378583908081\n",
      "Validation: Epoch [16], Batch [501/938], Loss: 0.8132363557815552\n",
      "Validation: Epoch [16], Batch [502/938], Loss: 0.9774065017700195\n",
      "Validation: Epoch [16], Batch [503/938], Loss: 0.4540201723575592\n",
      "Validation: Epoch [16], Batch [504/938], Loss: 0.6455869078636169\n",
      "Validation: Epoch [16], Batch [505/938], Loss: 0.6079607009887695\n",
      "Validation: Epoch [16], Batch [506/938], Loss: 0.47624439001083374\n",
      "Validation: Epoch [16], Batch [507/938], Loss: 0.6961807608604431\n",
      "Validation: Epoch [16], Batch [508/938], Loss: 0.7300878167152405\n",
      "Validation: Epoch [16], Batch [509/938], Loss: 0.597691535949707\n",
      "Validation: Epoch [16], Batch [510/938], Loss: 0.718880295753479\n",
      "Validation: Epoch [16], Batch [511/938], Loss: 0.7014746069908142\n",
      "Validation: Epoch [16], Batch [512/938], Loss: 0.7747414708137512\n",
      "Validation: Epoch [16], Batch [513/938], Loss: 0.6311818361282349\n",
      "Validation: Epoch [16], Batch [514/938], Loss: 0.7274608612060547\n",
      "Validation: Epoch [16], Batch [515/938], Loss: 0.6982119083404541\n",
      "Validation: Epoch [16], Batch [516/938], Loss: 0.4711129665374756\n",
      "Validation: Epoch [16], Batch [517/938], Loss: 0.6791089773178101\n",
      "Validation: Epoch [16], Batch [518/938], Loss: 0.915995717048645\n",
      "Validation: Epoch [16], Batch [519/938], Loss: 0.4886131286621094\n",
      "Validation: Epoch [16], Batch [520/938], Loss: 0.7033407092094421\n",
      "Validation: Epoch [16], Batch [521/938], Loss: 0.4865645170211792\n",
      "Validation: Epoch [16], Batch [522/938], Loss: 0.6688793897628784\n",
      "Validation: Epoch [16], Batch [523/938], Loss: 0.7728034853935242\n",
      "Validation: Epoch [16], Batch [524/938], Loss: 0.7375102639198303\n",
      "Validation: Epoch [16], Batch [525/938], Loss: 0.6245629191398621\n",
      "Validation: Epoch [16], Batch [526/938], Loss: 0.6088048219680786\n",
      "Validation: Epoch [16], Batch [527/938], Loss: 0.8819171190261841\n",
      "Validation: Epoch [16], Batch [528/938], Loss: 0.8178759813308716\n",
      "Validation: Epoch [16], Batch [529/938], Loss: 0.4928804039955139\n",
      "Validation: Epoch [16], Batch [530/938], Loss: 0.695608913898468\n",
      "Validation: Epoch [16], Batch [531/938], Loss: 0.7135496139526367\n",
      "Validation: Epoch [16], Batch [532/938], Loss: 0.8192527890205383\n",
      "Validation: Epoch [16], Batch [533/938], Loss: 0.7371845841407776\n",
      "Validation: Epoch [16], Batch [534/938], Loss: 0.6143822073936462\n",
      "Validation: Epoch [16], Batch [535/938], Loss: 0.5988991260528564\n",
      "Validation: Epoch [16], Batch [536/938], Loss: 0.49312615394592285\n",
      "Validation: Epoch [16], Batch [537/938], Loss: 0.6415066719055176\n",
      "Validation: Epoch [16], Batch [538/938], Loss: 0.7871087789535522\n",
      "Validation: Epoch [16], Batch [539/938], Loss: 0.6745922565460205\n",
      "Validation: Epoch [16], Batch [540/938], Loss: 0.6482266783714294\n",
      "Validation: Epoch [16], Batch [541/938], Loss: 0.635573148727417\n",
      "Validation: Epoch [16], Batch [542/938], Loss: 0.762603223323822\n",
      "Validation: Epoch [16], Batch [543/938], Loss: 0.5644780397415161\n",
      "Validation: Epoch [16], Batch [544/938], Loss: 0.6816357970237732\n",
      "Validation: Epoch [16], Batch [545/938], Loss: 0.7380169034004211\n",
      "Validation: Epoch [16], Batch [546/938], Loss: 0.45434245467185974\n",
      "Validation: Epoch [16], Batch [547/938], Loss: 0.6061170101165771\n",
      "Validation: Epoch [16], Batch [548/938], Loss: 0.9880845546722412\n",
      "Validation: Epoch [16], Batch [549/938], Loss: 0.7160429954528809\n",
      "Validation: Epoch [16], Batch [550/938], Loss: 0.6723969578742981\n",
      "Validation: Epoch [16], Batch [551/938], Loss: 0.6720800399780273\n",
      "Validation: Epoch [16], Batch [552/938], Loss: 0.6608888506889343\n",
      "Validation: Epoch [16], Batch [553/938], Loss: 0.7246394157409668\n",
      "Validation: Epoch [16], Batch [554/938], Loss: 0.6622609496116638\n",
      "Validation: Epoch [16], Batch [555/938], Loss: 0.9044305682182312\n",
      "Validation: Epoch [16], Batch [556/938], Loss: 0.6773090362548828\n",
      "Validation: Epoch [16], Batch [557/938], Loss: 0.5228755474090576\n",
      "Validation: Epoch [16], Batch [558/938], Loss: 0.5238859057426453\n",
      "Validation: Epoch [16], Batch [559/938], Loss: 0.48940718173980713\n",
      "Validation: Epoch [16], Batch [560/938], Loss: 0.8982093334197998\n",
      "Validation: Epoch [16], Batch [561/938], Loss: 0.5499764680862427\n",
      "Validation: Epoch [16], Batch [562/938], Loss: 0.8079043030738831\n",
      "Validation: Epoch [16], Batch [563/938], Loss: 0.44293996691703796\n",
      "Validation: Epoch [16], Batch [564/938], Loss: 0.7281250953674316\n",
      "Validation: Epoch [16], Batch [565/938], Loss: 0.6655418872833252\n",
      "Validation: Epoch [16], Batch [566/938], Loss: 0.7062802910804749\n",
      "Validation: Epoch [16], Batch [567/938], Loss: 0.7182459831237793\n",
      "Validation: Epoch [16], Batch [568/938], Loss: 0.3990103006362915\n",
      "Validation: Epoch [16], Batch [569/938], Loss: 0.6485439538955688\n",
      "Validation: Epoch [16], Batch [570/938], Loss: 0.9169815182685852\n",
      "Validation: Epoch [16], Batch [571/938], Loss: 0.6528502702713013\n",
      "Validation: Epoch [16], Batch [572/938], Loss: 0.7160392999649048\n",
      "Validation: Epoch [16], Batch [573/938], Loss: 0.7995021343231201\n",
      "Validation: Epoch [16], Batch [574/938], Loss: 0.8220661282539368\n",
      "Validation: Epoch [16], Batch [575/938], Loss: 0.6482583284378052\n",
      "Validation: Epoch [16], Batch [576/938], Loss: 0.5901656150817871\n",
      "Validation: Epoch [16], Batch [577/938], Loss: 0.571174681186676\n",
      "Validation: Epoch [16], Batch [578/938], Loss: 0.7982251644134521\n",
      "Validation: Epoch [16], Batch [579/938], Loss: 0.7236859798431396\n",
      "Validation: Epoch [16], Batch [580/938], Loss: 0.6909319162368774\n",
      "Validation: Epoch [16], Batch [581/938], Loss: 0.7195283770561218\n",
      "Validation: Epoch [16], Batch [582/938], Loss: 0.6311603784561157\n",
      "Validation: Epoch [16], Batch [583/938], Loss: 0.589083194732666\n",
      "Validation: Epoch [16], Batch [584/938], Loss: 0.6610561609268188\n",
      "Validation: Epoch [16], Batch [585/938], Loss: 0.6973080635070801\n",
      "Validation: Epoch [16], Batch [586/938], Loss: 0.45203346014022827\n",
      "Validation: Epoch [16], Batch [587/938], Loss: 0.7040412425994873\n",
      "Validation: Epoch [16], Batch [588/938], Loss: 0.7281091809272766\n",
      "Validation: Epoch [16], Batch [589/938], Loss: 0.5991864204406738\n",
      "Validation: Epoch [16], Batch [590/938], Loss: 0.7138801217079163\n",
      "Validation: Epoch [16], Batch [591/938], Loss: 0.7210301160812378\n",
      "Validation: Epoch [16], Batch [592/938], Loss: 0.7168467044830322\n",
      "Validation: Epoch [16], Batch [593/938], Loss: 0.7043936252593994\n",
      "Validation: Epoch [16], Batch [594/938], Loss: 0.8452023267745972\n",
      "Validation: Epoch [16], Batch [595/938], Loss: 0.5257608294487\n",
      "Validation: Epoch [16], Batch [596/938], Loss: 0.5154874324798584\n",
      "Validation: Epoch [16], Batch [597/938], Loss: 0.5882834196090698\n",
      "Validation: Epoch [16], Batch [598/938], Loss: 0.9729741215705872\n",
      "Validation: Epoch [16], Batch [599/938], Loss: 0.5482290983200073\n",
      "Validation: Epoch [16], Batch [600/938], Loss: 0.8218271732330322\n",
      "Validation: Epoch [16], Batch [601/938], Loss: 0.9633178114891052\n",
      "Validation: Epoch [16], Batch [602/938], Loss: 0.7355957627296448\n",
      "Validation: Epoch [16], Batch [603/938], Loss: 0.8559368252754211\n",
      "Validation: Epoch [16], Batch [604/938], Loss: 0.7170727252960205\n",
      "Validation: Epoch [16], Batch [605/938], Loss: 1.0546478033065796\n",
      "Validation: Epoch [16], Batch [606/938], Loss: 0.889077365398407\n",
      "Validation: Epoch [16], Batch [607/938], Loss: 0.6480944156646729\n",
      "Validation: Epoch [16], Batch [608/938], Loss: 0.6446988582611084\n",
      "Validation: Epoch [16], Batch [609/938], Loss: 0.714577317237854\n",
      "Validation: Epoch [16], Batch [610/938], Loss: 0.6510268449783325\n",
      "Validation: Epoch [16], Batch [611/938], Loss: 0.8701320290565491\n",
      "Validation: Epoch [16], Batch [612/938], Loss: 0.4752843379974365\n",
      "Validation: Epoch [16], Batch [613/938], Loss: 0.6825162768363953\n",
      "Validation: Epoch [16], Batch [614/938], Loss: 0.4863088130950928\n",
      "Validation: Epoch [16], Batch [615/938], Loss: 0.6396077275276184\n",
      "Validation: Epoch [16], Batch [616/938], Loss: 0.625318169593811\n",
      "Validation: Epoch [16], Batch [617/938], Loss: 0.6845272779464722\n",
      "Validation: Epoch [16], Batch [618/938], Loss: 0.7593883275985718\n",
      "Validation: Epoch [16], Batch [619/938], Loss: 0.9177800416946411\n",
      "Validation: Epoch [16], Batch [620/938], Loss: 0.8035621643066406\n",
      "Validation: Epoch [16], Batch [621/938], Loss: 0.6788955926895142\n",
      "Validation: Epoch [16], Batch [622/938], Loss: 0.48850199580192566\n",
      "Validation: Epoch [16], Batch [623/938], Loss: 0.8756489157676697\n",
      "Validation: Epoch [16], Batch [624/938], Loss: 0.6332972645759583\n",
      "Validation: Epoch [16], Batch [625/938], Loss: 0.6951835751533508\n",
      "Validation: Epoch [16], Batch [626/938], Loss: 0.6452787518501282\n",
      "Validation: Epoch [16], Batch [627/938], Loss: 0.7618730068206787\n",
      "Validation: Epoch [16], Batch [628/938], Loss: 0.6906787157058716\n",
      "Validation: Epoch [16], Batch [629/938], Loss: 0.7624043226242065\n",
      "Validation: Epoch [16], Batch [630/938], Loss: 0.7043913006782532\n",
      "Validation: Epoch [16], Batch [631/938], Loss: 0.5453348159790039\n",
      "Validation: Epoch [16], Batch [632/938], Loss: 0.7347849607467651\n",
      "Validation: Epoch [16], Batch [633/938], Loss: 0.4849185347557068\n",
      "Validation: Epoch [16], Batch [634/938], Loss: 0.7069060206413269\n",
      "Validation: Epoch [16], Batch [635/938], Loss: 0.608748197555542\n",
      "Validation: Epoch [16], Batch [636/938], Loss: 0.5760489106178284\n",
      "Validation: Epoch [16], Batch [637/938], Loss: 1.0668689012527466\n",
      "Validation: Epoch [16], Batch [638/938], Loss: 0.8010390996932983\n",
      "Validation: Epoch [16], Batch [639/938], Loss: 0.5771845579147339\n",
      "Validation: Epoch [16], Batch [640/938], Loss: 0.659684956073761\n",
      "Validation: Epoch [16], Batch [641/938], Loss: 0.9615408182144165\n",
      "Validation: Epoch [16], Batch [642/938], Loss: 0.9274495244026184\n",
      "Validation: Epoch [16], Batch [643/938], Loss: 0.878121554851532\n",
      "Validation: Epoch [16], Batch [644/938], Loss: 0.7041285037994385\n",
      "Validation: Epoch [16], Batch [645/938], Loss: 0.7326783537864685\n",
      "Validation: Epoch [16], Batch [646/938], Loss: 0.7532862424850464\n",
      "Validation: Epoch [16], Batch [647/938], Loss: 0.6434822678565979\n",
      "Validation: Epoch [16], Batch [648/938], Loss: 0.7617902159690857\n",
      "Validation: Epoch [16], Batch [649/938], Loss: 0.864678144454956\n",
      "Validation: Epoch [16], Batch [650/938], Loss: 0.5867441892623901\n",
      "Validation: Epoch [16], Batch [651/938], Loss: 0.7825177311897278\n",
      "Validation: Epoch [16], Batch [652/938], Loss: 1.0078967809677124\n",
      "Validation: Epoch [16], Batch [653/938], Loss: 0.5566040873527527\n",
      "Validation: Epoch [16], Batch [654/938], Loss: 0.6637612581253052\n",
      "Validation: Epoch [16], Batch [655/938], Loss: 0.6300001740455627\n",
      "Validation: Epoch [16], Batch [656/938], Loss: 0.7125983238220215\n",
      "Validation: Epoch [16], Batch [657/938], Loss: 0.8169599771499634\n",
      "Validation: Epoch [16], Batch [658/938], Loss: 0.7276189923286438\n",
      "Validation: Epoch [16], Batch [659/938], Loss: 0.6370663046836853\n",
      "Validation: Epoch [16], Batch [660/938], Loss: 0.9169983863830566\n",
      "Validation: Epoch [16], Batch [661/938], Loss: 0.5086958408355713\n",
      "Validation: Epoch [16], Batch [662/938], Loss: 0.7536348104476929\n",
      "Validation: Epoch [16], Batch [663/938], Loss: 0.8301070332527161\n",
      "Validation: Epoch [16], Batch [664/938], Loss: 0.6304036378860474\n",
      "Validation: Epoch [16], Batch [665/938], Loss: 0.804999589920044\n",
      "Validation: Epoch [16], Batch [666/938], Loss: 0.9131631851196289\n",
      "Validation: Epoch [16], Batch [667/938], Loss: 0.6374179720878601\n",
      "Validation: Epoch [16], Batch [668/938], Loss: 0.6941563487052917\n",
      "Validation: Epoch [16], Batch [669/938], Loss: 0.6138691902160645\n",
      "Validation: Epoch [16], Batch [670/938], Loss: 0.5573317408561707\n",
      "Validation: Epoch [16], Batch [671/938], Loss: 0.5769893527030945\n",
      "Validation: Epoch [16], Batch [672/938], Loss: 0.9641854166984558\n",
      "Validation: Epoch [16], Batch [673/938], Loss: 0.5681309700012207\n",
      "Validation: Epoch [16], Batch [674/938], Loss: 0.7153913378715515\n",
      "Validation: Epoch [16], Batch [675/938], Loss: 0.688297688961029\n",
      "Validation: Epoch [16], Batch [676/938], Loss: 0.7509246468544006\n",
      "Validation: Epoch [16], Batch [677/938], Loss: 0.7559368014335632\n",
      "Validation: Epoch [16], Batch [678/938], Loss: 0.6461885571479797\n",
      "Validation: Epoch [16], Batch [679/938], Loss: 0.7118863463401794\n",
      "Validation: Epoch [16], Batch [680/938], Loss: 0.5907630324363708\n",
      "Validation: Epoch [16], Batch [681/938], Loss: 0.5659855008125305\n",
      "Validation: Epoch [16], Batch [682/938], Loss: 0.6772555112838745\n",
      "Validation: Epoch [16], Batch [683/938], Loss: 0.7900926470756531\n",
      "Validation: Epoch [16], Batch [684/938], Loss: 0.7033199071884155\n",
      "Validation: Epoch [16], Batch [685/938], Loss: 0.617516040802002\n",
      "Validation: Epoch [16], Batch [686/938], Loss: 0.5082855224609375\n",
      "Validation: Epoch [16], Batch [687/938], Loss: 0.6841190457344055\n",
      "Validation: Epoch [16], Batch [688/938], Loss: 0.7097959518432617\n",
      "Validation: Epoch [16], Batch [689/938], Loss: 0.6331374049186707\n",
      "Validation: Epoch [16], Batch [690/938], Loss: 0.7890872359275818\n",
      "Validation: Epoch [16], Batch [691/938], Loss: 0.6173237562179565\n",
      "Validation: Epoch [16], Batch [692/938], Loss: 0.6688170433044434\n",
      "Validation: Epoch [16], Batch [693/938], Loss: 0.8422476053237915\n",
      "Validation: Epoch [16], Batch [694/938], Loss: 0.6507272124290466\n",
      "Validation: Epoch [16], Batch [695/938], Loss: 0.5327167510986328\n",
      "Validation: Epoch [16], Batch [696/938], Loss: 0.564303457736969\n",
      "Validation: Epoch [16], Batch [697/938], Loss: 0.6624797582626343\n",
      "Validation: Epoch [16], Batch [698/938], Loss: 0.5562065243721008\n",
      "Validation: Epoch [16], Batch [699/938], Loss: 0.636385440826416\n",
      "Validation: Epoch [16], Batch [700/938], Loss: 0.7307127118110657\n",
      "Validation: Epoch [16], Batch [701/938], Loss: 0.8590783476829529\n",
      "Validation: Epoch [16], Batch [702/938], Loss: 0.7267784476280212\n",
      "Validation: Epoch [16], Batch [703/938], Loss: 0.8020586371421814\n",
      "Validation: Epoch [16], Batch [704/938], Loss: 0.48620688915252686\n",
      "Validation: Epoch [16], Batch [705/938], Loss: 0.8430418372154236\n",
      "Validation: Epoch [16], Batch [706/938], Loss: 0.7362882494926453\n",
      "Validation: Epoch [16], Batch [707/938], Loss: 0.5148547887802124\n",
      "Validation: Epoch [16], Batch [708/938], Loss: 0.8171744346618652\n",
      "Validation: Epoch [16], Batch [709/938], Loss: 0.7484453320503235\n",
      "Validation: Epoch [16], Batch [710/938], Loss: 0.6498491764068604\n",
      "Validation: Epoch [16], Batch [711/938], Loss: 0.6785358786582947\n",
      "Validation: Epoch [16], Batch [712/938], Loss: 0.9144759178161621\n",
      "Validation: Epoch [16], Batch [713/938], Loss: 0.7550933957099915\n",
      "Validation: Epoch [16], Batch [714/938], Loss: 0.6843106150627136\n",
      "Validation: Epoch [16], Batch [715/938], Loss: 0.550121545791626\n",
      "Validation: Epoch [16], Batch [716/938], Loss: 0.7682304978370667\n",
      "Validation: Epoch [16], Batch [717/938], Loss: 0.7122623920440674\n",
      "Validation: Epoch [16], Batch [718/938], Loss: 0.6841341853141785\n",
      "Validation: Epoch [16], Batch [719/938], Loss: 0.5381414294242859\n",
      "Validation: Epoch [16], Batch [720/938], Loss: 0.4503054618835449\n",
      "Validation: Epoch [16], Batch [721/938], Loss: 0.5278790593147278\n",
      "Validation: Epoch [16], Batch [722/938], Loss: 0.9755945205688477\n",
      "Validation: Epoch [16], Batch [723/938], Loss: 0.8067716360092163\n",
      "Validation: Epoch [16], Batch [724/938], Loss: 1.10936439037323\n",
      "Validation: Epoch [16], Batch [725/938], Loss: 0.5561183094978333\n",
      "Validation: Epoch [16], Batch [726/938], Loss: 0.6130976676940918\n",
      "Validation: Epoch [16], Batch [727/938], Loss: 0.7934275269508362\n",
      "Validation: Epoch [16], Batch [728/938], Loss: 0.677243709564209\n",
      "Validation: Epoch [16], Batch [729/938], Loss: 0.6843218803405762\n",
      "Validation: Epoch [16], Batch [730/938], Loss: 0.6422404646873474\n",
      "Validation: Epoch [16], Batch [731/938], Loss: 0.6911680102348328\n",
      "Validation: Epoch [16], Batch [732/938], Loss: 0.6732252240180969\n",
      "Validation: Epoch [16], Batch [733/938], Loss: 0.5452746748924255\n",
      "Validation: Epoch [16], Batch [734/938], Loss: 0.6059378385543823\n",
      "Validation: Epoch [16], Batch [735/938], Loss: 0.6914154291152954\n",
      "Validation: Epoch [16], Batch [736/938], Loss: 0.6783111095428467\n",
      "Validation: Epoch [16], Batch [737/938], Loss: 0.7359894514083862\n",
      "Validation: Epoch [16], Batch [738/938], Loss: 0.7865492701530457\n",
      "Validation: Epoch [16], Batch [739/938], Loss: 0.5815143585205078\n",
      "Validation: Epoch [16], Batch [740/938], Loss: 0.7093390822410583\n",
      "Validation: Epoch [16], Batch [741/938], Loss: 0.6641639471054077\n",
      "Validation: Epoch [16], Batch [742/938], Loss: 0.5216349363327026\n",
      "Validation: Epoch [16], Batch [743/938], Loss: 0.6276301741600037\n",
      "Validation: Epoch [16], Batch [744/938], Loss: 0.5278168320655823\n",
      "Validation: Epoch [16], Batch [745/938], Loss: 0.7893664836883545\n",
      "Validation: Epoch [16], Batch [746/938], Loss: 0.5583911538124084\n",
      "Validation: Epoch [16], Batch [747/938], Loss: 0.9149572849273682\n",
      "Validation: Epoch [16], Batch [748/938], Loss: 0.5770534873008728\n",
      "Validation: Epoch [16], Batch [749/938], Loss: 0.9670448303222656\n",
      "Validation: Epoch [16], Batch [750/938], Loss: 0.9944593906402588\n",
      "Validation: Epoch [16], Batch [751/938], Loss: 0.5975701808929443\n",
      "Validation: Epoch [16], Batch [752/938], Loss: 0.8041465282440186\n",
      "Validation: Epoch [16], Batch [753/938], Loss: 0.6420949101448059\n",
      "Validation: Epoch [16], Batch [754/938], Loss: 0.6532208919525146\n",
      "Validation: Epoch [16], Batch [755/938], Loss: 0.6829236149787903\n",
      "Validation: Epoch [16], Batch [756/938], Loss: 0.4842488467693329\n",
      "Validation: Epoch [16], Batch [757/938], Loss: 0.5207394957542419\n",
      "Validation: Epoch [16], Batch [758/938], Loss: 0.8270168900489807\n",
      "Validation: Epoch [16], Batch [759/938], Loss: 0.9822103381156921\n",
      "Validation: Epoch [16], Batch [760/938], Loss: 0.7342746257781982\n",
      "Validation: Epoch [16], Batch [761/938], Loss: 0.5136889219284058\n",
      "Validation: Epoch [16], Batch [762/938], Loss: 0.7061910629272461\n",
      "Validation: Epoch [16], Batch [763/938], Loss: 0.7239057421684265\n",
      "Validation: Epoch [16], Batch [764/938], Loss: 0.6145470142364502\n",
      "Validation: Epoch [16], Batch [765/938], Loss: 0.7466177344322205\n",
      "Validation: Epoch [16], Batch [766/938], Loss: 0.760601282119751\n",
      "Validation: Epoch [16], Batch [767/938], Loss: 0.5742653012275696\n",
      "Validation: Epoch [16], Batch [768/938], Loss: 0.5671785473823547\n",
      "Validation: Epoch [16], Batch [769/938], Loss: 0.7692372798919678\n",
      "Validation: Epoch [16], Batch [770/938], Loss: 0.6744548082351685\n",
      "Validation: Epoch [16], Batch [771/938], Loss: 0.6608580350875854\n",
      "Validation: Epoch [16], Batch [772/938], Loss: 0.8668768405914307\n",
      "Validation: Epoch [16], Batch [773/938], Loss: 0.719498872756958\n",
      "Validation: Epoch [16], Batch [774/938], Loss: 0.8856398463249207\n",
      "Validation: Epoch [16], Batch [775/938], Loss: 0.6350183486938477\n",
      "Validation: Epoch [16], Batch [776/938], Loss: 0.6443806886672974\n",
      "Validation: Epoch [16], Batch [777/938], Loss: 0.608057975769043\n",
      "Validation: Epoch [16], Batch [778/938], Loss: 0.5846667885780334\n",
      "Validation: Epoch [16], Batch [779/938], Loss: 0.6647017002105713\n",
      "Validation: Epoch [16], Batch [780/938], Loss: 0.7994367480278015\n",
      "Validation: Epoch [16], Batch [781/938], Loss: 0.8479501008987427\n",
      "Validation: Epoch [16], Batch [782/938], Loss: 0.5676140785217285\n",
      "Validation: Epoch [16], Batch [783/938], Loss: 0.6307954788208008\n",
      "Validation: Epoch [16], Batch [784/938], Loss: 0.6117897033691406\n",
      "Validation: Epoch [16], Batch [785/938], Loss: 0.8190680146217346\n",
      "Validation: Epoch [16], Batch [786/938], Loss: 0.5071465969085693\n",
      "Validation: Epoch [16], Batch [787/938], Loss: 0.5721475481987\n",
      "Validation: Epoch [16], Batch [788/938], Loss: 0.7834994196891785\n",
      "Validation: Epoch [16], Batch [789/938], Loss: 0.537601888179779\n",
      "Validation: Epoch [16], Batch [790/938], Loss: 0.6865715384483337\n",
      "Validation: Epoch [16], Batch [791/938], Loss: 0.713761568069458\n",
      "Validation: Epoch [16], Batch [792/938], Loss: 0.7143605947494507\n",
      "Validation: Epoch [16], Batch [793/938], Loss: 0.7331962585449219\n",
      "Validation: Epoch [16], Batch [794/938], Loss: 0.8332431316375732\n",
      "Validation: Epoch [16], Batch [795/938], Loss: 0.7099265456199646\n",
      "Validation: Epoch [16], Batch [796/938], Loss: 0.49211159348487854\n",
      "Validation: Epoch [16], Batch [797/938], Loss: 0.47820985317230225\n",
      "Validation: Epoch [16], Batch [798/938], Loss: 0.6169306039810181\n",
      "Validation: Epoch [16], Batch [799/938], Loss: 0.5820760726928711\n",
      "Validation: Epoch [16], Batch [800/938], Loss: 0.6208372712135315\n",
      "Validation: Epoch [16], Batch [801/938], Loss: 0.652556300163269\n",
      "Validation: Epoch [16], Batch [802/938], Loss: 0.6150973439216614\n",
      "Validation: Epoch [16], Batch [803/938], Loss: 0.7817285060882568\n",
      "Validation: Epoch [16], Batch [804/938], Loss: 0.8407606482505798\n",
      "Validation: Epoch [16], Batch [805/938], Loss: 0.5719202160835266\n",
      "Validation: Epoch [16], Batch [806/938], Loss: 0.8720173835754395\n",
      "Validation: Epoch [16], Batch [807/938], Loss: 1.0352356433868408\n",
      "Validation: Epoch [16], Batch [808/938], Loss: 0.7116756439208984\n",
      "Validation: Epoch [16], Batch [809/938], Loss: 0.7190700173377991\n",
      "Validation: Epoch [16], Batch [810/938], Loss: 0.5692124366760254\n",
      "Validation: Epoch [16], Batch [811/938], Loss: 0.5906165242195129\n",
      "Validation: Epoch [16], Batch [812/938], Loss: 0.8303892016410828\n",
      "Validation: Epoch [16], Batch [813/938], Loss: 0.9578795433044434\n",
      "Validation: Epoch [16], Batch [814/938], Loss: 0.8043330907821655\n",
      "Validation: Epoch [16], Batch [815/938], Loss: 0.8029321432113647\n",
      "Validation: Epoch [16], Batch [816/938], Loss: 0.4912562966346741\n",
      "Validation: Epoch [16], Batch [817/938], Loss: 0.7487123012542725\n",
      "Validation: Epoch [16], Batch [818/938], Loss: 0.7117508053779602\n",
      "Validation: Epoch [16], Batch [819/938], Loss: 0.6263738870620728\n",
      "Validation: Epoch [16], Batch [820/938], Loss: 0.6504701972007751\n",
      "Validation: Epoch [16], Batch [821/938], Loss: 0.7256622314453125\n",
      "Validation: Epoch [16], Batch [822/938], Loss: 0.6528700590133667\n",
      "Validation: Epoch [16], Batch [823/938], Loss: 0.6199317574501038\n",
      "Validation: Epoch [16], Batch [824/938], Loss: 0.4403412938117981\n",
      "Validation: Epoch [16], Batch [825/938], Loss: 0.9937010407447815\n",
      "Validation: Epoch [16], Batch [826/938], Loss: 0.8914187550544739\n",
      "Validation: Epoch [16], Batch [827/938], Loss: 0.7366134524345398\n",
      "Validation: Epoch [16], Batch [828/938], Loss: 0.7036418914794922\n",
      "Validation: Epoch [16], Batch [829/938], Loss: 0.7446227073669434\n",
      "Validation: Epoch [16], Batch [830/938], Loss: 0.5940984487533569\n",
      "Validation: Epoch [16], Batch [831/938], Loss: 0.5917489528656006\n",
      "Validation: Epoch [16], Batch [832/938], Loss: 0.6050030589103699\n",
      "Validation: Epoch [16], Batch [833/938], Loss: 0.7736596465110779\n",
      "Validation: Epoch [16], Batch [834/938], Loss: 0.5855565667152405\n",
      "Validation: Epoch [16], Batch [835/938], Loss: 0.7830380797386169\n",
      "Validation: Epoch [16], Batch [836/938], Loss: 0.5544278621673584\n",
      "Validation: Epoch [16], Batch [837/938], Loss: 0.888856828212738\n",
      "Validation: Epoch [16], Batch [838/938], Loss: 1.0329325199127197\n",
      "Validation: Epoch [16], Batch [839/938], Loss: 0.6376628875732422\n",
      "Validation: Epoch [16], Batch [840/938], Loss: 0.5157178044319153\n",
      "Validation: Epoch [16], Batch [841/938], Loss: 0.7794135212898254\n",
      "Validation: Epoch [16], Batch [842/938], Loss: 0.47513681650161743\n",
      "Validation: Epoch [16], Batch [843/938], Loss: 0.6126617789268494\n",
      "Validation: Epoch [16], Batch [844/938], Loss: 0.4939403235912323\n",
      "Validation: Epoch [16], Batch [845/938], Loss: 0.5032004714012146\n",
      "Validation: Epoch [16], Batch [846/938], Loss: 0.6011040806770325\n",
      "Validation: Epoch [16], Batch [847/938], Loss: 0.41968920826911926\n",
      "Validation: Epoch [16], Batch [848/938], Loss: 0.5180376768112183\n",
      "Validation: Epoch [16], Batch [849/938], Loss: 0.4549902081489563\n",
      "Validation: Epoch [16], Batch [850/938], Loss: 0.5549384355545044\n",
      "Validation: Epoch [16], Batch [851/938], Loss: 0.6352437138557434\n",
      "Validation: Epoch [16], Batch [852/938], Loss: 0.8158725500106812\n",
      "Validation: Epoch [16], Batch [853/938], Loss: 0.5806921124458313\n",
      "Validation: Epoch [16], Batch [854/938], Loss: 0.5711352229118347\n",
      "Validation: Epoch [16], Batch [855/938], Loss: 0.6435698866844177\n",
      "Validation: Epoch [16], Batch [856/938], Loss: 0.72063148021698\n",
      "Validation: Epoch [16], Batch [857/938], Loss: 0.7722027897834778\n",
      "Validation: Epoch [16], Batch [858/938], Loss: 0.7699241042137146\n",
      "Validation: Epoch [16], Batch [859/938], Loss: 0.6682335138320923\n",
      "Validation: Epoch [16], Batch [860/938], Loss: 0.5061731338500977\n",
      "Validation: Epoch [16], Batch [861/938], Loss: 0.6456217169761658\n",
      "Validation: Epoch [16], Batch [862/938], Loss: 0.5562540292739868\n",
      "Validation: Epoch [16], Batch [863/938], Loss: 0.7293645739555359\n",
      "Validation: Epoch [16], Batch [864/938], Loss: 0.6102932095527649\n",
      "Validation: Epoch [16], Batch [865/938], Loss: 0.8335583806037903\n",
      "Validation: Epoch [16], Batch [866/938], Loss: 0.9761085510253906\n",
      "Validation: Epoch [16], Batch [867/938], Loss: 0.5567907691001892\n",
      "Validation: Epoch [16], Batch [868/938], Loss: 0.6993429660797119\n",
      "Validation: Epoch [16], Batch [869/938], Loss: 0.595097005367279\n",
      "Validation: Epoch [16], Batch [870/938], Loss: 0.6328550577163696\n",
      "Validation: Epoch [16], Batch [871/938], Loss: 0.6588448882102966\n",
      "Validation: Epoch [16], Batch [872/938], Loss: 0.5600712299346924\n",
      "Validation: Epoch [16], Batch [873/938], Loss: 0.626422107219696\n",
      "Validation: Epoch [16], Batch [874/938], Loss: 0.6707337498664856\n",
      "Validation: Epoch [16], Batch [875/938], Loss: 0.7221031188964844\n",
      "Validation: Epoch [16], Batch [876/938], Loss: 0.5598925352096558\n",
      "Validation: Epoch [16], Batch [877/938], Loss: 0.42762985825538635\n",
      "Validation: Epoch [16], Batch [878/938], Loss: 0.691058337688446\n",
      "Validation: Epoch [16], Batch [879/938], Loss: 0.5615712404251099\n",
      "Validation: Epoch [16], Batch [880/938], Loss: 0.8530802726745605\n",
      "Validation: Epoch [16], Batch [881/938], Loss: 0.9914194941520691\n",
      "Validation: Epoch [16], Batch [882/938], Loss: 0.5743823647499084\n",
      "Validation: Epoch [16], Batch [883/938], Loss: 0.5495546460151672\n",
      "Validation: Epoch [16], Batch [884/938], Loss: 0.7033023834228516\n",
      "Validation: Epoch [16], Batch [885/938], Loss: 0.7497845888137817\n",
      "Validation: Epoch [16], Batch [886/938], Loss: 0.8621727824211121\n",
      "Validation: Epoch [16], Batch [887/938], Loss: 0.559546709060669\n",
      "Validation: Epoch [16], Batch [888/938], Loss: 0.9912657737731934\n",
      "Validation: Epoch [16], Batch [889/938], Loss: 0.5045435428619385\n",
      "Validation: Epoch [16], Batch [890/938], Loss: 0.8134311437606812\n",
      "Validation: Epoch [16], Batch [891/938], Loss: 0.7548171877861023\n",
      "Validation: Epoch [16], Batch [892/938], Loss: 0.5245432257652283\n",
      "Validation: Epoch [16], Batch [893/938], Loss: 0.9813586473464966\n",
      "Validation: Epoch [16], Batch [894/938], Loss: 0.8179101347923279\n",
      "Validation: Epoch [16], Batch [895/938], Loss: 0.6434745192527771\n",
      "Validation: Epoch [16], Batch [896/938], Loss: 0.7712656259536743\n",
      "Validation: Epoch [16], Batch [897/938], Loss: 0.809699535369873\n",
      "Validation: Epoch [16], Batch [898/938], Loss: 0.6525560021400452\n",
      "Validation: Epoch [16], Batch [899/938], Loss: 0.49532032012939453\n",
      "Validation: Epoch [16], Batch [900/938], Loss: 0.6790143847465515\n",
      "Validation: Epoch [16], Batch [901/938], Loss: 0.6185672879219055\n",
      "Validation: Epoch [16], Batch [902/938], Loss: 0.4376145601272583\n",
      "Validation: Epoch [16], Batch [903/938], Loss: 0.669832706451416\n",
      "Validation: Epoch [16], Batch [904/938], Loss: 0.5346090793609619\n",
      "Validation: Epoch [16], Batch [905/938], Loss: 0.632553219795227\n",
      "Validation: Epoch [16], Batch [906/938], Loss: 0.5643479824066162\n",
      "Validation: Epoch [16], Batch [907/938], Loss: 0.8388281464576721\n",
      "Validation: Epoch [16], Batch [908/938], Loss: 0.9547649621963501\n",
      "Validation: Epoch [16], Batch [909/938], Loss: 0.5790417790412903\n",
      "Validation: Epoch [16], Batch [910/938], Loss: 0.5597758889198303\n",
      "Validation: Epoch [16], Batch [911/938], Loss: 0.7736934423446655\n",
      "Validation: Epoch [16], Batch [912/938], Loss: 0.7292088270187378\n",
      "Validation: Epoch [16], Batch [913/938], Loss: 0.7020184993743896\n",
      "Validation: Epoch [16], Batch [914/938], Loss: 1.0361747741699219\n",
      "Validation: Epoch [16], Batch [915/938], Loss: 0.5263814330101013\n",
      "Validation: Epoch [16], Batch [916/938], Loss: 0.609281063079834\n",
      "Validation: Epoch [16], Batch [917/938], Loss: 0.7687961459159851\n",
      "Validation: Epoch [16], Batch [918/938], Loss: 0.7573437094688416\n",
      "Validation: Epoch [16], Batch [919/938], Loss: 0.6458345651626587\n",
      "Validation: Epoch [16], Batch [920/938], Loss: 0.7490221858024597\n",
      "Validation: Epoch [16], Batch [921/938], Loss: 0.4827668070793152\n",
      "Validation: Epoch [16], Batch [922/938], Loss: 0.5062207579612732\n",
      "Validation: Epoch [16], Batch [923/938], Loss: 0.7518279552459717\n",
      "Validation: Epoch [16], Batch [924/938], Loss: 0.5930014252662659\n",
      "Validation: Epoch [16], Batch [925/938], Loss: 0.7952847480773926\n",
      "Validation: Epoch [16], Batch [926/938], Loss: 0.5731033086776733\n",
      "Validation: Epoch [16], Batch [927/938], Loss: 0.5316744446754456\n",
      "Validation: Epoch [16], Batch [928/938], Loss: 0.627940833568573\n",
      "Validation: Epoch [16], Batch [929/938], Loss: 0.7328412532806396\n",
      "Validation: Epoch [16], Batch [930/938], Loss: 0.610183596611023\n",
      "Validation: Epoch [16], Batch [931/938], Loss: 0.49758484959602356\n",
      "Validation: Epoch [16], Batch [932/938], Loss: 0.6869707107543945\n",
      "Validation: Epoch [16], Batch [933/938], Loss: 0.7039270401000977\n",
      "Validation: Epoch [16], Batch [934/938], Loss: 1.005537748336792\n",
      "Validation: Epoch [16], Batch [935/938], Loss: 0.6525839567184448\n",
      "Validation: Epoch [16], Batch [936/938], Loss: 0.8550065755844116\n",
      "Validation: Epoch [16], Batch [937/938], Loss: 0.7366020083427429\n",
      "Validation: Epoch [16], Batch [938/938], Loss: 0.5808703303337097\n",
      "Accuracy of test set: 0.7999333333333334\n",
      "Train: Epoch [17], Batch [1/938], Loss: 0.6356425881385803\n",
      "Train: Epoch [17], Batch [2/938], Loss: 0.6499112248420715\n",
      "Train: Epoch [17], Batch [3/938], Loss: 0.7458981871604919\n",
      "Train: Epoch [17], Batch [4/938], Loss: 0.8488050699234009\n",
      "Train: Epoch [17], Batch [5/938], Loss: 0.6378252506256104\n",
      "Train: Epoch [17], Batch [6/938], Loss: 0.565017580986023\n",
      "Train: Epoch [17], Batch [7/938], Loss: 0.5019587278366089\n",
      "Train: Epoch [17], Batch [8/938], Loss: 0.6630387306213379\n",
      "Train: Epoch [17], Batch [9/938], Loss: 0.6803381443023682\n",
      "Train: Epoch [17], Batch [10/938], Loss: 0.8804224729537964\n",
      "Train: Epoch [17], Batch [11/938], Loss: 0.6560642719268799\n",
      "Train: Epoch [17], Batch [12/938], Loss: 0.9378302097320557\n",
      "Train: Epoch [17], Batch [13/938], Loss: 0.6824365258216858\n",
      "Train: Epoch [17], Batch [14/938], Loss: 0.5342194437980652\n",
      "Train: Epoch [17], Batch [15/938], Loss: 0.6681687831878662\n",
      "Train: Epoch [17], Batch [16/938], Loss: 0.5675156116485596\n",
      "Train: Epoch [17], Batch [17/938], Loss: 0.7414071559906006\n",
      "Train: Epoch [17], Batch [18/938], Loss: 0.8183804750442505\n",
      "Train: Epoch [17], Batch [19/938], Loss: 0.9199934005737305\n",
      "Train: Epoch [17], Batch [20/938], Loss: 0.7104210257530212\n",
      "Train: Epoch [17], Batch [21/938], Loss: 0.7504452466964722\n",
      "Train: Epoch [17], Batch [22/938], Loss: 0.6847355961799622\n",
      "Train: Epoch [17], Batch [23/938], Loss: 0.8361637592315674\n",
      "Train: Epoch [17], Batch [24/938], Loss: 0.6918905377388\n",
      "Train: Epoch [17], Batch [25/938], Loss: 0.7321062088012695\n",
      "Train: Epoch [17], Batch [26/938], Loss: 0.6660653948783875\n",
      "Train: Epoch [17], Batch [27/938], Loss: 0.7958682775497437\n",
      "Train: Epoch [17], Batch [28/938], Loss: 0.6556987762451172\n",
      "Train: Epoch [17], Batch [29/938], Loss: 0.9590609669685364\n",
      "Train: Epoch [17], Batch [30/938], Loss: 0.9040805697441101\n",
      "Train: Epoch [17], Batch [31/938], Loss: 0.7768834829330444\n",
      "Train: Epoch [17], Batch [32/938], Loss: 0.6047806739807129\n",
      "Train: Epoch [17], Batch [33/938], Loss: 0.5972051620483398\n",
      "Train: Epoch [17], Batch [34/938], Loss: 0.6836449503898621\n",
      "Train: Epoch [17], Batch [35/938], Loss: 0.6523035764694214\n",
      "Train: Epoch [17], Batch [36/938], Loss: 0.7138960361480713\n",
      "Train: Epoch [17], Batch [37/938], Loss: 0.6217041015625\n",
      "Train: Epoch [17], Batch [38/938], Loss: 0.5963788032531738\n",
      "Train: Epoch [17], Batch [39/938], Loss: 0.5136686563491821\n",
      "Train: Epoch [17], Batch [40/938], Loss: 0.5194869637489319\n",
      "Train: Epoch [17], Batch [41/938], Loss: 0.756589949131012\n",
      "Train: Epoch [17], Batch [42/938], Loss: 0.5576471090316772\n",
      "Train: Epoch [17], Batch [43/938], Loss: 0.5705253481864929\n",
      "Train: Epoch [17], Batch [44/938], Loss: 0.7175714373588562\n",
      "Train: Epoch [17], Batch [45/938], Loss: 0.5552034378051758\n",
      "Train: Epoch [17], Batch [46/938], Loss: 0.6129705905914307\n",
      "Train: Epoch [17], Batch [47/938], Loss: 0.5578541159629822\n",
      "Train: Epoch [17], Batch [48/938], Loss: 0.6473865509033203\n",
      "Train: Epoch [17], Batch [49/938], Loss: 0.904340386390686\n",
      "Train: Epoch [17], Batch [50/938], Loss: 0.8024717569351196\n",
      "Train: Epoch [17], Batch [51/938], Loss: 0.7064230442047119\n",
      "Train: Epoch [17], Batch [52/938], Loss: 0.5683966875076294\n",
      "Train: Epoch [17], Batch [53/938], Loss: 0.7206292152404785\n",
      "Train: Epoch [17], Batch [54/938], Loss: 0.761874794960022\n",
      "Train: Epoch [17], Batch [55/938], Loss: 0.6639642715454102\n",
      "Train: Epoch [17], Batch [56/938], Loss: 0.8481177091598511\n",
      "Train: Epoch [17], Batch [57/938], Loss: 0.7967477440834045\n",
      "Train: Epoch [17], Batch [58/938], Loss: 1.0499756336212158\n",
      "Train: Epoch [17], Batch [59/938], Loss: 0.6360323429107666\n",
      "Train: Epoch [17], Batch [60/938], Loss: 0.6530497074127197\n",
      "Train: Epoch [17], Batch [61/938], Loss: 0.7904651165008545\n",
      "Train: Epoch [17], Batch [62/938], Loss: 0.8641334772109985\n",
      "Train: Epoch [17], Batch [63/938], Loss: 0.7290107011795044\n",
      "Train: Epoch [17], Batch [64/938], Loss: 0.8559145331382751\n",
      "Train: Epoch [17], Batch [65/938], Loss: 0.7288435697555542\n",
      "Train: Epoch [17], Batch [66/938], Loss: 0.6028472781181335\n",
      "Train: Epoch [17], Batch [67/938], Loss: 0.9104633927345276\n",
      "Train: Epoch [17], Batch [68/938], Loss: 0.798489511013031\n",
      "Train: Epoch [17], Batch [69/938], Loss: 0.7374944686889648\n",
      "Train: Epoch [17], Batch [70/938], Loss: 0.7031248211860657\n",
      "Train: Epoch [17], Batch [71/938], Loss: 0.7002940773963928\n",
      "Train: Epoch [17], Batch [72/938], Loss: 0.7672663927078247\n",
      "Train: Epoch [17], Batch [73/938], Loss: 0.8009374737739563\n",
      "Train: Epoch [17], Batch [74/938], Loss: 0.7331094741821289\n",
      "Train: Epoch [17], Batch [75/938], Loss: 0.5474019050598145\n",
      "Train: Epoch [17], Batch [76/938], Loss: 0.7353726625442505\n",
      "Train: Epoch [17], Batch [77/938], Loss: 0.46805503964424133\n",
      "Train: Epoch [17], Batch [78/938], Loss: 0.7430911660194397\n",
      "Train: Epoch [17], Batch [79/938], Loss: 0.7663034200668335\n",
      "Train: Epoch [17], Batch [80/938], Loss: 0.874473512172699\n",
      "Train: Epoch [17], Batch [81/938], Loss: 0.5705951452255249\n",
      "Train: Epoch [17], Batch [82/938], Loss: 0.7856258153915405\n",
      "Train: Epoch [17], Batch [83/938], Loss: 0.9458407163619995\n",
      "Train: Epoch [17], Batch [84/938], Loss: 0.8232362866401672\n",
      "Train: Epoch [17], Batch [85/938], Loss: 0.6580129861831665\n",
      "Train: Epoch [17], Batch [86/938], Loss: 0.6759726405143738\n",
      "Train: Epoch [17], Batch [87/938], Loss: 0.6152137517929077\n",
      "Train: Epoch [17], Batch [88/938], Loss: 0.8546298742294312\n",
      "Train: Epoch [17], Batch [89/938], Loss: 0.7512794137001038\n",
      "Train: Epoch [17], Batch [90/938], Loss: 0.4971267879009247\n",
      "Train: Epoch [17], Batch [91/938], Loss: 0.9664972424507141\n",
      "Train: Epoch [17], Batch [92/938], Loss: 0.6703997254371643\n",
      "Train: Epoch [17], Batch [93/938], Loss: 0.8256546854972839\n",
      "Train: Epoch [17], Batch [94/938], Loss: 0.8650248050689697\n",
      "Train: Epoch [17], Batch [95/938], Loss: 0.5957943201065063\n",
      "Train: Epoch [17], Batch [96/938], Loss: 0.6356107592582703\n",
      "Train: Epoch [17], Batch [97/938], Loss: 0.741232693195343\n",
      "Train: Epoch [17], Batch [98/938], Loss: 1.0102781057357788\n",
      "Train: Epoch [17], Batch [99/938], Loss: 0.48860278725624084\n",
      "Train: Epoch [17], Batch [100/938], Loss: 0.6325120329856873\n",
      "Train: Epoch [17], Batch [101/938], Loss: 0.5428858995437622\n",
      "Train: Epoch [17], Batch [102/938], Loss: 0.5966739058494568\n",
      "Train: Epoch [17], Batch [103/938], Loss: 0.585640549659729\n",
      "Train: Epoch [17], Batch [104/938], Loss: 0.6787687540054321\n",
      "Train: Epoch [17], Batch [105/938], Loss: 0.6040775179862976\n",
      "Train: Epoch [17], Batch [106/938], Loss: 0.6922507286071777\n",
      "Train: Epoch [17], Batch [107/938], Loss: 0.96388840675354\n",
      "Train: Epoch [17], Batch [108/938], Loss: 0.5083321332931519\n",
      "Train: Epoch [17], Batch [109/938], Loss: 0.58060622215271\n",
      "Train: Epoch [17], Batch [110/938], Loss: 0.7825801372528076\n",
      "Train: Epoch [17], Batch [111/938], Loss: 0.5603469610214233\n",
      "Train: Epoch [17], Batch [112/938], Loss: 0.8656744360923767\n",
      "Train: Epoch [17], Batch [113/938], Loss: 0.46368563175201416\n",
      "Train: Epoch [17], Batch [114/938], Loss: 0.7403979301452637\n",
      "Train: Epoch [17], Batch [115/938], Loss: 0.5873254537582397\n",
      "Train: Epoch [17], Batch [116/938], Loss: 0.7487364411354065\n",
      "Train: Epoch [17], Batch [117/938], Loss: 0.5475403666496277\n",
      "Train: Epoch [17], Batch [118/938], Loss: 0.640777587890625\n",
      "Train: Epoch [17], Batch [119/938], Loss: 0.7368274331092834\n",
      "Train: Epoch [17], Batch [120/938], Loss: 0.5809729099273682\n",
      "Train: Epoch [17], Batch [121/938], Loss: 0.7359248399734497\n",
      "Train: Epoch [17], Batch [122/938], Loss: 0.7414416074752808\n",
      "Train: Epoch [17], Batch [123/938], Loss: 0.8135175108909607\n",
      "Train: Epoch [17], Batch [124/938], Loss: 0.8365801572799683\n",
      "Train: Epoch [17], Batch [125/938], Loss: 0.6330371499061584\n",
      "Train: Epoch [17], Batch [126/938], Loss: 0.645346999168396\n",
      "Train: Epoch [17], Batch [127/938], Loss: 0.8252646923065186\n",
      "Train: Epoch [17], Batch [128/938], Loss: 0.41452834010124207\n",
      "Train: Epoch [17], Batch [129/938], Loss: 0.6693724989891052\n",
      "Train: Epoch [17], Batch [130/938], Loss: 0.5689431428909302\n",
      "Train: Epoch [17], Batch [131/938], Loss: 0.7668213248252869\n",
      "Train: Epoch [17], Batch [132/938], Loss: 0.5176460146903992\n",
      "Train: Epoch [17], Batch [133/938], Loss: 0.7705515027046204\n",
      "Train: Epoch [17], Batch [134/938], Loss: 0.6888044476509094\n",
      "Train: Epoch [17], Batch [135/938], Loss: 0.6838846206665039\n",
      "Train: Epoch [17], Batch [136/938], Loss: 0.6809242367744446\n",
      "Train: Epoch [17], Batch [137/938], Loss: 0.8929837942123413\n",
      "Train: Epoch [17], Batch [138/938], Loss: 0.532849907875061\n",
      "Train: Epoch [17], Batch [139/938], Loss: 0.6057658791542053\n",
      "Train: Epoch [17], Batch [140/938], Loss: 0.5633631944656372\n",
      "Train: Epoch [17], Batch [141/938], Loss: 0.5972951054573059\n",
      "Train: Epoch [17], Batch [142/938], Loss: 0.6403781175613403\n",
      "Train: Epoch [17], Batch [143/938], Loss: 0.5263545513153076\n",
      "Train: Epoch [17], Batch [144/938], Loss: 0.5878247022628784\n",
      "Train: Epoch [17], Batch [145/938], Loss: 0.45680755376815796\n",
      "Train: Epoch [17], Batch [146/938], Loss: 0.630765974521637\n",
      "Train: Epoch [17], Batch [147/938], Loss: 0.7772554159164429\n",
      "Train: Epoch [17], Batch [148/938], Loss: 0.7868638038635254\n",
      "Train: Epoch [17], Batch [149/938], Loss: 0.6211022138595581\n",
      "Train: Epoch [17], Batch [150/938], Loss: 0.7069882750511169\n",
      "Train: Epoch [17], Batch [151/938], Loss: 0.6950139403343201\n",
      "Train: Epoch [17], Batch [152/938], Loss: 0.6552359461784363\n",
      "Train: Epoch [17], Batch [153/938], Loss: 0.887309730052948\n",
      "Train: Epoch [17], Batch [154/938], Loss: 0.7379482388496399\n",
      "Train: Epoch [17], Batch [155/938], Loss: 0.7619321346282959\n",
      "Train: Epoch [17], Batch [156/938], Loss: 0.7642886638641357\n",
      "Train: Epoch [17], Batch [157/938], Loss: 0.7545962333679199\n",
      "Train: Epoch [17], Batch [158/938], Loss: 0.4516529142856598\n",
      "Train: Epoch [17], Batch [159/938], Loss: 0.6737175583839417\n",
      "Train: Epoch [17], Batch [160/938], Loss: 0.525801956653595\n",
      "Train: Epoch [17], Batch [161/938], Loss: 0.777470588684082\n",
      "Train: Epoch [17], Batch [162/938], Loss: 0.49973857402801514\n",
      "Train: Epoch [17], Batch [163/938], Loss: 0.7655426859855652\n",
      "Train: Epoch [17], Batch [164/938], Loss: 0.5812340974807739\n",
      "Train: Epoch [17], Batch [165/938], Loss: 0.34708455204963684\n",
      "Train: Epoch [17], Batch [166/938], Loss: 1.0026935338974\n",
      "Train: Epoch [17], Batch [167/938], Loss: 0.785190224647522\n",
      "Train: Epoch [17], Batch [168/938], Loss: 0.6392802596092224\n",
      "Train: Epoch [17], Batch [169/938], Loss: 0.6832624673843384\n",
      "Train: Epoch [17], Batch [170/938], Loss: 0.7442107796669006\n",
      "Train: Epoch [17], Batch [171/938], Loss: 0.6062369346618652\n",
      "Train: Epoch [17], Batch [172/938], Loss: 0.8683258295059204\n",
      "Train: Epoch [17], Batch [173/938], Loss: 0.6295053958892822\n",
      "Train: Epoch [17], Batch [174/938], Loss: 0.5962213277816772\n",
      "Train: Epoch [17], Batch [175/938], Loss: 0.6864319443702698\n",
      "Train: Epoch [17], Batch [176/938], Loss: 0.5756568908691406\n",
      "Train: Epoch [17], Batch [177/938], Loss: 0.7511626482009888\n",
      "Train: Epoch [17], Batch [178/938], Loss: 0.5447733998298645\n",
      "Train: Epoch [17], Batch [179/938], Loss: 0.6998776793479919\n",
      "Train: Epoch [17], Batch [180/938], Loss: 0.6051081418991089\n",
      "Train: Epoch [17], Batch [181/938], Loss: 0.5888941287994385\n",
      "Train: Epoch [17], Batch [182/938], Loss: 0.8691995143890381\n",
      "Train: Epoch [17], Batch [183/938], Loss: 0.5364499688148499\n",
      "Train: Epoch [17], Batch [184/938], Loss: 0.6696853041648865\n",
      "Train: Epoch [17], Batch [185/938], Loss: 0.7614212036132812\n",
      "Train: Epoch [17], Batch [186/938], Loss: 0.802052915096283\n",
      "Train: Epoch [17], Batch [187/938], Loss: 0.5638630986213684\n",
      "Train: Epoch [17], Batch [188/938], Loss: 0.5548041462898254\n",
      "Train: Epoch [17], Batch [189/938], Loss: 0.7093603014945984\n",
      "Train: Epoch [17], Batch [190/938], Loss: 0.7762205004692078\n",
      "Train: Epoch [17], Batch [191/938], Loss: 0.5728477239608765\n",
      "Train: Epoch [17], Batch [192/938], Loss: 0.8107382655143738\n",
      "Train: Epoch [17], Batch [193/938], Loss: 0.6556620597839355\n",
      "Train: Epoch [17], Batch [194/938], Loss: 0.8047071695327759\n",
      "Train: Epoch [17], Batch [195/938], Loss: 0.4803942143917084\n",
      "Train: Epoch [17], Batch [196/938], Loss: 0.5049952864646912\n",
      "Train: Epoch [17], Batch [197/938], Loss: 0.4163050055503845\n",
      "Train: Epoch [17], Batch [198/938], Loss: 0.8683847784996033\n",
      "Train: Epoch [17], Batch [199/938], Loss: 0.5560269355773926\n",
      "Train: Epoch [17], Batch [200/938], Loss: 0.813685953617096\n",
      "Train: Epoch [17], Batch [201/938], Loss: 0.6958788633346558\n",
      "Train: Epoch [17], Batch [202/938], Loss: 0.507775604724884\n",
      "Train: Epoch [17], Batch [203/938], Loss: 0.7140764594078064\n",
      "Train: Epoch [17], Batch [204/938], Loss: 0.7848089337348938\n",
      "Train: Epoch [17], Batch [205/938], Loss: 0.6273324489593506\n",
      "Train: Epoch [17], Batch [206/938], Loss: 0.40477776527404785\n",
      "Train: Epoch [17], Batch [207/938], Loss: 0.5996558666229248\n",
      "Train: Epoch [17], Batch [208/938], Loss: 0.7690908908843994\n",
      "Train: Epoch [17], Batch [209/938], Loss: 0.7872166037559509\n",
      "Train: Epoch [17], Batch [210/938], Loss: 0.5522521138191223\n",
      "Train: Epoch [17], Batch [211/938], Loss: 0.734529435634613\n",
      "Train: Epoch [17], Batch [212/938], Loss: 0.5000084638595581\n",
      "Train: Epoch [17], Batch [213/938], Loss: 0.5914751291275024\n",
      "Train: Epoch [17], Batch [214/938], Loss: 0.8369512557983398\n",
      "Train: Epoch [17], Batch [215/938], Loss: 0.39603930711746216\n",
      "Train: Epoch [17], Batch [216/938], Loss: 0.6205083131790161\n",
      "Train: Epoch [17], Batch [217/938], Loss: 0.6619287729263306\n",
      "Train: Epoch [17], Batch [218/938], Loss: 0.4628499746322632\n",
      "Train: Epoch [17], Batch [219/938], Loss: 0.7041478157043457\n",
      "Train: Epoch [17], Batch [220/938], Loss: 0.5806495547294617\n",
      "Train: Epoch [17], Batch [221/938], Loss: 0.7035413980484009\n",
      "Train: Epoch [17], Batch [222/938], Loss: 0.679606020450592\n",
      "Train: Epoch [17], Batch [223/938], Loss: 0.5278105735778809\n",
      "Train: Epoch [17], Batch [224/938], Loss: 0.6151827573776245\n",
      "Train: Epoch [17], Batch [225/938], Loss: 0.7574754953384399\n",
      "Train: Epoch [17], Batch [226/938], Loss: 0.6016873121261597\n",
      "Train: Epoch [17], Batch [227/938], Loss: 0.5074361562728882\n",
      "Train: Epoch [17], Batch [228/938], Loss: 0.6433614492416382\n",
      "Train: Epoch [17], Batch [229/938], Loss: 0.6719284057617188\n",
      "Train: Epoch [17], Batch [230/938], Loss: 0.8587803840637207\n",
      "Train: Epoch [17], Batch [231/938], Loss: 1.0042452812194824\n",
      "Train: Epoch [17], Batch [232/938], Loss: 0.4534763991832733\n",
      "Train: Epoch [17], Batch [233/938], Loss: 0.5613902807235718\n",
      "Train: Epoch [17], Batch [234/938], Loss: 0.8870825171470642\n",
      "Train: Epoch [17], Batch [235/938], Loss: 0.7958534359931946\n",
      "Train: Epoch [17], Batch [236/938], Loss: 0.6582183837890625\n",
      "Train: Epoch [17], Batch [237/938], Loss: 0.6528434157371521\n",
      "Train: Epoch [17], Batch [238/938], Loss: 0.5029916167259216\n",
      "Train: Epoch [17], Batch [239/938], Loss: 0.8490918278694153\n",
      "Train: Epoch [17], Batch [240/938], Loss: 0.8120676875114441\n",
      "Train: Epoch [17], Batch [241/938], Loss: 0.6160182952880859\n",
      "Train: Epoch [17], Batch [242/938], Loss: 0.47537025809288025\n",
      "Train: Epoch [17], Batch [243/938], Loss: 0.7087443470954895\n",
      "Train: Epoch [17], Batch [244/938], Loss: 0.7285881042480469\n",
      "Train: Epoch [17], Batch [245/938], Loss: 0.48350366950035095\n",
      "Train: Epoch [17], Batch [246/938], Loss: 0.4724758267402649\n",
      "Train: Epoch [17], Batch [247/938], Loss: 0.7175907492637634\n",
      "Train: Epoch [17], Batch [248/938], Loss: 0.768356442451477\n",
      "Train: Epoch [17], Batch [249/938], Loss: 0.6204946041107178\n",
      "Train: Epoch [17], Batch [250/938], Loss: 0.5078750252723694\n",
      "Train: Epoch [17], Batch [251/938], Loss: 0.4889034628868103\n",
      "Train: Epoch [17], Batch [252/938], Loss: 0.7284169793128967\n",
      "Train: Epoch [17], Batch [253/938], Loss: 0.7411156892776489\n",
      "Train: Epoch [17], Batch [254/938], Loss: 0.8365305662155151\n",
      "Train: Epoch [17], Batch [255/938], Loss: 0.5579274296760559\n",
      "Train: Epoch [17], Batch [256/938], Loss: 0.535944402217865\n",
      "Train: Epoch [17], Batch [257/938], Loss: 0.6524418592453003\n",
      "Train: Epoch [17], Batch [258/938], Loss: 0.7218078970909119\n",
      "Train: Epoch [17], Batch [259/938], Loss: 0.7738471031188965\n",
      "Train: Epoch [17], Batch [260/938], Loss: 0.7757492661476135\n",
      "Train: Epoch [17], Batch [261/938], Loss: 0.5129468441009521\n",
      "Train: Epoch [17], Batch [262/938], Loss: 0.6589393615722656\n",
      "Train: Epoch [17], Batch [263/938], Loss: 0.8704633712768555\n",
      "Train: Epoch [17], Batch [264/938], Loss: 0.53392094373703\n",
      "Train: Epoch [17], Batch [265/938], Loss: 0.6316255331039429\n",
      "Train: Epoch [17], Batch [266/938], Loss: 0.6259505152702332\n",
      "Train: Epoch [17], Batch [267/938], Loss: 0.6571436524391174\n",
      "Train: Epoch [17], Batch [268/938], Loss: 0.5616682171821594\n",
      "Train: Epoch [17], Batch [269/938], Loss: 0.8127423524856567\n",
      "Train: Epoch [17], Batch [270/938], Loss: 0.7653032541275024\n",
      "Train: Epoch [17], Batch [271/938], Loss: 0.7965847253799438\n",
      "Train: Epoch [17], Batch [272/938], Loss: 0.7376558184623718\n",
      "Train: Epoch [17], Batch [273/938], Loss: 0.7324556112289429\n",
      "Train: Epoch [17], Batch [274/938], Loss: 0.8807713985443115\n",
      "Train: Epoch [17], Batch [275/938], Loss: 0.6160620450973511\n",
      "Train: Epoch [17], Batch [276/938], Loss: 0.7338773012161255\n",
      "Train: Epoch [17], Batch [277/938], Loss: 0.9578282237052917\n",
      "Train: Epoch [17], Batch [278/938], Loss: 0.7778481841087341\n",
      "Train: Epoch [17], Batch [279/938], Loss: 0.6945684552192688\n",
      "Train: Epoch [17], Batch [280/938], Loss: 0.5811856985092163\n",
      "Train: Epoch [17], Batch [281/938], Loss: 0.7199464440345764\n",
      "Train: Epoch [17], Batch [282/938], Loss: 0.7178984880447388\n",
      "Train: Epoch [17], Batch [283/938], Loss: 0.3862556219100952\n",
      "Train: Epoch [17], Batch [284/938], Loss: 0.47417595982551575\n",
      "Train: Epoch [17], Batch [285/938], Loss: 0.7284126877784729\n",
      "Train: Epoch [17], Batch [286/938], Loss: 0.7176933884620667\n",
      "Train: Epoch [17], Batch [287/938], Loss: 0.6507067680358887\n",
      "Train: Epoch [17], Batch [288/938], Loss: 0.5657381415367126\n",
      "Train: Epoch [17], Batch [289/938], Loss: 0.9665576219558716\n",
      "Train: Epoch [17], Batch [290/938], Loss: 0.8234795928001404\n",
      "Train: Epoch [17], Batch [291/938], Loss: 0.6045206785202026\n",
      "Train: Epoch [17], Batch [292/938], Loss: 0.8149523138999939\n",
      "Train: Epoch [17], Batch [293/938], Loss: 0.7342028617858887\n",
      "Train: Epoch [17], Batch [294/938], Loss: 0.7827417254447937\n",
      "Train: Epoch [17], Batch [295/938], Loss: 0.8105261325836182\n",
      "Train: Epoch [17], Batch [296/938], Loss: 0.6627609133720398\n",
      "Train: Epoch [17], Batch [297/938], Loss: 0.5243760347366333\n",
      "Train: Epoch [17], Batch [298/938], Loss: 0.762537956237793\n",
      "Train: Epoch [17], Batch [299/938], Loss: 0.7786892652511597\n",
      "Train: Epoch [17], Batch [300/938], Loss: 0.6014800071716309\n",
      "Train: Epoch [17], Batch [301/938], Loss: 0.7546311616897583\n",
      "Train: Epoch [17], Batch [302/938], Loss: 0.6254863739013672\n",
      "Train: Epoch [17], Batch [303/938], Loss: 0.5479537844657898\n",
      "Train: Epoch [17], Batch [304/938], Loss: 0.6324341297149658\n",
      "Train: Epoch [17], Batch [305/938], Loss: 0.781621515750885\n",
      "Train: Epoch [17], Batch [306/938], Loss: 0.6098809838294983\n",
      "Train: Epoch [17], Batch [307/938], Loss: 0.8450402617454529\n",
      "Train: Epoch [17], Batch [308/938], Loss: 0.6028833389282227\n",
      "Train: Epoch [17], Batch [309/938], Loss: 0.48769643902778625\n",
      "Train: Epoch [17], Batch [310/938], Loss: 0.5773531198501587\n",
      "Train: Epoch [17], Batch [311/938], Loss: 0.6615495681762695\n",
      "Train: Epoch [17], Batch [312/938], Loss: 0.6140023469924927\n",
      "Train: Epoch [17], Batch [313/938], Loss: 0.7944149971008301\n",
      "Train: Epoch [17], Batch [314/938], Loss: 0.867713451385498\n",
      "Train: Epoch [17], Batch [315/938], Loss: 0.6085177063941956\n",
      "Train: Epoch [17], Batch [316/938], Loss: 0.6952582597732544\n",
      "Train: Epoch [17], Batch [317/938], Loss: 0.5866943597793579\n",
      "Train: Epoch [17], Batch [318/938], Loss: 0.6924025416374207\n",
      "Train: Epoch [17], Batch [319/938], Loss: 0.6984403729438782\n",
      "Train: Epoch [17], Batch [320/938], Loss: 0.6947396993637085\n",
      "Train: Epoch [17], Batch [321/938], Loss: 0.7621154189109802\n",
      "Train: Epoch [17], Batch [322/938], Loss: 0.4423733055591583\n",
      "Train: Epoch [17], Batch [323/938], Loss: 0.4942765235900879\n",
      "Train: Epoch [17], Batch [324/938], Loss: 0.763659656047821\n",
      "Train: Epoch [17], Batch [325/938], Loss: 0.7122570276260376\n",
      "Train: Epoch [17], Batch [326/938], Loss: 0.734611451625824\n",
      "Train: Epoch [17], Batch [327/938], Loss: 0.7701684236526489\n",
      "Train: Epoch [17], Batch [328/938], Loss: 0.6918419003486633\n",
      "Train: Epoch [17], Batch [329/938], Loss: 0.7689712643623352\n",
      "Train: Epoch [17], Batch [330/938], Loss: 0.6409846544265747\n",
      "Train: Epoch [17], Batch [331/938], Loss: 0.9184209704399109\n",
      "Train: Epoch [17], Batch [332/938], Loss: 0.7939087152481079\n",
      "Train: Epoch [17], Batch [333/938], Loss: 1.1243083477020264\n",
      "Train: Epoch [17], Batch [334/938], Loss: 0.6870592832565308\n",
      "Train: Epoch [17], Batch [335/938], Loss: 0.719119131565094\n",
      "Train: Epoch [17], Batch [336/938], Loss: 0.6465551853179932\n",
      "Train: Epoch [17], Batch [337/938], Loss: 0.8015732169151306\n",
      "Train: Epoch [17], Batch [338/938], Loss: 0.7943686246871948\n",
      "Train: Epoch [17], Batch [339/938], Loss: 0.6139484643936157\n",
      "Train: Epoch [17], Batch [340/938], Loss: 0.7145008444786072\n",
      "Train: Epoch [17], Batch [341/938], Loss: 0.5423043370246887\n",
      "Train: Epoch [17], Batch [342/938], Loss: 0.5501185059547424\n",
      "Train: Epoch [17], Batch [343/938], Loss: 0.6045661568641663\n",
      "Train: Epoch [17], Batch [344/938], Loss: 0.6040976643562317\n",
      "Train: Epoch [17], Batch [345/938], Loss: 0.8425112962722778\n",
      "Train: Epoch [17], Batch [346/938], Loss: 0.6739984750747681\n",
      "Train: Epoch [17], Batch [347/938], Loss: 0.7521088123321533\n",
      "Train: Epoch [17], Batch [348/938], Loss: 0.546629011631012\n",
      "Train: Epoch [17], Batch [349/938], Loss: 0.6567043662071228\n",
      "Train: Epoch [17], Batch [350/938], Loss: 0.5408250093460083\n",
      "Train: Epoch [17], Batch [351/938], Loss: 0.8005009293556213\n",
      "Train: Epoch [17], Batch [352/938], Loss: 0.589651346206665\n",
      "Train: Epoch [17], Batch [353/938], Loss: 0.8811761736869812\n",
      "Train: Epoch [17], Batch [354/938], Loss: 0.6819643378257751\n",
      "Train: Epoch [17], Batch [355/938], Loss: 0.5323790907859802\n",
      "Train: Epoch [17], Batch [356/938], Loss: 0.5643889307975769\n",
      "Train: Epoch [17], Batch [357/938], Loss: 0.7632930278778076\n",
      "Train: Epoch [17], Batch [358/938], Loss: 0.7092409133911133\n",
      "Train: Epoch [17], Batch [359/938], Loss: 0.6830431222915649\n",
      "Train: Epoch [17], Batch [360/938], Loss: 0.6805844306945801\n",
      "Train: Epoch [17], Batch [361/938], Loss: 0.6819594502449036\n",
      "Train: Epoch [17], Batch [362/938], Loss: 0.9095764756202698\n",
      "Train: Epoch [17], Batch [363/938], Loss: 0.5747857689857483\n",
      "Train: Epoch [17], Batch [364/938], Loss: 1.0850028991699219\n",
      "Train: Epoch [17], Batch [365/938], Loss: 0.6954383254051208\n",
      "Train: Epoch [17], Batch [366/938], Loss: 0.7276845574378967\n",
      "Train: Epoch [17], Batch [367/938], Loss: 0.7530099153518677\n",
      "Train: Epoch [17], Batch [368/938], Loss: 0.7484171986579895\n",
      "Train: Epoch [17], Batch [369/938], Loss: 0.6426196694374084\n",
      "Train: Epoch [17], Batch [370/938], Loss: 0.6413844227790833\n",
      "Train: Epoch [17], Batch [371/938], Loss: 0.7893362641334534\n",
      "Train: Epoch [17], Batch [372/938], Loss: 0.7148523330688477\n",
      "Train: Epoch [17], Batch [373/938], Loss: 0.5985915064811707\n",
      "Train: Epoch [17], Batch [374/938], Loss: 0.44923844933509827\n",
      "Train: Epoch [17], Batch [375/938], Loss: 0.6701965928077698\n",
      "Train: Epoch [17], Batch [376/938], Loss: 0.6621599793434143\n",
      "Train: Epoch [17], Batch [377/938], Loss: 0.4145268499851227\n",
      "Train: Epoch [17], Batch [378/938], Loss: 0.7088255286216736\n",
      "Train: Epoch [17], Batch [379/938], Loss: 0.6591725945472717\n",
      "Train: Epoch [17], Batch [380/938], Loss: 0.6398712992668152\n",
      "Train: Epoch [17], Batch [381/938], Loss: 0.8026949167251587\n",
      "Train: Epoch [17], Batch [382/938], Loss: 0.7202062606811523\n",
      "Train: Epoch [17], Batch [383/938], Loss: 0.7258185744285583\n",
      "Train: Epoch [17], Batch [384/938], Loss: 0.567948579788208\n",
      "Train: Epoch [17], Batch [385/938], Loss: 0.9093526601791382\n",
      "Train: Epoch [17], Batch [386/938], Loss: 0.6155121326446533\n",
      "Train: Epoch [17], Batch [387/938], Loss: 0.6770131587982178\n",
      "Train: Epoch [17], Batch [388/938], Loss: 0.8907192945480347\n",
      "Train: Epoch [17], Batch [389/938], Loss: 0.6891690492630005\n",
      "Train: Epoch [17], Batch [390/938], Loss: 0.6239771842956543\n",
      "Train: Epoch [17], Batch [391/938], Loss: 0.6496710777282715\n",
      "Train: Epoch [17], Batch [392/938], Loss: 0.6375536322593689\n",
      "Train: Epoch [17], Batch [393/938], Loss: 0.6604737043380737\n",
      "Train: Epoch [17], Batch [394/938], Loss: 0.6074568629264832\n",
      "Train: Epoch [17], Batch [395/938], Loss: 0.6440446972846985\n",
      "Train: Epoch [17], Batch [396/938], Loss: 0.792830228805542\n",
      "Train: Epoch [17], Batch [397/938], Loss: 0.8916606903076172\n",
      "Train: Epoch [17], Batch [398/938], Loss: 0.6527130007743835\n",
      "Train: Epoch [17], Batch [399/938], Loss: 0.8314579129219055\n",
      "Train: Epoch [17], Batch [400/938], Loss: 0.5380021929740906\n",
      "Train: Epoch [17], Batch [401/938], Loss: 0.7600669860839844\n",
      "Train: Epoch [17], Batch [402/938], Loss: 0.7179171442985535\n",
      "Train: Epoch [17], Batch [403/938], Loss: 0.679519534111023\n",
      "Train: Epoch [17], Batch [404/938], Loss: 0.5690371990203857\n",
      "Train: Epoch [17], Batch [405/938], Loss: 0.53411865234375\n",
      "Train: Epoch [17], Batch [406/938], Loss: 0.673457145690918\n",
      "Train: Epoch [17], Batch [407/938], Loss: 0.9095884561538696\n",
      "Train: Epoch [17], Batch [408/938], Loss: 0.6415773630142212\n",
      "Train: Epoch [17], Batch [409/938], Loss: 0.6191605925559998\n",
      "Train: Epoch [17], Batch [410/938], Loss: 0.7021297216415405\n",
      "Train: Epoch [17], Batch [411/938], Loss: 0.57038813829422\n",
      "Train: Epoch [17], Batch [412/938], Loss: 0.8114443421363831\n",
      "Train: Epoch [17], Batch [413/938], Loss: 0.7541460990905762\n",
      "Train: Epoch [17], Batch [414/938], Loss: 1.066176414489746\n",
      "Train: Epoch [17], Batch [415/938], Loss: 0.6852517127990723\n",
      "Train: Epoch [17], Batch [416/938], Loss: 0.4538831114768982\n",
      "Train: Epoch [17], Batch [417/938], Loss: 0.8639477491378784\n",
      "Train: Epoch [17], Batch [418/938], Loss: 0.42819973826408386\n",
      "Train: Epoch [17], Batch [419/938], Loss: 0.8440156579017639\n",
      "Train: Epoch [17], Batch [420/938], Loss: 0.7255573868751526\n",
      "Train: Epoch [17], Batch [421/938], Loss: 0.7443990111351013\n",
      "Train: Epoch [17], Batch [422/938], Loss: 0.8332840800285339\n",
      "Train: Epoch [17], Batch [423/938], Loss: 0.797184944152832\n",
      "Train: Epoch [17], Batch [424/938], Loss: 0.8703158497810364\n",
      "Train: Epoch [17], Batch [425/938], Loss: 0.7068847417831421\n",
      "Train: Epoch [17], Batch [426/938], Loss: 0.43061748147010803\n",
      "Train: Epoch [17], Batch [427/938], Loss: 0.8407315015792847\n",
      "Train: Epoch [17], Batch [428/938], Loss: 0.7845785617828369\n",
      "Train: Epoch [17], Batch [429/938], Loss: 0.613497257232666\n",
      "Train: Epoch [17], Batch [430/938], Loss: 0.5464984774589539\n",
      "Train: Epoch [17], Batch [431/938], Loss: 0.6836740374565125\n",
      "Train: Epoch [17], Batch [432/938], Loss: 0.7367931008338928\n",
      "Train: Epoch [17], Batch [433/938], Loss: 1.015188455581665\n",
      "Train: Epoch [17], Batch [434/938], Loss: 0.766088604927063\n",
      "Train: Epoch [17], Batch [435/938], Loss: 0.8009345531463623\n",
      "Train: Epoch [17], Batch [436/938], Loss: 0.8649435639381409\n",
      "Train: Epoch [17], Batch [437/938], Loss: 0.6704548001289368\n",
      "Train: Epoch [17], Batch [438/938], Loss: 0.6739253997802734\n",
      "Train: Epoch [17], Batch [439/938], Loss: 0.5722146034240723\n",
      "Train: Epoch [17], Batch [440/938], Loss: 0.8031214475631714\n",
      "Train: Epoch [17], Batch [441/938], Loss: 0.8500780463218689\n",
      "Train: Epoch [17], Batch [442/938], Loss: 0.6402710676193237\n",
      "Train: Epoch [17], Batch [443/938], Loss: 0.6223956346511841\n",
      "Train: Epoch [17], Batch [444/938], Loss: 0.6538134813308716\n",
      "Train: Epoch [17], Batch [445/938], Loss: 0.7454779744148254\n",
      "Train: Epoch [17], Batch [446/938], Loss: 0.8213329315185547\n",
      "Train: Epoch [17], Batch [447/938], Loss: 0.6488203406333923\n",
      "Train: Epoch [17], Batch [448/938], Loss: 0.6539427638053894\n",
      "Train: Epoch [17], Batch [449/938], Loss: 0.485992431640625\n",
      "Train: Epoch [17], Batch [450/938], Loss: 0.6068822741508484\n",
      "Train: Epoch [17], Batch [451/938], Loss: 0.8580082654953003\n",
      "Train: Epoch [17], Batch [452/938], Loss: 0.6844261288642883\n",
      "Train: Epoch [17], Batch [453/938], Loss: 0.6396843791007996\n",
      "Train: Epoch [17], Batch [454/938], Loss: 0.8446855545043945\n",
      "Train: Epoch [17], Batch [455/938], Loss: 0.6566857695579529\n",
      "Train: Epoch [17], Batch [456/938], Loss: 0.6110628247261047\n",
      "Train: Epoch [17], Batch [457/938], Loss: 0.639420747756958\n",
      "Train: Epoch [17], Batch [458/938], Loss: 0.7961690425872803\n",
      "Train: Epoch [17], Batch [459/938], Loss: 0.5289723873138428\n",
      "Train: Epoch [17], Batch [460/938], Loss: 0.5327012538909912\n",
      "Train: Epoch [17], Batch [461/938], Loss: 0.5905295014381409\n",
      "Train: Epoch [17], Batch [462/938], Loss: 0.881169855594635\n",
      "Train: Epoch [17], Batch [463/938], Loss: 0.6512572765350342\n",
      "Train: Epoch [17], Batch [464/938], Loss: 0.5412163734436035\n",
      "Train: Epoch [17], Batch [465/938], Loss: 0.5828376412391663\n",
      "Train: Epoch [17], Batch [466/938], Loss: 0.5763412117958069\n",
      "Train: Epoch [17], Batch [467/938], Loss: 0.5028343200683594\n",
      "Train: Epoch [17], Batch [468/938], Loss: 0.787261426448822\n",
      "Train: Epoch [17], Batch [469/938], Loss: 0.6522932052612305\n",
      "Train: Epoch [17], Batch [470/938], Loss: 0.5655919313430786\n",
      "Train: Epoch [17], Batch [471/938], Loss: 0.6990063190460205\n",
      "Train: Epoch [17], Batch [472/938], Loss: 0.7709543108940125\n",
      "Train: Epoch [17], Batch [473/938], Loss: 0.47752806544303894\n",
      "Train: Epoch [17], Batch [474/938], Loss: 0.5749961137771606\n",
      "Train: Epoch [17], Batch [475/938], Loss: 0.8227325081825256\n",
      "Train: Epoch [17], Batch [476/938], Loss: 0.7537989020347595\n",
      "Train: Epoch [17], Batch [477/938], Loss: 0.59177565574646\n",
      "Train: Epoch [17], Batch [478/938], Loss: 0.6282749772071838\n",
      "Train: Epoch [17], Batch [479/938], Loss: 0.821327805519104\n",
      "Train: Epoch [17], Batch [480/938], Loss: 0.7423751950263977\n",
      "Train: Epoch [17], Batch [481/938], Loss: 0.7012344598770142\n",
      "Train: Epoch [17], Batch [482/938], Loss: 0.6552568078041077\n",
      "Train: Epoch [17], Batch [483/938], Loss: 0.6821953058242798\n",
      "Train: Epoch [17], Batch [484/938], Loss: 0.6575826406478882\n",
      "Train: Epoch [17], Batch [485/938], Loss: 0.8722878694534302\n",
      "Train: Epoch [17], Batch [486/938], Loss: 0.6376358866691589\n",
      "Train: Epoch [17], Batch [487/938], Loss: 0.6921727657318115\n",
      "Train: Epoch [17], Batch [488/938], Loss: 0.7314883470535278\n",
      "Train: Epoch [17], Batch [489/938], Loss: 0.5791882872581482\n",
      "Train: Epoch [17], Batch [490/938], Loss: 0.9055153727531433\n",
      "Train: Epoch [17], Batch [491/938], Loss: 0.4975582957267761\n",
      "Train: Epoch [17], Batch [492/938], Loss: 0.5246168375015259\n",
      "Train: Epoch [17], Batch [493/938], Loss: 0.5109250545501709\n",
      "Train: Epoch [17], Batch [494/938], Loss: 0.6570379734039307\n",
      "Train: Epoch [17], Batch [495/938], Loss: 0.5194249749183655\n",
      "Train: Epoch [17], Batch [496/938], Loss: 0.7753443717956543\n",
      "Train: Epoch [17], Batch [497/938], Loss: 0.7379528880119324\n",
      "Train: Epoch [17], Batch [498/938], Loss: 0.7311692833900452\n",
      "Train: Epoch [17], Batch [499/938], Loss: 0.9140143990516663\n",
      "Train: Epoch [17], Batch [500/938], Loss: 0.3972020745277405\n",
      "Train: Epoch [17], Batch [501/938], Loss: 0.7361558675765991\n",
      "Train: Epoch [17], Batch [502/938], Loss: 0.5178582072257996\n",
      "Train: Epoch [17], Batch [503/938], Loss: 0.5473657250404358\n",
      "Train: Epoch [17], Batch [504/938], Loss: 0.7251797914505005\n",
      "Train: Epoch [17], Batch [505/938], Loss: 0.6282558441162109\n",
      "Train: Epoch [17], Batch [506/938], Loss: 0.5357230305671692\n",
      "Train: Epoch [17], Batch [507/938], Loss: 0.686444103717804\n",
      "Train: Epoch [17], Batch [508/938], Loss: 0.5728965401649475\n",
      "Train: Epoch [17], Batch [509/938], Loss: 0.6372525095939636\n",
      "Train: Epoch [17], Batch [510/938], Loss: 0.8917620182037354\n",
      "Train: Epoch [17], Batch [511/938], Loss: 0.6994884610176086\n",
      "Train: Epoch [17], Batch [512/938], Loss: 0.7161327004432678\n",
      "Train: Epoch [17], Batch [513/938], Loss: 0.4858333170413971\n",
      "Train: Epoch [17], Batch [514/938], Loss: 0.771604597568512\n",
      "Train: Epoch [17], Batch [515/938], Loss: 0.7751497030258179\n",
      "Train: Epoch [17], Batch [516/938], Loss: 0.60465008020401\n",
      "Train: Epoch [17], Batch [517/938], Loss: 0.3525885045528412\n",
      "Train: Epoch [17], Batch [518/938], Loss: 0.6229021549224854\n",
      "Train: Epoch [17], Batch [519/938], Loss: 0.7628835439682007\n",
      "Train: Epoch [17], Batch [520/938], Loss: 0.5521584749221802\n",
      "Train: Epoch [17], Batch [521/938], Loss: 0.6780164837837219\n",
      "Train: Epoch [17], Batch [522/938], Loss: 0.7676249146461487\n",
      "Train: Epoch [17], Batch [523/938], Loss: 0.7270363569259644\n",
      "Train: Epoch [17], Batch [524/938], Loss: 0.7434137463569641\n",
      "Train: Epoch [17], Batch [525/938], Loss: 0.7410107851028442\n",
      "Train: Epoch [17], Batch [526/938], Loss: 0.569602370262146\n",
      "Train: Epoch [17], Batch [527/938], Loss: 0.6448290348052979\n",
      "Train: Epoch [17], Batch [528/938], Loss: 0.7334828972816467\n",
      "Train: Epoch [17], Batch [529/938], Loss: 0.7188495993614197\n",
      "Train: Epoch [17], Batch [530/938], Loss: 0.7328344583511353\n",
      "Train: Epoch [17], Batch [531/938], Loss: 0.8836323618888855\n",
      "Train: Epoch [17], Batch [532/938], Loss: 0.7671031951904297\n",
      "Train: Epoch [17], Batch [533/938], Loss: 0.517862856388092\n",
      "Train: Epoch [17], Batch [534/938], Loss: 0.4433480203151703\n",
      "Train: Epoch [17], Batch [535/938], Loss: 0.5537670254707336\n",
      "Train: Epoch [17], Batch [536/938], Loss: 0.5248053073883057\n",
      "Train: Epoch [17], Batch [537/938], Loss: 0.6092447638511658\n",
      "Train: Epoch [17], Batch [538/938], Loss: 0.6985321640968323\n",
      "Train: Epoch [17], Batch [539/938], Loss: 0.663223147392273\n",
      "Train: Epoch [17], Batch [540/938], Loss: 0.7215225696563721\n",
      "Train: Epoch [17], Batch [541/938], Loss: 0.8060375452041626\n",
      "Train: Epoch [17], Batch [542/938], Loss: 0.46552610397338867\n",
      "Train: Epoch [17], Batch [543/938], Loss: 0.5089397430419922\n",
      "Train: Epoch [17], Batch [544/938], Loss: 0.4564337730407715\n",
      "Train: Epoch [17], Batch [545/938], Loss: 0.3847752511501312\n",
      "Train: Epoch [17], Batch [546/938], Loss: 0.7532616257667542\n",
      "Train: Epoch [17], Batch [547/938], Loss: 0.6284302473068237\n",
      "Train: Epoch [17], Batch [548/938], Loss: 0.7711707949638367\n",
      "Train: Epoch [17], Batch [549/938], Loss: 0.7514144778251648\n",
      "Train: Epoch [17], Batch [550/938], Loss: 0.8112534284591675\n",
      "Train: Epoch [17], Batch [551/938], Loss: 0.6365401148796082\n",
      "Train: Epoch [17], Batch [552/938], Loss: 0.5988819003105164\n",
      "Train: Epoch [17], Batch [553/938], Loss: 0.5092543363571167\n",
      "Train: Epoch [17], Batch [554/938], Loss: 0.7343301177024841\n",
      "Train: Epoch [17], Batch [555/938], Loss: 0.5172749161720276\n",
      "Train: Epoch [17], Batch [556/938], Loss: 0.7097544074058533\n",
      "Train: Epoch [17], Batch [557/938], Loss: 0.8735695481300354\n",
      "Train: Epoch [17], Batch [558/938], Loss: 0.7598161101341248\n",
      "Train: Epoch [17], Batch [559/938], Loss: 0.7259523868560791\n",
      "Train: Epoch [17], Batch [560/938], Loss: 0.9737774133682251\n",
      "Train: Epoch [17], Batch [561/938], Loss: 0.5772116184234619\n",
      "Train: Epoch [17], Batch [562/938], Loss: 0.7650368213653564\n",
      "Train: Epoch [17], Batch [563/938], Loss: 0.6404708623886108\n",
      "Train: Epoch [17], Batch [564/938], Loss: 0.8694961667060852\n",
      "Train: Epoch [17], Batch [565/938], Loss: 0.3245380222797394\n",
      "Train: Epoch [17], Batch [566/938], Loss: 0.6701864004135132\n",
      "Train: Epoch [17], Batch [567/938], Loss: 0.6955562829971313\n",
      "Train: Epoch [17], Batch [568/938], Loss: 0.442251592874527\n",
      "Train: Epoch [17], Batch [569/938], Loss: 0.5630240440368652\n",
      "Train: Epoch [17], Batch [570/938], Loss: 0.48984482884407043\n",
      "Train: Epoch [17], Batch [571/938], Loss: 0.7814066410064697\n",
      "Train: Epoch [17], Batch [572/938], Loss: 0.6514822840690613\n",
      "Train: Epoch [17], Batch [573/938], Loss: 0.826391875743866\n",
      "Train: Epoch [17], Batch [574/938], Loss: 0.59479820728302\n",
      "Train: Epoch [17], Batch [575/938], Loss: 0.613771915435791\n",
      "Train: Epoch [17], Batch [576/938], Loss: 0.49865052103996277\n",
      "Train: Epoch [17], Batch [577/938], Loss: 0.5269275307655334\n",
      "Train: Epoch [17], Batch [578/938], Loss: 0.6935210824012756\n",
      "Train: Epoch [17], Batch [579/938], Loss: 0.5237301588058472\n",
      "Train: Epoch [17], Batch [580/938], Loss: 0.6730330586433411\n",
      "Train: Epoch [17], Batch [581/938], Loss: 0.8147432804107666\n",
      "Train: Epoch [17], Batch [582/938], Loss: 0.563060998916626\n",
      "Train: Epoch [17], Batch [583/938], Loss: 0.5569090843200684\n",
      "Train: Epoch [17], Batch [584/938], Loss: 0.5224971175193787\n",
      "Train: Epoch [17], Batch [585/938], Loss: 0.5680240988731384\n",
      "Train: Epoch [17], Batch [586/938], Loss: 0.5422264337539673\n",
      "Train: Epoch [17], Batch [587/938], Loss: 0.6720984578132629\n",
      "Train: Epoch [17], Batch [588/938], Loss: 0.9798375368118286\n",
      "Train: Epoch [17], Batch [589/938], Loss: 0.4517804980278015\n",
      "Train: Epoch [17], Batch [590/938], Loss: 0.5746814012527466\n",
      "Train: Epoch [17], Batch [591/938], Loss: 0.8541296720504761\n",
      "Train: Epoch [17], Batch [592/938], Loss: 0.815879225730896\n",
      "Train: Epoch [17], Batch [593/938], Loss: 0.7499963045120239\n",
      "Train: Epoch [17], Batch [594/938], Loss: 0.5732475519180298\n",
      "Train: Epoch [17], Batch [595/938], Loss: 0.8375271558761597\n",
      "Train: Epoch [17], Batch [596/938], Loss: 1.1427855491638184\n",
      "Train: Epoch [17], Batch [597/938], Loss: 0.5648151636123657\n",
      "Train: Epoch [17], Batch [598/938], Loss: 0.5651200413703918\n",
      "Train: Epoch [17], Batch [599/938], Loss: 0.7832872867584229\n",
      "Train: Epoch [17], Batch [600/938], Loss: 0.7853237986564636\n",
      "Train: Epoch [17], Batch [601/938], Loss: 0.5570854544639587\n",
      "Train: Epoch [17], Batch [602/938], Loss: 0.6140859723091125\n",
      "Train: Epoch [17], Batch [603/938], Loss: 0.7940035462379456\n",
      "Train: Epoch [17], Batch [604/938], Loss: 0.868462324142456\n",
      "Train: Epoch [17], Batch [605/938], Loss: 0.7672774791717529\n",
      "Train: Epoch [17], Batch [606/938], Loss: 0.5982959270477295\n",
      "Train: Epoch [17], Batch [607/938], Loss: 0.5925508737564087\n",
      "Train: Epoch [17], Batch [608/938], Loss: 0.738717794418335\n",
      "Train: Epoch [17], Batch [609/938], Loss: 0.7617037296295166\n",
      "Train: Epoch [17], Batch [610/938], Loss: 0.6071443557739258\n",
      "Train: Epoch [17], Batch [611/938], Loss: 0.7184577584266663\n",
      "Train: Epoch [17], Batch [612/938], Loss: 0.5096545815467834\n",
      "Train: Epoch [17], Batch [613/938], Loss: 0.7862509489059448\n",
      "Train: Epoch [17], Batch [614/938], Loss: 0.481903612613678\n",
      "Train: Epoch [17], Batch [615/938], Loss: 0.9476233720779419\n",
      "Train: Epoch [17], Batch [616/938], Loss: 0.6194597482681274\n",
      "Train: Epoch [17], Batch [617/938], Loss: 0.6839882731437683\n",
      "Train: Epoch [17], Batch [618/938], Loss: 0.5943761467933655\n",
      "Train: Epoch [17], Batch [619/938], Loss: 0.5728553533554077\n",
      "Train: Epoch [17], Batch [620/938], Loss: 0.7262043952941895\n",
      "Train: Epoch [17], Batch [621/938], Loss: 0.8681269884109497\n",
      "Train: Epoch [17], Batch [622/938], Loss: 0.9149969220161438\n",
      "Train: Epoch [17], Batch [623/938], Loss: 0.7297250628471375\n",
      "Train: Epoch [17], Batch [624/938], Loss: 0.7021946310997009\n",
      "Train: Epoch [17], Batch [625/938], Loss: 0.5031196475028992\n",
      "Train: Epoch [17], Batch [626/938], Loss: 0.8128748536109924\n",
      "Train: Epoch [17], Batch [627/938], Loss: 0.4751925468444824\n",
      "Train: Epoch [17], Batch [628/938], Loss: 0.7710556983947754\n",
      "Train: Epoch [17], Batch [629/938], Loss: 0.4453507959842682\n",
      "Train: Epoch [17], Batch [630/938], Loss: 0.5226312279701233\n",
      "Train: Epoch [17], Batch [631/938], Loss: 0.8602165579795837\n",
      "Train: Epoch [17], Batch [632/938], Loss: 0.8789818286895752\n",
      "Train: Epoch [17], Batch [633/938], Loss: 0.7576268911361694\n",
      "Train: Epoch [17], Batch [634/938], Loss: 0.6407929062843323\n",
      "Train: Epoch [17], Batch [635/938], Loss: 0.7139506936073303\n",
      "Train: Epoch [17], Batch [636/938], Loss: 0.7405403256416321\n",
      "Train: Epoch [17], Batch [637/938], Loss: 0.8419540524482727\n",
      "Train: Epoch [17], Batch [638/938], Loss: 0.6190093755722046\n",
      "Train: Epoch [17], Batch [639/938], Loss: 0.5041220784187317\n",
      "Train: Epoch [17], Batch [640/938], Loss: 0.7233829498291016\n",
      "Train: Epoch [17], Batch [641/938], Loss: 0.9080241322517395\n",
      "Train: Epoch [17], Batch [642/938], Loss: 0.6077200174331665\n",
      "Train: Epoch [17], Batch [643/938], Loss: 0.7598237991333008\n",
      "Train: Epoch [17], Batch [644/938], Loss: 0.7264022827148438\n",
      "Train: Epoch [17], Batch [645/938], Loss: 0.8551180362701416\n",
      "Train: Epoch [17], Batch [646/938], Loss: 0.6616278886795044\n",
      "Train: Epoch [17], Batch [647/938], Loss: 0.5818656086921692\n",
      "Train: Epoch [17], Batch [648/938], Loss: 0.5945066213607788\n",
      "Train: Epoch [17], Batch [649/938], Loss: 0.5144461989402771\n",
      "Train: Epoch [17], Batch [650/938], Loss: 0.40313273668289185\n",
      "Train: Epoch [17], Batch [651/938], Loss: 0.7161180973052979\n",
      "Train: Epoch [17], Batch [652/938], Loss: 0.6706728339195251\n",
      "Train: Epoch [17], Batch [653/938], Loss: 0.6273642778396606\n",
      "Train: Epoch [17], Batch [654/938], Loss: 0.2845819890499115\n",
      "Train: Epoch [17], Batch [655/938], Loss: 0.6733070611953735\n",
      "Train: Epoch [17], Batch [656/938], Loss: 0.6041976809501648\n",
      "Train: Epoch [17], Batch [657/938], Loss: 0.5557677745819092\n",
      "Train: Epoch [17], Batch [658/938], Loss: 0.7317357659339905\n",
      "Train: Epoch [17], Batch [659/938], Loss: 0.5389203429222107\n",
      "Train: Epoch [17], Batch [660/938], Loss: 0.6345669031143188\n",
      "Train: Epoch [17], Batch [661/938], Loss: 0.6420041918754578\n",
      "Train: Epoch [17], Batch [662/938], Loss: 0.6868094801902771\n",
      "Train: Epoch [17], Batch [663/938], Loss: 0.6976357698440552\n",
      "Train: Epoch [17], Batch [664/938], Loss: 0.6700785160064697\n",
      "Train: Epoch [17], Batch [665/938], Loss: 0.777930498123169\n",
      "Train: Epoch [17], Batch [666/938], Loss: 0.6926939487457275\n",
      "Train: Epoch [17], Batch [667/938], Loss: 0.8072648644447327\n",
      "Train: Epoch [17], Batch [668/938], Loss: 0.6494293808937073\n",
      "Train: Epoch [17], Batch [669/938], Loss: 0.40104541182518005\n",
      "Train: Epoch [17], Batch [670/938], Loss: 0.7020522356033325\n",
      "Train: Epoch [17], Batch [671/938], Loss: 0.7432106733322144\n",
      "Train: Epoch [17], Batch [672/938], Loss: 0.3508390784263611\n",
      "Train: Epoch [17], Batch [673/938], Loss: 0.8221209049224854\n",
      "Train: Epoch [17], Batch [674/938], Loss: 0.4743453562259674\n",
      "Train: Epoch [17], Batch [675/938], Loss: 0.7564185261726379\n",
      "Train: Epoch [17], Batch [676/938], Loss: 0.6584428548812866\n",
      "Train: Epoch [17], Batch [677/938], Loss: 0.7252220511436462\n",
      "Train: Epoch [17], Batch [678/938], Loss: 0.5304041504859924\n",
      "Train: Epoch [17], Batch [679/938], Loss: 0.7894545793533325\n",
      "Train: Epoch [17], Batch [680/938], Loss: 0.6953628063201904\n",
      "Train: Epoch [17], Batch [681/938], Loss: 0.5908896923065186\n",
      "Train: Epoch [17], Batch [682/938], Loss: 0.7568907737731934\n",
      "Train: Epoch [17], Batch [683/938], Loss: 0.747312068939209\n",
      "Train: Epoch [17], Batch [684/938], Loss: 0.9250918030738831\n",
      "Train: Epoch [17], Batch [685/938], Loss: 0.7384443283081055\n",
      "Train: Epoch [17], Batch [686/938], Loss: 0.578183114528656\n",
      "Train: Epoch [17], Batch [687/938], Loss: 0.8598707914352417\n",
      "Train: Epoch [17], Batch [688/938], Loss: 0.804304301738739\n",
      "Train: Epoch [17], Batch [689/938], Loss: 0.9724864959716797\n",
      "Train: Epoch [17], Batch [690/938], Loss: 0.6218180060386658\n",
      "Train: Epoch [17], Batch [691/938], Loss: 0.5910432934761047\n",
      "Train: Epoch [17], Batch [692/938], Loss: 0.7105276584625244\n",
      "Train: Epoch [17], Batch [693/938], Loss: 0.5500586628913879\n",
      "Train: Epoch [17], Batch [694/938], Loss: 0.6472792029380798\n",
      "Train: Epoch [17], Batch [695/938], Loss: 0.6681009531021118\n",
      "Train: Epoch [17], Batch [696/938], Loss: 0.7336795926094055\n",
      "Train: Epoch [17], Batch [697/938], Loss: 0.9857995510101318\n",
      "Train: Epoch [17], Batch [698/938], Loss: 0.620134711265564\n",
      "Train: Epoch [17], Batch [699/938], Loss: 0.8382115364074707\n",
      "Train: Epoch [17], Batch [700/938], Loss: 0.9865023493766785\n",
      "Train: Epoch [17], Batch [701/938], Loss: 0.5940283536911011\n",
      "Train: Epoch [17], Batch [702/938], Loss: 0.7640535235404968\n",
      "Train: Epoch [17], Batch [703/938], Loss: 0.6363636255264282\n",
      "Train: Epoch [17], Batch [704/938], Loss: 0.4858802556991577\n",
      "Train: Epoch [17], Batch [705/938], Loss: 0.47707876563072205\n",
      "Train: Epoch [17], Batch [706/938], Loss: 0.6927894353866577\n",
      "Train: Epoch [17], Batch [707/938], Loss: 0.4445173740386963\n",
      "Train: Epoch [17], Batch [708/938], Loss: 0.4597599506378174\n",
      "Train: Epoch [17], Batch [709/938], Loss: 0.620343804359436\n",
      "Train: Epoch [17], Batch [710/938], Loss: 0.848893404006958\n",
      "Train: Epoch [17], Batch [711/938], Loss: 0.8170007467269897\n",
      "Train: Epoch [17], Batch [712/938], Loss: 0.9474146366119385\n",
      "Train: Epoch [17], Batch [713/938], Loss: 0.8805782794952393\n",
      "Train: Epoch [17], Batch [714/938], Loss: 0.560662031173706\n",
      "Train: Epoch [17], Batch [715/938], Loss: 0.4954819083213806\n",
      "Train: Epoch [17], Batch [716/938], Loss: 0.7241562604904175\n",
      "Train: Epoch [17], Batch [717/938], Loss: 0.5816793441772461\n",
      "Train: Epoch [17], Batch [718/938], Loss: 0.6800937056541443\n",
      "Train: Epoch [17], Batch [719/938], Loss: 0.5850581526756287\n",
      "Train: Epoch [17], Batch [720/938], Loss: 0.6884495615959167\n",
      "Train: Epoch [17], Batch [721/938], Loss: 0.5273855924606323\n",
      "Train: Epoch [17], Batch [722/938], Loss: 0.567106306552887\n",
      "Train: Epoch [17], Batch [723/938], Loss: 0.6895835995674133\n",
      "Train: Epoch [17], Batch [724/938], Loss: 0.9417542219161987\n",
      "Train: Epoch [17], Batch [725/938], Loss: 0.7475962042808533\n",
      "Train: Epoch [17], Batch [726/938], Loss: 0.5289449691772461\n",
      "Train: Epoch [17], Batch [727/938], Loss: 0.5513194799423218\n",
      "Train: Epoch [17], Batch [728/938], Loss: 0.9227421283721924\n",
      "Train: Epoch [17], Batch [729/938], Loss: 0.5280968546867371\n",
      "Train: Epoch [17], Batch [730/938], Loss: 0.6472798585891724\n",
      "Train: Epoch [17], Batch [731/938], Loss: 0.7575267553329468\n",
      "Train: Epoch [17], Batch [732/938], Loss: 0.8491473197937012\n",
      "Train: Epoch [17], Batch [733/938], Loss: 0.5746099948883057\n",
      "Train: Epoch [17], Batch [734/938], Loss: 0.47658562660217285\n",
      "Train: Epoch [17], Batch [735/938], Loss: 0.7529814839363098\n",
      "Train: Epoch [17], Batch [736/938], Loss: 0.689132034778595\n",
      "Train: Epoch [17], Batch [737/938], Loss: 0.8751126527786255\n",
      "Train: Epoch [17], Batch [738/938], Loss: 0.5385189056396484\n",
      "Train: Epoch [17], Batch [739/938], Loss: 0.6301659941673279\n",
      "Train: Epoch [17], Batch [740/938], Loss: 0.805761992931366\n",
      "Train: Epoch [17], Batch [741/938], Loss: 0.45577046275138855\n",
      "Train: Epoch [17], Batch [742/938], Loss: 0.7935750484466553\n",
      "Train: Epoch [17], Batch [743/938], Loss: 0.877339780330658\n",
      "Train: Epoch [17], Batch [744/938], Loss: 0.5289515256881714\n",
      "Train: Epoch [17], Batch [745/938], Loss: 0.6983472108840942\n",
      "Train: Epoch [17], Batch [746/938], Loss: 0.7849246859550476\n",
      "Train: Epoch [17], Batch [747/938], Loss: 0.5841249823570251\n",
      "Train: Epoch [17], Batch [748/938], Loss: 0.8808853626251221\n",
      "Train: Epoch [17], Batch [749/938], Loss: 0.7709847688674927\n",
      "Train: Epoch [17], Batch [750/938], Loss: 0.6866300106048584\n",
      "Train: Epoch [17], Batch [751/938], Loss: 1.025723934173584\n",
      "Train: Epoch [17], Batch [752/938], Loss: 0.5507897734642029\n",
      "Train: Epoch [17], Batch [753/938], Loss: 0.8042864203453064\n",
      "Train: Epoch [17], Batch [754/938], Loss: 0.5652821063995361\n",
      "Train: Epoch [17], Batch [755/938], Loss: 0.717317521572113\n",
      "Train: Epoch [17], Batch [756/938], Loss: 0.6346948742866516\n",
      "Train: Epoch [17], Batch [757/938], Loss: 0.6902457475662231\n",
      "Train: Epoch [17], Batch [758/938], Loss: 0.6783263683319092\n",
      "Train: Epoch [17], Batch [759/938], Loss: 0.6686086654663086\n",
      "Train: Epoch [17], Batch [760/938], Loss: 0.44890493154525757\n",
      "Train: Epoch [17], Batch [761/938], Loss: 0.6887553930282593\n",
      "Train: Epoch [17], Batch [762/938], Loss: 1.0195672512054443\n",
      "Train: Epoch [17], Batch [763/938], Loss: 0.8414729237556458\n",
      "Train: Epoch [17], Batch [764/938], Loss: 0.6438676714897156\n",
      "Train: Epoch [17], Batch [765/938], Loss: 0.6811901926994324\n",
      "Train: Epoch [17], Batch [766/938], Loss: 0.7503489851951599\n",
      "Train: Epoch [17], Batch [767/938], Loss: 0.6727497577667236\n",
      "Train: Epoch [17], Batch [768/938], Loss: 0.8601528406143188\n",
      "Train: Epoch [17], Batch [769/938], Loss: 0.7311975955963135\n",
      "Train: Epoch [17], Batch [770/938], Loss: 0.6860519051551819\n",
      "Train: Epoch [17], Batch [771/938], Loss: 0.5970582365989685\n",
      "Train: Epoch [17], Batch [772/938], Loss: 0.9776420593261719\n",
      "Train: Epoch [17], Batch [773/938], Loss: 0.7806059122085571\n",
      "Train: Epoch [17], Batch [774/938], Loss: 0.6774886250495911\n",
      "Train: Epoch [17], Batch [775/938], Loss: 0.8395980596542358\n",
      "Train: Epoch [17], Batch [776/938], Loss: 0.8943415284156799\n",
      "Train: Epoch [17], Batch [777/938], Loss: 0.749045193195343\n",
      "Train: Epoch [17], Batch [778/938], Loss: 0.7039891481399536\n",
      "Train: Epoch [17], Batch [779/938], Loss: 0.696857213973999\n",
      "Train: Epoch [17], Batch [780/938], Loss: 0.4934574067592621\n",
      "Train: Epoch [17], Batch [781/938], Loss: 0.6791999340057373\n",
      "Train: Epoch [17], Batch [782/938], Loss: 0.4842478036880493\n",
      "Train: Epoch [17], Batch [783/938], Loss: 0.4687057137489319\n",
      "Train: Epoch [17], Batch [784/938], Loss: 0.7525664567947388\n",
      "Train: Epoch [17], Batch [785/938], Loss: 0.7215923070907593\n",
      "Train: Epoch [17], Batch [786/938], Loss: 0.4786073565483093\n",
      "Train: Epoch [17], Batch [787/938], Loss: 0.4538051187992096\n",
      "Train: Epoch [17], Batch [788/938], Loss: 0.5981122255325317\n",
      "Train: Epoch [17], Batch [789/938], Loss: 0.8044838905334473\n",
      "Train: Epoch [17], Batch [790/938], Loss: 0.5253819227218628\n",
      "Train: Epoch [17], Batch [791/938], Loss: 0.900111973285675\n",
      "Train: Epoch [17], Batch [792/938], Loss: 0.8377275466918945\n",
      "Train: Epoch [17], Batch [793/938], Loss: 0.6701916456222534\n",
      "Train: Epoch [17], Batch [794/938], Loss: 0.7562196850776672\n",
      "Train: Epoch [17], Batch [795/938], Loss: 1.0372717380523682\n",
      "Train: Epoch [17], Batch [796/938], Loss: 0.9160752296447754\n",
      "Train: Epoch [17], Batch [797/938], Loss: 0.6203012466430664\n",
      "Train: Epoch [17], Batch [798/938], Loss: 0.6464025974273682\n",
      "Train: Epoch [17], Batch [799/938], Loss: 0.6358956098556519\n",
      "Train: Epoch [17], Batch [800/938], Loss: 0.8872486352920532\n",
      "Train: Epoch [17], Batch [801/938], Loss: 0.5500494241714478\n",
      "Train: Epoch [17], Batch [802/938], Loss: 0.7482333183288574\n",
      "Train: Epoch [17], Batch [803/938], Loss: 0.7175887823104858\n",
      "Train: Epoch [17], Batch [804/938], Loss: 0.6651853322982788\n",
      "Train: Epoch [17], Batch [805/938], Loss: 0.6906980872154236\n",
      "Train: Epoch [17], Batch [806/938], Loss: 0.7287548780441284\n",
      "Train: Epoch [17], Batch [807/938], Loss: 0.604789137840271\n",
      "Train: Epoch [17], Batch [808/938], Loss: 0.6397313475608826\n",
      "Train: Epoch [17], Batch [809/938], Loss: 0.39301151037216187\n",
      "Train: Epoch [17], Batch [810/938], Loss: 0.6900631189346313\n",
      "Train: Epoch [17], Batch [811/938], Loss: 0.4942428469657898\n",
      "Train: Epoch [17], Batch [812/938], Loss: 0.6381955146789551\n",
      "Train: Epoch [17], Batch [813/938], Loss: 0.7090883851051331\n",
      "Train: Epoch [17], Batch [814/938], Loss: 0.739030659198761\n",
      "Train: Epoch [17], Batch [815/938], Loss: 0.6729913949966431\n",
      "Train: Epoch [17], Batch [816/938], Loss: 0.9136459827423096\n",
      "Train: Epoch [17], Batch [817/938], Loss: 0.8405800461769104\n",
      "Train: Epoch [17], Batch [818/938], Loss: 0.5432822704315186\n",
      "Train: Epoch [17], Batch [819/938], Loss: 0.5871389508247375\n",
      "Train: Epoch [17], Batch [820/938], Loss: 0.8140847682952881\n",
      "Train: Epoch [17], Batch [821/938], Loss: 1.0048034191131592\n",
      "Train: Epoch [17], Batch [822/938], Loss: 0.5898995399475098\n",
      "Train: Epoch [17], Batch [823/938], Loss: 0.6493434309959412\n",
      "Train: Epoch [17], Batch [824/938], Loss: 0.6498438715934753\n",
      "Train: Epoch [17], Batch [825/938], Loss: 0.9370173215866089\n",
      "Train: Epoch [17], Batch [826/938], Loss: 0.5998544096946716\n",
      "Train: Epoch [17], Batch [827/938], Loss: 0.9293665885925293\n",
      "Train: Epoch [17], Batch [828/938], Loss: 0.8348820209503174\n",
      "Train: Epoch [17], Batch [829/938], Loss: 0.6385167241096497\n",
      "Train: Epoch [17], Batch [830/938], Loss: 0.7265787720680237\n",
      "Train: Epoch [17], Batch [831/938], Loss: 0.6513279676437378\n",
      "Train: Epoch [17], Batch [832/938], Loss: 0.49848371744155884\n",
      "Train: Epoch [17], Batch [833/938], Loss: 0.5671858191490173\n",
      "Train: Epoch [17], Batch [834/938], Loss: 0.6948120594024658\n",
      "Train: Epoch [17], Batch [835/938], Loss: 0.6662298440933228\n",
      "Train: Epoch [17], Batch [836/938], Loss: 0.8019453287124634\n",
      "Train: Epoch [17], Batch [837/938], Loss: 0.7694433927536011\n",
      "Train: Epoch [17], Batch [838/938], Loss: 0.5104596614837646\n",
      "Train: Epoch [17], Batch [839/938], Loss: 0.7542532682418823\n",
      "Train: Epoch [17], Batch [840/938], Loss: 0.7754250168800354\n",
      "Train: Epoch [17], Batch [841/938], Loss: 0.84620201587677\n",
      "Train: Epoch [17], Batch [842/938], Loss: 0.7284758687019348\n",
      "Train: Epoch [17], Batch [843/938], Loss: 0.4954169988632202\n",
      "Train: Epoch [17], Batch [844/938], Loss: 0.588422954082489\n",
      "Train: Epoch [17], Batch [845/938], Loss: 0.6097559332847595\n",
      "Train: Epoch [17], Batch [846/938], Loss: 0.8448784947395325\n",
      "Train: Epoch [17], Batch [847/938], Loss: 0.6095905900001526\n",
      "Train: Epoch [17], Batch [848/938], Loss: 0.5965754389762878\n",
      "Train: Epoch [17], Batch [849/938], Loss: 0.5687970519065857\n",
      "Train: Epoch [17], Batch [850/938], Loss: 0.6886346340179443\n",
      "Train: Epoch [17], Batch [851/938], Loss: 0.9595195055007935\n",
      "Train: Epoch [17], Batch [852/938], Loss: 0.8628331422805786\n",
      "Train: Epoch [17], Batch [853/938], Loss: 0.7076382040977478\n",
      "Train: Epoch [17], Batch [854/938], Loss: 0.7068039178848267\n",
      "Train: Epoch [17], Batch [855/938], Loss: 0.6189137697219849\n",
      "Train: Epoch [17], Batch [856/938], Loss: 0.7435708045959473\n",
      "Train: Epoch [17], Batch [857/938], Loss: 0.8140604496002197\n",
      "Train: Epoch [17], Batch [858/938], Loss: 0.6307775378227234\n",
      "Train: Epoch [17], Batch [859/938], Loss: 0.5077080130577087\n",
      "Train: Epoch [17], Batch [860/938], Loss: 0.8801841735839844\n",
      "Train: Epoch [17], Batch [861/938], Loss: 0.5718106031417847\n",
      "Train: Epoch [17], Batch [862/938], Loss: 0.6625196933746338\n",
      "Train: Epoch [17], Batch [863/938], Loss: 0.8154008388519287\n",
      "Train: Epoch [17], Batch [864/938], Loss: 0.7076757550239563\n",
      "Train: Epoch [17], Batch [865/938], Loss: 0.5270078182220459\n",
      "Train: Epoch [17], Batch [866/938], Loss: 0.7407546639442444\n",
      "Train: Epoch [17], Batch [867/938], Loss: 0.7042744755744934\n",
      "Train: Epoch [17], Batch [868/938], Loss: 0.9106613397598267\n",
      "Train: Epoch [17], Batch [869/938], Loss: 0.5178202390670776\n",
      "Train: Epoch [17], Batch [870/938], Loss: 0.8412803411483765\n",
      "Train: Epoch [17], Batch [871/938], Loss: 0.6230553388595581\n",
      "Train: Epoch [17], Batch [872/938], Loss: 0.8314763903617859\n",
      "Train: Epoch [17], Batch [873/938], Loss: 0.7092034816741943\n",
      "Train: Epoch [17], Batch [874/938], Loss: 0.681118369102478\n",
      "Train: Epoch [17], Batch [875/938], Loss: 0.5768797993659973\n",
      "Train: Epoch [17], Batch [876/938], Loss: 0.7517129778862\n",
      "Train: Epoch [17], Batch [877/938], Loss: 0.49492180347442627\n",
      "Train: Epoch [17], Batch [878/938], Loss: 0.5652252435684204\n",
      "Train: Epoch [17], Batch [879/938], Loss: 0.4871708154678345\n",
      "Train: Epoch [17], Batch [880/938], Loss: 0.6492139101028442\n",
      "Train: Epoch [17], Batch [881/938], Loss: 0.5205687284469604\n",
      "Train: Epoch [17], Batch [882/938], Loss: 0.8776458501815796\n",
      "Train: Epoch [17], Batch [883/938], Loss: 0.6943733096122742\n",
      "Train: Epoch [17], Batch [884/938], Loss: 0.8418571949005127\n",
      "Train: Epoch [17], Batch [885/938], Loss: 0.8699010014533997\n",
      "Train: Epoch [17], Batch [886/938], Loss: 0.47851526737213135\n",
      "Train: Epoch [17], Batch [887/938], Loss: 0.7245442271232605\n",
      "Train: Epoch [17], Batch [888/938], Loss: 0.6207720041275024\n",
      "Train: Epoch [17], Batch [889/938], Loss: 0.49262064695358276\n",
      "Train: Epoch [17], Batch [890/938], Loss: 0.603874683380127\n",
      "Train: Epoch [17], Batch [891/938], Loss: 0.5832722187042236\n",
      "Train: Epoch [17], Batch [892/938], Loss: 0.6328160166740417\n",
      "Train: Epoch [17], Batch [893/938], Loss: 0.7665979266166687\n",
      "Train: Epoch [17], Batch [894/938], Loss: 0.6866406798362732\n",
      "Train: Epoch [17], Batch [895/938], Loss: 0.5668846368789673\n",
      "Train: Epoch [17], Batch [896/938], Loss: 0.7869484424591064\n",
      "Train: Epoch [17], Batch [897/938], Loss: 0.6864598989486694\n",
      "Train: Epoch [17], Batch [898/938], Loss: 0.9729743599891663\n",
      "Train: Epoch [17], Batch [899/938], Loss: 0.620808482170105\n",
      "Train: Epoch [17], Batch [900/938], Loss: 0.6332510113716125\n",
      "Train: Epoch [17], Batch [901/938], Loss: 0.6458317637443542\n",
      "Train: Epoch [17], Batch [902/938], Loss: 0.6122745275497437\n",
      "Train: Epoch [17], Batch [903/938], Loss: 0.9526183605194092\n",
      "Train: Epoch [17], Batch [904/938], Loss: 0.8167387247085571\n",
      "Train: Epoch [17], Batch [905/938], Loss: 0.5270525813102722\n",
      "Train: Epoch [17], Batch [906/938], Loss: 0.6152552962303162\n",
      "Train: Epoch [17], Batch [907/938], Loss: 0.7474229335784912\n",
      "Train: Epoch [17], Batch [908/938], Loss: 0.6762167811393738\n",
      "Train: Epoch [17], Batch [909/938], Loss: 0.6608633995056152\n",
      "Train: Epoch [17], Batch [910/938], Loss: 0.4944334626197815\n",
      "Train: Epoch [17], Batch [911/938], Loss: 0.5098475813865662\n",
      "Train: Epoch [17], Batch [912/938], Loss: 0.8658401966094971\n",
      "Train: Epoch [17], Batch [913/938], Loss: 0.6621119379997253\n",
      "Train: Epoch [17], Batch [914/938], Loss: 0.5151352882385254\n",
      "Train: Epoch [17], Batch [915/938], Loss: 0.5639110207557678\n",
      "Train: Epoch [17], Batch [916/938], Loss: 0.7652177214622498\n",
      "Train: Epoch [17], Batch [917/938], Loss: 0.7152324914932251\n",
      "Train: Epoch [17], Batch [918/938], Loss: 0.7345762848854065\n",
      "Train: Epoch [17], Batch [919/938], Loss: 0.7552722692489624\n",
      "Train: Epoch [17], Batch [920/938], Loss: 0.5763095021247864\n",
      "Train: Epoch [17], Batch [921/938], Loss: 0.6032283902168274\n",
      "Train: Epoch [17], Batch [922/938], Loss: 0.6360450983047485\n",
      "Train: Epoch [17], Batch [923/938], Loss: 0.689500093460083\n",
      "Train: Epoch [17], Batch [924/938], Loss: 0.9872725009918213\n",
      "Train: Epoch [17], Batch [925/938], Loss: 0.6761370301246643\n",
      "Train: Epoch [17], Batch [926/938], Loss: 0.8982495069503784\n",
      "Train: Epoch [17], Batch [927/938], Loss: 0.8234714269638062\n",
      "Train: Epoch [17], Batch [928/938], Loss: 0.6364916563034058\n",
      "Train: Epoch [17], Batch [929/938], Loss: 0.7065113186836243\n",
      "Train: Epoch [17], Batch [930/938], Loss: 0.7544703483581543\n",
      "Train: Epoch [17], Batch [931/938], Loss: 0.5782157182693481\n",
      "Train: Epoch [17], Batch [932/938], Loss: 0.6531482338905334\n",
      "Train: Epoch [17], Batch [933/938], Loss: 0.5623573064804077\n",
      "Train: Epoch [17], Batch [934/938], Loss: 0.7935565710067749\n",
      "Train: Epoch [17], Batch [935/938], Loss: 0.6569491028785706\n",
      "Train: Epoch [17], Batch [936/938], Loss: 0.5506569743156433\n",
      "Train: Epoch [17], Batch [937/938], Loss: 0.635583758354187\n",
      "Train: Epoch [17], Batch [938/938], Loss: 0.8238112926483154\n",
      "Accuracy of train set: 0.7958\n",
      "Validation: Epoch [17], Batch [1/938], Loss: 0.8355218172073364\n",
      "Validation: Epoch [17], Batch [2/938], Loss: 1.0109740495681763\n",
      "Validation: Epoch [17], Batch [3/938], Loss: 0.5070286393165588\n",
      "Validation: Epoch [17], Batch [4/938], Loss: 0.7740723490715027\n",
      "Validation: Epoch [17], Batch [5/938], Loss: 0.8274739384651184\n",
      "Validation: Epoch [17], Batch [6/938], Loss: 0.9502691626548767\n",
      "Validation: Epoch [17], Batch [7/938], Loss: 0.5939777493476868\n",
      "Validation: Epoch [17], Batch [8/938], Loss: 0.7612445950508118\n",
      "Validation: Epoch [17], Batch [9/938], Loss: 0.6107726097106934\n",
      "Validation: Epoch [17], Batch [10/938], Loss: 0.721403956413269\n",
      "Validation: Epoch [17], Batch [11/938], Loss: 0.8050171732902527\n",
      "Validation: Epoch [17], Batch [12/938], Loss: 0.7096540927886963\n",
      "Validation: Epoch [17], Batch [13/938], Loss: 0.7510664463043213\n",
      "Validation: Epoch [17], Batch [14/938], Loss: 0.8517837524414062\n",
      "Validation: Epoch [17], Batch [15/938], Loss: 0.852222740650177\n",
      "Validation: Epoch [17], Batch [16/938], Loss: 0.6266492605209351\n",
      "Validation: Epoch [17], Batch [17/938], Loss: 1.1031605005264282\n",
      "Validation: Epoch [17], Batch [18/938], Loss: 0.4901195764541626\n",
      "Validation: Epoch [17], Batch [19/938], Loss: 0.805677592754364\n",
      "Validation: Epoch [17], Batch [20/938], Loss: 0.7215194702148438\n",
      "Validation: Epoch [17], Batch [21/938], Loss: 0.9339008927345276\n",
      "Validation: Epoch [17], Batch [22/938], Loss: 0.8050572276115417\n",
      "Validation: Epoch [17], Batch [23/938], Loss: 0.48479706048965454\n",
      "Validation: Epoch [17], Batch [24/938], Loss: 0.874372124671936\n",
      "Validation: Epoch [17], Batch [25/938], Loss: 0.6713992357254028\n",
      "Validation: Epoch [17], Batch [26/938], Loss: 0.9329087138175964\n",
      "Validation: Epoch [17], Batch [27/938], Loss: 0.6153252124786377\n",
      "Validation: Epoch [17], Batch [28/938], Loss: 0.6091615557670593\n",
      "Validation: Epoch [17], Batch [29/938], Loss: 0.7133374214172363\n",
      "Validation: Epoch [17], Batch [30/938], Loss: 0.5450845956802368\n",
      "Validation: Epoch [17], Batch [31/938], Loss: 0.6250631213188171\n",
      "Validation: Epoch [17], Batch [32/938], Loss: 0.8736743330955505\n",
      "Validation: Epoch [17], Batch [33/938], Loss: 0.5505332946777344\n",
      "Validation: Epoch [17], Batch [34/938], Loss: 0.9795223474502563\n",
      "Validation: Epoch [17], Batch [35/938], Loss: 0.6512666940689087\n",
      "Validation: Epoch [17], Batch [36/938], Loss: 0.6284903287887573\n",
      "Validation: Epoch [17], Batch [37/938], Loss: 0.7994028329849243\n",
      "Validation: Epoch [17], Batch [38/938], Loss: 0.8534590601921082\n",
      "Validation: Epoch [17], Batch [39/938], Loss: 0.7719417810440063\n",
      "Validation: Epoch [17], Batch [40/938], Loss: 0.622505247592926\n",
      "Validation: Epoch [17], Batch [41/938], Loss: 0.6199122667312622\n",
      "Validation: Epoch [17], Batch [42/938], Loss: 0.3429795801639557\n",
      "Validation: Epoch [17], Batch [43/938], Loss: 0.7934706807136536\n",
      "Validation: Epoch [17], Batch [44/938], Loss: 0.6866641044616699\n",
      "Validation: Epoch [17], Batch [45/938], Loss: 0.8040722608566284\n",
      "Validation: Epoch [17], Batch [46/938], Loss: 0.6133551001548767\n",
      "Validation: Epoch [17], Batch [47/938], Loss: 0.5933648347854614\n",
      "Validation: Epoch [17], Batch [48/938], Loss: 0.9629843235015869\n",
      "Validation: Epoch [17], Batch [49/938], Loss: 0.8493390083312988\n",
      "Validation: Epoch [17], Batch [50/938], Loss: 0.6985192894935608\n",
      "Validation: Epoch [17], Batch [51/938], Loss: 0.917364776134491\n",
      "Validation: Epoch [17], Batch [52/938], Loss: 0.6714888215065002\n",
      "Validation: Epoch [17], Batch [53/938], Loss: 0.657602846622467\n",
      "Validation: Epoch [17], Batch [54/938], Loss: 0.5319984555244446\n",
      "Validation: Epoch [17], Batch [55/938], Loss: 0.6012895703315735\n",
      "Validation: Epoch [17], Batch [56/938], Loss: 0.5791028141975403\n",
      "Validation: Epoch [17], Batch [57/938], Loss: 0.6414929628372192\n",
      "Validation: Epoch [17], Batch [58/938], Loss: 0.8222505450248718\n",
      "Validation: Epoch [17], Batch [59/938], Loss: 0.8520686626434326\n",
      "Validation: Epoch [17], Batch [60/938], Loss: 0.7536458969116211\n",
      "Validation: Epoch [17], Batch [61/938], Loss: 0.6770995855331421\n",
      "Validation: Epoch [17], Batch [62/938], Loss: 0.7922082543373108\n",
      "Validation: Epoch [17], Batch [63/938], Loss: 0.6769735217094421\n",
      "Validation: Epoch [17], Batch [64/938], Loss: 0.47533243894577026\n",
      "Validation: Epoch [17], Batch [65/938], Loss: 0.577814519405365\n",
      "Validation: Epoch [17], Batch [66/938], Loss: 0.7916240692138672\n",
      "Validation: Epoch [17], Batch [67/938], Loss: 0.7153598666191101\n",
      "Validation: Epoch [17], Batch [68/938], Loss: 0.8645623326301575\n",
      "Validation: Epoch [17], Batch [69/938], Loss: 0.5510858297348022\n",
      "Validation: Epoch [17], Batch [70/938], Loss: 0.5736644268035889\n",
      "Validation: Epoch [17], Batch [71/938], Loss: 0.9388233423233032\n",
      "Validation: Epoch [17], Batch [72/938], Loss: 0.7474658489227295\n",
      "Validation: Epoch [17], Batch [73/938], Loss: 0.6247244477272034\n",
      "Validation: Epoch [17], Batch [74/938], Loss: 0.8353158235549927\n",
      "Validation: Epoch [17], Batch [75/938], Loss: 0.5783953070640564\n",
      "Validation: Epoch [17], Batch [76/938], Loss: 0.9027478694915771\n",
      "Validation: Epoch [17], Batch [77/938], Loss: 0.6831502914428711\n",
      "Validation: Epoch [17], Batch [78/938], Loss: 0.6785228252410889\n",
      "Validation: Epoch [17], Batch [79/938], Loss: 0.8306022882461548\n",
      "Validation: Epoch [17], Batch [80/938], Loss: 0.8531317710876465\n",
      "Validation: Epoch [17], Batch [81/938], Loss: 0.5190588235855103\n",
      "Validation: Epoch [17], Batch [82/938], Loss: 0.7512280941009521\n",
      "Validation: Epoch [17], Batch [83/938], Loss: 0.5488448739051819\n",
      "Validation: Epoch [17], Batch [84/938], Loss: 0.725116491317749\n",
      "Validation: Epoch [17], Batch [85/938], Loss: 0.6367249488830566\n",
      "Validation: Epoch [17], Batch [86/938], Loss: 0.6623654365539551\n",
      "Validation: Epoch [17], Batch [87/938], Loss: 0.7054674625396729\n",
      "Validation: Epoch [17], Batch [88/938], Loss: 0.8707337379455566\n",
      "Validation: Epoch [17], Batch [89/938], Loss: 0.8282704949378967\n",
      "Validation: Epoch [17], Batch [90/938], Loss: 0.7347936630249023\n",
      "Validation: Epoch [17], Batch [91/938], Loss: 0.8734372854232788\n",
      "Validation: Epoch [17], Batch [92/938], Loss: 0.5173367857933044\n",
      "Validation: Epoch [17], Batch [93/938], Loss: 0.7171204090118408\n",
      "Validation: Epoch [17], Batch [94/938], Loss: 0.6549364328384399\n",
      "Validation: Epoch [17], Batch [95/938], Loss: 0.6942393779754639\n",
      "Validation: Epoch [17], Batch [96/938], Loss: 0.7218029499053955\n",
      "Validation: Epoch [17], Batch [97/938], Loss: 0.5930063724517822\n",
      "Validation: Epoch [17], Batch [98/938], Loss: 0.6934810280799866\n",
      "Validation: Epoch [17], Batch [99/938], Loss: 0.540290117263794\n",
      "Validation: Epoch [17], Batch [100/938], Loss: 0.6426030993461609\n",
      "Validation: Epoch [17], Batch [101/938], Loss: 0.524835467338562\n",
      "Validation: Epoch [17], Batch [102/938], Loss: 0.6204532384872437\n",
      "Validation: Epoch [17], Batch [103/938], Loss: 0.46087169647216797\n",
      "Validation: Epoch [17], Batch [104/938], Loss: 0.7363411784172058\n",
      "Validation: Epoch [17], Batch [105/938], Loss: 0.6264593601226807\n",
      "Validation: Epoch [17], Batch [106/938], Loss: 0.5458499789237976\n",
      "Validation: Epoch [17], Batch [107/938], Loss: 0.5870547294616699\n",
      "Validation: Epoch [17], Batch [108/938], Loss: 0.8974527716636658\n",
      "Validation: Epoch [17], Batch [109/938], Loss: 0.6724937558174133\n",
      "Validation: Epoch [17], Batch [110/938], Loss: 0.5068225860595703\n",
      "Validation: Epoch [17], Batch [111/938], Loss: 0.6484986543655396\n",
      "Validation: Epoch [17], Batch [112/938], Loss: 0.6546238660812378\n",
      "Validation: Epoch [17], Batch [113/938], Loss: 0.5864599347114563\n",
      "Validation: Epoch [17], Batch [114/938], Loss: 0.6292111873626709\n",
      "Validation: Epoch [17], Batch [115/938], Loss: 0.5874099731445312\n",
      "Validation: Epoch [17], Batch [116/938], Loss: 0.5264425277709961\n",
      "Validation: Epoch [17], Batch [117/938], Loss: 0.6136260628700256\n",
      "Validation: Epoch [17], Batch [118/938], Loss: 0.7606878876686096\n",
      "Validation: Epoch [17], Batch [119/938], Loss: 0.6293824911117554\n",
      "Validation: Epoch [17], Batch [120/938], Loss: 0.7151276469230652\n",
      "Validation: Epoch [17], Batch [121/938], Loss: 0.6855530738830566\n",
      "Validation: Epoch [17], Batch [122/938], Loss: 0.4016787111759186\n",
      "Validation: Epoch [17], Batch [123/938], Loss: 0.6102501749992371\n",
      "Validation: Epoch [17], Batch [124/938], Loss: 0.6131442785263062\n",
      "Validation: Epoch [17], Batch [125/938], Loss: 0.720263659954071\n",
      "Validation: Epoch [17], Batch [126/938], Loss: 0.7656739354133606\n",
      "Validation: Epoch [17], Batch [127/938], Loss: 0.5696894526481628\n",
      "Validation: Epoch [17], Batch [128/938], Loss: 0.8719323873519897\n",
      "Validation: Epoch [17], Batch [129/938], Loss: 0.8130512237548828\n",
      "Validation: Epoch [17], Batch [130/938], Loss: 0.6458888649940491\n",
      "Validation: Epoch [17], Batch [131/938], Loss: 0.6681063175201416\n",
      "Validation: Epoch [17], Batch [132/938], Loss: 0.7047652006149292\n",
      "Validation: Epoch [17], Batch [133/938], Loss: 0.5494656562805176\n",
      "Validation: Epoch [17], Batch [134/938], Loss: 0.7993024587631226\n",
      "Validation: Epoch [17], Batch [135/938], Loss: 0.6682194471359253\n",
      "Validation: Epoch [17], Batch [136/938], Loss: 0.9426975846290588\n",
      "Validation: Epoch [17], Batch [137/938], Loss: 0.8264102339744568\n",
      "Validation: Epoch [17], Batch [138/938], Loss: 0.6628075838088989\n",
      "Validation: Epoch [17], Batch [139/938], Loss: 0.7277896404266357\n",
      "Validation: Epoch [17], Batch [140/938], Loss: 0.9844839572906494\n",
      "Validation: Epoch [17], Batch [141/938], Loss: 0.52437824010849\n",
      "Validation: Epoch [17], Batch [142/938], Loss: 0.6887643933296204\n",
      "Validation: Epoch [17], Batch [143/938], Loss: 1.055304765701294\n",
      "Validation: Epoch [17], Batch [144/938], Loss: 0.6051558256149292\n",
      "Validation: Epoch [17], Batch [145/938], Loss: 0.43396008014678955\n",
      "Validation: Epoch [17], Batch [146/938], Loss: 0.7294665575027466\n",
      "Validation: Epoch [17], Batch [147/938], Loss: 0.8522757887840271\n",
      "Validation: Epoch [17], Batch [148/938], Loss: 0.6486128568649292\n",
      "Validation: Epoch [17], Batch [149/938], Loss: 0.8043422102928162\n",
      "Validation: Epoch [17], Batch [150/938], Loss: 0.6023356914520264\n",
      "Validation: Epoch [17], Batch [151/938], Loss: 0.7289590239524841\n",
      "Validation: Epoch [17], Batch [152/938], Loss: 0.7458697557449341\n",
      "Validation: Epoch [17], Batch [153/938], Loss: 0.6930745840072632\n",
      "Validation: Epoch [17], Batch [154/938], Loss: 0.5753349661827087\n",
      "Validation: Epoch [17], Batch [155/938], Loss: 0.7718536257743835\n",
      "Validation: Epoch [17], Batch [156/938], Loss: 0.79076087474823\n",
      "Validation: Epoch [17], Batch [157/938], Loss: 0.7546147108078003\n",
      "Validation: Epoch [17], Batch [158/938], Loss: 0.6608814597129822\n",
      "Validation: Epoch [17], Batch [159/938], Loss: 0.7173281908035278\n",
      "Validation: Epoch [17], Batch [160/938], Loss: 0.6634252071380615\n",
      "Validation: Epoch [17], Batch [161/938], Loss: 0.7179588675498962\n",
      "Validation: Epoch [17], Batch [162/938], Loss: 0.7283998727798462\n",
      "Validation: Epoch [17], Batch [163/938], Loss: 1.0273975133895874\n",
      "Validation: Epoch [17], Batch [164/938], Loss: 0.6159539818763733\n",
      "Validation: Epoch [17], Batch [165/938], Loss: 0.5752732157707214\n",
      "Validation: Epoch [17], Batch [166/938], Loss: 0.769578754901886\n",
      "Validation: Epoch [17], Batch [167/938], Loss: 0.6102721095085144\n",
      "Validation: Epoch [17], Batch [168/938], Loss: 0.48908230662345886\n",
      "Validation: Epoch [17], Batch [169/938], Loss: 0.6801291704177856\n",
      "Validation: Epoch [17], Batch [170/938], Loss: 0.8455626368522644\n",
      "Validation: Epoch [17], Batch [171/938], Loss: 0.813312292098999\n",
      "Validation: Epoch [17], Batch [172/938], Loss: 0.5914487242698669\n",
      "Validation: Epoch [17], Batch [173/938], Loss: 0.4513530731201172\n",
      "Validation: Epoch [17], Batch [174/938], Loss: 0.7725997567176819\n",
      "Validation: Epoch [17], Batch [175/938], Loss: 0.5544127225875854\n",
      "Validation: Epoch [17], Batch [176/938], Loss: 0.8844776153564453\n",
      "Validation: Epoch [17], Batch [177/938], Loss: 0.7028700113296509\n",
      "Validation: Epoch [17], Batch [178/938], Loss: 0.7581108808517456\n",
      "Validation: Epoch [17], Batch [179/938], Loss: 0.6765113472938538\n",
      "Validation: Epoch [17], Batch [180/938], Loss: 0.6335552334785461\n",
      "Validation: Epoch [17], Batch [181/938], Loss: 0.7220351696014404\n",
      "Validation: Epoch [17], Batch [182/938], Loss: 0.6422185897827148\n",
      "Validation: Epoch [17], Batch [183/938], Loss: 0.5462163686752319\n",
      "Validation: Epoch [17], Batch [184/938], Loss: 0.5342223644256592\n",
      "Validation: Epoch [17], Batch [185/938], Loss: 0.560988187789917\n",
      "Validation: Epoch [17], Batch [186/938], Loss: 0.6456934809684753\n",
      "Validation: Epoch [17], Batch [187/938], Loss: 0.6329261660575867\n",
      "Validation: Epoch [17], Batch [188/938], Loss: 0.7878450751304626\n",
      "Validation: Epoch [17], Batch [189/938], Loss: 0.9154639840126038\n",
      "Validation: Epoch [17], Batch [190/938], Loss: 0.6187524795532227\n",
      "Validation: Epoch [17], Batch [191/938], Loss: 0.7571889758110046\n",
      "Validation: Epoch [17], Batch [192/938], Loss: 0.6402865052223206\n",
      "Validation: Epoch [17], Batch [193/938], Loss: 0.7035233378410339\n",
      "Validation: Epoch [17], Batch [194/938], Loss: 0.7481428384780884\n",
      "Validation: Epoch [17], Batch [195/938], Loss: 0.8585560917854309\n",
      "Validation: Epoch [17], Batch [196/938], Loss: 0.9444366097450256\n",
      "Validation: Epoch [17], Batch [197/938], Loss: 0.6505737900733948\n",
      "Validation: Epoch [17], Batch [198/938], Loss: 0.8433345556259155\n",
      "Validation: Epoch [17], Batch [199/938], Loss: 0.6882911920547485\n",
      "Validation: Epoch [17], Batch [200/938], Loss: 0.509620726108551\n",
      "Validation: Epoch [17], Batch [201/938], Loss: 0.6527946591377258\n",
      "Validation: Epoch [17], Batch [202/938], Loss: 0.7976077795028687\n",
      "Validation: Epoch [17], Batch [203/938], Loss: 0.5210597515106201\n",
      "Validation: Epoch [17], Batch [204/938], Loss: 0.890207827091217\n",
      "Validation: Epoch [17], Batch [205/938], Loss: 0.7151618599891663\n",
      "Validation: Epoch [17], Batch [206/938], Loss: 0.8248000144958496\n",
      "Validation: Epoch [17], Batch [207/938], Loss: 0.6176791191101074\n",
      "Validation: Epoch [17], Batch [208/938], Loss: 0.7029623985290527\n",
      "Validation: Epoch [17], Batch [209/938], Loss: 0.6025426983833313\n",
      "Validation: Epoch [17], Batch [210/938], Loss: 0.7191405892372131\n",
      "Validation: Epoch [17], Batch [211/938], Loss: 0.5673753023147583\n",
      "Validation: Epoch [17], Batch [212/938], Loss: 0.680290937423706\n",
      "Validation: Epoch [17], Batch [213/938], Loss: 0.8274903893470764\n",
      "Validation: Epoch [17], Batch [214/938], Loss: 0.8005598783493042\n",
      "Validation: Epoch [17], Batch [215/938], Loss: 0.7091415524482727\n",
      "Validation: Epoch [17], Batch [216/938], Loss: 0.4870914816856384\n",
      "Validation: Epoch [17], Batch [217/938], Loss: 0.5300334692001343\n",
      "Validation: Epoch [17], Batch [218/938], Loss: 0.44711923599243164\n",
      "Validation: Epoch [17], Batch [219/938], Loss: 0.5018554925918579\n",
      "Validation: Epoch [17], Batch [220/938], Loss: 0.9026646018028259\n",
      "Validation: Epoch [17], Batch [221/938], Loss: 0.7584605813026428\n",
      "Validation: Epoch [17], Batch [222/938], Loss: 0.6960766911506653\n",
      "Validation: Epoch [17], Batch [223/938], Loss: 0.4910726547241211\n",
      "Validation: Epoch [17], Batch [224/938], Loss: 0.42117488384246826\n",
      "Validation: Epoch [17], Batch [225/938], Loss: 0.7369358539581299\n",
      "Validation: Epoch [17], Batch [226/938], Loss: 0.6379940509796143\n",
      "Validation: Epoch [17], Batch [227/938], Loss: 0.7473921179771423\n",
      "Validation: Epoch [17], Batch [228/938], Loss: 0.41207757592201233\n",
      "Validation: Epoch [17], Batch [229/938], Loss: 0.7377504110336304\n",
      "Validation: Epoch [17], Batch [230/938], Loss: 0.9644472599029541\n",
      "Validation: Epoch [17], Batch [231/938], Loss: 0.691543698310852\n",
      "Validation: Epoch [17], Batch [232/938], Loss: 0.8497902154922485\n",
      "Validation: Epoch [17], Batch [233/938], Loss: 0.7039873600006104\n",
      "Validation: Epoch [17], Batch [234/938], Loss: 0.6560265421867371\n",
      "Validation: Epoch [17], Batch [235/938], Loss: 0.7153702974319458\n",
      "Validation: Epoch [17], Batch [236/938], Loss: 0.6526230573654175\n",
      "Validation: Epoch [17], Batch [237/938], Loss: 0.8458135724067688\n",
      "Validation: Epoch [17], Batch [238/938], Loss: 0.74033522605896\n",
      "Validation: Epoch [17], Batch [239/938], Loss: 0.41369104385375977\n",
      "Validation: Epoch [17], Batch [240/938], Loss: 0.7161399722099304\n",
      "Validation: Epoch [17], Batch [241/938], Loss: 0.7003554105758667\n",
      "Validation: Epoch [17], Batch [242/938], Loss: 0.698298454284668\n",
      "Validation: Epoch [17], Batch [243/938], Loss: 0.4987106919288635\n",
      "Validation: Epoch [17], Batch [244/938], Loss: 0.7733081579208374\n",
      "Validation: Epoch [17], Batch [245/938], Loss: 0.6977386474609375\n",
      "Validation: Epoch [17], Batch [246/938], Loss: 0.8418522477149963\n",
      "Validation: Epoch [17], Batch [247/938], Loss: 0.6428260207176208\n",
      "Validation: Epoch [17], Batch [248/938], Loss: 0.545621395111084\n",
      "Validation: Epoch [17], Batch [249/938], Loss: 0.4363614618778229\n",
      "Validation: Epoch [17], Batch [250/938], Loss: 0.699099063873291\n",
      "Validation: Epoch [17], Batch [251/938], Loss: 0.7940908670425415\n",
      "Validation: Epoch [17], Batch [252/938], Loss: 0.5964744687080383\n",
      "Validation: Epoch [17], Batch [253/938], Loss: 0.7953197360038757\n",
      "Validation: Epoch [17], Batch [254/938], Loss: 0.8344253301620483\n",
      "Validation: Epoch [17], Batch [255/938], Loss: 0.6680227518081665\n",
      "Validation: Epoch [17], Batch [256/938], Loss: 0.5033086538314819\n",
      "Validation: Epoch [17], Batch [257/938], Loss: 0.8848556876182556\n",
      "Validation: Epoch [17], Batch [258/938], Loss: 0.5111621022224426\n",
      "Validation: Epoch [17], Batch [259/938], Loss: 0.7644069194793701\n",
      "Validation: Epoch [17], Batch [260/938], Loss: 0.82511305809021\n",
      "Validation: Epoch [17], Batch [261/938], Loss: 0.5733610391616821\n",
      "Validation: Epoch [17], Batch [262/938], Loss: 0.6914924383163452\n",
      "Validation: Epoch [17], Batch [263/938], Loss: 0.6823179721832275\n",
      "Validation: Epoch [17], Batch [264/938], Loss: 0.6307554244995117\n",
      "Validation: Epoch [17], Batch [265/938], Loss: 0.7208764553070068\n",
      "Validation: Epoch [17], Batch [266/938], Loss: 0.6971529722213745\n",
      "Validation: Epoch [17], Batch [267/938], Loss: 0.6777502298355103\n",
      "Validation: Epoch [17], Batch [268/938], Loss: 0.480154812335968\n",
      "Validation: Epoch [17], Batch [269/938], Loss: 0.7597277164459229\n",
      "Validation: Epoch [17], Batch [270/938], Loss: 0.5862773060798645\n",
      "Validation: Epoch [17], Batch [271/938], Loss: 0.9003771543502808\n",
      "Validation: Epoch [17], Batch [272/938], Loss: 0.4886144995689392\n",
      "Validation: Epoch [17], Batch [273/938], Loss: 0.7929747104644775\n",
      "Validation: Epoch [17], Batch [274/938], Loss: 0.7309548258781433\n",
      "Validation: Epoch [17], Batch [275/938], Loss: 0.9129343032836914\n",
      "Validation: Epoch [17], Batch [276/938], Loss: 0.9284024834632874\n",
      "Validation: Epoch [17], Batch [277/938], Loss: 0.5592164397239685\n",
      "Validation: Epoch [17], Batch [278/938], Loss: 0.824004054069519\n",
      "Validation: Epoch [17], Batch [279/938], Loss: 0.754071056842804\n",
      "Validation: Epoch [17], Batch [280/938], Loss: 0.7183913588523865\n",
      "Validation: Epoch [17], Batch [281/938], Loss: 0.7543429732322693\n",
      "Validation: Epoch [17], Batch [282/938], Loss: 0.8000002503395081\n",
      "Validation: Epoch [17], Batch [283/938], Loss: 0.6735485792160034\n",
      "Validation: Epoch [17], Batch [284/938], Loss: 0.563237726688385\n",
      "Validation: Epoch [17], Batch [285/938], Loss: 0.5260376930236816\n",
      "Validation: Epoch [17], Batch [286/938], Loss: 0.634922981262207\n",
      "Validation: Epoch [17], Batch [287/938], Loss: 0.6462317705154419\n",
      "Validation: Epoch [17], Batch [288/938], Loss: 0.6814382672309875\n",
      "Validation: Epoch [17], Batch [289/938], Loss: 0.543873131275177\n",
      "Validation: Epoch [17], Batch [290/938], Loss: 0.8365730047225952\n",
      "Validation: Epoch [17], Batch [291/938], Loss: 0.5865475535392761\n",
      "Validation: Epoch [17], Batch [292/938], Loss: 0.8010427355766296\n",
      "Validation: Epoch [17], Batch [293/938], Loss: 0.6884897351264954\n",
      "Validation: Epoch [17], Batch [294/938], Loss: 0.6773885488510132\n",
      "Validation: Epoch [17], Batch [295/938], Loss: 0.48469170928001404\n",
      "Validation: Epoch [17], Batch [296/938], Loss: 0.7584945559501648\n",
      "Validation: Epoch [17], Batch [297/938], Loss: 0.6426188349723816\n",
      "Validation: Epoch [17], Batch [298/938], Loss: 0.8021557331085205\n",
      "Validation: Epoch [17], Batch [299/938], Loss: 0.6008705496788025\n",
      "Validation: Epoch [17], Batch [300/938], Loss: 0.4981361925601959\n",
      "Validation: Epoch [17], Batch [301/938], Loss: 0.7371384501457214\n",
      "Validation: Epoch [17], Batch [302/938], Loss: 0.6006640195846558\n",
      "Validation: Epoch [17], Batch [303/938], Loss: 0.7305130958557129\n",
      "Validation: Epoch [17], Batch [304/938], Loss: 0.27226346731185913\n",
      "Validation: Epoch [17], Batch [305/938], Loss: 0.6839533448219299\n",
      "Validation: Epoch [17], Batch [306/938], Loss: 0.5982102155685425\n",
      "Validation: Epoch [17], Batch [307/938], Loss: 0.8783719539642334\n",
      "Validation: Epoch [17], Batch [308/938], Loss: 0.6003777980804443\n",
      "Validation: Epoch [17], Batch [309/938], Loss: 0.5020178556442261\n",
      "Validation: Epoch [17], Batch [310/938], Loss: 0.3599967360496521\n",
      "Validation: Epoch [17], Batch [311/938], Loss: 0.7007693648338318\n",
      "Validation: Epoch [17], Batch [312/938], Loss: 0.6511103510856628\n",
      "Validation: Epoch [17], Batch [313/938], Loss: 0.575546383857727\n",
      "Validation: Epoch [17], Batch [314/938], Loss: 0.48624950647354126\n",
      "Validation: Epoch [17], Batch [315/938], Loss: 0.6888545155525208\n",
      "Validation: Epoch [17], Batch [316/938], Loss: 0.49039357900619507\n",
      "Validation: Epoch [17], Batch [317/938], Loss: 0.6676584482192993\n",
      "Validation: Epoch [17], Batch [318/938], Loss: 0.45178455114364624\n",
      "Validation: Epoch [17], Batch [319/938], Loss: 0.7694979310035706\n",
      "Validation: Epoch [17], Batch [320/938], Loss: 0.5374463200569153\n",
      "Validation: Epoch [17], Batch [321/938], Loss: 0.6816486120223999\n",
      "Validation: Epoch [17], Batch [322/938], Loss: 0.7630817294120789\n",
      "Validation: Epoch [17], Batch [323/938], Loss: 0.703217089176178\n",
      "Validation: Epoch [17], Batch [324/938], Loss: 0.576896071434021\n",
      "Validation: Epoch [17], Batch [325/938], Loss: 0.5560909509658813\n",
      "Validation: Epoch [17], Batch [326/938], Loss: 0.5500554442405701\n",
      "Validation: Epoch [17], Batch [327/938], Loss: 0.5434328317642212\n",
      "Validation: Epoch [17], Batch [328/938], Loss: 0.9131945967674255\n",
      "Validation: Epoch [17], Batch [329/938], Loss: 0.700600266456604\n",
      "Validation: Epoch [17], Batch [330/938], Loss: 0.8660301566123962\n",
      "Validation: Epoch [17], Batch [331/938], Loss: 0.6158814430236816\n",
      "Validation: Epoch [17], Batch [332/938], Loss: 0.41494566202163696\n",
      "Validation: Epoch [17], Batch [333/938], Loss: 0.6131620407104492\n",
      "Validation: Epoch [17], Batch [334/938], Loss: 0.5422669649124146\n",
      "Validation: Epoch [17], Batch [335/938], Loss: 0.7511173486709595\n",
      "Validation: Epoch [17], Batch [336/938], Loss: 0.6347576379776001\n",
      "Validation: Epoch [17], Batch [337/938], Loss: 0.6220718622207642\n",
      "Validation: Epoch [17], Batch [338/938], Loss: 0.7311388254165649\n",
      "Validation: Epoch [17], Batch [339/938], Loss: 0.6127699613571167\n",
      "Validation: Epoch [17], Batch [340/938], Loss: 0.7025297284126282\n",
      "Validation: Epoch [17], Batch [341/938], Loss: 0.721375584602356\n",
      "Validation: Epoch [17], Batch [342/938], Loss: 0.681308388710022\n",
      "Validation: Epoch [17], Batch [343/938], Loss: 0.6391551494598389\n",
      "Validation: Epoch [17], Batch [344/938], Loss: 0.9146339893341064\n",
      "Validation: Epoch [17], Batch [345/938], Loss: 0.7073433995246887\n",
      "Validation: Epoch [17], Batch [346/938], Loss: 0.7765284180641174\n",
      "Validation: Epoch [17], Batch [347/938], Loss: 0.646806001663208\n",
      "Validation: Epoch [17], Batch [348/938], Loss: 0.6327815055847168\n",
      "Validation: Epoch [17], Batch [349/938], Loss: 0.6110860109329224\n",
      "Validation: Epoch [17], Batch [350/938], Loss: 0.6243208646774292\n",
      "Validation: Epoch [17], Batch [351/938], Loss: 0.698503315448761\n",
      "Validation: Epoch [17], Batch [352/938], Loss: 0.7023797035217285\n",
      "Validation: Epoch [17], Batch [353/938], Loss: 0.6810692548751831\n",
      "Validation: Epoch [17], Batch [354/938], Loss: 0.9264557957649231\n",
      "Validation: Epoch [17], Batch [355/938], Loss: 0.7276661992073059\n",
      "Validation: Epoch [17], Batch [356/938], Loss: 0.5729153752326965\n",
      "Validation: Epoch [17], Batch [357/938], Loss: 0.6669460535049438\n",
      "Validation: Epoch [17], Batch [358/938], Loss: 0.5638079047203064\n",
      "Validation: Epoch [17], Batch [359/938], Loss: 0.5952481627464294\n",
      "Validation: Epoch [17], Batch [360/938], Loss: 0.7027848958969116\n",
      "Validation: Epoch [17], Batch [361/938], Loss: 0.5558916330337524\n",
      "Validation: Epoch [17], Batch [362/938], Loss: 0.7575874328613281\n",
      "Validation: Epoch [17], Batch [363/938], Loss: 0.7857570648193359\n",
      "Validation: Epoch [17], Batch [364/938], Loss: 0.8054966926574707\n",
      "Validation: Epoch [17], Batch [365/938], Loss: 0.7742602229118347\n",
      "Validation: Epoch [17], Batch [366/938], Loss: 0.8774189949035645\n",
      "Validation: Epoch [17], Batch [367/938], Loss: 0.5722503662109375\n",
      "Validation: Epoch [17], Batch [368/938], Loss: 0.5030562877655029\n",
      "Validation: Epoch [17], Batch [369/938], Loss: 0.5630449652671814\n",
      "Validation: Epoch [17], Batch [370/938], Loss: 0.6049625873565674\n",
      "Validation: Epoch [17], Batch [371/938], Loss: 0.7257877588272095\n",
      "Validation: Epoch [17], Batch [372/938], Loss: 0.7146098613739014\n",
      "Validation: Epoch [17], Batch [373/938], Loss: 0.7964969277381897\n",
      "Validation: Epoch [17], Batch [374/938], Loss: 0.5080926418304443\n",
      "Validation: Epoch [17], Batch [375/938], Loss: 0.7009781002998352\n",
      "Validation: Epoch [17], Batch [376/938], Loss: 0.6925300359725952\n",
      "Validation: Epoch [17], Batch [377/938], Loss: 0.47267597913742065\n",
      "Validation: Epoch [17], Batch [378/938], Loss: 0.886486291885376\n",
      "Validation: Epoch [17], Batch [379/938], Loss: 0.5372267365455627\n",
      "Validation: Epoch [17], Batch [380/938], Loss: 0.641773521900177\n",
      "Validation: Epoch [17], Batch [381/938], Loss: 0.8564103245735168\n",
      "Validation: Epoch [17], Batch [382/938], Loss: 0.6638409495353699\n",
      "Validation: Epoch [17], Batch [383/938], Loss: 0.9703139662742615\n",
      "Validation: Epoch [17], Batch [384/938], Loss: 0.629667341709137\n",
      "Validation: Epoch [17], Batch [385/938], Loss: 0.8338811993598938\n",
      "Validation: Epoch [17], Batch [386/938], Loss: 0.6913310885429382\n",
      "Validation: Epoch [17], Batch [387/938], Loss: 0.7380053400993347\n",
      "Validation: Epoch [17], Batch [388/938], Loss: 0.7867270708084106\n",
      "Validation: Epoch [17], Batch [389/938], Loss: 0.9952619075775146\n",
      "Validation: Epoch [17], Batch [390/938], Loss: 0.673644483089447\n",
      "Validation: Epoch [17], Batch [391/938], Loss: 0.9325187802314758\n",
      "Validation: Epoch [17], Batch [392/938], Loss: 0.7260392308235168\n",
      "Validation: Epoch [17], Batch [393/938], Loss: 0.5489898324012756\n",
      "Validation: Epoch [17], Batch [394/938], Loss: 0.6204933524131775\n",
      "Validation: Epoch [17], Batch [395/938], Loss: 0.7362831234931946\n",
      "Validation: Epoch [17], Batch [396/938], Loss: 0.6507107019424438\n",
      "Validation: Epoch [17], Batch [397/938], Loss: 0.7252574563026428\n",
      "Validation: Epoch [17], Batch [398/938], Loss: 0.6681199073791504\n",
      "Validation: Epoch [17], Batch [399/938], Loss: 0.616635262966156\n",
      "Validation: Epoch [17], Batch [400/938], Loss: 0.6262286901473999\n",
      "Validation: Epoch [17], Batch [401/938], Loss: 0.8096017241477966\n",
      "Validation: Epoch [17], Batch [402/938], Loss: 0.7802926301956177\n",
      "Validation: Epoch [17], Batch [403/938], Loss: 0.4582923948764801\n",
      "Validation: Epoch [17], Batch [404/938], Loss: 0.6378840208053589\n",
      "Validation: Epoch [17], Batch [405/938], Loss: 0.850000262260437\n",
      "Validation: Epoch [17], Batch [406/938], Loss: 0.5643056631088257\n",
      "Validation: Epoch [17], Batch [407/938], Loss: 0.6332528591156006\n",
      "Validation: Epoch [17], Batch [408/938], Loss: 0.7197904586791992\n",
      "Validation: Epoch [17], Batch [409/938], Loss: 0.7722837328910828\n",
      "Validation: Epoch [17], Batch [410/938], Loss: 0.5798725485801697\n",
      "Validation: Epoch [17], Batch [411/938], Loss: 0.5672619342803955\n",
      "Validation: Epoch [17], Batch [412/938], Loss: 0.4570446312427521\n",
      "Validation: Epoch [17], Batch [413/938], Loss: 0.7445406317710876\n",
      "Validation: Epoch [17], Batch [414/938], Loss: 0.7865775227546692\n",
      "Validation: Epoch [17], Batch [415/938], Loss: 0.7068518400192261\n",
      "Validation: Epoch [17], Batch [416/938], Loss: 0.788849949836731\n",
      "Validation: Epoch [17], Batch [417/938], Loss: 0.6485182642936707\n",
      "Validation: Epoch [17], Batch [418/938], Loss: 0.810055136680603\n",
      "Validation: Epoch [17], Batch [419/938], Loss: 0.7343615293502808\n",
      "Validation: Epoch [17], Batch [420/938], Loss: 0.8346547484397888\n",
      "Validation: Epoch [17], Batch [421/938], Loss: 0.8937209844589233\n",
      "Validation: Epoch [17], Batch [422/938], Loss: 0.6910105347633362\n",
      "Validation: Epoch [17], Batch [423/938], Loss: 0.8273531794548035\n",
      "Validation: Epoch [17], Batch [424/938], Loss: 0.6887215375900269\n",
      "Validation: Epoch [17], Batch [425/938], Loss: 0.5836452841758728\n",
      "Validation: Epoch [17], Batch [426/938], Loss: 0.8002207279205322\n",
      "Validation: Epoch [17], Batch [427/938], Loss: 0.8029587268829346\n",
      "Validation: Epoch [17], Batch [428/938], Loss: 1.0226364135742188\n",
      "Validation: Epoch [17], Batch [429/938], Loss: 0.38885945081710815\n",
      "Validation: Epoch [17], Batch [430/938], Loss: 0.5351289510726929\n",
      "Validation: Epoch [17], Batch [431/938], Loss: 0.41686367988586426\n",
      "Validation: Epoch [17], Batch [432/938], Loss: 0.659993588924408\n",
      "Validation: Epoch [17], Batch [433/938], Loss: 0.7506199479103088\n",
      "Validation: Epoch [17], Batch [434/938], Loss: 0.7298257350921631\n",
      "Validation: Epoch [17], Batch [435/938], Loss: 0.6224319934844971\n",
      "Validation: Epoch [17], Batch [436/938], Loss: 0.6090087890625\n",
      "Validation: Epoch [17], Batch [437/938], Loss: 0.7768619656562805\n",
      "Validation: Epoch [17], Batch [438/938], Loss: 0.7898002862930298\n",
      "Validation: Epoch [17], Batch [439/938], Loss: 0.8137736320495605\n",
      "Validation: Epoch [17], Batch [440/938], Loss: 0.824862003326416\n",
      "Validation: Epoch [17], Batch [441/938], Loss: 0.829703688621521\n",
      "Validation: Epoch [17], Batch [442/938], Loss: 0.44824349880218506\n",
      "Validation: Epoch [17], Batch [443/938], Loss: 0.8792188763618469\n",
      "Validation: Epoch [17], Batch [444/938], Loss: 0.6091943383216858\n",
      "Validation: Epoch [17], Batch [445/938], Loss: 0.724237322807312\n",
      "Validation: Epoch [17], Batch [446/938], Loss: 0.5986312031745911\n",
      "Validation: Epoch [17], Batch [447/938], Loss: 0.6218106150627136\n",
      "Validation: Epoch [17], Batch [448/938], Loss: 0.7169219255447388\n",
      "Validation: Epoch [17], Batch [449/938], Loss: 0.7966742515563965\n",
      "Validation: Epoch [17], Batch [450/938], Loss: 0.668740451335907\n",
      "Validation: Epoch [17], Batch [451/938], Loss: 0.3782472014427185\n",
      "Validation: Epoch [17], Batch [452/938], Loss: 0.6632898449897766\n",
      "Validation: Epoch [17], Batch [453/938], Loss: 0.6414937376976013\n",
      "Validation: Epoch [17], Batch [454/938], Loss: 0.8336336612701416\n",
      "Validation: Epoch [17], Batch [455/938], Loss: 0.9190730452537537\n",
      "Validation: Epoch [17], Batch [456/938], Loss: 0.7220160365104675\n",
      "Validation: Epoch [17], Batch [457/938], Loss: 0.6432109475135803\n",
      "Validation: Epoch [17], Batch [458/938], Loss: 0.6430103182792664\n",
      "Validation: Epoch [17], Batch [459/938], Loss: 0.5214247107505798\n",
      "Validation: Epoch [17], Batch [460/938], Loss: 0.5908392667770386\n",
      "Validation: Epoch [17], Batch [461/938], Loss: 0.9018586874008179\n",
      "Validation: Epoch [17], Batch [462/938], Loss: 1.0558037757873535\n",
      "Validation: Epoch [17], Batch [463/938], Loss: 0.6232664585113525\n",
      "Validation: Epoch [17], Batch [464/938], Loss: 0.8877933621406555\n",
      "Validation: Epoch [17], Batch [465/938], Loss: 0.8751404285430908\n",
      "Validation: Epoch [17], Batch [466/938], Loss: 0.5645190477371216\n",
      "Validation: Epoch [17], Batch [467/938], Loss: 0.6454845070838928\n",
      "Validation: Epoch [17], Batch [468/938], Loss: 0.4484192728996277\n",
      "Validation: Epoch [17], Batch [469/938], Loss: 0.7921610474586487\n",
      "Validation: Epoch [17], Batch [470/938], Loss: 0.7211390733718872\n",
      "Validation: Epoch [17], Batch [471/938], Loss: 0.8374831080436707\n",
      "Validation: Epoch [17], Batch [472/938], Loss: 0.7622723579406738\n",
      "Validation: Epoch [17], Batch [473/938], Loss: 0.6790516376495361\n",
      "Validation: Epoch [17], Batch [474/938], Loss: 0.7180770635604858\n",
      "Validation: Epoch [17], Batch [475/938], Loss: 0.6680468320846558\n",
      "Validation: Epoch [17], Batch [476/938], Loss: 0.6565619707107544\n",
      "Validation: Epoch [17], Batch [477/938], Loss: 0.6223655939102173\n",
      "Validation: Epoch [17], Batch [478/938], Loss: 0.6109611988067627\n",
      "Validation: Epoch [17], Batch [479/938], Loss: 0.5528243780136108\n",
      "Validation: Epoch [17], Batch [480/938], Loss: 0.46259355545043945\n",
      "Validation: Epoch [17], Batch [481/938], Loss: 0.6853646636009216\n",
      "Validation: Epoch [17], Batch [482/938], Loss: 0.7511668801307678\n",
      "Validation: Epoch [17], Batch [483/938], Loss: 0.5961915254592896\n",
      "Validation: Epoch [17], Batch [484/938], Loss: 0.5920641422271729\n",
      "Validation: Epoch [17], Batch [485/938], Loss: 0.6007996201515198\n",
      "Validation: Epoch [17], Batch [486/938], Loss: 0.7591269612312317\n",
      "Validation: Epoch [17], Batch [487/938], Loss: 0.7357725501060486\n",
      "Validation: Epoch [17], Batch [488/938], Loss: 0.5937999486923218\n",
      "Validation: Epoch [17], Batch [489/938], Loss: 0.6955875158309937\n",
      "Validation: Epoch [17], Batch [490/938], Loss: 0.9179242849349976\n",
      "Validation: Epoch [17], Batch [491/938], Loss: 0.5252860188484192\n",
      "Validation: Epoch [17], Batch [492/938], Loss: 0.6869782209396362\n",
      "Validation: Epoch [17], Batch [493/938], Loss: 0.6501269936561584\n",
      "Validation: Epoch [17], Batch [494/938], Loss: 0.7035131454467773\n",
      "Validation: Epoch [17], Batch [495/938], Loss: 0.6518694758415222\n",
      "Validation: Epoch [17], Batch [496/938], Loss: 0.8359426856040955\n",
      "Validation: Epoch [17], Batch [497/938], Loss: 0.5368903279304504\n",
      "Validation: Epoch [17], Batch [498/938], Loss: 0.641039252281189\n",
      "Validation: Epoch [17], Batch [499/938], Loss: 0.5614324808120728\n",
      "Validation: Epoch [17], Batch [500/938], Loss: 0.5883776545524597\n",
      "Validation: Epoch [17], Batch [501/938], Loss: 0.8014854192733765\n",
      "Validation: Epoch [17], Batch [502/938], Loss: 0.7207136750221252\n",
      "Validation: Epoch [17], Batch [503/938], Loss: 0.6525879502296448\n",
      "Validation: Epoch [17], Batch [504/938], Loss: 0.8177447319030762\n",
      "Validation: Epoch [17], Batch [505/938], Loss: 0.5819928646087646\n",
      "Validation: Epoch [17], Batch [506/938], Loss: 0.8112272024154663\n",
      "Validation: Epoch [17], Batch [507/938], Loss: 0.6740036606788635\n",
      "Validation: Epoch [17], Batch [508/938], Loss: 0.8573409914970398\n",
      "Validation: Epoch [17], Batch [509/938], Loss: 0.6826849579811096\n",
      "Validation: Epoch [17], Batch [510/938], Loss: 0.8799111843109131\n",
      "Validation: Epoch [17], Batch [511/938], Loss: 0.47585341334342957\n",
      "Validation: Epoch [17], Batch [512/938], Loss: 0.6433649063110352\n",
      "Validation: Epoch [17], Batch [513/938], Loss: 0.6380263566970825\n",
      "Validation: Epoch [17], Batch [514/938], Loss: 0.858134925365448\n",
      "Validation: Epoch [17], Batch [515/938], Loss: 0.7065861225128174\n",
      "Validation: Epoch [17], Batch [516/938], Loss: 0.6288029551506042\n",
      "Validation: Epoch [17], Batch [517/938], Loss: 0.4596892297267914\n",
      "Validation: Epoch [17], Batch [518/938], Loss: 0.950957179069519\n",
      "Validation: Epoch [17], Batch [519/938], Loss: 0.58830726146698\n",
      "Validation: Epoch [17], Batch [520/938], Loss: 0.7099870443344116\n",
      "Validation: Epoch [17], Batch [521/938], Loss: 0.5871419906616211\n",
      "Validation: Epoch [17], Batch [522/938], Loss: 0.6717129349708557\n",
      "Validation: Epoch [17], Batch [523/938], Loss: 0.5551977753639221\n",
      "Validation: Epoch [17], Batch [524/938], Loss: 0.7076236605644226\n",
      "Validation: Epoch [17], Batch [525/938], Loss: 0.8685323596000671\n",
      "Validation: Epoch [17], Batch [526/938], Loss: 0.9918941855430603\n",
      "Validation: Epoch [17], Batch [527/938], Loss: 1.0154815912246704\n",
      "Validation: Epoch [17], Batch [528/938], Loss: 0.5963704586029053\n",
      "Validation: Epoch [17], Batch [529/938], Loss: 0.7754126191139221\n",
      "Validation: Epoch [17], Batch [530/938], Loss: 0.5500701069831848\n",
      "Validation: Epoch [17], Batch [531/938], Loss: 0.5602965950965881\n",
      "Validation: Epoch [17], Batch [532/938], Loss: 0.7094606161117554\n",
      "Validation: Epoch [17], Batch [533/938], Loss: 0.7256802320480347\n",
      "Validation: Epoch [17], Batch [534/938], Loss: 0.7029053568840027\n",
      "Validation: Epoch [17], Batch [535/938], Loss: 0.7797037959098816\n",
      "Validation: Epoch [17], Batch [536/938], Loss: 0.6860359907150269\n",
      "Validation: Epoch [17], Batch [537/938], Loss: 0.6547706723213196\n",
      "Validation: Epoch [17], Batch [538/938], Loss: 0.6058306694030762\n",
      "Validation: Epoch [17], Batch [539/938], Loss: 0.4913784861564636\n",
      "Validation: Epoch [17], Batch [540/938], Loss: 0.5843209624290466\n",
      "Validation: Epoch [17], Batch [541/938], Loss: 0.7842481732368469\n",
      "Validation: Epoch [17], Batch [542/938], Loss: 0.8092786073684692\n",
      "Validation: Epoch [17], Batch [543/938], Loss: 0.6110478639602661\n",
      "Validation: Epoch [17], Batch [544/938], Loss: 0.6800398230552673\n",
      "Validation: Epoch [17], Batch [545/938], Loss: 0.5616514086723328\n",
      "Validation: Epoch [17], Batch [546/938], Loss: 0.49561434984207153\n",
      "Validation: Epoch [17], Batch [547/938], Loss: 0.4325760304927826\n",
      "Validation: Epoch [17], Batch [548/938], Loss: 0.4580632150173187\n",
      "Validation: Epoch [17], Batch [549/938], Loss: 0.6821624040603638\n",
      "Validation: Epoch [17], Batch [550/938], Loss: 0.659498929977417\n",
      "Validation: Epoch [17], Batch [551/938], Loss: 0.6088212728500366\n",
      "Validation: Epoch [17], Batch [552/938], Loss: 0.6811467409133911\n",
      "Validation: Epoch [17], Batch [553/938], Loss: 0.6099323630332947\n",
      "Validation: Epoch [17], Batch [554/938], Loss: 0.4960314631462097\n",
      "Validation: Epoch [17], Batch [555/938], Loss: 0.6072410941123962\n",
      "Validation: Epoch [17], Batch [556/938], Loss: 0.6070895195007324\n",
      "Validation: Epoch [17], Batch [557/938], Loss: 0.7698098421096802\n",
      "Validation: Epoch [17], Batch [558/938], Loss: 0.4475322365760803\n",
      "Validation: Epoch [17], Batch [559/938], Loss: 0.6466562747955322\n",
      "Validation: Epoch [17], Batch [560/938], Loss: 0.6556775569915771\n",
      "Validation: Epoch [17], Batch [561/938], Loss: 0.8513020873069763\n",
      "Validation: Epoch [17], Batch [562/938], Loss: 0.7614860534667969\n",
      "Validation: Epoch [17], Batch [563/938], Loss: 0.6657366156578064\n",
      "Validation: Epoch [17], Batch [564/938], Loss: 0.9067201614379883\n",
      "Validation: Epoch [17], Batch [565/938], Loss: 0.41018038988113403\n",
      "Validation: Epoch [17], Batch [566/938], Loss: 0.7820581197738647\n",
      "Validation: Epoch [17], Batch [567/938], Loss: 0.528797447681427\n",
      "Validation: Epoch [17], Batch [568/938], Loss: 0.8953971266746521\n",
      "Validation: Epoch [17], Batch [569/938], Loss: 0.6976799964904785\n",
      "Validation: Epoch [17], Batch [570/938], Loss: 0.8836045861244202\n",
      "Validation: Epoch [17], Batch [571/938], Loss: 0.6066991686820984\n",
      "Validation: Epoch [17], Batch [572/938], Loss: 0.6060822010040283\n",
      "Validation: Epoch [17], Batch [573/938], Loss: 0.6049187183380127\n",
      "Validation: Epoch [17], Batch [574/938], Loss: 0.502802848815918\n",
      "Validation: Epoch [17], Batch [575/938], Loss: 0.7452360987663269\n",
      "Validation: Epoch [17], Batch [576/938], Loss: 0.670340895652771\n",
      "Validation: Epoch [17], Batch [577/938], Loss: 0.510070264339447\n",
      "Validation: Epoch [17], Batch [578/938], Loss: 0.6509237289428711\n",
      "Validation: Epoch [17], Batch [579/938], Loss: 0.5704184770584106\n",
      "Validation: Epoch [17], Batch [580/938], Loss: 0.8177900910377502\n",
      "Validation: Epoch [17], Batch [581/938], Loss: 0.6811874508857727\n",
      "Validation: Epoch [17], Batch [582/938], Loss: 0.6469680666923523\n",
      "Validation: Epoch [17], Batch [583/938], Loss: 0.6088929772377014\n",
      "Validation: Epoch [17], Batch [584/938], Loss: 0.5543623566627502\n",
      "Validation: Epoch [17], Batch [585/938], Loss: 0.49427592754364014\n",
      "Validation: Epoch [17], Batch [586/938], Loss: 0.5925158858299255\n",
      "Validation: Epoch [17], Batch [587/938], Loss: 0.8361493945121765\n",
      "Validation: Epoch [17], Batch [588/938], Loss: 0.5148049592971802\n",
      "Validation: Epoch [17], Batch [589/938], Loss: 0.9165924191474915\n",
      "Validation: Epoch [17], Batch [590/938], Loss: 0.7845553159713745\n",
      "Validation: Epoch [17], Batch [591/938], Loss: 0.6684231758117676\n",
      "Validation: Epoch [17], Batch [592/938], Loss: 0.710304856300354\n",
      "Validation: Epoch [17], Batch [593/938], Loss: 0.877771258354187\n",
      "Validation: Epoch [17], Batch [594/938], Loss: 0.7774538397789001\n",
      "Validation: Epoch [17], Batch [595/938], Loss: 0.6450366377830505\n",
      "Validation: Epoch [17], Batch [596/938], Loss: 0.4689450263977051\n",
      "Validation: Epoch [17], Batch [597/938], Loss: 0.8000345230102539\n",
      "Validation: Epoch [17], Batch [598/938], Loss: 0.7687426805496216\n",
      "Validation: Epoch [17], Batch [599/938], Loss: 0.7873260974884033\n",
      "Validation: Epoch [17], Batch [600/938], Loss: 0.6673065423965454\n",
      "Validation: Epoch [17], Batch [601/938], Loss: 0.8333955407142639\n",
      "Validation: Epoch [17], Batch [602/938], Loss: 0.6311073303222656\n",
      "Validation: Epoch [17], Batch [603/938], Loss: 0.43450459837913513\n",
      "Validation: Epoch [17], Batch [604/938], Loss: 0.739870548248291\n",
      "Validation: Epoch [17], Batch [605/938], Loss: 0.6706385016441345\n",
      "Validation: Epoch [17], Batch [606/938], Loss: 0.5710712671279907\n",
      "Validation: Epoch [17], Batch [607/938], Loss: 0.504875898361206\n",
      "Validation: Epoch [17], Batch [608/938], Loss: 0.8996956944465637\n",
      "Validation: Epoch [17], Batch [609/938], Loss: 0.679987907409668\n",
      "Validation: Epoch [17], Batch [610/938], Loss: 0.5658648014068604\n",
      "Validation: Epoch [17], Batch [611/938], Loss: 0.7681560516357422\n",
      "Validation: Epoch [17], Batch [612/938], Loss: 0.6207067966461182\n",
      "Validation: Epoch [17], Batch [613/938], Loss: 0.9132134914398193\n",
      "Validation: Epoch [17], Batch [614/938], Loss: 0.4759010374546051\n",
      "Validation: Epoch [17], Batch [615/938], Loss: 0.6421070098876953\n",
      "Validation: Epoch [17], Batch [616/938], Loss: 0.6067888736724854\n",
      "Validation: Epoch [17], Batch [617/938], Loss: 0.6832230091094971\n",
      "Validation: Epoch [17], Batch [618/938], Loss: 0.696975827217102\n",
      "Validation: Epoch [17], Batch [619/938], Loss: 0.7955511212348938\n",
      "Validation: Epoch [17], Batch [620/938], Loss: 0.7256792187690735\n",
      "Validation: Epoch [17], Batch [621/938], Loss: 0.6702397465705872\n",
      "Validation: Epoch [17], Batch [622/938], Loss: 0.703337550163269\n",
      "Validation: Epoch [17], Batch [623/938], Loss: 0.6660245656967163\n",
      "Validation: Epoch [17], Batch [624/938], Loss: 0.8528833985328674\n",
      "Validation: Epoch [17], Batch [625/938], Loss: 0.5975279211997986\n",
      "Validation: Epoch [17], Batch [626/938], Loss: 0.557105302810669\n",
      "Validation: Epoch [17], Batch [627/938], Loss: 0.6681672930717468\n",
      "Validation: Epoch [17], Batch [628/938], Loss: 0.6641089916229248\n",
      "Validation: Epoch [17], Batch [629/938], Loss: 0.6070812344551086\n",
      "Validation: Epoch [17], Batch [630/938], Loss: 0.6753891110420227\n",
      "Validation: Epoch [17], Batch [631/938], Loss: 0.7352476119995117\n",
      "Validation: Epoch [17], Batch [632/938], Loss: 0.7251198291778564\n",
      "Validation: Epoch [17], Batch [633/938], Loss: 0.7721006274223328\n",
      "Validation: Epoch [17], Batch [634/938], Loss: 0.6663261651992798\n",
      "Validation: Epoch [17], Batch [635/938], Loss: 0.7190453410148621\n",
      "Validation: Epoch [17], Batch [636/938], Loss: 0.7410370111465454\n",
      "Validation: Epoch [17], Batch [637/938], Loss: 0.903419554233551\n",
      "Validation: Epoch [17], Batch [638/938], Loss: 0.493421733379364\n",
      "Validation: Epoch [17], Batch [639/938], Loss: 0.7386367917060852\n",
      "Validation: Epoch [17], Batch [640/938], Loss: 0.7501779198646545\n",
      "Validation: Epoch [17], Batch [641/938], Loss: 0.6031301021575928\n",
      "Validation: Epoch [17], Batch [642/938], Loss: 0.7688995599746704\n",
      "Validation: Epoch [17], Batch [643/938], Loss: 0.5780178308486938\n",
      "Validation: Epoch [17], Batch [644/938], Loss: 0.7242434024810791\n",
      "Validation: Epoch [17], Batch [645/938], Loss: 0.6278833746910095\n",
      "Validation: Epoch [17], Batch [646/938], Loss: 0.5926427245140076\n",
      "Validation: Epoch [17], Batch [647/938], Loss: 0.8936803936958313\n",
      "Validation: Epoch [17], Batch [648/938], Loss: 0.562243640422821\n",
      "Validation: Epoch [17], Batch [649/938], Loss: 0.7551370859146118\n",
      "Validation: Epoch [17], Batch [650/938], Loss: 0.7387043237686157\n",
      "Validation: Epoch [17], Batch [651/938], Loss: 0.8947681188583374\n",
      "Validation: Epoch [17], Batch [652/938], Loss: 0.7742505073547363\n",
      "Validation: Epoch [17], Batch [653/938], Loss: 0.6081855893135071\n",
      "Validation: Epoch [17], Batch [654/938], Loss: 0.7513116002082825\n",
      "Validation: Epoch [17], Batch [655/938], Loss: 0.8591458201408386\n",
      "Validation: Epoch [17], Batch [656/938], Loss: 0.6459401845932007\n",
      "Validation: Epoch [17], Batch [657/938], Loss: 0.5164916515350342\n",
      "Validation: Epoch [17], Batch [658/938], Loss: 0.6106548309326172\n",
      "Validation: Epoch [17], Batch [659/938], Loss: 0.7763479351997375\n",
      "Validation: Epoch [17], Batch [660/938], Loss: 0.7276940941810608\n",
      "Validation: Epoch [17], Batch [661/938], Loss: 0.7489805221557617\n",
      "Validation: Epoch [17], Batch [662/938], Loss: 0.6124642491340637\n",
      "Validation: Epoch [17], Batch [663/938], Loss: 0.6695241332054138\n",
      "Validation: Epoch [17], Batch [664/938], Loss: 0.6188565492630005\n",
      "Validation: Epoch [17], Batch [665/938], Loss: 0.6387314200401306\n",
      "Validation: Epoch [17], Batch [666/938], Loss: 0.7963448762893677\n",
      "Validation: Epoch [17], Batch [667/938], Loss: 0.536759614944458\n",
      "Validation: Epoch [17], Batch [668/938], Loss: 0.7959070205688477\n",
      "Validation: Epoch [17], Batch [669/938], Loss: 0.759130597114563\n",
      "Validation: Epoch [17], Batch [670/938], Loss: 0.8665611743927002\n",
      "Validation: Epoch [17], Batch [671/938], Loss: 0.7313850522041321\n",
      "Validation: Epoch [17], Batch [672/938], Loss: 0.549054741859436\n",
      "Validation: Epoch [17], Batch [673/938], Loss: 0.6587792634963989\n",
      "Validation: Epoch [17], Batch [674/938], Loss: 0.7626568078994751\n",
      "Validation: Epoch [17], Batch [675/938], Loss: 0.5958354473114014\n",
      "Validation: Epoch [17], Batch [676/938], Loss: 0.6232866644859314\n",
      "Validation: Epoch [17], Batch [677/938], Loss: 0.7866301536560059\n",
      "Validation: Epoch [17], Batch [678/938], Loss: 0.7391432523727417\n",
      "Validation: Epoch [17], Batch [679/938], Loss: 0.39126402139663696\n",
      "Validation: Epoch [17], Batch [680/938], Loss: 0.8386919498443604\n",
      "Validation: Epoch [17], Batch [681/938], Loss: 0.6583184599876404\n",
      "Validation: Epoch [17], Batch [682/938], Loss: 0.9246588349342346\n",
      "Validation: Epoch [17], Batch [683/938], Loss: 0.7330523729324341\n",
      "Validation: Epoch [17], Batch [684/938], Loss: 0.5305682420730591\n",
      "Validation: Epoch [17], Batch [685/938], Loss: 0.5753799676895142\n",
      "Validation: Epoch [17], Batch [686/938], Loss: 0.5401301383972168\n",
      "Validation: Epoch [17], Batch [687/938], Loss: 0.7891310453414917\n",
      "Validation: Epoch [17], Batch [688/938], Loss: 0.608326256275177\n",
      "Validation: Epoch [17], Batch [689/938], Loss: 0.4073312282562256\n",
      "Validation: Epoch [17], Batch [690/938], Loss: 0.6516907811164856\n",
      "Validation: Epoch [17], Batch [691/938], Loss: 0.6084223985671997\n",
      "Validation: Epoch [17], Batch [692/938], Loss: 0.7930299043655396\n",
      "Validation: Epoch [17], Batch [693/938], Loss: 0.6766373515129089\n",
      "Validation: Epoch [17], Batch [694/938], Loss: 0.7141855359077454\n",
      "Validation: Epoch [17], Batch [695/938], Loss: 0.699304461479187\n",
      "Validation: Epoch [17], Batch [696/938], Loss: 0.6345967054367065\n",
      "Validation: Epoch [17], Batch [697/938], Loss: 0.5259342789649963\n",
      "Validation: Epoch [17], Batch [698/938], Loss: 0.7474333047866821\n",
      "Validation: Epoch [17], Batch [699/938], Loss: 0.6113170981407166\n",
      "Validation: Epoch [17], Batch [700/938], Loss: 0.5634483098983765\n",
      "Validation: Epoch [17], Batch [701/938], Loss: 0.6123745441436768\n",
      "Validation: Epoch [17], Batch [702/938], Loss: 0.6491091847419739\n",
      "Validation: Epoch [17], Batch [703/938], Loss: 0.5865901112556458\n",
      "Validation: Epoch [17], Batch [704/938], Loss: 0.8086320161819458\n",
      "Validation: Epoch [17], Batch [705/938], Loss: 0.6683516502380371\n",
      "Validation: Epoch [17], Batch [706/938], Loss: 0.6689590811729431\n",
      "Validation: Epoch [17], Batch [707/938], Loss: 0.7570371627807617\n",
      "Validation: Epoch [17], Batch [708/938], Loss: 0.6103754043579102\n",
      "Validation: Epoch [17], Batch [709/938], Loss: 1.0181126594543457\n",
      "Validation: Epoch [17], Batch [710/938], Loss: 0.5452725291252136\n",
      "Validation: Epoch [17], Batch [711/938], Loss: 0.5663814544677734\n",
      "Validation: Epoch [17], Batch [712/938], Loss: 0.6822043061256409\n",
      "Validation: Epoch [17], Batch [713/938], Loss: 0.6285039186477661\n",
      "Validation: Epoch [17], Batch [714/938], Loss: 0.6650945544242859\n",
      "Validation: Epoch [17], Batch [715/938], Loss: 0.6471420526504517\n",
      "Validation: Epoch [17], Batch [716/938], Loss: 0.9369230270385742\n",
      "Validation: Epoch [17], Batch [717/938], Loss: 0.6618692874908447\n",
      "Validation: Epoch [17], Batch [718/938], Loss: 0.8085741400718689\n",
      "Validation: Epoch [17], Batch [719/938], Loss: 0.7079240679740906\n",
      "Validation: Epoch [17], Batch [720/938], Loss: 0.8269022107124329\n",
      "Validation: Epoch [17], Batch [721/938], Loss: 0.8842942118644714\n",
      "Validation: Epoch [17], Batch [722/938], Loss: 0.706341028213501\n",
      "Validation: Epoch [17], Batch [723/938], Loss: 0.575221836566925\n",
      "Validation: Epoch [17], Batch [724/938], Loss: 0.5398948788642883\n",
      "Validation: Epoch [17], Batch [725/938], Loss: 0.5950866341590881\n",
      "Validation: Epoch [17], Batch [726/938], Loss: 0.7020638585090637\n",
      "Validation: Epoch [17], Batch [727/938], Loss: 0.7849062085151672\n",
      "Validation: Epoch [17], Batch [728/938], Loss: 0.7943682074546814\n",
      "Validation: Epoch [17], Batch [729/938], Loss: 0.6174326539039612\n",
      "Validation: Epoch [17], Batch [730/938], Loss: 0.7255291938781738\n",
      "Validation: Epoch [17], Batch [731/938], Loss: 0.5890040397644043\n",
      "Validation: Epoch [17], Batch [732/938], Loss: 0.9464660882949829\n",
      "Validation: Epoch [17], Batch [733/938], Loss: 0.7678212523460388\n",
      "Validation: Epoch [17], Batch [734/938], Loss: 0.41874316334724426\n",
      "Validation: Epoch [17], Batch [735/938], Loss: 0.6866509914398193\n",
      "Validation: Epoch [17], Batch [736/938], Loss: 0.7344567775726318\n",
      "Validation: Epoch [17], Batch [737/938], Loss: 0.9457017183303833\n",
      "Validation: Epoch [17], Batch [738/938], Loss: 0.5977269411087036\n",
      "Validation: Epoch [17], Batch [739/938], Loss: 0.7245941162109375\n",
      "Validation: Epoch [17], Batch [740/938], Loss: 0.6445318460464478\n",
      "Validation: Epoch [17], Batch [741/938], Loss: 0.6236981153488159\n",
      "Validation: Epoch [17], Batch [742/938], Loss: 0.7017014622688293\n",
      "Validation: Epoch [17], Batch [743/938], Loss: 0.7972953915596008\n",
      "Validation: Epoch [17], Batch [744/938], Loss: 0.6047449111938477\n",
      "Validation: Epoch [17], Batch [745/938], Loss: 1.1017296314239502\n",
      "Validation: Epoch [17], Batch [746/938], Loss: 0.7353101968765259\n",
      "Validation: Epoch [17], Batch [747/938], Loss: 0.7321639657020569\n",
      "Validation: Epoch [17], Batch [748/938], Loss: 0.5936076641082764\n",
      "Validation: Epoch [17], Batch [749/938], Loss: 0.6360388994216919\n",
      "Validation: Epoch [17], Batch [750/938], Loss: 0.6647236943244934\n",
      "Validation: Epoch [17], Batch [751/938], Loss: 0.7403635382652283\n",
      "Validation: Epoch [17], Batch [752/938], Loss: 1.0566167831420898\n",
      "Validation: Epoch [17], Batch [753/938], Loss: 0.6011341214179993\n",
      "Validation: Epoch [17], Batch [754/938], Loss: 0.6459596157073975\n",
      "Validation: Epoch [17], Batch [755/938], Loss: 0.7782472372055054\n",
      "Validation: Epoch [17], Batch [756/938], Loss: 0.7914718985557556\n",
      "Validation: Epoch [17], Batch [757/938], Loss: 0.665346086025238\n",
      "Validation: Epoch [17], Batch [758/938], Loss: 0.7052865624427795\n",
      "Validation: Epoch [17], Batch [759/938], Loss: 0.6772736310958862\n",
      "Validation: Epoch [17], Batch [760/938], Loss: 0.47312644124031067\n",
      "Validation: Epoch [17], Batch [761/938], Loss: 0.7747540473937988\n",
      "Validation: Epoch [17], Batch [762/938], Loss: 0.7555615901947021\n",
      "Validation: Epoch [17], Batch [763/938], Loss: 0.7072815299034119\n",
      "Validation: Epoch [17], Batch [764/938], Loss: 0.7597175240516663\n",
      "Validation: Epoch [17], Batch [765/938], Loss: 0.7937048077583313\n",
      "Validation: Epoch [17], Batch [766/938], Loss: 0.8812767863273621\n",
      "Validation: Epoch [17], Batch [767/938], Loss: 0.7464183568954468\n",
      "Validation: Epoch [17], Batch [768/938], Loss: 0.6556974649429321\n",
      "Validation: Epoch [17], Batch [769/938], Loss: 0.6491937637329102\n",
      "Validation: Epoch [17], Batch [770/938], Loss: 0.6885443925857544\n",
      "Validation: Epoch [17], Batch [771/938], Loss: 0.7722448110580444\n",
      "Validation: Epoch [17], Batch [772/938], Loss: 0.5155515074729919\n",
      "Validation: Epoch [17], Batch [773/938], Loss: 0.7668151259422302\n",
      "Validation: Epoch [17], Batch [774/938], Loss: 0.6913758516311646\n",
      "Validation: Epoch [17], Batch [775/938], Loss: 0.7859765291213989\n",
      "Validation: Epoch [17], Batch [776/938], Loss: 0.55977463722229\n",
      "Validation: Epoch [17], Batch [777/938], Loss: 0.571532666683197\n",
      "Validation: Epoch [17], Batch [778/938], Loss: 0.6189388632774353\n",
      "Validation: Epoch [17], Batch [779/938], Loss: 0.6706641912460327\n",
      "Validation: Epoch [17], Batch [780/938], Loss: 0.5147956013679504\n",
      "Validation: Epoch [17], Batch [781/938], Loss: 0.900874674320221\n",
      "Validation: Epoch [17], Batch [782/938], Loss: 0.42707422375679016\n",
      "Validation: Epoch [17], Batch [783/938], Loss: 0.7702591419219971\n",
      "Validation: Epoch [17], Batch [784/938], Loss: 0.7341181039810181\n",
      "Validation: Epoch [17], Batch [785/938], Loss: 0.5180849432945251\n",
      "Validation: Epoch [17], Batch [786/938], Loss: 0.673999547958374\n",
      "Validation: Epoch [17], Batch [787/938], Loss: 0.99315345287323\n",
      "Validation: Epoch [17], Batch [788/938], Loss: 0.6091107130050659\n",
      "Validation: Epoch [17], Batch [789/938], Loss: 0.892261266708374\n",
      "Validation: Epoch [17], Batch [790/938], Loss: 0.7410237193107605\n",
      "Validation: Epoch [17], Batch [791/938], Loss: 0.615105152130127\n",
      "Validation: Epoch [17], Batch [792/938], Loss: 0.7852461338043213\n",
      "Validation: Epoch [17], Batch [793/938], Loss: 0.4647790789604187\n",
      "Validation: Epoch [17], Batch [794/938], Loss: 0.7108107805252075\n",
      "Validation: Epoch [17], Batch [795/938], Loss: 0.7009491920471191\n",
      "Validation: Epoch [17], Batch [796/938], Loss: 0.6317346096038818\n",
      "Validation: Epoch [17], Batch [797/938], Loss: 0.7078282833099365\n",
      "Validation: Epoch [17], Batch [798/938], Loss: 0.6870249509811401\n",
      "Validation: Epoch [17], Batch [799/938], Loss: 0.5944498777389526\n",
      "Validation: Epoch [17], Batch [800/938], Loss: 0.720325231552124\n",
      "Validation: Epoch [17], Batch [801/938], Loss: 0.5455635190010071\n",
      "Validation: Epoch [17], Batch [802/938], Loss: 0.6342777609825134\n",
      "Validation: Epoch [17], Batch [803/938], Loss: 0.6997246742248535\n",
      "Validation: Epoch [17], Batch [804/938], Loss: 0.8123940229415894\n",
      "Validation: Epoch [17], Batch [805/938], Loss: 0.790869951248169\n",
      "Validation: Epoch [17], Batch [806/938], Loss: 0.6993235349655151\n",
      "Validation: Epoch [17], Batch [807/938], Loss: 0.607748806476593\n",
      "Validation: Epoch [17], Batch [808/938], Loss: 0.560477077960968\n",
      "Validation: Epoch [17], Batch [809/938], Loss: 0.748772144317627\n",
      "Validation: Epoch [17], Batch [810/938], Loss: 0.7270953059196472\n",
      "Validation: Epoch [17], Batch [811/938], Loss: 0.8283745050430298\n",
      "Validation: Epoch [17], Batch [812/938], Loss: 0.7937421798706055\n",
      "Validation: Epoch [17], Batch [813/938], Loss: 0.8683410286903381\n",
      "Validation: Epoch [17], Batch [814/938], Loss: 0.621320903301239\n",
      "Validation: Epoch [17], Batch [815/938], Loss: 0.3811390995979309\n",
      "Validation: Epoch [17], Batch [816/938], Loss: 0.5984546542167664\n",
      "Validation: Epoch [17], Batch [817/938], Loss: 0.8199537992477417\n",
      "Validation: Epoch [17], Batch [818/938], Loss: 0.681716799736023\n",
      "Validation: Epoch [17], Batch [819/938], Loss: 0.6463085412979126\n",
      "Validation: Epoch [17], Batch [820/938], Loss: 0.5290389060974121\n",
      "Validation: Epoch [17], Batch [821/938], Loss: 0.7585309147834778\n",
      "Validation: Epoch [17], Batch [822/938], Loss: 0.3694661557674408\n",
      "Validation: Epoch [17], Batch [823/938], Loss: 0.7863248586654663\n",
      "Validation: Epoch [17], Batch [824/938], Loss: 0.6374219059944153\n",
      "Validation: Epoch [17], Batch [825/938], Loss: 0.6065060496330261\n",
      "Validation: Epoch [17], Batch [826/938], Loss: 0.751814603805542\n",
      "Validation: Epoch [17], Batch [827/938], Loss: 0.7689415812492371\n",
      "Validation: Epoch [17], Batch [828/938], Loss: 0.7117685079574585\n",
      "Validation: Epoch [17], Batch [829/938], Loss: 0.8520476818084717\n",
      "Validation: Epoch [17], Batch [830/938], Loss: 0.7948676347732544\n",
      "Validation: Epoch [17], Batch [831/938], Loss: 0.8053100109100342\n",
      "Validation: Epoch [17], Batch [832/938], Loss: 0.6130578517913818\n",
      "Validation: Epoch [17], Batch [833/938], Loss: 0.7300914525985718\n",
      "Validation: Epoch [17], Batch [834/938], Loss: 0.5754273533821106\n",
      "Validation: Epoch [17], Batch [835/938], Loss: 0.8989405632019043\n",
      "Validation: Epoch [17], Batch [836/938], Loss: 0.5189874768257141\n",
      "Validation: Epoch [17], Batch [837/938], Loss: 0.6850804686546326\n",
      "Validation: Epoch [17], Batch [838/938], Loss: 0.820448637008667\n",
      "Validation: Epoch [17], Batch [839/938], Loss: 0.7077488899230957\n",
      "Validation: Epoch [17], Batch [840/938], Loss: 1.0060837268829346\n",
      "Validation: Epoch [17], Batch [841/938], Loss: 0.6674551963806152\n",
      "Validation: Epoch [17], Batch [842/938], Loss: 0.6149696707725525\n",
      "Validation: Epoch [17], Batch [843/938], Loss: 0.8406161665916443\n",
      "Validation: Epoch [17], Batch [844/938], Loss: 0.7358365654945374\n",
      "Validation: Epoch [17], Batch [845/938], Loss: 0.7352091073989868\n",
      "Validation: Epoch [17], Batch [846/938], Loss: 0.7680568099021912\n",
      "Validation: Epoch [17], Batch [847/938], Loss: 0.6449425220489502\n",
      "Validation: Epoch [17], Batch [848/938], Loss: 0.6653539538383484\n",
      "Validation: Epoch [17], Batch [849/938], Loss: 0.6149724721908569\n",
      "Validation: Epoch [17], Batch [850/938], Loss: 0.7050946950912476\n",
      "Validation: Epoch [17], Batch [851/938], Loss: 0.6416865587234497\n",
      "Validation: Epoch [17], Batch [852/938], Loss: 0.6524001359939575\n",
      "Validation: Epoch [17], Batch [853/938], Loss: 0.7427992820739746\n",
      "Validation: Epoch [17], Batch [854/938], Loss: 0.5962046384811401\n",
      "Validation: Epoch [17], Batch [855/938], Loss: 0.6485117077827454\n",
      "Validation: Epoch [17], Batch [856/938], Loss: 0.6803679466247559\n",
      "Validation: Epoch [17], Batch [857/938], Loss: 0.6903010606765747\n",
      "Validation: Epoch [17], Batch [858/938], Loss: 0.8188241720199585\n",
      "Validation: Epoch [17], Batch [859/938], Loss: 0.5800319314002991\n",
      "Validation: Epoch [17], Batch [860/938], Loss: 0.7010023593902588\n",
      "Validation: Epoch [17], Batch [861/938], Loss: 0.5616862177848816\n",
      "Validation: Epoch [17], Batch [862/938], Loss: 0.7489335536956787\n",
      "Validation: Epoch [17], Batch [863/938], Loss: 0.6239144206047058\n",
      "Validation: Epoch [17], Batch [864/938], Loss: 0.7140480875968933\n",
      "Validation: Epoch [17], Batch [865/938], Loss: 0.7718943357467651\n",
      "Validation: Epoch [17], Batch [866/938], Loss: 0.5125484466552734\n",
      "Validation: Epoch [17], Batch [867/938], Loss: 0.5892482995986938\n",
      "Validation: Epoch [17], Batch [868/938], Loss: 0.6327316164970398\n",
      "Validation: Epoch [17], Batch [869/938], Loss: 0.8911644220352173\n",
      "Validation: Epoch [17], Batch [870/938], Loss: 0.7343407869338989\n",
      "Validation: Epoch [17], Batch [871/938], Loss: 0.5822985172271729\n",
      "Validation: Epoch [17], Batch [872/938], Loss: 0.9391650557518005\n",
      "Validation: Epoch [17], Batch [873/938], Loss: 0.8738217353820801\n",
      "Validation: Epoch [17], Batch [874/938], Loss: 0.4567490220069885\n",
      "Validation: Epoch [17], Batch [875/938], Loss: 0.7315282821655273\n",
      "Validation: Epoch [17], Batch [876/938], Loss: 0.7926966547966003\n",
      "Validation: Epoch [17], Batch [877/938], Loss: 0.47389134764671326\n",
      "Validation: Epoch [17], Batch [878/938], Loss: 0.5174893736839294\n",
      "Validation: Epoch [17], Batch [879/938], Loss: 0.9025012850761414\n",
      "Validation: Epoch [17], Batch [880/938], Loss: 0.5717299580574036\n",
      "Validation: Epoch [17], Batch [881/938], Loss: 0.4408102035522461\n",
      "Validation: Epoch [17], Batch [882/938], Loss: 0.5784980058670044\n",
      "Validation: Epoch [17], Batch [883/938], Loss: 0.6311264634132385\n",
      "Validation: Epoch [17], Batch [884/938], Loss: 0.6907179355621338\n",
      "Validation: Epoch [17], Batch [885/938], Loss: 0.4893111288547516\n",
      "Validation: Epoch [17], Batch [886/938], Loss: 0.4767390787601471\n",
      "Validation: Epoch [17], Batch [887/938], Loss: 0.6643315553665161\n",
      "Validation: Epoch [17], Batch [888/938], Loss: 0.6351408362388611\n",
      "Validation: Epoch [17], Batch [889/938], Loss: 0.5891535878181458\n",
      "Validation: Epoch [17], Batch [890/938], Loss: 0.8719054460525513\n",
      "Validation: Epoch [17], Batch [891/938], Loss: 0.7461661696434021\n",
      "Validation: Epoch [17], Batch [892/938], Loss: 0.6807530522346497\n",
      "Validation: Epoch [17], Batch [893/938], Loss: 0.5551272034645081\n",
      "Validation: Epoch [17], Batch [894/938], Loss: 0.6673343777656555\n",
      "Validation: Epoch [17], Batch [895/938], Loss: 0.5199248790740967\n",
      "Validation: Epoch [17], Batch [896/938], Loss: 0.734164834022522\n",
      "Validation: Epoch [17], Batch [897/938], Loss: 0.6796684265136719\n",
      "Validation: Epoch [17], Batch [898/938], Loss: 0.5527024269104004\n",
      "Validation: Epoch [17], Batch [899/938], Loss: 0.9374103546142578\n",
      "Validation: Epoch [17], Batch [900/938], Loss: 0.7762408256530762\n",
      "Validation: Epoch [17], Batch [901/938], Loss: 0.43494197726249695\n",
      "Validation: Epoch [17], Batch [902/938], Loss: 0.5640602707862854\n",
      "Validation: Epoch [17], Batch [903/938], Loss: 0.6869544982910156\n",
      "Validation: Epoch [17], Batch [904/938], Loss: 0.48615702986717224\n",
      "Validation: Epoch [17], Batch [905/938], Loss: 0.7713623046875\n",
      "Validation: Epoch [17], Batch [906/938], Loss: 0.6274261474609375\n",
      "Validation: Epoch [17], Batch [907/938], Loss: 0.8115143179893494\n",
      "Validation: Epoch [17], Batch [908/938], Loss: 0.8067550659179688\n",
      "Validation: Epoch [17], Batch [909/938], Loss: 0.9511072635650635\n",
      "Validation: Epoch [17], Batch [910/938], Loss: 0.8105520606040955\n",
      "Validation: Epoch [17], Batch [911/938], Loss: 0.9322717189788818\n",
      "Validation: Epoch [17], Batch [912/938], Loss: 0.5952365398406982\n",
      "Validation: Epoch [17], Batch [913/938], Loss: 0.6955874562263489\n",
      "Validation: Epoch [17], Batch [914/938], Loss: 0.6781783699989319\n",
      "Validation: Epoch [17], Batch [915/938], Loss: 0.6403286457061768\n",
      "Validation: Epoch [17], Batch [916/938], Loss: 0.9131678938865662\n",
      "Validation: Epoch [17], Batch [917/938], Loss: 0.7333928346633911\n",
      "Validation: Epoch [17], Batch [918/938], Loss: 0.6576433181762695\n",
      "Validation: Epoch [17], Batch [919/938], Loss: 0.6332055926322937\n",
      "Validation: Epoch [17], Batch [920/938], Loss: 0.7609539031982422\n",
      "Validation: Epoch [17], Batch [921/938], Loss: 0.6019396185874939\n",
      "Validation: Epoch [17], Batch [922/938], Loss: 0.6922823190689087\n",
      "Validation: Epoch [17], Batch [923/938], Loss: 0.5905926823616028\n",
      "Validation: Epoch [17], Batch [924/938], Loss: 0.6883417963981628\n",
      "Validation: Epoch [17], Batch [925/938], Loss: 0.8482399582862854\n",
      "Validation: Epoch [17], Batch [926/938], Loss: 0.5923519730567932\n",
      "Validation: Epoch [17], Batch [927/938], Loss: 0.6415265202522278\n",
      "Validation: Epoch [17], Batch [928/938], Loss: 0.5341302752494812\n",
      "Validation: Epoch [17], Batch [929/938], Loss: 1.0208148956298828\n",
      "Validation: Epoch [17], Batch [930/938], Loss: 0.7211275100708008\n",
      "Validation: Epoch [17], Batch [931/938], Loss: 0.5771476626396179\n",
      "Validation: Epoch [17], Batch [932/938], Loss: 0.7987440824508667\n",
      "Validation: Epoch [17], Batch [933/938], Loss: 0.5189707279205322\n",
      "Validation: Epoch [17], Batch [934/938], Loss: 0.6948370337486267\n",
      "Validation: Epoch [17], Batch [935/938], Loss: 0.8024274110794067\n",
      "Validation: Epoch [17], Batch [936/938], Loss: 0.5099907517433167\n",
      "Validation: Epoch [17], Batch [937/938], Loss: 0.7050639986991882\n",
      "Validation: Epoch [17], Batch [938/938], Loss: 0.5136001110076904\n",
      "Accuracy of test set: 0.8085833333333333\n",
      "Train: Epoch [18], Batch [1/938], Loss: 0.8936906456947327\n",
      "Train: Epoch [18], Batch [2/938], Loss: 0.732355535030365\n",
      "Train: Epoch [18], Batch [3/938], Loss: 0.6322383284568787\n",
      "Train: Epoch [18], Batch [4/938], Loss: 0.6935811042785645\n",
      "Train: Epoch [18], Batch [5/938], Loss: 0.6749299168586731\n",
      "Train: Epoch [18], Batch [6/938], Loss: 0.7153668403625488\n",
      "Train: Epoch [18], Batch [7/938], Loss: 0.5564864873886108\n",
      "Train: Epoch [18], Batch [8/938], Loss: 0.6450611352920532\n",
      "Train: Epoch [18], Batch [9/938], Loss: 0.8949141502380371\n",
      "Train: Epoch [18], Batch [10/938], Loss: 0.682014524936676\n",
      "Train: Epoch [18], Batch [11/938], Loss: 0.7992352247238159\n",
      "Train: Epoch [18], Batch [12/938], Loss: 0.4913942813873291\n",
      "Train: Epoch [18], Batch [13/938], Loss: 0.769396960735321\n",
      "Train: Epoch [18], Batch [14/938], Loss: 0.8036640882492065\n",
      "Train: Epoch [18], Batch [15/938], Loss: 0.5917595624923706\n",
      "Train: Epoch [18], Batch [16/938], Loss: 0.7714911103248596\n",
      "Train: Epoch [18], Batch [17/938], Loss: 0.5309067964553833\n",
      "Train: Epoch [18], Batch [18/938], Loss: 0.8058705925941467\n",
      "Train: Epoch [18], Batch [19/938], Loss: 0.7997981309890747\n",
      "Train: Epoch [18], Batch [20/938], Loss: 0.3854513168334961\n",
      "Train: Epoch [18], Batch [21/938], Loss: 0.9095786809921265\n",
      "Train: Epoch [18], Batch [22/938], Loss: 0.483462929725647\n",
      "Train: Epoch [18], Batch [23/938], Loss: 0.5672358274459839\n",
      "Train: Epoch [18], Batch [24/938], Loss: 0.5849639773368835\n",
      "Train: Epoch [18], Batch [25/938], Loss: 0.7301395535469055\n",
      "Train: Epoch [18], Batch [26/938], Loss: 0.618920087814331\n",
      "Train: Epoch [18], Batch [27/938], Loss: 0.7098872661590576\n",
      "Train: Epoch [18], Batch [28/938], Loss: 0.6448339223861694\n",
      "Train: Epoch [18], Batch [29/938], Loss: 0.3092029392719269\n",
      "Train: Epoch [18], Batch [30/938], Loss: 0.5021819472312927\n",
      "Train: Epoch [18], Batch [31/938], Loss: 0.5493194460868835\n",
      "Train: Epoch [18], Batch [32/938], Loss: 0.5067968368530273\n",
      "Train: Epoch [18], Batch [33/938], Loss: 0.7098132967948914\n",
      "Train: Epoch [18], Batch [34/938], Loss: 0.9922693371772766\n",
      "Train: Epoch [18], Batch [35/938], Loss: 0.7200651168823242\n",
      "Train: Epoch [18], Batch [36/938], Loss: 0.5587918758392334\n",
      "Train: Epoch [18], Batch [37/938], Loss: 0.797698438167572\n",
      "Train: Epoch [18], Batch [38/938], Loss: 0.8309701681137085\n",
      "Train: Epoch [18], Batch [39/938], Loss: 0.46189963817596436\n",
      "Train: Epoch [18], Batch [40/938], Loss: 0.6198084354400635\n",
      "Train: Epoch [18], Batch [41/938], Loss: 0.6588313579559326\n",
      "Train: Epoch [18], Batch [42/938], Loss: 0.7174761891365051\n",
      "Train: Epoch [18], Batch [43/938], Loss: 0.5739187598228455\n",
      "Train: Epoch [18], Batch [44/938], Loss: 0.6770222783088684\n",
      "Train: Epoch [18], Batch [45/938], Loss: 0.7893227934837341\n",
      "Train: Epoch [18], Batch [46/938], Loss: 0.8954389691352844\n",
      "Train: Epoch [18], Batch [47/938], Loss: 0.6605905890464783\n",
      "Train: Epoch [18], Batch [48/938], Loss: 0.6235024929046631\n",
      "Train: Epoch [18], Batch [49/938], Loss: 0.5925424098968506\n",
      "Train: Epoch [18], Batch [50/938], Loss: 0.4602321684360504\n",
      "Train: Epoch [18], Batch [51/938], Loss: 0.4738463759422302\n",
      "Train: Epoch [18], Batch [52/938], Loss: 0.48587965965270996\n",
      "Train: Epoch [18], Batch [53/938], Loss: 0.4488122761249542\n",
      "Train: Epoch [18], Batch [54/938], Loss: 0.7021076679229736\n",
      "Train: Epoch [18], Batch [55/938], Loss: 0.9080607295036316\n",
      "Train: Epoch [18], Batch [56/938], Loss: 0.7522210478782654\n",
      "Train: Epoch [18], Batch [57/938], Loss: 0.6853559613227844\n",
      "Train: Epoch [18], Batch [58/938], Loss: 0.8085942268371582\n",
      "Train: Epoch [18], Batch [59/938], Loss: 0.6599714159965515\n",
      "Train: Epoch [18], Batch [60/938], Loss: 0.6176729202270508\n",
      "Train: Epoch [18], Batch [61/938], Loss: 0.7006502747535706\n",
      "Train: Epoch [18], Batch [62/938], Loss: 0.872085452079773\n",
      "Train: Epoch [18], Batch [63/938], Loss: 0.9627568125724792\n",
      "Train: Epoch [18], Batch [64/938], Loss: 0.7961632609367371\n",
      "Train: Epoch [18], Batch [65/938], Loss: 0.4285464882850647\n",
      "Train: Epoch [18], Batch [66/938], Loss: 0.7467586398124695\n",
      "Train: Epoch [18], Batch [67/938], Loss: 0.8047829866409302\n",
      "Train: Epoch [18], Batch [68/938], Loss: 0.782052755355835\n",
      "Train: Epoch [18], Batch [69/938], Loss: 0.3847486078739166\n",
      "Train: Epoch [18], Batch [70/938], Loss: 0.8344818949699402\n",
      "Train: Epoch [18], Batch [71/938], Loss: 0.8094133138656616\n",
      "Train: Epoch [18], Batch [72/938], Loss: 0.7059426307678223\n",
      "Train: Epoch [18], Batch [73/938], Loss: 0.8420300483703613\n",
      "Train: Epoch [18], Batch [74/938], Loss: 0.6472555994987488\n",
      "Train: Epoch [18], Batch [75/938], Loss: 0.999553918838501\n",
      "Train: Epoch [18], Batch [76/938], Loss: 0.7766879796981812\n",
      "Train: Epoch [18], Batch [77/938], Loss: 0.6449650526046753\n",
      "Train: Epoch [18], Batch [78/938], Loss: 0.6229943037033081\n",
      "Train: Epoch [18], Batch [79/938], Loss: 0.666619062423706\n",
      "Train: Epoch [18], Batch [80/938], Loss: 1.04047429561615\n",
      "Train: Epoch [18], Batch [81/938], Loss: 0.75960373878479\n",
      "Train: Epoch [18], Batch [82/938], Loss: 0.4495323896408081\n",
      "Train: Epoch [18], Batch [83/938], Loss: 0.4139012396335602\n",
      "Train: Epoch [18], Batch [84/938], Loss: 0.6774901151657104\n",
      "Train: Epoch [18], Batch [85/938], Loss: 0.6041440367698669\n",
      "Train: Epoch [18], Batch [86/938], Loss: 0.7521420121192932\n",
      "Train: Epoch [18], Batch [87/938], Loss: 0.7173683643341064\n",
      "Train: Epoch [18], Batch [88/938], Loss: 0.6480255126953125\n",
      "Train: Epoch [18], Batch [89/938], Loss: 0.4958324432373047\n",
      "Train: Epoch [18], Batch [90/938], Loss: 0.5618466734886169\n",
      "Train: Epoch [18], Batch [91/938], Loss: 0.625298798084259\n",
      "Train: Epoch [18], Batch [92/938], Loss: 0.5413131713867188\n",
      "Train: Epoch [18], Batch [93/938], Loss: 0.660408079624176\n",
      "Train: Epoch [18], Batch [94/938], Loss: 0.5844663381576538\n",
      "Train: Epoch [18], Batch [95/938], Loss: 0.4383963346481323\n",
      "Train: Epoch [18], Batch [96/938], Loss: 0.5106915831565857\n",
      "Train: Epoch [18], Batch [97/938], Loss: 0.814312756061554\n",
      "Train: Epoch [18], Batch [98/938], Loss: 0.8827868700027466\n",
      "Train: Epoch [18], Batch [99/938], Loss: 0.7004328966140747\n",
      "Train: Epoch [18], Batch [100/938], Loss: 0.9412162899971008\n",
      "Train: Epoch [18], Batch [101/938], Loss: 0.6594442129135132\n",
      "Train: Epoch [18], Batch [102/938], Loss: 0.7762538194656372\n",
      "Train: Epoch [18], Batch [103/938], Loss: 0.6529631018638611\n",
      "Train: Epoch [18], Batch [104/938], Loss: 0.5691295266151428\n",
      "Train: Epoch [18], Batch [105/938], Loss: 0.6579263210296631\n",
      "Train: Epoch [18], Batch [106/938], Loss: 0.7599545121192932\n",
      "Train: Epoch [18], Batch [107/938], Loss: 0.5657576322555542\n",
      "Train: Epoch [18], Batch [108/938], Loss: 0.6101145148277283\n",
      "Train: Epoch [18], Batch [109/938], Loss: 0.6341610550880432\n",
      "Train: Epoch [18], Batch [110/938], Loss: 0.6983991861343384\n",
      "Train: Epoch [18], Batch [111/938], Loss: 0.77469801902771\n",
      "Train: Epoch [18], Batch [112/938], Loss: 0.5587725639343262\n",
      "Train: Epoch [18], Batch [113/938], Loss: 0.5377656817436218\n",
      "Train: Epoch [18], Batch [114/938], Loss: 0.633981466293335\n",
      "Train: Epoch [18], Batch [115/938], Loss: 0.6541105508804321\n",
      "Train: Epoch [18], Batch [116/938], Loss: 0.7190808057785034\n",
      "Train: Epoch [18], Batch [117/938], Loss: 0.6493299603462219\n",
      "Train: Epoch [18], Batch [118/938], Loss: 0.7736121416091919\n",
      "Train: Epoch [18], Batch [119/938], Loss: 0.8609204888343811\n",
      "Train: Epoch [18], Batch [120/938], Loss: 0.5733665823936462\n",
      "Train: Epoch [18], Batch [121/938], Loss: 0.6145091652870178\n",
      "Train: Epoch [18], Batch [122/938], Loss: 0.7026984095573425\n",
      "Train: Epoch [18], Batch [123/938], Loss: 0.7873761653900146\n",
      "Train: Epoch [18], Batch [124/938], Loss: 0.7002406120300293\n",
      "Train: Epoch [18], Batch [125/938], Loss: 0.7573449015617371\n",
      "Train: Epoch [18], Batch [126/938], Loss: 0.6441174149513245\n",
      "Train: Epoch [18], Batch [127/938], Loss: 0.8200565576553345\n",
      "Train: Epoch [18], Batch [128/938], Loss: 0.5265456438064575\n",
      "Train: Epoch [18], Batch [129/938], Loss: 0.8165174722671509\n",
      "Train: Epoch [18], Batch [130/938], Loss: 0.6896439790725708\n",
      "Train: Epoch [18], Batch [131/938], Loss: 0.6366400718688965\n",
      "Train: Epoch [18], Batch [132/938], Loss: 0.826920747756958\n",
      "Train: Epoch [18], Batch [133/938], Loss: 0.7389301657676697\n",
      "Train: Epoch [18], Batch [134/938], Loss: 0.652344822883606\n",
      "Train: Epoch [18], Batch [135/938], Loss: 0.561306893825531\n",
      "Train: Epoch [18], Batch [136/938], Loss: 0.73052579164505\n",
      "Train: Epoch [18], Batch [137/938], Loss: 0.5497986078262329\n",
      "Train: Epoch [18], Batch [138/938], Loss: 0.691345751285553\n",
      "Train: Epoch [18], Batch [139/938], Loss: 0.8479094505310059\n",
      "Train: Epoch [18], Batch [140/938], Loss: 0.6351948380470276\n",
      "Train: Epoch [18], Batch [141/938], Loss: 0.6860224008560181\n",
      "Train: Epoch [18], Batch [142/938], Loss: 0.8614808320999146\n",
      "Train: Epoch [18], Batch [143/938], Loss: 0.8057931661605835\n",
      "Train: Epoch [18], Batch [144/938], Loss: 0.6512136459350586\n",
      "Train: Epoch [18], Batch [145/938], Loss: 0.726599395275116\n",
      "Train: Epoch [18], Batch [146/938], Loss: 0.6614090800285339\n",
      "Train: Epoch [18], Batch [147/938], Loss: 0.7126252055168152\n",
      "Train: Epoch [18], Batch [148/938], Loss: 0.5931649804115295\n",
      "Train: Epoch [18], Batch [149/938], Loss: 0.5583608746528625\n",
      "Train: Epoch [18], Batch [150/938], Loss: 0.6057076454162598\n",
      "Train: Epoch [18], Batch [151/938], Loss: 0.7740170955657959\n",
      "Train: Epoch [18], Batch [152/938], Loss: 0.6039581298828125\n",
      "Train: Epoch [18], Batch [153/938], Loss: 0.6711575388908386\n",
      "Train: Epoch [18], Batch [154/938], Loss: 0.48702627420425415\n",
      "Train: Epoch [18], Batch [155/938], Loss: 0.6696021556854248\n",
      "Train: Epoch [18], Batch [156/938], Loss: 0.813849925994873\n",
      "Train: Epoch [18], Batch [157/938], Loss: 0.8797075152397156\n",
      "Train: Epoch [18], Batch [158/938], Loss: 0.6060892939567566\n",
      "Train: Epoch [18], Batch [159/938], Loss: 0.6359175443649292\n",
      "Train: Epoch [18], Batch [160/938], Loss: 0.5961576700210571\n",
      "Train: Epoch [18], Batch [161/938], Loss: 0.7638328075408936\n",
      "Train: Epoch [18], Batch [162/938], Loss: 0.6186098456382751\n",
      "Train: Epoch [18], Batch [163/938], Loss: 0.4834084212779999\n",
      "Train: Epoch [18], Batch [164/938], Loss: 0.6582192778587341\n",
      "Train: Epoch [18], Batch [165/938], Loss: 0.582218587398529\n",
      "Train: Epoch [18], Batch [166/938], Loss: 0.850742518901825\n",
      "Train: Epoch [18], Batch [167/938], Loss: 0.6324139833450317\n",
      "Train: Epoch [18], Batch [168/938], Loss: 0.5424044132232666\n",
      "Train: Epoch [18], Batch [169/938], Loss: 0.7147020101547241\n",
      "Train: Epoch [18], Batch [170/938], Loss: 0.6876619458198547\n",
      "Train: Epoch [18], Batch [171/938], Loss: 0.8520433902740479\n",
      "Train: Epoch [18], Batch [172/938], Loss: 0.5622259378433228\n",
      "Train: Epoch [18], Batch [173/938], Loss: 0.694241464138031\n",
      "Train: Epoch [18], Batch [174/938], Loss: 0.6815239191055298\n",
      "Train: Epoch [18], Batch [175/938], Loss: 0.6642348170280457\n",
      "Train: Epoch [18], Batch [176/938], Loss: 0.776692271232605\n",
      "Train: Epoch [18], Batch [177/938], Loss: 0.5405959486961365\n",
      "Train: Epoch [18], Batch [178/938], Loss: 0.611177384853363\n",
      "Train: Epoch [18], Batch [179/938], Loss: 0.5209327340126038\n",
      "Train: Epoch [18], Batch [180/938], Loss: 0.6323863863945007\n",
      "Train: Epoch [18], Batch [181/938], Loss: 0.6621344089508057\n",
      "Train: Epoch [18], Batch [182/938], Loss: 0.5694827437400818\n",
      "Train: Epoch [18], Batch [183/938], Loss: 0.600784957408905\n",
      "Train: Epoch [18], Batch [184/938], Loss: 0.8002665042877197\n",
      "Train: Epoch [18], Batch [185/938], Loss: 0.5844507217407227\n",
      "Train: Epoch [18], Batch [186/938], Loss: 0.6250713467597961\n",
      "Train: Epoch [18], Batch [187/938], Loss: 0.8028894662857056\n",
      "Train: Epoch [18], Batch [188/938], Loss: 0.663581371307373\n",
      "Train: Epoch [18], Batch [189/938], Loss: 0.7281909584999084\n",
      "Train: Epoch [18], Batch [190/938], Loss: 0.6053540706634521\n",
      "Train: Epoch [18], Batch [191/938], Loss: 0.8005868196487427\n",
      "Train: Epoch [18], Batch [192/938], Loss: 0.5739753842353821\n",
      "Train: Epoch [18], Batch [193/938], Loss: 0.6418533325195312\n",
      "Train: Epoch [18], Batch [194/938], Loss: 0.4610958695411682\n",
      "Train: Epoch [18], Batch [195/938], Loss: 0.5661987662315369\n",
      "Train: Epoch [18], Batch [196/938], Loss: 0.8758721351623535\n",
      "Train: Epoch [18], Batch [197/938], Loss: 0.7569552659988403\n",
      "Train: Epoch [18], Batch [198/938], Loss: 0.6949241161346436\n",
      "Train: Epoch [18], Batch [199/938], Loss: 0.8204374313354492\n",
      "Train: Epoch [18], Batch [200/938], Loss: 0.5732840895652771\n",
      "Train: Epoch [18], Batch [201/938], Loss: 0.8466581106185913\n",
      "Train: Epoch [18], Batch [202/938], Loss: 0.38311994075775146\n",
      "Train: Epoch [18], Batch [203/938], Loss: 0.9019200801849365\n",
      "Train: Epoch [18], Batch [204/938], Loss: 0.5165131092071533\n",
      "Train: Epoch [18], Batch [205/938], Loss: 0.7910698056221008\n",
      "Train: Epoch [18], Batch [206/938], Loss: 0.7841084003448486\n",
      "Train: Epoch [18], Batch [207/938], Loss: 0.6758623719215393\n",
      "Train: Epoch [18], Batch [208/938], Loss: 0.5200579166412354\n",
      "Train: Epoch [18], Batch [209/938], Loss: 0.6305074691772461\n",
      "Train: Epoch [18], Batch [210/938], Loss: 0.48908373713493347\n",
      "Train: Epoch [18], Batch [211/938], Loss: 0.7242706418037415\n",
      "Train: Epoch [18], Batch [212/938], Loss: 0.701079249382019\n",
      "Train: Epoch [18], Batch [213/938], Loss: 0.4968762993812561\n",
      "Train: Epoch [18], Batch [214/938], Loss: 0.6702454686164856\n",
      "Train: Epoch [18], Batch [215/938], Loss: 0.6159694790840149\n",
      "Train: Epoch [18], Batch [216/938], Loss: 0.8299087285995483\n",
      "Train: Epoch [18], Batch [217/938], Loss: 0.8005070686340332\n",
      "Train: Epoch [18], Batch [218/938], Loss: 0.7060359120368958\n",
      "Train: Epoch [18], Batch [219/938], Loss: 0.5069564580917358\n",
      "Train: Epoch [18], Batch [220/938], Loss: 0.5983031988143921\n",
      "Train: Epoch [18], Batch [221/938], Loss: 0.6546667218208313\n",
      "Train: Epoch [18], Batch [222/938], Loss: 0.5674034953117371\n",
      "Train: Epoch [18], Batch [223/938], Loss: 0.8626254796981812\n",
      "Train: Epoch [18], Batch [224/938], Loss: 0.7350301742553711\n",
      "Train: Epoch [18], Batch [225/938], Loss: 0.8773479461669922\n",
      "Train: Epoch [18], Batch [226/938], Loss: 0.6245907545089722\n",
      "Train: Epoch [18], Batch [227/938], Loss: 0.803876519203186\n",
      "Train: Epoch [18], Batch [228/938], Loss: 0.4438536763191223\n",
      "Train: Epoch [18], Batch [229/938], Loss: 0.9036177396774292\n",
      "Train: Epoch [18], Batch [230/938], Loss: 0.8755163550376892\n",
      "Train: Epoch [18], Batch [231/938], Loss: 0.629603922367096\n",
      "Train: Epoch [18], Batch [232/938], Loss: 0.528856635093689\n",
      "Train: Epoch [18], Batch [233/938], Loss: 0.5962519645690918\n",
      "Train: Epoch [18], Batch [234/938], Loss: 0.5760267376899719\n",
      "Train: Epoch [18], Batch [235/938], Loss: 0.6623706221580505\n",
      "Train: Epoch [18], Batch [236/938], Loss: 0.5878192782402039\n",
      "Train: Epoch [18], Batch [237/938], Loss: 0.7180033326148987\n",
      "Train: Epoch [18], Batch [238/938], Loss: 0.5499750375747681\n",
      "Train: Epoch [18], Batch [239/938], Loss: 0.5964315533638\n",
      "Train: Epoch [18], Batch [240/938], Loss: 0.7736696600914001\n",
      "Train: Epoch [18], Batch [241/938], Loss: 0.6239154934883118\n",
      "Train: Epoch [18], Batch [242/938], Loss: 0.7268387079238892\n",
      "Train: Epoch [18], Batch [243/938], Loss: 0.878065824508667\n",
      "Train: Epoch [18], Batch [244/938], Loss: 0.3639415502548218\n",
      "Train: Epoch [18], Batch [245/938], Loss: 0.5822752118110657\n",
      "Train: Epoch [18], Batch [246/938], Loss: 0.45105525851249695\n",
      "Train: Epoch [18], Batch [247/938], Loss: 0.728551983833313\n",
      "Train: Epoch [18], Batch [248/938], Loss: 0.7750763297080994\n",
      "Train: Epoch [18], Batch [249/938], Loss: 0.5530964732170105\n",
      "Train: Epoch [18], Batch [250/938], Loss: 0.5001615881919861\n",
      "Train: Epoch [18], Batch [251/938], Loss: 0.7632212042808533\n",
      "Train: Epoch [18], Batch [252/938], Loss: 0.7895478010177612\n",
      "Train: Epoch [18], Batch [253/938], Loss: 0.7436990737915039\n",
      "Train: Epoch [18], Batch [254/938], Loss: 0.675275981426239\n",
      "Train: Epoch [18], Batch [255/938], Loss: 0.5652168393135071\n",
      "Train: Epoch [18], Batch [256/938], Loss: 0.7623030543327332\n",
      "Train: Epoch [18], Batch [257/938], Loss: 0.6543256044387817\n",
      "Train: Epoch [18], Batch [258/938], Loss: 0.721144437789917\n",
      "Train: Epoch [18], Batch [259/938], Loss: 0.5851197838783264\n",
      "Train: Epoch [18], Batch [260/938], Loss: 0.6557206511497498\n",
      "Train: Epoch [18], Batch [261/938], Loss: 0.7577817440032959\n",
      "Train: Epoch [18], Batch [262/938], Loss: 0.8747214674949646\n",
      "Train: Epoch [18], Batch [263/938], Loss: 0.5354077219963074\n",
      "Train: Epoch [18], Batch [264/938], Loss: 0.4589231610298157\n",
      "Train: Epoch [18], Batch [265/938], Loss: 0.8188403844833374\n",
      "Train: Epoch [18], Batch [266/938], Loss: 0.7600092887878418\n",
      "Train: Epoch [18], Batch [267/938], Loss: 0.49801504611968994\n",
      "Train: Epoch [18], Batch [268/938], Loss: 0.800204873085022\n",
      "Train: Epoch [18], Batch [269/938], Loss: 0.66639643907547\n",
      "Train: Epoch [18], Batch [270/938], Loss: 0.5789207220077515\n",
      "Train: Epoch [18], Batch [271/938], Loss: 0.5657492876052856\n",
      "Train: Epoch [18], Batch [272/938], Loss: 0.7038857340812683\n",
      "Train: Epoch [18], Batch [273/938], Loss: 0.9157683253288269\n",
      "Train: Epoch [18], Batch [274/938], Loss: 0.5466197729110718\n",
      "Train: Epoch [18], Batch [275/938], Loss: 0.8142495155334473\n",
      "Train: Epoch [18], Batch [276/938], Loss: 0.7904931306838989\n",
      "Train: Epoch [18], Batch [277/938], Loss: 0.7354269623756409\n",
      "Train: Epoch [18], Batch [278/938], Loss: 0.872405469417572\n",
      "Train: Epoch [18], Batch [279/938], Loss: 0.6506743431091309\n",
      "Train: Epoch [18], Batch [280/938], Loss: 0.7111428380012512\n",
      "Train: Epoch [18], Batch [281/938], Loss: 0.666730523109436\n",
      "Train: Epoch [18], Batch [282/938], Loss: 0.6237116456031799\n",
      "Train: Epoch [18], Batch [283/938], Loss: 0.5147638916969299\n",
      "Train: Epoch [18], Batch [284/938], Loss: 0.5289262533187866\n",
      "Train: Epoch [18], Batch [285/938], Loss: 0.6109263896942139\n",
      "Train: Epoch [18], Batch [286/938], Loss: 0.6617458462715149\n",
      "Train: Epoch [18], Batch [287/938], Loss: 0.7375802993774414\n",
      "Train: Epoch [18], Batch [288/938], Loss: 0.5986831784248352\n",
      "Train: Epoch [18], Batch [289/938], Loss: 0.5932151675224304\n",
      "Train: Epoch [18], Batch [290/938], Loss: 0.7483477592468262\n",
      "Train: Epoch [18], Batch [291/938], Loss: 0.6344252228736877\n",
      "Train: Epoch [18], Batch [292/938], Loss: 0.9373047947883606\n",
      "Train: Epoch [18], Batch [293/938], Loss: 0.7157769799232483\n",
      "Train: Epoch [18], Batch [294/938], Loss: 0.8083963394165039\n",
      "Train: Epoch [18], Batch [295/938], Loss: 0.7417044639587402\n",
      "Train: Epoch [18], Batch [296/938], Loss: 0.7196159362792969\n",
      "Train: Epoch [18], Batch [297/938], Loss: 0.7779585719108582\n",
      "Train: Epoch [18], Batch [298/938], Loss: 0.7932778000831604\n",
      "Train: Epoch [18], Batch [299/938], Loss: 0.6279408931732178\n",
      "Train: Epoch [18], Batch [300/938], Loss: 0.7666304707527161\n",
      "Train: Epoch [18], Batch [301/938], Loss: 0.7880097031593323\n",
      "Train: Epoch [18], Batch [302/938], Loss: 0.5203949809074402\n",
      "Train: Epoch [18], Batch [303/938], Loss: 0.7577239274978638\n",
      "Train: Epoch [18], Batch [304/938], Loss: 0.7286738157272339\n",
      "Train: Epoch [18], Batch [305/938], Loss: 0.6985476613044739\n",
      "Train: Epoch [18], Batch [306/938], Loss: 0.935944676399231\n",
      "Train: Epoch [18], Batch [307/938], Loss: 0.6726993918418884\n",
      "Train: Epoch [18], Batch [308/938], Loss: 0.6032233834266663\n",
      "Train: Epoch [18], Batch [309/938], Loss: 0.6237846612930298\n",
      "Train: Epoch [18], Batch [310/938], Loss: 0.6199848651885986\n",
      "Train: Epoch [18], Batch [311/938], Loss: 0.9955929517745972\n",
      "Train: Epoch [18], Batch [312/938], Loss: 0.6880093812942505\n",
      "Train: Epoch [18], Batch [313/938], Loss: 0.7141427397727966\n",
      "Train: Epoch [18], Batch [314/938], Loss: 0.5510822534561157\n",
      "Train: Epoch [18], Batch [315/938], Loss: 0.7427905797958374\n",
      "Train: Epoch [18], Batch [316/938], Loss: 0.6899972558021545\n",
      "Train: Epoch [18], Batch [317/938], Loss: 0.6460436582565308\n",
      "Train: Epoch [18], Batch [318/938], Loss: 0.4642658233642578\n",
      "Train: Epoch [18], Batch [319/938], Loss: 0.6870912909507751\n",
      "Train: Epoch [18], Batch [320/938], Loss: 0.7723982930183411\n",
      "Train: Epoch [18], Batch [321/938], Loss: 0.7400453686714172\n",
      "Train: Epoch [18], Batch [322/938], Loss: 0.8121811747550964\n",
      "Train: Epoch [18], Batch [323/938], Loss: 0.5991801023483276\n",
      "Train: Epoch [18], Batch [324/938], Loss: 0.7661511301994324\n",
      "Train: Epoch [18], Batch [325/938], Loss: 0.7009326219558716\n",
      "Train: Epoch [18], Batch [326/938], Loss: 0.6286841630935669\n",
      "Train: Epoch [18], Batch [327/938], Loss: 0.8007895946502686\n",
      "Train: Epoch [18], Batch [328/938], Loss: 0.7891020178794861\n",
      "Train: Epoch [18], Batch [329/938], Loss: 0.6179786324501038\n",
      "Train: Epoch [18], Batch [330/938], Loss: 0.5104154348373413\n",
      "Train: Epoch [18], Batch [331/938], Loss: 0.651057779788971\n",
      "Train: Epoch [18], Batch [332/938], Loss: 0.7130997180938721\n",
      "Train: Epoch [18], Batch [333/938], Loss: 0.6160805225372314\n",
      "Train: Epoch [18], Batch [334/938], Loss: 0.7985066175460815\n",
      "Train: Epoch [18], Batch [335/938], Loss: 0.5785204172134399\n",
      "Train: Epoch [18], Batch [336/938], Loss: 0.8166446685791016\n",
      "Train: Epoch [18], Batch [337/938], Loss: 0.7534444332122803\n",
      "Train: Epoch [18], Batch [338/938], Loss: 0.5072287321090698\n",
      "Train: Epoch [18], Batch [339/938], Loss: 0.5899969935417175\n",
      "Train: Epoch [18], Batch [340/938], Loss: 0.8238393068313599\n",
      "Train: Epoch [18], Batch [341/938], Loss: 0.678729772567749\n",
      "Train: Epoch [18], Batch [342/938], Loss: 0.7472619414329529\n",
      "Train: Epoch [18], Batch [343/938], Loss: 0.7476745247840881\n",
      "Train: Epoch [18], Batch [344/938], Loss: 0.8803106546401978\n",
      "Train: Epoch [18], Batch [345/938], Loss: 0.5163659453392029\n",
      "Train: Epoch [18], Batch [346/938], Loss: 0.40859946608543396\n",
      "Train: Epoch [18], Batch [347/938], Loss: 1.0280572175979614\n",
      "Train: Epoch [18], Batch [348/938], Loss: 0.7585058808326721\n",
      "Train: Epoch [18], Batch [349/938], Loss: 0.6101149916648865\n",
      "Train: Epoch [18], Batch [350/938], Loss: 0.5800906419754028\n",
      "Train: Epoch [18], Batch [351/938], Loss: 0.8344919681549072\n",
      "Train: Epoch [18], Batch [352/938], Loss: 0.8946599960327148\n",
      "Train: Epoch [18], Batch [353/938], Loss: 0.720180094242096\n",
      "Train: Epoch [18], Batch [354/938], Loss: 0.8832969069480896\n",
      "Train: Epoch [18], Batch [355/938], Loss: 0.48570072650909424\n",
      "Train: Epoch [18], Batch [356/938], Loss: 0.681597113609314\n",
      "Train: Epoch [18], Batch [357/938], Loss: 0.6781545877456665\n",
      "Train: Epoch [18], Batch [358/938], Loss: 0.8970643877983093\n",
      "Train: Epoch [18], Batch [359/938], Loss: 0.5365169048309326\n",
      "Train: Epoch [18], Batch [360/938], Loss: 0.5639947652816772\n",
      "Train: Epoch [18], Batch [361/938], Loss: 0.4805065989494324\n",
      "Train: Epoch [18], Batch [362/938], Loss: 0.6949634552001953\n",
      "Train: Epoch [18], Batch [363/938], Loss: 0.5923405289649963\n",
      "Train: Epoch [18], Batch [364/938], Loss: 0.6067427396774292\n",
      "Train: Epoch [18], Batch [365/938], Loss: 0.6255411505699158\n",
      "Train: Epoch [18], Batch [366/938], Loss: 0.563900351524353\n",
      "Train: Epoch [18], Batch [367/938], Loss: 0.8529293537139893\n",
      "Train: Epoch [18], Batch [368/938], Loss: 0.49542421102523804\n",
      "Train: Epoch [18], Batch [369/938], Loss: 0.4516947269439697\n",
      "Train: Epoch [18], Batch [370/938], Loss: 0.4186466634273529\n",
      "Train: Epoch [18], Batch [371/938], Loss: 0.4998610019683838\n",
      "Train: Epoch [18], Batch [372/938], Loss: 0.7016889452934265\n",
      "Train: Epoch [18], Batch [373/938], Loss: 0.7108338475227356\n",
      "Train: Epoch [18], Batch [374/938], Loss: 0.42071351408958435\n",
      "Train: Epoch [18], Batch [375/938], Loss: 0.7713022232055664\n",
      "Train: Epoch [18], Batch [376/938], Loss: 0.6278796792030334\n",
      "Train: Epoch [18], Batch [377/938], Loss: 0.5013201236724854\n",
      "Train: Epoch [18], Batch [378/938], Loss: 0.5653175115585327\n",
      "Train: Epoch [18], Batch [379/938], Loss: 0.42264509201049805\n",
      "Train: Epoch [18], Batch [380/938], Loss: 0.7163686752319336\n",
      "Train: Epoch [18], Batch [381/938], Loss: 0.7175495028495789\n",
      "Train: Epoch [18], Batch [382/938], Loss: 0.6529608368873596\n",
      "Train: Epoch [18], Batch [383/938], Loss: 0.5957476496696472\n",
      "Train: Epoch [18], Batch [384/938], Loss: 1.020090103149414\n",
      "Train: Epoch [18], Batch [385/938], Loss: 0.923712432384491\n",
      "Train: Epoch [18], Batch [386/938], Loss: 0.6825183629989624\n",
      "Train: Epoch [18], Batch [387/938], Loss: 0.6609758138656616\n",
      "Train: Epoch [18], Batch [388/938], Loss: 0.5056740045547485\n",
      "Train: Epoch [18], Batch [389/938], Loss: 0.8422586917877197\n",
      "Train: Epoch [18], Batch [390/938], Loss: 0.9751162528991699\n",
      "Train: Epoch [18], Batch [391/938], Loss: 0.6140226125717163\n",
      "Train: Epoch [18], Batch [392/938], Loss: 0.5728503465652466\n",
      "Train: Epoch [18], Batch [393/938], Loss: 1.1780810356140137\n",
      "Train: Epoch [18], Batch [394/938], Loss: 0.5238536596298218\n",
      "Train: Epoch [18], Batch [395/938], Loss: 0.5198380947113037\n",
      "Train: Epoch [18], Batch [396/938], Loss: 0.7604352235794067\n",
      "Train: Epoch [18], Batch [397/938], Loss: 0.6643694043159485\n",
      "Train: Epoch [18], Batch [398/938], Loss: 0.584275484085083\n",
      "Train: Epoch [18], Batch [399/938], Loss: 0.5410564541816711\n",
      "Train: Epoch [18], Batch [400/938], Loss: 0.7604247331619263\n",
      "Train: Epoch [18], Batch [401/938], Loss: 0.6714568734169006\n",
      "Train: Epoch [18], Batch [402/938], Loss: 0.7770435214042664\n",
      "Train: Epoch [18], Batch [403/938], Loss: 0.624141275882721\n",
      "Train: Epoch [18], Batch [404/938], Loss: 0.6447890400886536\n",
      "Train: Epoch [18], Batch [405/938], Loss: 0.7474012970924377\n",
      "Train: Epoch [18], Batch [406/938], Loss: 0.7390697598457336\n",
      "Train: Epoch [18], Batch [407/938], Loss: 0.6945115327835083\n",
      "Train: Epoch [18], Batch [408/938], Loss: 0.5152794122695923\n",
      "Train: Epoch [18], Batch [409/938], Loss: 0.5900067090988159\n",
      "Train: Epoch [18], Batch [410/938], Loss: 0.5629516839981079\n",
      "Train: Epoch [18], Batch [411/938], Loss: 0.6425122618675232\n",
      "Train: Epoch [18], Batch [412/938], Loss: 0.8352401852607727\n",
      "Train: Epoch [18], Batch [413/938], Loss: 0.6319909691810608\n",
      "Train: Epoch [18], Batch [414/938], Loss: 0.6552819013595581\n",
      "Train: Epoch [18], Batch [415/938], Loss: 0.6316724419593811\n",
      "Train: Epoch [18], Batch [416/938], Loss: 0.7915918231010437\n",
      "Train: Epoch [18], Batch [417/938], Loss: 0.6574563384056091\n",
      "Train: Epoch [18], Batch [418/938], Loss: 0.3880116939544678\n",
      "Train: Epoch [18], Batch [419/938], Loss: 0.8636668920516968\n",
      "Train: Epoch [18], Batch [420/938], Loss: 0.5473076105117798\n",
      "Train: Epoch [18], Batch [421/938], Loss: 0.7054992914199829\n",
      "Train: Epoch [18], Batch [422/938], Loss: 0.8365111947059631\n",
      "Train: Epoch [18], Batch [423/938], Loss: 0.7879091501235962\n",
      "Train: Epoch [18], Batch [424/938], Loss: 0.8578435778617859\n",
      "Train: Epoch [18], Batch [425/938], Loss: 0.7061758041381836\n",
      "Train: Epoch [18], Batch [426/938], Loss: 0.7583118677139282\n",
      "Train: Epoch [18], Batch [427/938], Loss: 0.4607767164707184\n",
      "Train: Epoch [18], Batch [428/938], Loss: 0.8113600611686707\n",
      "Train: Epoch [18], Batch [429/938], Loss: 0.4241967499256134\n",
      "Train: Epoch [18], Batch [430/938], Loss: 0.7866681814193726\n",
      "Train: Epoch [18], Batch [431/938], Loss: 1.2204114198684692\n",
      "Train: Epoch [18], Batch [432/938], Loss: 0.45074304938316345\n",
      "Train: Epoch [18], Batch [433/938], Loss: 0.6781200170516968\n",
      "Train: Epoch [18], Batch [434/938], Loss: 0.8764818906784058\n",
      "Train: Epoch [18], Batch [435/938], Loss: 0.5677658915519714\n",
      "Train: Epoch [18], Batch [436/938], Loss: 0.45776644349098206\n",
      "Train: Epoch [18], Batch [437/938], Loss: 0.5786232352256775\n",
      "Train: Epoch [18], Batch [438/938], Loss: 0.8340346217155457\n",
      "Train: Epoch [18], Batch [439/938], Loss: 0.4971383213996887\n",
      "Train: Epoch [18], Batch [440/938], Loss: 0.9092648029327393\n",
      "Train: Epoch [18], Batch [441/938], Loss: 0.5640544891357422\n",
      "Train: Epoch [18], Batch [442/938], Loss: 0.7027475833892822\n",
      "Train: Epoch [18], Batch [443/938], Loss: 0.7611812353134155\n",
      "Train: Epoch [18], Batch [444/938], Loss: 0.5405978560447693\n",
      "Train: Epoch [18], Batch [445/938], Loss: 0.5653037428855896\n",
      "Train: Epoch [18], Batch [446/938], Loss: 0.9790536761283875\n",
      "Train: Epoch [18], Batch [447/938], Loss: 0.8476772308349609\n",
      "Train: Epoch [18], Batch [448/938], Loss: 0.6767247319221497\n",
      "Train: Epoch [18], Batch [449/938], Loss: 0.5590767860412598\n",
      "Train: Epoch [18], Batch [450/938], Loss: 0.7211666703224182\n",
      "Train: Epoch [18], Batch [451/938], Loss: 0.8435166478157043\n",
      "Train: Epoch [18], Batch [452/938], Loss: 0.7764164209365845\n",
      "Train: Epoch [18], Batch [453/938], Loss: 0.7437567710876465\n",
      "Train: Epoch [18], Batch [454/938], Loss: 0.6849727630615234\n",
      "Train: Epoch [18], Batch [455/938], Loss: 0.6934595108032227\n",
      "Train: Epoch [18], Batch [456/938], Loss: 0.7016529440879822\n",
      "Train: Epoch [18], Batch [457/938], Loss: 0.7745414972305298\n",
      "Train: Epoch [18], Batch [458/938], Loss: 0.7210825085639954\n",
      "Train: Epoch [18], Batch [459/938], Loss: 0.3765745759010315\n",
      "Train: Epoch [18], Batch [460/938], Loss: 0.6616693735122681\n",
      "Train: Epoch [18], Batch [461/938], Loss: 0.71260005235672\n",
      "Train: Epoch [18], Batch [462/938], Loss: 0.5767670273780823\n",
      "Train: Epoch [18], Batch [463/938], Loss: 0.5780943036079407\n",
      "Train: Epoch [18], Batch [464/938], Loss: 0.6612671613693237\n",
      "Train: Epoch [18], Batch [465/938], Loss: 0.7950485944747925\n",
      "Train: Epoch [18], Batch [466/938], Loss: 0.678186297416687\n",
      "Train: Epoch [18], Batch [467/938], Loss: 0.8536346554756165\n",
      "Train: Epoch [18], Batch [468/938], Loss: 0.497232049703598\n",
      "Train: Epoch [18], Batch [469/938], Loss: 0.5041314959526062\n",
      "Train: Epoch [18], Batch [470/938], Loss: 0.6043181419372559\n",
      "Train: Epoch [18], Batch [471/938], Loss: 0.49329087138175964\n",
      "Train: Epoch [18], Batch [472/938], Loss: 0.825147807598114\n",
      "Train: Epoch [18], Batch [473/938], Loss: 0.6108956933021545\n",
      "Train: Epoch [18], Batch [474/938], Loss: 0.5438745021820068\n",
      "Train: Epoch [18], Batch [475/938], Loss: 0.8312785625457764\n",
      "Train: Epoch [18], Batch [476/938], Loss: 0.686408519744873\n",
      "Train: Epoch [18], Batch [477/938], Loss: 0.5559950470924377\n",
      "Train: Epoch [18], Batch [478/938], Loss: 0.7254611253738403\n",
      "Train: Epoch [18], Batch [479/938], Loss: 0.5143277049064636\n",
      "Train: Epoch [18], Batch [480/938], Loss: 0.6059313416481018\n",
      "Train: Epoch [18], Batch [481/938], Loss: 0.7500949501991272\n",
      "Train: Epoch [18], Batch [482/938], Loss: 0.7789788842201233\n",
      "Train: Epoch [18], Batch [483/938], Loss: 0.8434426784515381\n",
      "Train: Epoch [18], Batch [484/938], Loss: 0.8812963962554932\n",
      "Train: Epoch [18], Batch [485/938], Loss: 0.7195444107055664\n",
      "Train: Epoch [18], Batch [486/938], Loss: 0.9444708824157715\n",
      "Train: Epoch [18], Batch [487/938], Loss: 0.6410082578659058\n",
      "Train: Epoch [18], Batch [488/938], Loss: 0.48866331577301025\n",
      "Train: Epoch [18], Batch [489/938], Loss: 0.5979151725769043\n",
      "Train: Epoch [18], Batch [490/938], Loss: 0.4062311053276062\n",
      "Train: Epoch [18], Batch [491/938], Loss: 0.8372130393981934\n",
      "Train: Epoch [18], Batch [492/938], Loss: 0.9120000600814819\n",
      "Train: Epoch [18], Batch [493/938], Loss: 0.5946168899536133\n",
      "Train: Epoch [18], Batch [494/938], Loss: 0.7371349334716797\n",
      "Train: Epoch [18], Batch [495/938], Loss: 0.784392237663269\n",
      "Train: Epoch [18], Batch [496/938], Loss: 0.6633418798446655\n",
      "Train: Epoch [18], Batch [497/938], Loss: 0.6123968362808228\n",
      "Train: Epoch [18], Batch [498/938], Loss: 0.622442901134491\n",
      "Train: Epoch [18], Batch [499/938], Loss: 0.7567348480224609\n",
      "Train: Epoch [18], Batch [500/938], Loss: 0.7008904814720154\n",
      "Train: Epoch [18], Batch [501/938], Loss: 0.9224791526794434\n",
      "Train: Epoch [18], Batch [502/938], Loss: 0.6835640668869019\n",
      "Train: Epoch [18], Batch [503/938], Loss: 0.6757325530052185\n",
      "Train: Epoch [18], Batch [504/938], Loss: 0.8081506490707397\n",
      "Train: Epoch [18], Batch [505/938], Loss: 0.8934355974197388\n",
      "Train: Epoch [18], Batch [506/938], Loss: 1.095996379852295\n",
      "Train: Epoch [18], Batch [507/938], Loss: 0.5844222903251648\n",
      "Train: Epoch [18], Batch [508/938], Loss: 0.5145774483680725\n",
      "Train: Epoch [18], Batch [509/938], Loss: 0.49584758281707764\n",
      "Train: Epoch [18], Batch [510/938], Loss: 0.5782632827758789\n",
      "Train: Epoch [18], Batch [511/938], Loss: 0.6204242706298828\n",
      "Train: Epoch [18], Batch [512/938], Loss: 0.677811324596405\n",
      "Train: Epoch [18], Batch [513/938], Loss: 0.630624532699585\n",
      "Train: Epoch [18], Batch [514/938], Loss: 0.6038193702697754\n",
      "Train: Epoch [18], Batch [515/938], Loss: 0.6672934293746948\n",
      "Train: Epoch [18], Batch [516/938], Loss: 0.8230603933334351\n",
      "Train: Epoch [18], Batch [517/938], Loss: 0.7472286224365234\n",
      "Train: Epoch [18], Batch [518/938], Loss: 0.7030625343322754\n",
      "Train: Epoch [18], Batch [519/938], Loss: 0.9344453811645508\n",
      "Train: Epoch [18], Batch [520/938], Loss: 0.8538026809692383\n",
      "Train: Epoch [18], Batch [521/938], Loss: 0.6666433811187744\n",
      "Train: Epoch [18], Batch [522/938], Loss: 0.5968679189682007\n",
      "Train: Epoch [18], Batch [523/938], Loss: 0.5323470234870911\n",
      "Train: Epoch [18], Batch [524/938], Loss: 0.7038248777389526\n",
      "Train: Epoch [18], Batch [525/938], Loss: 0.675832211971283\n",
      "Train: Epoch [18], Batch [526/938], Loss: 0.7400630712509155\n",
      "Train: Epoch [18], Batch [527/938], Loss: 0.6236881613731384\n",
      "Train: Epoch [18], Batch [528/938], Loss: 0.7290385961532593\n",
      "Train: Epoch [18], Batch [529/938], Loss: 0.7092260122299194\n",
      "Train: Epoch [18], Batch [530/938], Loss: 0.5750960111618042\n",
      "Train: Epoch [18], Batch [531/938], Loss: 0.4752410054206848\n",
      "Train: Epoch [18], Batch [532/938], Loss: 0.5285898447036743\n",
      "Train: Epoch [18], Batch [533/938], Loss: 0.6004740595817566\n",
      "Train: Epoch [18], Batch [534/938], Loss: 0.7377555966377258\n",
      "Train: Epoch [18], Batch [535/938], Loss: 0.9095224738121033\n",
      "Train: Epoch [18], Batch [536/938], Loss: 0.39540204405784607\n",
      "Train: Epoch [18], Batch [537/938], Loss: 0.7749934792518616\n",
      "Train: Epoch [18], Batch [538/938], Loss: 0.7495349049568176\n",
      "Train: Epoch [18], Batch [539/938], Loss: 0.5728238821029663\n",
      "Train: Epoch [18], Batch [540/938], Loss: 0.6653470993041992\n",
      "Train: Epoch [18], Batch [541/938], Loss: 0.623704195022583\n",
      "Train: Epoch [18], Batch [542/938], Loss: 0.9444997310638428\n",
      "Train: Epoch [18], Batch [543/938], Loss: 0.8947470784187317\n",
      "Train: Epoch [18], Batch [544/938], Loss: 0.4005274474620819\n",
      "Train: Epoch [18], Batch [545/938], Loss: 0.6606636643409729\n",
      "Train: Epoch [18], Batch [546/938], Loss: 0.5083028674125671\n",
      "Train: Epoch [18], Batch [547/938], Loss: 0.5980243682861328\n",
      "Train: Epoch [18], Batch [548/938], Loss: 0.5075329542160034\n",
      "Train: Epoch [18], Batch [549/938], Loss: 0.9492632150650024\n",
      "Train: Epoch [18], Batch [550/938], Loss: 1.0653764009475708\n",
      "Train: Epoch [18], Batch [551/938], Loss: 0.5745143294334412\n",
      "Train: Epoch [18], Batch [552/938], Loss: 0.6387202739715576\n",
      "Train: Epoch [18], Batch [553/938], Loss: 0.549956202507019\n",
      "Train: Epoch [18], Batch [554/938], Loss: 0.42462456226348877\n",
      "Train: Epoch [18], Batch [555/938], Loss: 0.6250700354576111\n",
      "Train: Epoch [18], Batch [556/938], Loss: 0.6748307943344116\n",
      "Train: Epoch [18], Batch [557/938], Loss: 0.8000469207763672\n",
      "Train: Epoch [18], Batch [558/938], Loss: 0.7799739241600037\n",
      "Train: Epoch [18], Batch [559/938], Loss: 0.6600298285484314\n",
      "Train: Epoch [18], Batch [560/938], Loss: 0.8637689352035522\n",
      "Train: Epoch [18], Batch [561/938], Loss: 0.7224743366241455\n",
      "Train: Epoch [18], Batch [562/938], Loss: 0.5877508521080017\n",
      "Train: Epoch [18], Batch [563/938], Loss: 0.5173550248146057\n",
      "Train: Epoch [18], Batch [564/938], Loss: 0.7367364168167114\n",
      "Train: Epoch [18], Batch [565/938], Loss: 0.6705349683761597\n",
      "Train: Epoch [18], Batch [566/938], Loss: 0.7060211300849915\n",
      "Train: Epoch [18], Batch [567/938], Loss: 0.6042107343673706\n",
      "Train: Epoch [18], Batch [568/938], Loss: 0.5851271152496338\n",
      "Train: Epoch [18], Batch [569/938], Loss: 0.7087568640708923\n",
      "Train: Epoch [18], Batch [570/938], Loss: 0.7893561124801636\n",
      "Train: Epoch [18], Batch [571/938], Loss: 0.8604902029037476\n",
      "Train: Epoch [18], Batch [572/938], Loss: 0.65767502784729\n",
      "Train: Epoch [18], Batch [573/938], Loss: 0.7530397772789001\n",
      "Train: Epoch [18], Batch [574/938], Loss: 0.6227547526359558\n",
      "Train: Epoch [18], Batch [575/938], Loss: 0.6261502504348755\n",
      "Train: Epoch [18], Batch [576/938], Loss: 0.8601951599121094\n",
      "Train: Epoch [18], Batch [577/938], Loss: 0.6067233085632324\n",
      "Train: Epoch [18], Batch [578/938], Loss: 0.7346815466880798\n",
      "Train: Epoch [18], Batch [579/938], Loss: 0.5254833102226257\n",
      "Train: Epoch [18], Batch [580/938], Loss: 0.5221536159515381\n",
      "Train: Epoch [18], Batch [581/938], Loss: 0.546789824962616\n",
      "Train: Epoch [18], Batch [582/938], Loss: 0.5942084193229675\n",
      "Train: Epoch [18], Batch [583/938], Loss: 0.8147748708724976\n",
      "Train: Epoch [18], Batch [584/938], Loss: 0.7444823384284973\n",
      "Train: Epoch [18], Batch [585/938], Loss: 0.46634459495544434\n",
      "Train: Epoch [18], Batch [586/938], Loss: 0.6563844680786133\n",
      "Train: Epoch [18], Batch [587/938], Loss: 0.6791770458221436\n",
      "Train: Epoch [18], Batch [588/938], Loss: 0.5435062050819397\n",
      "Train: Epoch [18], Batch [589/938], Loss: 0.7447848916053772\n",
      "Train: Epoch [18], Batch [590/938], Loss: 0.688684344291687\n",
      "Train: Epoch [18], Batch [591/938], Loss: 0.45966672897338867\n",
      "Train: Epoch [18], Batch [592/938], Loss: 0.6396028995513916\n",
      "Train: Epoch [18], Batch [593/938], Loss: 0.7581955790519714\n",
      "Train: Epoch [18], Batch [594/938], Loss: 0.6519365906715393\n",
      "Train: Epoch [18], Batch [595/938], Loss: 0.7339051961898804\n",
      "Train: Epoch [18], Batch [596/938], Loss: 0.6526923179626465\n",
      "Train: Epoch [18], Batch [597/938], Loss: 0.6856492161750793\n",
      "Train: Epoch [18], Batch [598/938], Loss: 0.5239937901496887\n",
      "Train: Epoch [18], Batch [599/938], Loss: 0.6219543218612671\n",
      "Train: Epoch [18], Batch [600/938], Loss: 0.713045597076416\n",
      "Train: Epoch [18], Batch [601/938], Loss: 0.6211807727813721\n",
      "Train: Epoch [18], Batch [602/938], Loss: 0.6733008027076721\n",
      "Train: Epoch [18], Batch [603/938], Loss: 0.42179274559020996\n",
      "Train: Epoch [18], Batch [604/938], Loss: 0.7133466601371765\n",
      "Train: Epoch [18], Batch [605/938], Loss: 0.4535098373889923\n",
      "Train: Epoch [18], Batch [606/938], Loss: 0.7146719098091125\n",
      "Train: Epoch [18], Batch [607/938], Loss: 0.40256303548812866\n",
      "Train: Epoch [18], Batch [608/938], Loss: 0.6632208824157715\n",
      "Train: Epoch [18], Batch [609/938], Loss: 0.5873807072639465\n",
      "Train: Epoch [18], Batch [610/938], Loss: 0.7100081443786621\n",
      "Train: Epoch [18], Batch [611/938], Loss: 0.49828389286994934\n",
      "Train: Epoch [18], Batch [612/938], Loss: 0.5947429537773132\n",
      "Train: Epoch [18], Batch [613/938], Loss: 0.577893853187561\n",
      "Train: Epoch [18], Batch [614/938], Loss: 0.7458623051643372\n",
      "Train: Epoch [18], Batch [615/938], Loss: 0.6578604578971863\n",
      "Train: Epoch [18], Batch [616/938], Loss: 0.8089148998260498\n",
      "Train: Epoch [18], Batch [617/938], Loss: 0.7920633554458618\n",
      "Train: Epoch [18], Batch [618/938], Loss: 0.67378830909729\n",
      "Train: Epoch [18], Batch [619/938], Loss: 0.6030774712562561\n",
      "Train: Epoch [18], Batch [620/938], Loss: 0.5313690304756165\n",
      "Train: Epoch [18], Batch [621/938], Loss: 0.7102085947990417\n",
      "Train: Epoch [18], Batch [622/938], Loss: 0.5023337006568909\n",
      "Train: Epoch [18], Batch [623/938], Loss: 0.9251118302345276\n",
      "Train: Epoch [18], Batch [624/938], Loss: 0.4913600981235504\n",
      "Train: Epoch [18], Batch [625/938], Loss: 0.6188398003578186\n",
      "Train: Epoch [18], Batch [626/938], Loss: 0.7764471769332886\n",
      "Train: Epoch [18], Batch [627/938], Loss: 0.4154530167579651\n",
      "Train: Epoch [18], Batch [628/938], Loss: 0.6829307079315186\n",
      "Train: Epoch [18], Batch [629/938], Loss: 0.6099256277084351\n",
      "Train: Epoch [18], Batch [630/938], Loss: 0.6279963254928589\n",
      "Train: Epoch [18], Batch [631/938], Loss: 0.7384170889854431\n",
      "Train: Epoch [18], Batch [632/938], Loss: 0.7995732426643372\n",
      "Train: Epoch [18], Batch [633/938], Loss: 0.6630391478538513\n",
      "Train: Epoch [18], Batch [634/938], Loss: 0.5851782560348511\n",
      "Train: Epoch [18], Batch [635/938], Loss: 0.717680811882019\n",
      "Train: Epoch [18], Batch [636/938], Loss: 0.7995599508285522\n",
      "Train: Epoch [18], Batch [637/938], Loss: 0.527644693851471\n",
      "Train: Epoch [18], Batch [638/938], Loss: 0.7656443119049072\n",
      "Train: Epoch [18], Batch [639/938], Loss: 0.7874736189842224\n",
      "Train: Epoch [18], Batch [640/938], Loss: 0.902481734752655\n",
      "Train: Epoch [18], Batch [641/938], Loss: 0.46112626791000366\n",
      "Train: Epoch [18], Batch [642/938], Loss: 0.6470799446105957\n",
      "Train: Epoch [18], Batch [643/938], Loss: 0.5761834383010864\n",
      "Train: Epoch [18], Batch [644/938], Loss: 0.6583739519119263\n",
      "Train: Epoch [18], Batch [645/938], Loss: 0.4945770800113678\n",
      "Train: Epoch [18], Batch [646/938], Loss: 0.7392914295196533\n",
      "Train: Epoch [18], Batch [647/938], Loss: 0.8289978504180908\n",
      "Train: Epoch [18], Batch [648/938], Loss: 0.6314688920974731\n",
      "Train: Epoch [18], Batch [649/938], Loss: 0.6563839316368103\n",
      "Train: Epoch [18], Batch [650/938], Loss: 0.6754838824272156\n",
      "Train: Epoch [18], Batch [651/938], Loss: 0.46770262718200684\n",
      "Train: Epoch [18], Batch [652/938], Loss: 0.6228059530258179\n",
      "Train: Epoch [18], Batch [653/938], Loss: 0.4129408597946167\n",
      "Train: Epoch [18], Batch [654/938], Loss: 0.8822873830795288\n",
      "Train: Epoch [18], Batch [655/938], Loss: 0.719897449016571\n",
      "Train: Epoch [18], Batch [656/938], Loss: 0.8040792346000671\n",
      "Train: Epoch [18], Batch [657/938], Loss: 0.6357592344284058\n",
      "Train: Epoch [18], Batch [658/938], Loss: 0.706495463848114\n",
      "Train: Epoch [18], Batch [659/938], Loss: 0.5815889835357666\n",
      "Train: Epoch [18], Batch [660/938], Loss: 0.7244672179222107\n",
      "Train: Epoch [18], Batch [661/938], Loss: 0.5193299055099487\n",
      "Train: Epoch [18], Batch [662/938], Loss: 0.8439002633094788\n",
      "Train: Epoch [18], Batch [663/938], Loss: 0.6041590571403503\n",
      "Train: Epoch [18], Batch [664/938], Loss: 0.47453388571739197\n",
      "Train: Epoch [18], Batch [665/938], Loss: 0.5972784161567688\n",
      "Train: Epoch [18], Batch [666/938], Loss: 0.7651338577270508\n",
      "Train: Epoch [18], Batch [667/938], Loss: 0.5775564908981323\n",
      "Train: Epoch [18], Batch [668/938], Loss: 0.684334397315979\n",
      "Train: Epoch [18], Batch [669/938], Loss: 0.5969595313072205\n",
      "Train: Epoch [18], Batch [670/938], Loss: 0.667056143283844\n",
      "Train: Epoch [18], Batch [671/938], Loss: 0.6140057444572449\n",
      "Train: Epoch [18], Batch [672/938], Loss: 0.754122793674469\n",
      "Train: Epoch [18], Batch [673/938], Loss: 0.6264930963516235\n",
      "Train: Epoch [18], Batch [674/938], Loss: 0.5712282061576843\n",
      "Train: Epoch [18], Batch [675/938], Loss: 0.8080780506134033\n",
      "Train: Epoch [18], Batch [676/938], Loss: 0.8136749863624573\n",
      "Train: Epoch [18], Batch [677/938], Loss: 0.765418291091919\n",
      "Train: Epoch [18], Batch [678/938], Loss: 0.6605592966079712\n",
      "Train: Epoch [18], Batch [679/938], Loss: 0.7941220998764038\n",
      "Train: Epoch [18], Batch [680/938], Loss: 0.6688874959945679\n",
      "Train: Epoch [18], Batch [681/938], Loss: 0.774989128112793\n",
      "Train: Epoch [18], Batch [682/938], Loss: 0.9920864105224609\n",
      "Train: Epoch [18], Batch [683/938], Loss: 0.7221897840499878\n",
      "Train: Epoch [18], Batch [684/938], Loss: 0.9984769821166992\n",
      "Train: Epoch [18], Batch [685/938], Loss: 0.7666649222373962\n",
      "Train: Epoch [18], Batch [686/938], Loss: 0.8110668659210205\n",
      "Train: Epoch [18], Batch [687/938], Loss: 0.6846303343772888\n",
      "Train: Epoch [18], Batch [688/938], Loss: 0.7141848802566528\n",
      "Train: Epoch [18], Batch [689/938], Loss: 0.5825594663619995\n",
      "Train: Epoch [18], Batch [690/938], Loss: 0.5579184889793396\n",
      "Train: Epoch [18], Batch [691/938], Loss: 0.6944072246551514\n",
      "Train: Epoch [18], Batch [692/938], Loss: 0.6291697025299072\n",
      "Train: Epoch [18], Batch [693/938], Loss: 0.6922673583030701\n",
      "Train: Epoch [18], Batch [694/938], Loss: 0.7137885689735413\n",
      "Train: Epoch [18], Batch [695/938], Loss: 0.6367982625961304\n",
      "Train: Epoch [18], Batch [696/938], Loss: 0.5302090048789978\n",
      "Train: Epoch [18], Batch [697/938], Loss: 0.44632288813591003\n",
      "Train: Epoch [18], Batch [698/938], Loss: 0.5366349816322327\n",
      "Train: Epoch [18], Batch [699/938], Loss: 0.7475963234901428\n",
      "Train: Epoch [18], Batch [700/938], Loss: 0.6201459169387817\n",
      "Train: Epoch [18], Batch [701/938], Loss: 0.5305685997009277\n",
      "Train: Epoch [18], Batch [702/938], Loss: 0.6793069243431091\n",
      "Train: Epoch [18], Batch [703/938], Loss: 0.6953748464584351\n",
      "Train: Epoch [18], Batch [704/938], Loss: 0.781879186630249\n",
      "Train: Epoch [18], Batch [705/938], Loss: 0.8995945453643799\n",
      "Train: Epoch [18], Batch [706/938], Loss: 0.5513513088226318\n",
      "Train: Epoch [18], Batch [707/938], Loss: 0.8425573110580444\n",
      "Train: Epoch [18], Batch [708/938], Loss: 0.755966305732727\n",
      "Train: Epoch [18], Batch [709/938], Loss: 0.6955981850624084\n",
      "Train: Epoch [18], Batch [710/938], Loss: 0.6600790619850159\n",
      "Train: Epoch [18], Batch [711/938], Loss: 0.6654838919639587\n",
      "Train: Epoch [18], Batch [712/938], Loss: 0.8843190670013428\n",
      "Train: Epoch [18], Batch [713/938], Loss: 0.7117300033569336\n",
      "Train: Epoch [18], Batch [714/938], Loss: 0.7726975083351135\n",
      "Train: Epoch [18], Batch [715/938], Loss: 0.4769299030303955\n",
      "Train: Epoch [18], Batch [716/938], Loss: 0.9666028618812561\n",
      "Train: Epoch [18], Batch [717/938], Loss: 0.6101683378219604\n",
      "Train: Epoch [18], Batch [718/938], Loss: 0.6162039041519165\n",
      "Train: Epoch [18], Batch [719/938], Loss: 0.6445561051368713\n",
      "Train: Epoch [18], Batch [720/938], Loss: 0.719491183757782\n",
      "Train: Epoch [18], Batch [721/938], Loss: 0.719508707523346\n",
      "Train: Epoch [18], Batch [722/938], Loss: 0.5315482020378113\n",
      "Train: Epoch [18], Batch [723/938], Loss: 0.5814708471298218\n",
      "Train: Epoch [18], Batch [724/938], Loss: 0.6677802205085754\n",
      "Train: Epoch [18], Batch [725/938], Loss: 0.6744568943977356\n",
      "Train: Epoch [18], Batch [726/938], Loss: 0.6918963193893433\n",
      "Train: Epoch [18], Batch [727/938], Loss: 0.6300066709518433\n",
      "Train: Epoch [18], Batch [728/938], Loss: 0.6692857146263123\n",
      "Train: Epoch [18], Batch [729/938], Loss: 0.6149889826774597\n",
      "Train: Epoch [18], Batch [730/938], Loss: 0.6751778721809387\n",
      "Train: Epoch [18], Batch [731/938], Loss: 0.7285600900650024\n",
      "Train: Epoch [18], Batch [732/938], Loss: 0.6145108342170715\n",
      "Train: Epoch [18], Batch [733/938], Loss: 0.5197905898094177\n",
      "Train: Epoch [18], Batch [734/938], Loss: 0.5271441340446472\n",
      "Train: Epoch [18], Batch [735/938], Loss: 0.6889709234237671\n",
      "Train: Epoch [18], Batch [736/938], Loss: 0.34188851714134216\n",
      "Train: Epoch [18], Batch [737/938], Loss: 0.5407067537307739\n",
      "Train: Epoch [18], Batch [738/938], Loss: 0.7826228141784668\n",
      "Train: Epoch [18], Batch [739/938], Loss: 0.6745944619178772\n",
      "Train: Epoch [18], Batch [740/938], Loss: 0.5679101943969727\n",
      "Train: Epoch [18], Batch [741/938], Loss: 0.6573092341423035\n",
      "Train: Epoch [18], Batch [742/938], Loss: 0.7648213505744934\n",
      "Train: Epoch [18], Batch [743/938], Loss: 0.6935255527496338\n",
      "Train: Epoch [18], Batch [744/938], Loss: 0.6436530351638794\n",
      "Train: Epoch [18], Batch [745/938], Loss: 0.7171359062194824\n",
      "Train: Epoch [18], Batch [746/938], Loss: 0.6580079793930054\n",
      "Train: Epoch [18], Batch [747/938], Loss: 0.4219112694263458\n",
      "Train: Epoch [18], Batch [748/938], Loss: 0.750308096408844\n",
      "Train: Epoch [18], Batch [749/938], Loss: 0.6245285272598267\n",
      "Train: Epoch [18], Batch [750/938], Loss: 0.7778406739234924\n",
      "Train: Epoch [18], Batch [751/938], Loss: 0.7275053262710571\n",
      "Train: Epoch [18], Batch [752/938], Loss: 0.6385871767997742\n",
      "Train: Epoch [18], Batch [753/938], Loss: 0.6033867597579956\n",
      "Train: Epoch [18], Batch [754/938], Loss: 0.7243497371673584\n",
      "Train: Epoch [18], Batch [755/938], Loss: 0.6933193802833557\n",
      "Train: Epoch [18], Batch [756/938], Loss: 0.4809766709804535\n",
      "Train: Epoch [18], Batch [757/938], Loss: 0.3743380904197693\n",
      "Train: Epoch [18], Batch [758/938], Loss: 0.7289208173751831\n",
      "Train: Epoch [18], Batch [759/938], Loss: 0.6677923202514648\n",
      "Train: Epoch [18], Batch [760/938], Loss: 0.6014723777770996\n",
      "Train: Epoch [18], Batch [761/938], Loss: 0.6860582828521729\n",
      "Train: Epoch [18], Batch [762/938], Loss: 0.6730679273605347\n",
      "Train: Epoch [18], Batch [763/938], Loss: 0.46320804953575134\n",
      "Train: Epoch [18], Batch [764/938], Loss: 0.7152930498123169\n",
      "Train: Epoch [18], Batch [765/938], Loss: 0.6622969508171082\n",
      "Train: Epoch [18], Batch [766/938], Loss: 0.5710742473602295\n",
      "Train: Epoch [18], Batch [767/938], Loss: 0.6551914215087891\n",
      "Train: Epoch [18], Batch [768/938], Loss: 0.709739625453949\n",
      "Train: Epoch [18], Batch [769/938], Loss: 0.9507679343223572\n",
      "Train: Epoch [18], Batch [770/938], Loss: 0.5391761064529419\n",
      "Train: Epoch [18], Batch [771/938], Loss: 0.31895503401756287\n",
      "Train: Epoch [18], Batch [772/938], Loss: 0.8988953828811646\n",
      "Train: Epoch [18], Batch [773/938], Loss: 0.644993007183075\n",
      "Train: Epoch [18], Batch [774/938], Loss: 0.7553533911705017\n",
      "Train: Epoch [18], Batch [775/938], Loss: 0.6709393262863159\n",
      "Train: Epoch [18], Batch [776/938], Loss: 0.7316758632659912\n",
      "Train: Epoch [18], Batch [777/938], Loss: 0.49637851119041443\n",
      "Train: Epoch [18], Batch [778/938], Loss: 0.682351291179657\n",
      "Train: Epoch [18], Batch [779/938], Loss: 0.7240845561027527\n",
      "Train: Epoch [18], Batch [780/938], Loss: 0.4580342769622803\n",
      "Train: Epoch [18], Batch [781/938], Loss: 0.814738392829895\n",
      "Train: Epoch [18], Batch [782/938], Loss: 0.4032039940357208\n",
      "Train: Epoch [18], Batch [783/938], Loss: 0.5867890119552612\n",
      "Train: Epoch [18], Batch [784/938], Loss: 0.6163438558578491\n",
      "Train: Epoch [18], Batch [785/938], Loss: 0.5478178858757019\n",
      "Train: Epoch [18], Batch [786/938], Loss: 0.5315845608711243\n",
      "Train: Epoch [18], Batch [787/938], Loss: 0.8672469258308411\n",
      "Train: Epoch [18], Batch [788/938], Loss: 0.5901445150375366\n",
      "Train: Epoch [18], Batch [789/938], Loss: 0.8820467591285706\n",
      "Train: Epoch [18], Batch [790/938], Loss: 0.7096826434135437\n",
      "Train: Epoch [18], Batch [791/938], Loss: 0.6103540658950806\n",
      "Train: Epoch [18], Batch [792/938], Loss: 0.7426064610481262\n",
      "Train: Epoch [18], Batch [793/938], Loss: 0.7934601306915283\n",
      "Train: Epoch [18], Batch [794/938], Loss: 0.6534748077392578\n",
      "Train: Epoch [18], Batch [795/938], Loss: 0.672967255115509\n",
      "Train: Epoch [18], Batch [796/938], Loss: 0.633399486541748\n",
      "Train: Epoch [18], Batch [797/938], Loss: 0.6040471196174622\n",
      "Train: Epoch [18], Batch [798/938], Loss: 0.8981432318687439\n",
      "Train: Epoch [18], Batch [799/938], Loss: 0.762711763381958\n",
      "Train: Epoch [18], Batch [800/938], Loss: 0.8942412734031677\n",
      "Train: Epoch [18], Batch [801/938], Loss: 0.8258368968963623\n",
      "Train: Epoch [18], Batch [802/938], Loss: 0.5085325241088867\n",
      "Train: Epoch [18], Batch [803/938], Loss: 0.5940163135528564\n",
      "Train: Epoch [18], Batch [804/938], Loss: 0.5830755829811096\n",
      "Train: Epoch [18], Batch [805/938], Loss: 0.7104241847991943\n",
      "Train: Epoch [18], Batch [806/938], Loss: 0.6779986619949341\n",
      "Train: Epoch [18], Batch [807/938], Loss: 0.6079704761505127\n",
      "Train: Epoch [18], Batch [808/938], Loss: 0.7359355688095093\n",
      "Train: Epoch [18], Batch [809/938], Loss: 0.585168719291687\n",
      "Train: Epoch [18], Batch [810/938], Loss: 0.6181790828704834\n",
      "Train: Epoch [18], Batch [811/938], Loss: 0.9295896291732788\n",
      "Train: Epoch [18], Batch [812/938], Loss: 0.42266178131103516\n",
      "Train: Epoch [18], Batch [813/938], Loss: 0.6711527705192566\n",
      "Train: Epoch [18], Batch [814/938], Loss: 0.5353780388832092\n",
      "Train: Epoch [18], Batch [815/938], Loss: 1.0637460947036743\n",
      "Train: Epoch [18], Batch [816/938], Loss: 0.7971403002738953\n",
      "Train: Epoch [18], Batch [817/938], Loss: 0.6856124997138977\n",
      "Train: Epoch [18], Batch [818/938], Loss: 0.6855552792549133\n",
      "Train: Epoch [18], Batch [819/938], Loss: 0.44089365005493164\n",
      "Train: Epoch [18], Batch [820/938], Loss: 0.8799413442611694\n",
      "Train: Epoch [18], Batch [821/938], Loss: 0.604953408241272\n",
      "Train: Epoch [18], Batch [822/938], Loss: 0.5107545256614685\n",
      "Train: Epoch [18], Batch [823/938], Loss: 0.7583687901496887\n",
      "Train: Epoch [18], Batch [824/938], Loss: 0.6235864758491516\n",
      "Train: Epoch [18], Batch [825/938], Loss: 0.3998716473579407\n",
      "Train: Epoch [18], Batch [826/938], Loss: 0.7101650238037109\n",
      "Train: Epoch [18], Batch [827/938], Loss: 0.6218947172164917\n",
      "Train: Epoch [18], Batch [828/938], Loss: 0.944390058517456\n",
      "Train: Epoch [18], Batch [829/938], Loss: 0.6313251852989197\n",
      "Train: Epoch [18], Batch [830/938], Loss: 0.8323168754577637\n",
      "Train: Epoch [18], Batch [831/938], Loss: 0.7451526522636414\n",
      "Train: Epoch [18], Batch [832/938], Loss: 0.8546926379203796\n",
      "Train: Epoch [18], Batch [833/938], Loss: 0.6142656803131104\n",
      "Train: Epoch [18], Batch [834/938], Loss: 0.7036117315292358\n",
      "Train: Epoch [18], Batch [835/938], Loss: 0.6243906021118164\n",
      "Train: Epoch [18], Batch [836/938], Loss: 0.7002440690994263\n",
      "Train: Epoch [18], Batch [837/938], Loss: 0.6619890332221985\n",
      "Train: Epoch [18], Batch [838/938], Loss: 0.7889156341552734\n",
      "Train: Epoch [18], Batch [839/938], Loss: 0.5623278617858887\n",
      "Train: Epoch [18], Batch [840/938], Loss: 0.8961912393569946\n",
      "Train: Epoch [18], Batch [841/938], Loss: 0.8543869256973267\n",
      "Train: Epoch [18], Batch [842/938], Loss: 0.5606586337089539\n",
      "Train: Epoch [18], Batch [843/938], Loss: 0.7364931702613831\n",
      "Train: Epoch [18], Batch [844/938], Loss: 0.7104053497314453\n",
      "Train: Epoch [18], Batch [845/938], Loss: 0.6277704238891602\n",
      "Train: Epoch [18], Batch [846/938], Loss: 0.5077256560325623\n",
      "Train: Epoch [18], Batch [847/938], Loss: 0.5110488533973694\n",
      "Train: Epoch [18], Batch [848/938], Loss: 0.6200197339057922\n",
      "Train: Epoch [18], Batch [849/938], Loss: 0.5983368158340454\n",
      "Train: Epoch [18], Batch [850/938], Loss: 0.7268544435501099\n",
      "Train: Epoch [18], Batch [851/938], Loss: 0.719003438949585\n",
      "Train: Epoch [18], Batch [852/938], Loss: 0.5488895177841187\n",
      "Train: Epoch [18], Batch [853/938], Loss: 0.5593658685684204\n",
      "Train: Epoch [18], Batch [854/938], Loss: 0.5030766725540161\n",
      "Train: Epoch [18], Batch [855/938], Loss: 0.6262862086296082\n",
      "Train: Epoch [18], Batch [856/938], Loss: 0.5483039617538452\n",
      "Train: Epoch [18], Batch [857/938], Loss: 0.8405627608299255\n",
      "Train: Epoch [18], Batch [858/938], Loss: 0.718744695186615\n",
      "Train: Epoch [18], Batch [859/938], Loss: 0.6835245490074158\n",
      "Train: Epoch [18], Batch [860/938], Loss: 0.757870614528656\n",
      "Train: Epoch [18], Batch [861/938], Loss: 1.1054637432098389\n",
      "Train: Epoch [18], Batch [862/938], Loss: 0.6590611338615417\n",
      "Train: Epoch [18], Batch [863/938], Loss: 0.8294859528541565\n",
      "Train: Epoch [18], Batch [864/938], Loss: 0.7797922492027283\n",
      "Train: Epoch [18], Batch [865/938], Loss: 0.6764612793922424\n",
      "Train: Epoch [18], Batch [866/938], Loss: 0.7813629508018494\n",
      "Train: Epoch [18], Batch [867/938], Loss: 0.5228551030158997\n",
      "Train: Epoch [18], Batch [868/938], Loss: 0.7144302725791931\n",
      "Train: Epoch [18], Batch [869/938], Loss: 0.5652065277099609\n",
      "Train: Epoch [18], Batch [870/938], Loss: 0.7476948499679565\n",
      "Train: Epoch [18], Batch [871/938], Loss: 0.6646396517753601\n",
      "Train: Epoch [18], Batch [872/938], Loss: 0.48677122592926025\n",
      "Train: Epoch [18], Batch [873/938], Loss: 0.8010988235473633\n",
      "Train: Epoch [18], Batch [874/938], Loss: 0.5275135040283203\n",
      "Train: Epoch [18], Batch [875/938], Loss: 0.6949281692504883\n",
      "Train: Epoch [18], Batch [876/938], Loss: 0.904028058052063\n",
      "Train: Epoch [18], Batch [877/938], Loss: 0.7409050464630127\n",
      "Train: Epoch [18], Batch [878/938], Loss: 0.7133752107620239\n",
      "Train: Epoch [18], Batch [879/938], Loss: 0.7315306663513184\n",
      "Train: Epoch [18], Batch [880/938], Loss: 0.7311857342720032\n",
      "Train: Epoch [18], Batch [881/938], Loss: 0.5846690535545349\n",
      "Train: Epoch [18], Batch [882/938], Loss: 0.48114079236984253\n",
      "Train: Epoch [18], Batch [883/938], Loss: 0.7732579112052917\n",
      "Train: Epoch [18], Batch [884/938], Loss: 0.5600309371948242\n",
      "Train: Epoch [18], Batch [885/938], Loss: 0.6993747353553772\n",
      "Train: Epoch [18], Batch [886/938], Loss: 0.5701488852500916\n",
      "Train: Epoch [18], Batch [887/938], Loss: 0.5383537411689758\n",
      "Train: Epoch [18], Batch [888/938], Loss: 0.4819095730781555\n",
      "Train: Epoch [18], Batch [889/938], Loss: 0.6413988471031189\n",
      "Train: Epoch [18], Batch [890/938], Loss: 0.8229341506958008\n",
      "Train: Epoch [18], Batch [891/938], Loss: 0.5453371405601501\n",
      "Train: Epoch [18], Batch [892/938], Loss: 0.41166919469833374\n",
      "Train: Epoch [18], Batch [893/938], Loss: 0.6343035697937012\n",
      "Train: Epoch [18], Batch [894/938], Loss: 0.5143123269081116\n",
      "Train: Epoch [18], Batch [895/938], Loss: 0.591464638710022\n",
      "Train: Epoch [18], Batch [896/938], Loss: 0.5911358594894409\n",
      "Train: Epoch [18], Batch [897/938], Loss: 0.8089426755905151\n",
      "Train: Epoch [18], Batch [898/938], Loss: 0.5629619359970093\n",
      "Train: Epoch [18], Batch [899/938], Loss: 0.546822190284729\n",
      "Train: Epoch [18], Batch [900/938], Loss: 0.837286651134491\n",
      "Train: Epoch [18], Batch [901/938], Loss: 0.6681270003318787\n",
      "Train: Epoch [18], Batch [902/938], Loss: 0.5398921370506287\n",
      "Train: Epoch [18], Batch [903/938], Loss: 0.5996958613395691\n",
      "Train: Epoch [18], Batch [904/938], Loss: 0.8448836803436279\n",
      "Train: Epoch [18], Batch [905/938], Loss: 0.6261589527130127\n",
      "Train: Epoch [18], Batch [906/938], Loss: 0.38306093215942383\n",
      "Train: Epoch [18], Batch [907/938], Loss: 0.6352057456970215\n",
      "Train: Epoch [18], Batch [908/938], Loss: 0.7654966115951538\n",
      "Train: Epoch [18], Batch [909/938], Loss: 0.9751161932945251\n",
      "Train: Epoch [18], Batch [910/938], Loss: 0.7506779432296753\n",
      "Train: Epoch [18], Batch [911/938], Loss: 0.6728959679603577\n",
      "Train: Epoch [18], Batch [912/938], Loss: 0.5386853814125061\n",
      "Train: Epoch [18], Batch [913/938], Loss: 0.6659780144691467\n",
      "Train: Epoch [18], Batch [914/938], Loss: 0.5397970080375671\n",
      "Train: Epoch [18], Batch [915/938], Loss: 0.49136245250701904\n",
      "Train: Epoch [18], Batch [916/938], Loss: 0.9368395805358887\n",
      "Train: Epoch [18], Batch [917/938], Loss: 0.3771447539329529\n",
      "Train: Epoch [18], Batch [918/938], Loss: 0.64066481590271\n",
      "Train: Epoch [18], Batch [919/938], Loss: 0.6934448480606079\n",
      "Train: Epoch [18], Batch [920/938], Loss: 0.8273265957832336\n",
      "Train: Epoch [18], Batch [921/938], Loss: 0.5236005783081055\n",
      "Train: Epoch [18], Batch [922/938], Loss: 0.5373203754425049\n",
      "Train: Epoch [18], Batch [923/938], Loss: 0.4307107627391815\n",
      "Train: Epoch [18], Batch [924/938], Loss: 0.7340905070304871\n",
      "Train: Epoch [18], Batch [925/938], Loss: 0.6789969205856323\n",
      "Train: Epoch [18], Batch [926/938], Loss: 0.6434429287910461\n",
      "Train: Epoch [18], Batch [927/938], Loss: 0.5475417375564575\n",
      "Train: Epoch [18], Batch [928/938], Loss: 0.8406803011894226\n",
      "Train: Epoch [18], Batch [929/938], Loss: 0.9652425050735474\n",
      "Train: Epoch [18], Batch [930/938], Loss: 0.5215646624565125\n",
      "Train: Epoch [18], Batch [931/938], Loss: 0.80174720287323\n",
      "Train: Epoch [18], Batch [932/938], Loss: 0.5730774998664856\n",
      "Train: Epoch [18], Batch [933/938], Loss: 0.7263396382331848\n",
      "Train: Epoch [18], Batch [934/938], Loss: 0.7806600332260132\n",
      "Train: Epoch [18], Batch [935/938], Loss: 0.6933083534240723\n",
      "Train: Epoch [18], Batch [936/938], Loss: 0.8883522748947144\n",
      "Train: Epoch [18], Batch [937/938], Loss: 0.78657066822052\n",
      "Train: Epoch [18], Batch [938/938], Loss: 0.7846032381057739\n",
      "Accuracy of train set: 0.7999\n",
      "Validation: Epoch [18], Batch [1/938], Loss: 0.9457135796546936\n",
      "Validation: Epoch [18], Batch [2/938], Loss: 0.932565450668335\n",
      "Validation: Epoch [18], Batch [3/938], Loss: 0.6429826021194458\n",
      "Validation: Epoch [18], Batch [4/938], Loss: 0.5820803046226501\n",
      "Validation: Epoch [18], Batch [5/938], Loss: 0.8829977512359619\n",
      "Validation: Epoch [18], Batch [6/938], Loss: 0.4788716435432434\n",
      "Validation: Epoch [18], Batch [7/938], Loss: 1.0440146923065186\n",
      "Validation: Epoch [18], Batch [8/938], Loss: 0.6652476787567139\n",
      "Validation: Epoch [18], Batch [9/938], Loss: 0.8255658149719238\n",
      "Validation: Epoch [18], Batch [10/938], Loss: 0.50295490026474\n",
      "Validation: Epoch [18], Batch [11/938], Loss: 0.6519097685813904\n",
      "Validation: Epoch [18], Batch [12/938], Loss: 0.7075297832489014\n",
      "Validation: Epoch [18], Batch [13/938], Loss: 0.7474043369293213\n",
      "Validation: Epoch [18], Batch [14/938], Loss: 0.8739562034606934\n",
      "Validation: Epoch [18], Batch [15/938], Loss: 1.055588722229004\n",
      "Validation: Epoch [18], Batch [16/938], Loss: 0.6743186116218567\n",
      "Validation: Epoch [18], Batch [17/938], Loss: 0.70694899559021\n",
      "Validation: Epoch [18], Batch [18/938], Loss: 0.7964479923248291\n",
      "Validation: Epoch [18], Batch [19/938], Loss: 1.045253872871399\n",
      "Validation: Epoch [18], Batch [20/938], Loss: 0.8306213617324829\n",
      "Validation: Epoch [18], Batch [21/938], Loss: 0.7101891040802002\n",
      "Validation: Epoch [18], Batch [22/938], Loss: 0.6219700574874878\n",
      "Validation: Epoch [18], Batch [23/938], Loss: 0.7057453393936157\n",
      "Validation: Epoch [18], Batch [24/938], Loss: 0.6425442695617676\n",
      "Validation: Epoch [18], Batch [25/938], Loss: 0.5813131928443909\n",
      "Validation: Epoch [18], Batch [26/938], Loss: 0.780441164970398\n",
      "Validation: Epoch [18], Batch [27/938], Loss: 0.7269830107688904\n",
      "Validation: Epoch [18], Batch [28/938], Loss: 0.6211832761764526\n",
      "Validation: Epoch [18], Batch [29/938], Loss: 0.6792901158332825\n",
      "Validation: Epoch [18], Batch [30/938], Loss: 0.6431169509887695\n",
      "Validation: Epoch [18], Batch [31/938], Loss: 0.8893812894821167\n",
      "Validation: Epoch [18], Batch [32/938], Loss: 1.051353096961975\n",
      "Validation: Epoch [18], Batch [33/938], Loss: 1.2747117280960083\n",
      "Validation: Epoch [18], Batch [34/938], Loss: 0.7424023151397705\n",
      "Validation: Epoch [18], Batch [35/938], Loss: 0.6698423624038696\n",
      "Validation: Epoch [18], Batch [36/938], Loss: 0.8356146812438965\n",
      "Validation: Epoch [18], Batch [37/938], Loss: 0.4377708435058594\n",
      "Validation: Epoch [18], Batch [38/938], Loss: 0.8250644207000732\n",
      "Validation: Epoch [18], Batch [39/938], Loss: 0.5754933953285217\n",
      "Validation: Epoch [18], Batch [40/938], Loss: 0.6192585229873657\n",
      "Validation: Epoch [18], Batch [41/938], Loss: 0.8681929111480713\n",
      "Validation: Epoch [18], Batch [42/938], Loss: 0.5015706419944763\n",
      "Validation: Epoch [18], Batch [43/938], Loss: 0.7538858652114868\n",
      "Validation: Epoch [18], Batch [44/938], Loss: 0.6590327620506287\n",
      "Validation: Epoch [18], Batch [45/938], Loss: 0.8778624534606934\n",
      "Validation: Epoch [18], Batch [46/938], Loss: 0.6376944780349731\n",
      "Validation: Epoch [18], Batch [47/938], Loss: 0.8076549768447876\n",
      "Validation: Epoch [18], Batch [48/938], Loss: 0.7244153022766113\n",
      "Validation: Epoch [18], Batch [49/938], Loss: 0.7221534848213196\n",
      "Validation: Epoch [18], Batch [50/938], Loss: 0.7910414934158325\n",
      "Validation: Epoch [18], Batch [51/938], Loss: 0.8725572228431702\n",
      "Validation: Epoch [18], Batch [52/938], Loss: 0.6316207647323608\n",
      "Validation: Epoch [18], Batch [53/938], Loss: 0.7142791748046875\n",
      "Validation: Epoch [18], Batch [54/938], Loss: 0.9615994691848755\n",
      "Validation: Epoch [18], Batch [55/938], Loss: 0.5730008482933044\n",
      "Validation: Epoch [18], Batch [56/938], Loss: 0.692634105682373\n",
      "Validation: Epoch [18], Batch [57/938], Loss: 0.6981626749038696\n",
      "Validation: Epoch [18], Batch [58/938], Loss: 0.6028727293014526\n",
      "Validation: Epoch [18], Batch [59/938], Loss: 0.690329372882843\n",
      "Validation: Epoch [18], Batch [60/938], Loss: 0.8786045908927917\n",
      "Validation: Epoch [18], Batch [61/938], Loss: 0.68204265832901\n",
      "Validation: Epoch [18], Batch [62/938], Loss: 0.6835157871246338\n",
      "Validation: Epoch [18], Batch [63/938], Loss: 0.6742624044418335\n",
      "Validation: Epoch [18], Batch [64/938], Loss: 0.682453989982605\n",
      "Validation: Epoch [18], Batch [65/938], Loss: 0.6918456554412842\n",
      "Validation: Epoch [18], Batch [66/938], Loss: 0.7184916734695435\n",
      "Validation: Epoch [18], Batch [67/938], Loss: 0.8473150730133057\n",
      "Validation: Epoch [18], Batch [68/938], Loss: 0.8186847567558289\n",
      "Validation: Epoch [18], Batch [69/938], Loss: 0.9330419301986694\n",
      "Validation: Epoch [18], Batch [70/938], Loss: 0.8282377123832703\n",
      "Validation: Epoch [18], Batch [71/938], Loss: 0.7405160665512085\n",
      "Validation: Epoch [18], Batch [72/938], Loss: 0.5659874677658081\n",
      "Validation: Epoch [18], Batch [73/938], Loss: 0.8531060814857483\n",
      "Validation: Epoch [18], Batch [74/938], Loss: 0.6124963760375977\n",
      "Validation: Epoch [18], Batch [75/938], Loss: 0.8452779650688171\n",
      "Validation: Epoch [18], Batch [76/938], Loss: 0.8689937591552734\n",
      "Validation: Epoch [18], Batch [77/938], Loss: 0.9261517524719238\n",
      "Validation: Epoch [18], Batch [78/938], Loss: 0.7461337447166443\n",
      "Validation: Epoch [18], Batch [79/938], Loss: 0.7929760217666626\n",
      "Validation: Epoch [18], Batch [80/938], Loss: 0.9078544974327087\n",
      "Validation: Epoch [18], Batch [81/938], Loss: 0.3918449580669403\n",
      "Validation: Epoch [18], Batch [82/938], Loss: 0.6958041191101074\n",
      "Validation: Epoch [18], Batch [83/938], Loss: 0.6333260536193848\n",
      "Validation: Epoch [18], Batch [84/938], Loss: 0.8030985593795776\n",
      "Validation: Epoch [18], Batch [85/938], Loss: 0.7133557796478271\n",
      "Validation: Epoch [18], Batch [86/938], Loss: 0.5283834934234619\n",
      "Validation: Epoch [18], Batch [87/938], Loss: 0.6909688711166382\n",
      "Validation: Epoch [18], Batch [88/938], Loss: 0.8265084028244019\n",
      "Validation: Epoch [18], Batch [89/938], Loss: 0.577279806137085\n",
      "Validation: Epoch [18], Batch [90/938], Loss: 0.8333296775817871\n",
      "Validation: Epoch [18], Batch [91/938], Loss: 0.8216613531112671\n",
      "Validation: Epoch [18], Batch [92/938], Loss: 0.7441530227661133\n",
      "Validation: Epoch [18], Batch [93/938], Loss: 0.8438218235969543\n",
      "Validation: Epoch [18], Batch [94/938], Loss: 0.46055513620376587\n",
      "Validation: Epoch [18], Batch [95/938], Loss: 0.7751186490058899\n",
      "Validation: Epoch [18], Batch [96/938], Loss: 0.8523671627044678\n",
      "Validation: Epoch [18], Batch [97/938], Loss: 0.7909767627716064\n",
      "Validation: Epoch [18], Batch [98/938], Loss: 0.7511751055717468\n",
      "Validation: Epoch [18], Batch [99/938], Loss: 0.7246167659759521\n",
      "Validation: Epoch [18], Batch [100/938], Loss: 0.6152024269104004\n",
      "Validation: Epoch [18], Batch [101/938], Loss: 0.5718741416931152\n",
      "Validation: Epoch [18], Batch [102/938], Loss: 0.8056390285491943\n",
      "Validation: Epoch [18], Batch [103/938], Loss: 0.9451431035995483\n",
      "Validation: Epoch [18], Batch [104/938], Loss: 0.4920358657836914\n",
      "Validation: Epoch [18], Batch [105/938], Loss: 0.6575450897216797\n",
      "Validation: Epoch [18], Batch [106/938], Loss: 0.6928908824920654\n",
      "Validation: Epoch [18], Batch [107/938], Loss: 0.6642581224441528\n",
      "Validation: Epoch [18], Batch [108/938], Loss: 0.6498135924339294\n",
      "Validation: Epoch [18], Batch [109/938], Loss: 0.6047720909118652\n",
      "Validation: Epoch [18], Batch [110/938], Loss: 0.6493239402770996\n",
      "Validation: Epoch [18], Batch [111/938], Loss: 0.6720149517059326\n",
      "Validation: Epoch [18], Batch [112/938], Loss: 0.6425873041152954\n",
      "Validation: Epoch [18], Batch [113/938], Loss: 0.6991082429885864\n",
      "Validation: Epoch [18], Batch [114/938], Loss: 0.7303464412689209\n",
      "Validation: Epoch [18], Batch [115/938], Loss: 0.6256898045539856\n",
      "Validation: Epoch [18], Batch [116/938], Loss: 0.8675373792648315\n",
      "Validation: Epoch [18], Batch [117/938], Loss: 0.5455326437950134\n",
      "Validation: Epoch [18], Batch [118/938], Loss: 0.603249192237854\n",
      "Validation: Epoch [18], Batch [119/938], Loss: 0.6566237807273865\n",
      "Validation: Epoch [18], Batch [120/938], Loss: 0.7577809691429138\n",
      "Validation: Epoch [18], Batch [121/938], Loss: 0.7619566917419434\n",
      "Validation: Epoch [18], Batch [122/938], Loss: 0.7215126156806946\n",
      "Validation: Epoch [18], Batch [123/938], Loss: 0.6571100354194641\n",
      "Validation: Epoch [18], Batch [124/938], Loss: 0.5294280648231506\n",
      "Validation: Epoch [18], Batch [125/938], Loss: 0.6981684565544128\n",
      "Validation: Epoch [18], Batch [126/938], Loss: 1.0277104377746582\n",
      "Validation: Epoch [18], Batch [127/938], Loss: 0.599801778793335\n",
      "Validation: Epoch [18], Batch [128/938], Loss: 0.7820451259613037\n",
      "Validation: Epoch [18], Batch [129/938], Loss: 0.6797450184822083\n",
      "Validation: Epoch [18], Batch [130/938], Loss: 0.5922982692718506\n",
      "Validation: Epoch [18], Batch [131/938], Loss: 0.8528407216072083\n",
      "Validation: Epoch [18], Batch [132/938], Loss: 0.8343064785003662\n",
      "Validation: Epoch [18], Batch [133/938], Loss: 0.8895034790039062\n",
      "Validation: Epoch [18], Batch [134/938], Loss: 0.5569289326667786\n",
      "Validation: Epoch [18], Batch [135/938], Loss: 0.6792858839035034\n",
      "Validation: Epoch [18], Batch [136/938], Loss: 0.7096807956695557\n",
      "Validation: Epoch [18], Batch [137/938], Loss: 0.441413938999176\n",
      "Validation: Epoch [18], Batch [138/938], Loss: 0.620184063911438\n",
      "Validation: Epoch [18], Batch [139/938], Loss: 0.5822075605392456\n",
      "Validation: Epoch [18], Batch [140/938], Loss: 0.8502113223075867\n",
      "Validation: Epoch [18], Batch [141/938], Loss: 0.7980002164840698\n",
      "Validation: Epoch [18], Batch [142/938], Loss: 0.7930847406387329\n",
      "Validation: Epoch [18], Batch [143/938], Loss: 0.6291651129722595\n",
      "Validation: Epoch [18], Batch [144/938], Loss: 0.6576247215270996\n",
      "Validation: Epoch [18], Batch [145/938], Loss: 0.8102002143859863\n",
      "Validation: Epoch [18], Batch [146/938], Loss: 0.7169572114944458\n",
      "Validation: Epoch [18], Batch [147/938], Loss: 0.5125664472579956\n",
      "Validation: Epoch [18], Batch [148/938], Loss: 0.747655987739563\n",
      "Validation: Epoch [18], Batch [149/938], Loss: 0.8473498821258545\n",
      "Validation: Epoch [18], Batch [150/938], Loss: 0.7630107998847961\n",
      "Validation: Epoch [18], Batch [151/938], Loss: 0.605475664138794\n",
      "Validation: Epoch [18], Batch [152/938], Loss: 1.0233068466186523\n",
      "Validation: Epoch [18], Batch [153/938], Loss: 0.7455441355705261\n",
      "Validation: Epoch [18], Batch [154/938], Loss: 0.7418050765991211\n",
      "Validation: Epoch [18], Batch [155/938], Loss: 0.7328627705574036\n",
      "Validation: Epoch [18], Batch [156/938], Loss: 0.5994614362716675\n",
      "Validation: Epoch [18], Batch [157/938], Loss: 0.7137219905853271\n",
      "Validation: Epoch [18], Batch [158/938], Loss: 0.7269425988197327\n",
      "Validation: Epoch [18], Batch [159/938], Loss: 0.7015289664268494\n",
      "Validation: Epoch [18], Batch [160/938], Loss: 0.6697816848754883\n",
      "Validation: Epoch [18], Batch [161/938], Loss: 0.8421548008918762\n",
      "Validation: Epoch [18], Batch [162/938], Loss: 0.5376696586608887\n",
      "Validation: Epoch [18], Batch [163/938], Loss: 0.5416435599327087\n",
      "Validation: Epoch [18], Batch [164/938], Loss: 0.6872808933258057\n",
      "Validation: Epoch [18], Batch [165/938], Loss: 0.5630731582641602\n",
      "Validation: Epoch [18], Batch [166/938], Loss: 0.5740746855735779\n",
      "Validation: Epoch [18], Batch [167/938], Loss: 0.6161109209060669\n",
      "Validation: Epoch [18], Batch [168/938], Loss: 0.5448606610298157\n",
      "Validation: Epoch [18], Batch [169/938], Loss: 0.829939603805542\n",
      "Validation: Epoch [18], Batch [170/938], Loss: 0.7008636593818665\n",
      "Validation: Epoch [18], Batch [171/938], Loss: 0.9687193632125854\n",
      "Validation: Epoch [18], Batch [172/938], Loss: 0.5956442356109619\n",
      "Validation: Epoch [18], Batch [173/938], Loss: 0.5555475354194641\n",
      "Validation: Epoch [18], Batch [174/938], Loss: 0.5717212557792664\n",
      "Validation: Epoch [18], Batch [175/938], Loss: 0.7086457014083862\n",
      "Validation: Epoch [18], Batch [176/938], Loss: 0.5870319604873657\n",
      "Validation: Epoch [18], Batch [177/938], Loss: 0.7622955441474915\n",
      "Validation: Epoch [18], Batch [178/938], Loss: 0.7935887575149536\n",
      "Validation: Epoch [18], Batch [179/938], Loss: 0.7641332149505615\n",
      "Validation: Epoch [18], Batch [180/938], Loss: 1.0420963764190674\n",
      "Validation: Epoch [18], Batch [181/938], Loss: 0.8297377228736877\n",
      "Validation: Epoch [18], Batch [182/938], Loss: 0.8244792222976685\n",
      "Validation: Epoch [18], Batch [183/938], Loss: 0.6229803562164307\n",
      "Validation: Epoch [18], Batch [184/938], Loss: 0.7164973020553589\n",
      "Validation: Epoch [18], Batch [185/938], Loss: 0.5876556038856506\n",
      "Validation: Epoch [18], Batch [186/938], Loss: 0.6110411882400513\n",
      "Validation: Epoch [18], Batch [187/938], Loss: 0.7433004379272461\n",
      "Validation: Epoch [18], Batch [188/938], Loss: 0.6113758087158203\n",
      "Validation: Epoch [18], Batch [189/938], Loss: 0.6232036352157593\n",
      "Validation: Epoch [18], Batch [190/938], Loss: 0.6510266065597534\n",
      "Validation: Epoch [18], Batch [191/938], Loss: 0.8828927874565125\n",
      "Validation: Epoch [18], Batch [192/938], Loss: 0.8048861026763916\n",
      "Validation: Epoch [18], Batch [193/938], Loss: 0.64932781457901\n",
      "Validation: Epoch [18], Batch [194/938], Loss: 0.7088823318481445\n",
      "Validation: Epoch [18], Batch [195/938], Loss: 1.0319185256958008\n",
      "Validation: Epoch [18], Batch [196/938], Loss: 0.6540159583091736\n",
      "Validation: Epoch [18], Batch [197/938], Loss: 0.5446178317070007\n",
      "Validation: Epoch [18], Batch [198/938], Loss: 0.9624283313751221\n",
      "Validation: Epoch [18], Batch [199/938], Loss: 0.8388029336929321\n",
      "Validation: Epoch [18], Batch [200/938], Loss: 0.44818276166915894\n",
      "Validation: Epoch [18], Batch [201/938], Loss: 0.924830973148346\n",
      "Validation: Epoch [18], Batch [202/938], Loss: 0.4956248104572296\n",
      "Validation: Epoch [18], Batch [203/938], Loss: 0.5677926540374756\n",
      "Validation: Epoch [18], Batch [204/938], Loss: 0.7481154799461365\n",
      "Validation: Epoch [18], Batch [205/938], Loss: 0.5623151063919067\n",
      "Validation: Epoch [18], Batch [206/938], Loss: 0.5932435989379883\n",
      "Validation: Epoch [18], Batch [207/938], Loss: 0.7523581981658936\n",
      "Validation: Epoch [18], Batch [208/938], Loss: 0.793070375919342\n",
      "Validation: Epoch [18], Batch [209/938], Loss: 0.8005739450454712\n",
      "Validation: Epoch [18], Batch [210/938], Loss: 0.8924709558486938\n",
      "Validation: Epoch [18], Batch [211/938], Loss: 0.7397758960723877\n",
      "Validation: Epoch [18], Batch [212/938], Loss: 0.7153644561767578\n",
      "Validation: Epoch [18], Batch [213/938], Loss: 0.9565587043762207\n",
      "Validation: Epoch [18], Batch [214/938], Loss: 0.6639577150344849\n",
      "Validation: Epoch [18], Batch [215/938], Loss: 0.7625336050987244\n",
      "Validation: Epoch [18], Batch [216/938], Loss: 0.6889510154724121\n",
      "Validation: Epoch [18], Batch [217/938], Loss: 0.6880432367324829\n",
      "Validation: Epoch [18], Batch [218/938], Loss: 0.8453819155693054\n",
      "Validation: Epoch [18], Batch [219/938], Loss: 1.0047286748886108\n",
      "Validation: Epoch [18], Batch [220/938], Loss: 0.6396915912628174\n",
      "Validation: Epoch [18], Batch [221/938], Loss: 0.758757472038269\n",
      "Validation: Epoch [18], Batch [222/938], Loss: 0.7365998029708862\n",
      "Validation: Epoch [18], Batch [223/938], Loss: 0.5772925019264221\n",
      "Validation: Epoch [18], Batch [224/938], Loss: 0.6510474681854248\n",
      "Validation: Epoch [18], Batch [225/938], Loss: 0.9043402075767517\n",
      "Validation: Epoch [18], Batch [226/938], Loss: 0.9226927161216736\n",
      "Validation: Epoch [18], Batch [227/938], Loss: 0.8022620677947998\n",
      "Validation: Epoch [18], Batch [228/938], Loss: 0.7650812864303589\n",
      "Validation: Epoch [18], Batch [229/938], Loss: 0.6247346997261047\n",
      "Validation: Epoch [18], Batch [230/938], Loss: 0.7193687558174133\n",
      "Validation: Epoch [18], Batch [231/938], Loss: 0.6153092980384827\n",
      "Validation: Epoch [18], Batch [232/938], Loss: 0.8997536301612854\n",
      "Validation: Epoch [18], Batch [233/938], Loss: 0.7887245416641235\n",
      "Validation: Epoch [18], Batch [234/938], Loss: 0.8630090355873108\n",
      "Validation: Epoch [18], Batch [235/938], Loss: 0.7351348996162415\n",
      "Validation: Epoch [18], Batch [236/938], Loss: 0.6713545322418213\n",
      "Validation: Epoch [18], Batch [237/938], Loss: 0.578574538230896\n",
      "Validation: Epoch [18], Batch [238/938], Loss: 0.7748637199401855\n",
      "Validation: Epoch [18], Batch [239/938], Loss: 0.6726104021072388\n",
      "Validation: Epoch [18], Batch [240/938], Loss: 0.834203839302063\n",
      "Validation: Epoch [18], Batch [241/938], Loss: 0.5750486850738525\n",
      "Validation: Epoch [18], Batch [242/938], Loss: 0.7583684921264648\n",
      "Validation: Epoch [18], Batch [243/938], Loss: 0.9930539727210999\n",
      "Validation: Epoch [18], Batch [244/938], Loss: 1.0196704864501953\n",
      "Validation: Epoch [18], Batch [245/938], Loss: 0.9272826910018921\n",
      "Validation: Epoch [18], Batch [246/938], Loss: 0.5071386694908142\n",
      "Validation: Epoch [18], Batch [247/938], Loss: 0.8037819862365723\n",
      "Validation: Epoch [18], Batch [248/938], Loss: 1.0530123710632324\n",
      "Validation: Epoch [18], Batch [249/938], Loss: 0.6844009160995483\n",
      "Validation: Epoch [18], Batch [250/938], Loss: 0.9312493801116943\n",
      "Validation: Epoch [18], Batch [251/938], Loss: 0.7949708104133606\n",
      "Validation: Epoch [18], Batch [252/938], Loss: 0.770190954208374\n",
      "Validation: Epoch [18], Batch [253/938], Loss: 0.5336143374443054\n",
      "Validation: Epoch [18], Batch [254/938], Loss: 0.5992617011070251\n",
      "Validation: Epoch [18], Batch [255/938], Loss: 0.843013346195221\n",
      "Validation: Epoch [18], Batch [256/938], Loss: 0.7141782641410828\n",
      "Validation: Epoch [18], Batch [257/938], Loss: 0.7397468686103821\n",
      "Validation: Epoch [18], Batch [258/938], Loss: 0.6299663186073303\n",
      "Validation: Epoch [18], Batch [259/938], Loss: 0.9457460045814514\n",
      "Validation: Epoch [18], Batch [260/938], Loss: 0.7109570503234863\n",
      "Validation: Epoch [18], Batch [261/938], Loss: 0.5939148664474487\n",
      "Validation: Epoch [18], Batch [262/938], Loss: 0.7700048685073853\n",
      "Validation: Epoch [18], Batch [263/938], Loss: 0.5855268836021423\n",
      "Validation: Epoch [18], Batch [264/938], Loss: 0.6507775783538818\n",
      "Validation: Epoch [18], Batch [265/938], Loss: 0.6862398982048035\n",
      "Validation: Epoch [18], Batch [266/938], Loss: 0.7889158129692078\n",
      "Validation: Epoch [18], Batch [267/938], Loss: 0.6052020788192749\n",
      "Validation: Epoch [18], Batch [268/938], Loss: 0.9069393873214722\n",
      "Validation: Epoch [18], Batch [269/938], Loss: 0.675347089767456\n",
      "Validation: Epoch [18], Batch [270/938], Loss: 0.7797642946243286\n",
      "Validation: Epoch [18], Batch [271/938], Loss: 0.8049062490463257\n",
      "Validation: Epoch [18], Batch [272/938], Loss: 0.6111588478088379\n",
      "Validation: Epoch [18], Batch [273/938], Loss: 0.41535255312919617\n",
      "Validation: Epoch [18], Batch [274/938], Loss: 0.9550713300704956\n",
      "Validation: Epoch [18], Batch [275/938], Loss: 0.5678750872612\n",
      "Validation: Epoch [18], Batch [276/938], Loss: 0.9386869668960571\n",
      "Validation: Epoch [18], Batch [277/938], Loss: 0.9466840028762817\n",
      "Validation: Epoch [18], Batch [278/938], Loss: 0.5911511778831482\n",
      "Validation: Epoch [18], Batch [279/938], Loss: 0.7569822072982788\n",
      "Validation: Epoch [18], Batch [280/938], Loss: 0.8685144186019897\n",
      "Validation: Epoch [18], Batch [281/938], Loss: 0.5204442739486694\n",
      "Validation: Epoch [18], Batch [282/938], Loss: 0.7216013073921204\n",
      "Validation: Epoch [18], Batch [283/938], Loss: 0.6581384539604187\n",
      "Validation: Epoch [18], Batch [284/938], Loss: 0.7694457173347473\n",
      "Validation: Epoch [18], Batch [285/938], Loss: 0.9044995903968811\n",
      "Validation: Epoch [18], Batch [286/938], Loss: 0.7115321755409241\n",
      "Validation: Epoch [18], Batch [287/938], Loss: 0.8229402303695679\n",
      "Validation: Epoch [18], Batch [288/938], Loss: 0.6885311007499695\n",
      "Validation: Epoch [18], Batch [289/938], Loss: 0.8122060894966125\n",
      "Validation: Epoch [18], Batch [290/938], Loss: 0.8153162002563477\n",
      "Validation: Epoch [18], Batch [291/938], Loss: 0.8652490973472595\n",
      "Validation: Epoch [18], Batch [292/938], Loss: 0.6594959497451782\n",
      "Validation: Epoch [18], Batch [293/938], Loss: 0.8688948750495911\n",
      "Validation: Epoch [18], Batch [294/938], Loss: 0.6644506454467773\n",
      "Validation: Epoch [18], Batch [295/938], Loss: 0.720934271812439\n",
      "Validation: Epoch [18], Batch [296/938], Loss: 0.5343793630599976\n",
      "Validation: Epoch [18], Batch [297/938], Loss: 0.7322074770927429\n",
      "Validation: Epoch [18], Batch [298/938], Loss: 0.7836363315582275\n",
      "Validation: Epoch [18], Batch [299/938], Loss: 0.8976297974586487\n",
      "Validation: Epoch [18], Batch [300/938], Loss: 0.9418550729751587\n",
      "Validation: Epoch [18], Batch [301/938], Loss: 0.675868034362793\n",
      "Validation: Epoch [18], Batch [302/938], Loss: 0.8161303997039795\n",
      "Validation: Epoch [18], Batch [303/938], Loss: 0.6968002319335938\n",
      "Validation: Epoch [18], Batch [304/938], Loss: 0.8096605539321899\n",
      "Validation: Epoch [18], Batch [305/938], Loss: 0.4895624816417694\n",
      "Validation: Epoch [18], Batch [306/938], Loss: 0.6921850442886353\n",
      "Validation: Epoch [18], Batch [307/938], Loss: 0.7426464557647705\n",
      "Validation: Epoch [18], Batch [308/938], Loss: 0.9575260877609253\n",
      "Validation: Epoch [18], Batch [309/938], Loss: 0.6017436981201172\n",
      "Validation: Epoch [18], Batch [310/938], Loss: 0.8784263730049133\n",
      "Validation: Epoch [18], Batch [311/938], Loss: 0.7428086996078491\n",
      "Validation: Epoch [18], Batch [312/938], Loss: 0.5515885949134827\n",
      "Validation: Epoch [18], Batch [313/938], Loss: 0.8851911425590515\n",
      "Validation: Epoch [18], Batch [314/938], Loss: 0.6031596660614014\n",
      "Validation: Epoch [18], Batch [315/938], Loss: 0.7075693607330322\n",
      "Validation: Epoch [18], Batch [316/938], Loss: 0.7758611440658569\n",
      "Validation: Epoch [18], Batch [317/938], Loss: 0.6715648770332336\n",
      "Validation: Epoch [18], Batch [318/938], Loss: 0.5143302083015442\n",
      "Validation: Epoch [18], Batch [319/938], Loss: 0.7480268478393555\n",
      "Validation: Epoch [18], Batch [320/938], Loss: 0.7213178277015686\n",
      "Validation: Epoch [18], Batch [321/938], Loss: 0.5360249280929565\n",
      "Validation: Epoch [18], Batch [322/938], Loss: 0.8111048936843872\n",
      "Validation: Epoch [18], Batch [323/938], Loss: 0.5930207371711731\n",
      "Validation: Epoch [18], Batch [324/938], Loss: 0.6005467772483826\n",
      "Validation: Epoch [18], Batch [325/938], Loss: 0.5161141157150269\n",
      "Validation: Epoch [18], Batch [326/938], Loss: 0.7883989810943604\n",
      "Validation: Epoch [18], Batch [327/938], Loss: 0.7055475115776062\n",
      "Validation: Epoch [18], Batch [328/938], Loss: 0.7716148495674133\n",
      "Validation: Epoch [18], Batch [329/938], Loss: 0.7674559950828552\n",
      "Validation: Epoch [18], Batch [330/938], Loss: 0.8174941539764404\n",
      "Validation: Epoch [18], Batch [331/938], Loss: 0.8689780235290527\n",
      "Validation: Epoch [18], Batch [332/938], Loss: 0.5525608658790588\n",
      "Validation: Epoch [18], Batch [333/938], Loss: 0.7092700004577637\n",
      "Validation: Epoch [18], Batch [334/938], Loss: 0.6754537224769592\n",
      "Validation: Epoch [18], Batch [335/938], Loss: 0.8869175910949707\n",
      "Validation: Epoch [18], Batch [336/938], Loss: 0.681979238986969\n",
      "Validation: Epoch [18], Batch [337/938], Loss: 0.9596307277679443\n",
      "Validation: Epoch [18], Batch [338/938], Loss: 1.0476064682006836\n",
      "Validation: Epoch [18], Batch [339/938], Loss: 0.7120058536529541\n",
      "Validation: Epoch [18], Batch [340/938], Loss: 1.0043179988861084\n",
      "Validation: Epoch [18], Batch [341/938], Loss: 0.7796544432640076\n",
      "Validation: Epoch [18], Batch [342/938], Loss: 0.46282219886779785\n",
      "Validation: Epoch [18], Batch [343/938], Loss: 0.7366147637367249\n",
      "Validation: Epoch [18], Batch [344/938], Loss: 0.7249537706375122\n",
      "Validation: Epoch [18], Batch [345/938], Loss: 0.8531647324562073\n",
      "Validation: Epoch [18], Batch [346/938], Loss: 0.9147350192070007\n",
      "Validation: Epoch [18], Batch [347/938], Loss: 0.9497265815734863\n",
      "Validation: Epoch [18], Batch [348/938], Loss: 0.7525991201400757\n",
      "Validation: Epoch [18], Batch [349/938], Loss: 1.0406229496002197\n",
      "Validation: Epoch [18], Batch [350/938], Loss: 1.0499767065048218\n",
      "Validation: Epoch [18], Batch [351/938], Loss: 0.540546178817749\n",
      "Validation: Epoch [18], Batch [352/938], Loss: 0.6062621474266052\n",
      "Validation: Epoch [18], Batch [353/938], Loss: 0.7148405313491821\n",
      "Validation: Epoch [18], Batch [354/938], Loss: 0.8520246744155884\n",
      "Validation: Epoch [18], Batch [355/938], Loss: 0.6473093032836914\n",
      "Validation: Epoch [18], Batch [356/938], Loss: 0.7074390053749084\n",
      "Validation: Epoch [18], Batch [357/938], Loss: 0.5318962931632996\n",
      "Validation: Epoch [18], Batch [358/938], Loss: 0.5843661427497864\n",
      "Validation: Epoch [18], Batch [359/938], Loss: 0.8144519925117493\n",
      "Validation: Epoch [18], Batch [360/938], Loss: 0.6945977807044983\n",
      "Validation: Epoch [18], Batch [361/938], Loss: 0.787390947341919\n",
      "Validation: Epoch [18], Batch [362/938], Loss: 0.5892842411994934\n",
      "Validation: Epoch [18], Batch [363/938], Loss: 0.6951935291290283\n",
      "Validation: Epoch [18], Batch [364/938], Loss: 0.7563300132751465\n",
      "Validation: Epoch [18], Batch [365/938], Loss: 0.6864711046218872\n",
      "Validation: Epoch [18], Batch [366/938], Loss: 0.6848361492156982\n",
      "Validation: Epoch [18], Batch [367/938], Loss: 0.6732591390609741\n",
      "Validation: Epoch [18], Batch [368/938], Loss: 0.6096092462539673\n",
      "Validation: Epoch [18], Batch [369/938], Loss: 1.0675941705703735\n",
      "Validation: Epoch [18], Batch [370/938], Loss: 0.9266008138656616\n",
      "Validation: Epoch [18], Batch [371/938], Loss: 0.6168825626373291\n",
      "Validation: Epoch [18], Batch [372/938], Loss: 0.5130055546760559\n",
      "Validation: Epoch [18], Batch [373/938], Loss: 0.5141093134880066\n",
      "Validation: Epoch [18], Batch [374/938], Loss: 0.6969801783561707\n",
      "Validation: Epoch [18], Batch [375/938], Loss: 0.7593240141868591\n",
      "Validation: Epoch [18], Batch [376/938], Loss: 0.8142451047897339\n",
      "Validation: Epoch [18], Batch [377/938], Loss: 0.7692474126815796\n",
      "Validation: Epoch [18], Batch [378/938], Loss: 0.6905999183654785\n",
      "Validation: Epoch [18], Batch [379/938], Loss: 0.7820348739624023\n",
      "Validation: Epoch [18], Batch [380/938], Loss: 0.647582471370697\n",
      "Validation: Epoch [18], Batch [381/938], Loss: 0.6982081532478333\n",
      "Validation: Epoch [18], Batch [382/938], Loss: 0.6123239398002625\n",
      "Validation: Epoch [18], Batch [383/938], Loss: 0.8240565657615662\n",
      "Validation: Epoch [18], Batch [384/938], Loss: 0.9555999040603638\n",
      "Validation: Epoch [18], Batch [385/938], Loss: 0.8074058294296265\n",
      "Validation: Epoch [18], Batch [386/938], Loss: 0.741936981678009\n",
      "Validation: Epoch [18], Batch [387/938], Loss: 0.9748600721359253\n",
      "Validation: Epoch [18], Batch [388/938], Loss: 1.1078863143920898\n",
      "Validation: Epoch [18], Batch [389/938], Loss: 0.7018238306045532\n",
      "Validation: Epoch [18], Batch [390/938], Loss: 0.8156961798667908\n",
      "Validation: Epoch [18], Batch [391/938], Loss: 0.7356310486793518\n",
      "Validation: Epoch [18], Batch [392/938], Loss: 0.7571356296539307\n",
      "Validation: Epoch [18], Batch [393/938], Loss: 0.6388046145439148\n",
      "Validation: Epoch [18], Batch [394/938], Loss: 0.8593870997428894\n",
      "Validation: Epoch [18], Batch [395/938], Loss: 0.8023245334625244\n",
      "Validation: Epoch [18], Batch [396/938], Loss: 0.5992025136947632\n",
      "Validation: Epoch [18], Batch [397/938], Loss: 1.0463135242462158\n",
      "Validation: Epoch [18], Batch [398/938], Loss: 0.7021857500076294\n",
      "Validation: Epoch [18], Batch [399/938], Loss: 0.5680236220359802\n",
      "Validation: Epoch [18], Batch [400/938], Loss: 1.0177557468414307\n",
      "Validation: Epoch [18], Batch [401/938], Loss: 0.6139355897903442\n",
      "Validation: Epoch [18], Batch [402/938], Loss: 0.6483173966407776\n",
      "Validation: Epoch [18], Batch [403/938], Loss: 0.6853209733963013\n",
      "Validation: Epoch [18], Batch [404/938], Loss: 0.7461326718330383\n",
      "Validation: Epoch [18], Batch [405/938], Loss: 0.8712558746337891\n",
      "Validation: Epoch [18], Batch [406/938], Loss: 0.6720845103263855\n",
      "Validation: Epoch [18], Batch [407/938], Loss: 0.8096070289611816\n",
      "Validation: Epoch [18], Batch [408/938], Loss: 0.6220113635063171\n",
      "Validation: Epoch [18], Batch [409/938], Loss: 0.6757050156593323\n",
      "Validation: Epoch [18], Batch [410/938], Loss: 0.8030314445495605\n",
      "Validation: Epoch [18], Batch [411/938], Loss: 0.882546603679657\n",
      "Validation: Epoch [18], Batch [412/938], Loss: 0.5451675653457642\n",
      "Validation: Epoch [18], Batch [413/938], Loss: 0.5443805456161499\n",
      "Validation: Epoch [18], Batch [414/938], Loss: 0.5699046850204468\n",
      "Validation: Epoch [18], Batch [415/938], Loss: 0.9025173187255859\n",
      "Validation: Epoch [18], Batch [416/938], Loss: 0.886684238910675\n",
      "Validation: Epoch [18], Batch [417/938], Loss: 0.6760066151618958\n",
      "Validation: Epoch [18], Batch [418/938], Loss: 0.6267274618148804\n",
      "Validation: Epoch [18], Batch [419/938], Loss: 0.43334728479385376\n",
      "Validation: Epoch [18], Batch [420/938], Loss: 0.8136312365531921\n",
      "Validation: Epoch [18], Batch [421/938], Loss: 0.6583144664764404\n",
      "Validation: Epoch [18], Batch [422/938], Loss: 0.9432838559150696\n",
      "Validation: Epoch [18], Batch [423/938], Loss: 0.8508491516113281\n",
      "Validation: Epoch [18], Batch [424/938], Loss: 0.7152319550514221\n",
      "Validation: Epoch [18], Batch [425/938], Loss: 0.8946948051452637\n",
      "Validation: Epoch [18], Batch [426/938], Loss: 0.9414071440696716\n",
      "Validation: Epoch [18], Batch [427/938], Loss: 0.6687057614326477\n",
      "Validation: Epoch [18], Batch [428/938], Loss: 0.4962060749530792\n",
      "Validation: Epoch [18], Batch [429/938], Loss: 0.6541153788566589\n",
      "Validation: Epoch [18], Batch [430/938], Loss: 0.7546088695526123\n",
      "Validation: Epoch [18], Batch [431/938], Loss: 0.6828611493110657\n",
      "Validation: Epoch [18], Batch [432/938], Loss: 0.78167724609375\n",
      "Validation: Epoch [18], Batch [433/938], Loss: 0.7852082252502441\n",
      "Validation: Epoch [18], Batch [434/938], Loss: 0.7804393768310547\n",
      "Validation: Epoch [18], Batch [435/938], Loss: 0.7972636818885803\n",
      "Validation: Epoch [18], Batch [436/938], Loss: 0.796347439289093\n",
      "Validation: Epoch [18], Batch [437/938], Loss: 0.7334616184234619\n",
      "Validation: Epoch [18], Batch [438/938], Loss: 0.8342942595481873\n",
      "Validation: Epoch [18], Batch [439/938], Loss: 0.6667156219482422\n",
      "Validation: Epoch [18], Batch [440/938], Loss: 0.6794672012329102\n",
      "Validation: Epoch [18], Batch [441/938], Loss: 0.6344245672225952\n",
      "Validation: Epoch [18], Batch [442/938], Loss: 0.7682039737701416\n",
      "Validation: Epoch [18], Batch [443/938], Loss: 0.8053755760192871\n",
      "Validation: Epoch [18], Batch [444/938], Loss: 0.684266984462738\n",
      "Validation: Epoch [18], Batch [445/938], Loss: 0.7926132082939148\n",
      "Validation: Epoch [18], Batch [446/938], Loss: 0.4676234722137451\n",
      "Validation: Epoch [18], Batch [447/938], Loss: 0.9681323766708374\n",
      "Validation: Epoch [18], Batch [448/938], Loss: 0.8324090838432312\n",
      "Validation: Epoch [18], Batch [449/938], Loss: 0.6500787734985352\n",
      "Validation: Epoch [18], Batch [450/938], Loss: 0.5461753606796265\n",
      "Validation: Epoch [18], Batch [451/938], Loss: 0.46673938632011414\n",
      "Validation: Epoch [18], Batch [452/938], Loss: 0.5509126782417297\n",
      "Validation: Epoch [18], Batch [453/938], Loss: 0.7488915920257568\n",
      "Validation: Epoch [18], Batch [454/938], Loss: 0.9435095191001892\n",
      "Validation: Epoch [18], Batch [455/938], Loss: 0.7298598289489746\n",
      "Validation: Epoch [18], Batch [456/938], Loss: 0.7309868931770325\n",
      "Validation: Epoch [18], Batch [457/938], Loss: 0.7209422588348389\n",
      "Validation: Epoch [18], Batch [458/938], Loss: 0.7300685048103333\n",
      "Validation: Epoch [18], Batch [459/938], Loss: 0.6781015396118164\n",
      "Validation: Epoch [18], Batch [460/938], Loss: 0.9331809878349304\n",
      "Validation: Epoch [18], Batch [461/938], Loss: 0.7310287356376648\n",
      "Validation: Epoch [18], Batch [462/938], Loss: 0.7948912382125854\n",
      "Validation: Epoch [18], Batch [463/938], Loss: 0.6937925815582275\n",
      "Validation: Epoch [18], Batch [464/938], Loss: 0.5472404360771179\n",
      "Validation: Epoch [18], Batch [465/938], Loss: 0.6751896739006042\n",
      "Validation: Epoch [18], Batch [466/938], Loss: 0.7052253484725952\n",
      "Validation: Epoch [18], Batch [467/938], Loss: 0.7763765454292297\n",
      "Validation: Epoch [18], Batch [468/938], Loss: 0.7499445080757141\n",
      "Validation: Epoch [18], Batch [469/938], Loss: 0.8104726076126099\n",
      "Validation: Epoch [18], Batch [470/938], Loss: 0.8687387108802795\n",
      "Validation: Epoch [18], Batch [471/938], Loss: 0.5065224766731262\n",
      "Validation: Epoch [18], Batch [472/938], Loss: 0.6431257724761963\n",
      "Validation: Epoch [18], Batch [473/938], Loss: 0.7675551772117615\n",
      "Validation: Epoch [18], Batch [474/938], Loss: 0.7459023594856262\n",
      "Validation: Epoch [18], Batch [475/938], Loss: 0.6042170524597168\n",
      "Validation: Epoch [18], Batch [476/938], Loss: 0.6305357217788696\n",
      "Validation: Epoch [18], Batch [477/938], Loss: 0.6123713850975037\n",
      "Validation: Epoch [18], Batch [478/938], Loss: 0.8235628008842468\n",
      "Validation: Epoch [18], Batch [479/938], Loss: 0.9642144441604614\n",
      "Validation: Epoch [18], Batch [480/938], Loss: 0.8573164939880371\n",
      "Validation: Epoch [18], Batch [481/938], Loss: 0.6921967267990112\n",
      "Validation: Epoch [18], Batch [482/938], Loss: 0.8148606419563293\n",
      "Validation: Epoch [18], Batch [483/938], Loss: 0.8054497241973877\n",
      "Validation: Epoch [18], Batch [484/938], Loss: 0.9330246448516846\n",
      "Validation: Epoch [18], Batch [485/938], Loss: 0.4732358157634735\n",
      "Validation: Epoch [18], Batch [486/938], Loss: 0.6580529808998108\n",
      "Validation: Epoch [18], Batch [487/938], Loss: 1.0240494012832642\n",
      "Validation: Epoch [18], Batch [488/938], Loss: 0.5245155096054077\n",
      "Validation: Epoch [18], Batch [489/938], Loss: 0.8590276837348938\n",
      "Validation: Epoch [18], Batch [490/938], Loss: 0.7845921516418457\n",
      "Validation: Epoch [18], Batch [491/938], Loss: 0.4873530864715576\n",
      "Validation: Epoch [18], Batch [492/938], Loss: 0.7626932263374329\n",
      "Validation: Epoch [18], Batch [493/938], Loss: 0.6104716658592224\n",
      "Validation: Epoch [18], Batch [494/938], Loss: 0.5938315987586975\n",
      "Validation: Epoch [18], Batch [495/938], Loss: 0.5160245895385742\n",
      "Validation: Epoch [18], Batch [496/938], Loss: 0.6009469032287598\n",
      "Validation: Epoch [18], Batch [497/938], Loss: 0.7163525819778442\n",
      "Validation: Epoch [18], Batch [498/938], Loss: 0.5402948260307312\n",
      "Validation: Epoch [18], Batch [499/938], Loss: 0.8352063298225403\n",
      "Validation: Epoch [18], Batch [500/938], Loss: 0.588823676109314\n",
      "Validation: Epoch [18], Batch [501/938], Loss: 0.6368089914321899\n",
      "Validation: Epoch [18], Batch [502/938], Loss: 0.6274232864379883\n",
      "Validation: Epoch [18], Batch [503/938], Loss: 0.5201481580734253\n",
      "Validation: Epoch [18], Batch [504/938], Loss: 0.705001711845398\n",
      "Validation: Epoch [18], Batch [505/938], Loss: 0.7709590792655945\n",
      "Validation: Epoch [18], Batch [506/938], Loss: 0.8236643671989441\n",
      "Validation: Epoch [18], Batch [507/938], Loss: 0.6116941571235657\n",
      "Validation: Epoch [18], Batch [508/938], Loss: 0.5915573835372925\n",
      "Validation: Epoch [18], Batch [509/938], Loss: 0.531797468662262\n",
      "Validation: Epoch [18], Batch [510/938], Loss: 0.7978784441947937\n",
      "Validation: Epoch [18], Batch [511/938], Loss: 0.9518404006958008\n",
      "Validation: Epoch [18], Batch [512/938], Loss: 0.8421126008033752\n",
      "Validation: Epoch [18], Batch [513/938], Loss: 0.8190557956695557\n",
      "Validation: Epoch [18], Batch [514/938], Loss: 0.7542015314102173\n",
      "Validation: Epoch [18], Batch [515/938], Loss: 0.6674647927284241\n",
      "Validation: Epoch [18], Batch [516/938], Loss: 0.6442375183105469\n",
      "Validation: Epoch [18], Batch [517/938], Loss: 0.736748456954956\n",
      "Validation: Epoch [18], Batch [518/938], Loss: 0.834854006767273\n",
      "Validation: Epoch [18], Batch [519/938], Loss: 0.5568749904632568\n",
      "Validation: Epoch [18], Batch [520/938], Loss: 0.6582934260368347\n",
      "Validation: Epoch [18], Batch [521/938], Loss: 0.8334453105926514\n",
      "Validation: Epoch [18], Batch [522/938], Loss: 0.7230324149131775\n",
      "Validation: Epoch [18], Batch [523/938], Loss: 0.6348652839660645\n",
      "Validation: Epoch [18], Batch [524/938], Loss: 0.7244580388069153\n",
      "Validation: Epoch [18], Batch [525/938], Loss: 0.6030076146125793\n",
      "Validation: Epoch [18], Batch [526/938], Loss: 0.655716061592102\n",
      "Validation: Epoch [18], Batch [527/938], Loss: 0.5902220606803894\n",
      "Validation: Epoch [18], Batch [528/938], Loss: 0.6943926811218262\n",
      "Validation: Epoch [18], Batch [529/938], Loss: 0.5481861233711243\n",
      "Validation: Epoch [18], Batch [530/938], Loss: 0.74164879322052\n",
      "Validation: Epoch [18], Batch [531/938], Loss: 0.7590283751487732\n",
      "Validation: Epoch [18], Batch [532/938], Loss: 0.7450892925262451\n",
      "Validation: Epoch [18], Batch [533/938], Loss: 0.4729619026184082\n",
      "Validation: Epoch [18], Batch [534/938], Loss: 0.5342944860458374\n",
      "Validation: Epoch [18], Batch [535/938], Loss: 1.0820307731628418\n",
      "Validation: Epoch [18], Batch [536/938], Loss: 0.9348978996276855\n",
      "Validation: Epoch [18], Batch [537/938], Loss: 0.9327151775360107\n",
      "Validation: Epoch [18], Batch [538/938], Loss: 0.7452290058135986\n",
      "Validation: Epoch [18], Batch [539/938], Loss: 0.904636561870575\n",
      "Validation: Epoch [18], Batch [540/938], Loss: 0.8504090309143066\n",
      "Validation: Epoch [18], Batch [541/938], Loss: 1.0350258350372314\n",
      "Validation: Epoch [18], Batch [542/938], Loss: 0.7918863296508789\n",
      "Validation: Epoch [18], Batch [543/938], Loss: 0.7317730784416199\n",
      "Validation: Epoch [18], Batch [544/938], Loss: 0.8226251006126404\n",
      "Validation: Epoch [18], Batch [545/938], Loss: 0.7491762638092041\n",
      "Validation: Epoch [18], Batch [546/938], Loss: 0.6749707460403442\n",
      "Validation: Epoch [18], Batch [547/938], Loss: 0.44839298725128174\n",
      "Validation: Epoch [18], Batch [548/938], Loss: 0.831703245639801\n",
      "Validation: Epoch [18], Batch [549/938], Loss: 0.7425320148468018\n",
      "Validation: Epoch [18], Batch [550/938], Loss: 0.41980427503585815\n",
      "Validation: Epoch [18], Batch [551/938], Loss: 0.7969483733177185\n",
      "Validation: Epoch [18], Batch [552/938], Loss: 0.7892134189605713\n",
      "Validation: Epoch [18], Batch [553/938], Loss: 0.618922233581543\n",
      "Validation: Epoch [18], Batch [554/938], Loss: 0.888359546661377\n",
      "Validation: Epoch [18], Batch [555/938], Loss: 0.5386877655982971\n",
      "Validation: Epoch [18], Batch [556/938], Loss: 0.7040737867355347\n",
      "Validation: Epoch [18], Batch [557/938], Loss: 0.744300901889801\n",
      "Validation: Epoch [18], Batch [558/938], Loss: 0.7592031359672546\n",
      "Validation: Epoch [18], Batch [559/938], Loss: 0.932249128818512\n",
      "Validation: Epoch [18], Batch [560/938], Loss: 0.818816602230072\n",
      "Validation: Epoch [18], Batch [561/938], Loss: 0.830461859703064\n",
      "Validation: Epoch [18], Batch [562/938], Loss: 0.6737217307090759\n",
      "Validation: Epoch [18], Batch [563/938], Loss: 0.5789000391960144\n",
      "Validation: Epoch [18], Batch [564/938], Loss: 1.0682874917984009\n",
      "Validation: Epoch [18], Batch [565/938], Loss: 0.48642438650131226\n",
      "Validation: Epoch [18], Batch [566/938], Loss: 0.7892686128616333\n",
      "Validation: Epoch [18], Batch [567/938], Loss: 0.7981982231140137\n",
      "Validation: Epoch [18], Batch [568/938], Loss: 0.7554857134819031\n",
      "Validation: Epoch [18], Batch [569/938], Loss: 0.6975451707839966\n",
      "Validation: Epoch [18], Batch [570/938], Loss: 0.6898418068885803\n",
      "Validation: Epoch [18], Batch [571/938], Loss: 0.6639033555984497\n",
      "Validation: Epoch [18], Batch [572/938], Loss: 0.5940507650375366\n",
      "Validation: Epoch [18], Batch [573/938], Loss: 0.8646092414855957\n",
      "Validation: Epoch [18], Batch [574/938], Loss: 0.8561670184135437\n",
      "Validation: Epoch [18], Batch [575/938], Loss: 0.9764459133148193\n",
      "Validation: Epoch [18], Batch [576/938], Loss: 0.7310662269592285\n",
      "Validation: Epoch [18], Batch [577/938], Loss: 0.5668753385543823\n",
      "Validation: Epoch [18], Batch [578/938], Loss: 0.9096832871437073\n",
      "Validation: Epoch [18], Batch [579/938], Loss: 0.7952706813812256\n",
      "Validation: Epoch [18], Batch [580/938], Loss: 0.8264009356498718\n",
      "Validation: Epoch [18], Batch [581/938], Loss: 0.7940125465393066\n",
      "Validation: Epoch [18], Batch [582/938], Loss: 0.5719583034515381\n",
      "Validation: Epoch [18], Batch [583/938], Loss: 1.092182993888855\n",
      "Validation: Epoch [18], Batch [584/938], Loss: 0.6535553932189941\n",
      "Validation: Epoch [18], Batch [585/938], Loss: 0.6587095260620117\n",
      "Validation: Epoch [18], Batch [586/938], Loss: 0.7683277726173401\n",
      "Validation: Epoch [18], Batch [587/938], Loss: 0.6578691005706787\n",
      "Validation: Epoch [18], Batch [588/938], Loss: 0.6729281544685364\n",
      "Validation: Epoch [18], Batch [589/938], Loss: 0.7001158595085144\n",
      "Validation: Epoch [18], Batch [590/938], Loss: 0.6625205278396606\n",
      "Validation: Epoch [18], Batch [591/938], Loss: 0.6513782143592834\n",
      "Validation: Epoch [18], Batch [592/938], Loss: 0.8599828481674194\n",
      "Validation: Epoch [18], Batch [593/938], Loss: 0.6755359172821045\n",
      "Validation: Epoch [18], Batch [594/938], Loss: 0.526893675327301\n",
      "Validation: Epoch [18], Batch [595/938], Loss: 0.7662825584411621\n",
      "Validation: Epoch [18], Batch [596/938], Loss: 0.6499282717704773\n",
      "Validation: Epoch [18], Batch [597/938], Loss: 0.6024249196052551\n",
      "Validation: Epoch [18], Batch [598/938], Loss: 0.8532244563102722\n",
      "Validation: Epoch [18], Batch [599/938], Loss: 1.1474272012710571\n",
      "Validation: Epoch [18], Batch [600/938], Loss: 0.4810370206832886\n",
      "Validation: Epoch [18], Batch [601/938], Loss: 0.7415133118629456\n",
      "Validation: Epoch [18], Batch [602/938], Loss: 0.6345250606536865\n",
      "Validation: Epoch [18], Batch [603/938], Loss: 0.6616613268852234\n",
      "Validation: Epoch [18], Batch [604/938], Loss: 0.8189290761947632\n",
      "Validation: Epoch [18], Batch [605/938], Loss: 0.584016740322113\n",
      "Validation: Epoch [18], Batch [606/938], Loss: 0.7285951375961304\n",
      "Validation: Epoch [18], Batch [607/938], Loss: 0.5992016792297363\n",
      "Validation: Epoch [18], Batch [608/938], Loss: 0.6531860828399658\n",
      "Validation: Epoch [18], Batch [609/938], Loss: 0.6298982501029968\n",
      "Validation: Epoch [18], Batch [610/938], Loss: 0.6531289219856262\n",
      "Validation: Epoch [18], Batch [611/938], Loss: 1.2611865997314453\n",
      "Validation: Epoch [18], Batch [612/938], Loss: 0.7880059480667114\n",
      "Validation: Epoch [18], Batch [613/938], Loss: 0.7442703247070312\n",
      "Validation: Epoch [18], Batch [614/938], Loss: 0.7910134792327881\n",
      "Validation: Epoch [18], Batch [615/938], Loss: 0.6644616723060608\n",
      "Validation: Epoch [18], Batch [616/938], Loss: 0.7181274890899658\n",
      "Validation: Epoch [18], Batch [617/938], Loss: 0.7682714462280273\n",
      "Validation: Epoch [18], Batch [618/938], Loss: 1.061216115951538\n",
      "Validation: Epoch [18], Batch [619/938], Loss: 0.9746738076210022\n",
      "Validation: Epoch [18], Batch [620/938], Loss: 0.9378460049629211\n",
      "Validation: Epoch [18], Batch [621/938], Loss: 0.7486398220062256\n",
      "Validation: Epoch [18], Batch [622/938], Loss: 0.9408414363861084\n",
      "Validation: Epoch [18], Batch [623/938], Loss: 0.7614977359771729\n",
      "Validation: Epoch [18], Batch [624/938], Loss: 0.7041935324668884\n",
      "Validation: Epoch [18], Batch [625/938], Loss: 0.8935275077819824\n",
      "Validation: Epoch [18], Batch [626/938], Loss: 0.7197061777114868\n",
      "Validation: Epoch [18], Batch [627/938], Loss: 0.6172446012496948\n",
      "Validation: Epoch [18], Batch [628/938], Loss: 0.9400153160095215\n",
      "Validation: Epoch [18], Batch [629/938], Loss: 0.7677018046379089\n",
      "Validation: Epoch [18], Batch [630/938], Loss: 0.6431003212928772\n",
      "Validation: Epoch [18], Batch [631/938], Loss: 0.5071225762367249\n",
      "Validation: Epoch [18], Batch [632/938], Loss: 0.7295433282852173\n",
      "Validation: Epoch [18], Batch [633/938], Loss: 0.40231087803840637\n",
      "Validation: Epoch [18], Batch [634/938], Loss: 0.5203146934509277\n",
      "Validation: Epoch [18], Batch [635/938], Loss: 0.7160224914550781\n",
      "Validation: Epoch [18], Batch [636/938], Loss: 0.6571055054664612\n",
      "Validation: Epoch [18], Batch [637/938], Loss: 0.6540465354919434\n",
      "Validation: Epoch [18], Batch [638/938], Loss: 1.005789041519165\n",
      "Validation: Epoch [18], Batch [639/938], Loss: 0.9096877574920654\n",
      "Validation: Epoch [18], Batch [640/938], Loss: 0.8714767098426819\n",
      "Validation: Epoch [18], Batch [641/938], Loss: 0.8260811567306519\n",
      "Validation: Epoch [18], Batch [642/938], Loss: 0.8575395345687866\n",
      "Validation: Epoch [18], Batch [643/938], Loss: 0.5841184258460999\n",
      "Validation: Epoch [18], Batch [644/938], Loss: 0.7278571128845215\n",
      "Validation: Epoch [18], Batch [645/938], Loss: 0.597217857837677\n",
      "Validation: Epoch [18], Batch [646/938], Loss: 0.6815248727798462\n",
      "Validation: Epoch [18], Batch [647/938], Loss: 0.6751901507377625\n",
      "Validation: Epoch [18], Batch [648/938], Loss: 0.802649199962616\n",
      "Validation: Epoch [18], Batch [649/938], Loss: 0.7190999984741211\n",
      "Validation: Epoch [18], Batch [650/938], Loss: 0.9199564456939697\n",
      "Validation: Epoch [18], Batch [651/938], Loss: 1.0501482486724854\n",
      "Validation: Epoch [18], Batch [652/938], Loss: 1.0815011262893677\n",
      "Validation: Epoch [18], Batch [653/938], Loss: 0.766048789024353\n",
      "Validation: Epoch [18], Batch [654/938], Loss: 0.8514251708984375\n",
      "Validation: Epoch [18], Batch [655/938], Loss: 0.7965805530548096\n",
      "Validation: Epoch [18], Batch [656/938], Loss: 0.8352646827697754\n",
      "Validation: Epoch [18], Batch [657/938], Loss: 0.7430676221847534\n",
      "Validation: Epoch [18], Batch [658/938], Loss: 0.708870530128479\n",
      "Validation: Epoch [18], Batch [659/938], Loss: 0.5396343469619751\n",
      "Validation: Epoch [18], Batch [660/938], Loss: 0.719860851764679\n",
      "Validation: Epoch [18], Batch [661/938], Loss: 0.815007746219635\n",
      "Validation: Epoch [18], Batch [662/938], Loss: 0.6722702980041504\n",
      "Validation: Epoch [18], Batch [663/938], Loss: 0.8019286394119263\n",
      "Validation: Epoch [18], Batch [664/938], Loss: 0.6899692416191101\n",
      "Validation: Epoch [18], Batch [665/938], Loss: 0.826436460018158\n",
      "Validation: Epoch [18], Batch [666/938], Loss: 0.7227678894996643\n",
      "Validation: Epoch [18], Batch [667/938], Loss: 0.6471291184425354\n",
      "Validation: Epoch [18], Batch [668/938], Loss: 0.7399476766586304\n",
      "Validation: Epoch [18], Batch [669/938], Loss: 0.5578947067260742\n",
      "Validation: Epoch [18], Batch [670/938], Loss: 0.9918543100357056\n",
      "Validation: Epoch [18], Batch [671/938], Loss: 0.6801611185073853\n",
      "Validation: Epoch [18], Batch [672/938], Loss: 0.7378758192062378\n",
      "Validation: Epoch [18], Batch [673/938], Loss: 0.8023889064788818\n",
      "Validation: Epoch [18], Batch [674/938], Loss: 0.363065630197525\n",
      "Validation: Epoch [18], Batch [675/938], Loss: 0.7720076441764832\n",
      "Validation: Epoch [18], Batch [676/938], Loss: 0.6318333148956299\n",
      "Validation: Epoch [18], Batch [677/938], Loss: 0.8379169702529907\n",
      "Validation: Epoch [18], Batch [678/938], Loss: 0.6998372077941895\n",
      "Validation: Epoch [18], Batch [679/938], Loss: 0.6336728930473328\n",
      "Validation: Epoch [18], Batch [680/938], Loss: 0.8262090086936951\n",
      "Validation: Epoch [18], Batch [681/938], Loss: 0.6494767665863037\n",
      "Validation: Epoch [18], Batch [682/938], Loss: 0.832462728023529\n",
      "Validation: Epoch [18], Batch [683/938], Loss: 0.7320106029510498\n",
      "Validation: Epoch [18], Batch [684/938], Loss: 0.6845472455024719\n",
      "Validation: Epoch [18], Batch [685/938], Loss: 0.632969856262207\n",
      "Validation: Epoch [18], Batch [686/938], Loss: 0.6551790833473206\n",
      "Validation: Epoch [18], Batch [687/938], Loss: 0.6995223164558411\n",
      "Validation: Epoch [18], Batch [688/938], Loss: 0.7014636993408203\n",
      "Validation: Epoch [18], Batch [689/938], Loss: 0.8913698196411133\n",
      "Validation: Epoch [18], Batch [690/938], Loss: 0.6833867430686951\n",
      "Validation: Epoch [18], Batch [691/938], Loss: 0.6245287656784058\n",
      "Validation: Epoch [18], Batch [692/938], Loss: 0.7573100328445435\n",
      "Validation: Epoch [18], Batch [693/938], Loss: 0.8642588257789612\n",
      "Validation: Epoch [18], Batch [694/938], Loss: 0.7688533663749695\n",
      "Validation: Epoch [18], Batch [695/938], Loss: 0.6416861414909363\n",
      "Validation: Epoch [18], Batch [696/938], Loss: 0.7947826385498047\n",
      "Validation: Epoch [18], Batch [697/938], Loss: 0.8822535276412964\n",
      "Validation: Epoch [18], Batch [698/938], Loss: 0.8759706616401672\n",
      "Validation: Epoch [18], Batch [699/938], Loss: 0.6148601770401001\n",
      "Validation: Epoch [18], Batch [700/938], Loss: 0.7971231937408447\n",
      "Validation: Epoch [18], Batch [701/938], Loss: 0.8602955341339111\n",
      "Validation: Epoch [18], Batch [702/938], Loss: 0.8818668127059937\n",
      "Validation: Epoch [18], Batch [703/938], Loss: 0.8782401084899902\n",
      "Validation: Epoch [18], Batch [704/938], Loss: 0.588172435760498\n",
      "Validation: Epoch [18], Batch [705/938], Loss: 0.635470986366272\n",
      "Validation: Epoch [18], Batch [706/938], Loss: 0.7878711223602295\n",
      "Validation: Epoch [18], Batch [707/938], Loss: 0.801742672920227\n",
      "Validation: Epoch [18], Batch [708/938], Loss: 0.8851428627967834\n",
      "Validation: Epoch [18], Batch [709/938], Loss: 0.5490579009056091\n",
      "Validation: Epoch [18], Batch [710/938], Loss: 0.5860944986343384\n",
      "Validation: Epoch [18], Batch [711/938], Loss: 0.6989843845367432\n",
      "Validation: Epoch [18], Batch [712/938], Loss: 0.8138207197189331\n",
      "Validation: Epoch [18], Batch [713/938], Loss: 0.7678080201148987\n",
      "Validation: Epoch [18], Batch [714/938], Loss: 0.5560112595558167\n",
      "Validation: Epoch [18], Batch [715/938], Loss: 0.6652200222015381\n",
      "Validation: Epoch [18], Batch [716/938], Loss: 0.7134566903114319\n",
      "Validation: Epoch [18], Batch [717/938], Loss: 0.7013754844665527\n",
      "Validation: Epoch [18], Batch [718/938], Loss: 0.6197560429573059\n",
      "Validation: Epoch [18], Batch [719/938], Loss: 1.1528915166854858\n",
      "Validation: Epoch [18], Batch [720/938], Loss: 0.8051917552947998\n",
      "Validation: Epoch [18], Batch [721/938], Loss: 0.5872509479522705\n",
      "Validation: Epoch [18], Batch [722/938], Loss: 0.6263673305511475\n",
      "Validation: Epoch [18], Batch [723/938], Loss: 0.6788039207458496\n",
      "Validation: Epoch [18], Batch [724/938], Loss: 0.7726393342018127\n",
      "Validation: Epoch [18], Batch [725/938], Loss: 0.8303508162498474\n",
      "Validation: Epoch [18], Batch [726/938], Loss: 0.7040600776672363\n",
      "Validation: Epoch [18], Batch [727/938], Loss: 0.8100500106811523\n",
      "Validation: Epoch [18], Batch [728/938], Loss: 0.6593337059020996\n",
      "Validation: Epoch [18], Batch [729/938], Loss: 0.7692376971244812\n",
      "Validation: Epoch [18], Batch [730/938], Loss: 0.5976464748382568\n",
      "Validation: Epoch [18], Batch [731/938], Loss: 0.5567343831062317\n",
      "Validation: Epoch [18], Batch [732/938], Loss: 0.6244539022445679\n",
      "Validation: Epoch [18], Batch [733/938], Loss: 0.760016679763794\n",
      "Validation: Epoch [18], Batch [734/938], Loss: 0.5960206985473633\n",
      "Validation: Epoch [18], Batch [735/938], Loss: 0.8221729397773743\n",
      "Validation: Epoch [18], Batch [736/938], Loss: 0.7569900155067444\n",
      "Validation: Epoch [18], Batch [737/938], Loss: 0.7173452377319336\n",
      "Validation: Epoch [18], Batch [738/938], Loss: 0.9113405346870422\n",
      "Validation: Epoch [18], Batch [739/938], Loss: 0.6643123626708984\n",
      "Validation: Epoch [18], Batch [740/938], Loss: 0.8414086699485779\n",
      "Validation: Epoch [18], Batch [741/938], Loss: 0.9821380376815796\n",
      "Validation: Epoch [18], Batch [742/938], Loss: 0.718849778175354\n",
      "Validation: Epoch [18], Batch [743/938], Loss: 0.6777451038360596\n",
      "Validation: Epoch [18], Batch [744/938], Loss: 0.6045529842376709\n",
      "Validation: Epoch [18], Batch [745/938], Loss: 0.6654987335205078\n",
      "Validation: Epoch [18], Batch [746/938], Loss: 0.7952398061752319\n",
      "Validation: Epoch [18], Batch [747/938], Loss: 0.7770212888717651\n",
      "Validation: Epoch [18], Batch [748/938], Loss: 0.6126086115837097\n",
      "Validation: Epoch [18], Batch [749/938], Loss: 0.838017463684082\n",
      "Validation: Epoch [18], Batch [750/938], Loss: 0.8035166263580322\n",
      "Validation: Epoch [18], Batch [751/938], Loss: 0.6840696930885315\n",
      "Validation: Epoch [18], Batch [752/938], Loss: 0.7839382290840149\n",
      "Validation: Epoch [18], Batch [753/938], Loss: 0.7723881006240845\n",
      "Validation: Epoch [18], Batch [754/938], Loss: 0.7319852709770203\n",
      "Validation: Epoch [18], Batch [755/938], Loss: 0.9535163640975952\n",
      "Validation: Epoch [18], Batch [756/938], Loss: 0.7566834688186646\n",
      "Validation: Epoch [18], Batch [757/938], Loss: 0.5472695827484131\n",
      "Validation: Epoch [18], Batch [758/938], Loss: 0.8839110136032104\n",
      "Validation: Epoch [18], Batch [759/938], Loss: 0.6124956607818604\n",
      "Validation: Epoch [18], Batch [760/938], Loss: 0.4547237753868103\n",
      "Validation: Epoch [18], Batch [761/938], Loss: 0.713233232498169\n",
      "Validation: Epoch [18], Batch [762/938], Loss: 0.6675171852111816\n",
      "Validation: Epoch [18], Batch [763/938], Loss: 0.6703701019287109\n",
      "Validation: Epoch [18], Batch [764/938], Loss: 0.6897968053817749\n",
      "Validation: Epoch [18], Batch [765/938], Loss: 0.9446959495544434\n",
      "Validation: Epoch [18], Batch [766/938], Loss: 0.8356500864028931\n",
      "Validation: Epoch [18], Batch [767/938], Loss: 0.7337868213653564\n",
      "Validation: Epoch [18], Batch [768/938], Loss: 0.7507167458534241\n",
      "Validation: Epoch [18], Batch [769/938], Loss: 0.6276434659957886\n",
      "Validation: Epoch [18], Batch [770/938], Loss: 0.46420830488204956\n",
      "Validation: Epoch [18], Batch [771/938], Loss: 0.6109054684638977\n",
      "Validation: Epoch [18], Batch [772/938], Loss: 0.6890373229980469\n",
      "Validation: Epoch [18], Batch [773/938], Loss: 0.7408198118209839\n",
      "Validation: Epoch [18], Batch [774/938], Loss: 0.6429092288017273\n",
      "Validation: Epoch [18], Batch [775/938], Loss: 0.7837499380111694\n",
      "Validation: Epoch [18], Batch [776/938], Loss: 0.7951148152351379\n",
      "Validation: Epoch [18], Batch [777/938], Loss: 0.8608419895172119\n",
      "Validation: Epoch [18], Batch [778/938], Loss: 0.7071256041526794\n",
      "Validation: Epoch [18], Batch [779/938], Loss: 0.7336564064025879\n",
      "Validation: Epoch [18], Batch [780/938], Loss: 0.8402284383773804\n",
      "Validation: Epoch [18], Batch [781/938], Loss: 0.7002696394920349\n",
      "Validation: Epoch [18], Batch [782/938], Loss: 0.6987249851226807\n",
      "Validation: Epoch [18], Batch [783/938], Loss: 0.990276575088501\n",
      "Validation: Epoch [18], Batch [784/938], Loss: 0.8635352253913879\n",
      "Validation: Epoch [18], Batch [785/938], Loss: 0.6849431991577148\n",
      "Validation: Epoch [18], Batch [786/938], Loss: 0.6524559259414673\n",
      "Validation: Epoch [18], Batch [787/938], Loss: 0.7430130839347839\n",
      "Validation: Epoch [18], Batch [788/938], Loss: 0.6981115341186523\n",
      "Validation: Epoch [18], Batch [789/938], Loss: 0.870081901550293\n",
      "Validation: Epoch [18], Batch [790/938], Loss: 0.7137653827667236\n",
      "Validation: Epoch [18], Batch [791/938], Loss: 0.7552379965782166\n",
      "Validation: Epoch [18], Batch [792/938], Loss: 0.7682069540023804\n",
      "Validation: Epoch [18], Batch [793/938], Loss: 0.7167474031448364\n",
      "Validation: Epoch [18], Batch [794/938], Loss: 0.7211503386497498\n",
      "Validation: Epoch [18], Batch [795/938], Loss: 0.8556175231933594\n",
      "Validation: Epoch [18], Batch [796/938], Loss: 0.6815143823623657\n",
      "Validation: Epoch [18], Batch [797/938], Loss: 0.593840479850769\n",
      "Validation: Epoch [18], Batch [798/938], Loss: 0.6818935871124268\n",
      "Validation: Epoch [18], Batch [799/938], Loss: 0.5585669875144958\n",
      "Validation: Epoch [18], Batch [800/938], Loss: 0.5750313401222229\n",
      "Validation: Epoch [18], Batch [801/938], Loss: 0.5794926881790161\n",
      "Validation: Epoch [18], Batch [802/938], Loss: 0.41099318861961365\n",
      "Validation: Epoch [18], Batch [803/938], Loss: 0.6839669346809387\n",
      "Validation: Epoch [18], Batch [804/938], Loss: 0.5691999793052673\n",
      "Validation: Epoch [18], Batch [805/938], Loss: 0.6581770777702332\n",
      "Validation: Epoch [18], Batch [806/938], Loss: 0.9368125200271606\n",
      "Validation: Epoch [18], Batch [807/938], Loss: 0.820724368095398\n",
      "Validation: Epoch [18], Batch [808/938], Loss: 0.5767278075218201\n",
      "Validation: Epoch [18], Batch [809/938], Loss: 1.0057631731033325\n",
      "Validation: Epoch [18], Batch [810/938], Loss: 0.43025389313697815\n",
      "Validation: Epoch [18], Batch [811/938], Loss: 0.7381977438926697\n",
      "Validation: Epoch [18], Batch [812/938], Loss: 0.7499133348464966\n",
      "Validation: Epoch [18], Batch [813/938], Loss: 0.7366510033607483\n",
      "Validation: Epoch [18], Batch [814/938], Loss: 0.5729964971542358\n",
      "Validation: Epoch [18], Batch [815/938], Loss: 0.6295673251152039\n",
      "Validation: Epoch [18], Batch [816/938], Loss: 0.6678500175476074\n",
      "Validation: Epoch [18], Batch [817/938], Loss: 0.7871668338775635\n",
      "Validation: Epoch [18], Batch [818/938], Loss: 0.8050776720046997\n",
      "Validation: Epoch [18], Batch [819/938], Loss: 0.5979878306388855\n",
      "Validation: Epoch [18], Batch [820/938], Loss: 0.8971179723739624\n",
      "Validation: Epoch [18], Batch [821/938], Loss: 0.7732420563697815\n",
      "Validation: Epoch [18], Batch [822/938], Loss: 0.852921724319458\n",
      "Validation: Epoch [18], Batch [823/938], Loss: 0.5135095715522766\n",
      "Validation: Epoch [18], Batch [824/938], Loss: 0.8581405878067017\n",
      "Validation: Epoch [18], Batch [825/938], Loss: 0.865098774433136\n",
      "Validation: Epoch [18], Batch [826/938], Loss: 0.639391303062439\n",
      "Validation: Epoch [18], Batch [827/938], Loss: 0.6302725672721863\n",
      "Validation: Epoch [18], Batch [828/938], Loss: 0.5918242931365967\n",
      "Validation: Epoch [18], Batch [829/938], Loss: 0.6521762013435364\n",
      "Validation: Epoch [18], Batch [830/938], Loss: 0.601509690284729\n",
      "Validation: Epoch [18], Batch [831/938], Loss: 0.5935245752334595\n",
      "Validation: Epoch [18], Batch [832/938], Loss: 0.7127553224563599\n",
      "Validation: Epoch [18], Batch [833/938], Loss: 0.6688119769096375\n",
      "Validation: Epoch [18], Batch [834/938], Loss: 0.6202590465545654\n",
      "Validation: Epoch [18], Batch [835/938], Loss: 0.609713077545166\n",
      "Validation: Epoch [18], Batch [836/938], Loss: 0.44975557923316956\n",
      "Validation: Epoch [18], Batch [837/938], Loss: 0.7061168551445007\n",
      "Validation: Epoch [18], Batch [838/938], Loss: 0.6248618960380554\n",
      "Validation: Epoch [18], Batch [839/938], Loss: 0.5954726934432983\n",
      "Validation: Epoch [18], Batch [840/938], Loss: 0.9375096559524536\n",
      "Validation: Epoch [18], Batch [841/938], Loss: 0.5191315412521362\n",
      "Validation: Epoch [18], Batch [842/938], Loss: 0.8161203861236572\n",
      "Validation: Epoch [18], Batch [843/938], Loss: 0.7065377235412598\n",
      "Validation: Epoch [18], Batch [844/938], Loss: 0.7612544894218445\n",
      "Validation: Epoch [18], Batch [845/938], Loss: 0.6982972025871277\n",
      "Validation: Epoch [18], Batch [846/938], Loss: 0.6667076945304871\n",
      "Validation: Epoch [18], Batch [847/938], Loss: 0.677775502204895\n",
      "Validation: Epoch [18], Batch [848/938], Loss: 0.9324800968170166\n",
      "Validation: Epoch [18], Batch [849/938], Loss: 0.7620169520378113\n",
      "Validation: Epoch [18], Batch [850/938], Loss: 0.6268704533576965\n",
      "Validation: Epoch [18], Batch [851/938], Loss: 0.4779345393180847\n",
      "Validation: Epoch [18], Batch [852/938], Loss: 0.9644752740859985\n",
      "Validation: Epoch [18], Batch [853/938], Loss: 0.8579010367393494\n",
      "Validation: Epoch [18], Batch [854/938], Loss: 0.6918694376945496\n",
      "Validation: Epoch [18], Batch [855/938], Loss: 1.0560777187347412\n",
      "Validation: Epoch [18], Batch [856/938], Loss: 0.7373192310333252\n",
      "Validation: Epoch [18], Batch [857/938], Loss: 0.7504632472991943\n",
      "Validation: Epoch [18], Batch [858/938], Loss: 0.4896172285079956\n",
      "Validation: Epoch [18], Batch [859/938], Loss: 0.6209126710891724\n",
      "Validation: Epoch [18], Batch [860/938], Loss: 0.6418280005455017\n",
      "Validation: Epoch [18], Batch [861/938], Loss: 0.382423460483551\n",
      "Validation: Epoch [18], Batch [862/938], Loss: 0.6067823767662048\n",
      "Validation: Epoch [18], Batch [863/938], Loss: 0.6102259159088135\n",
      "Validation: Epoch [18], Batch [864/938], Loss: 1.0071686506271362\n",
      "Validation: Epoch [18], Batch [865/938], Loss: 0.7749313116073608\n",
      "Validation: Epoch [18], Batch [866/938], Loss: 0.6754977107048035\n",
      "Validation: Epoch [18], Batch [867/938], Loss: 0.744020938873291\n",
      "Validation: Epoch [18], Batch [868/938], Loss: 0.9357041120529175\n",
      "Validation: Epoch [18], Batch [869/938], Loss: 0.7372707724571228\n",
      "Validation: Epoch [18], Batch [870/938], Loss: 0.7343701124191284\n",
      "Validation: Epoch [18], Batch [871/938], Loss: 0.7187142372131348\n",
      "Validation: Epoch [18], Batch [872/938], Loss: 1.0873152017593384\n",
      "Validation: Epoch [18], Batch [873/938], Loss: 0.8468621373176575\n",
      "Validation: Epoch [18], Batch [874/938], Loss: 0.802863359451294\n",
      "Validation: Epoch [18], Batch [875/938], Loss: 0.5373594760894775\n",
      "Validation: Epoch [18], Batch [876/938], Loss: 0.6380789279937744\n",
      "Validation: Epoch [18], Batch [877/938], Loss: 0.6666162014007568\n",
      "Validation: Epoch [18], Batch [878/938], Loss: 0.5040764212608337\n",
      "Validation: Epoch [18], Batch [879/938], Loss: 0.9352718591690063\n",
      "Validation: Epoch [18], Batch [880/938], Loss: 0.6696732044219971\n",
      "Validation: Epoch [18], Batch [881/938], Loss: 0.6424664258956909\n",
      "Validation: Epoch [18], Batch [882/938], Loss: 0.5936169624328613\n",
      "Validation: Epoch [18], Batch [883/938], Loss: 0.6328307390213013\n",
      "Validation: Epoch [18], Batch [884/938], Loss: 0.5391116142272949\n",
      "Validation: Epoch [18], Batch [885/938], Loss: 0.7313482165336609\n",
      "Validation: Epoch [18], Batch [886/938], Loss: 0.7144498229026794\n",
      "Validation: Epoch [18], Batch [887/938], Loss: 0.5385761260986328\n",
      "Validation: Epoch [18], Batch [888/938], Loss: 0.7458885908126831\n",
      "Validation: Epoch [18], Batch [889/938], Loss: 0.7759758830070496\n",
      "Validation: Epoch [18], Batch [890/938], Loss: 1.002516746520996\n",
      "Validation: Epoch [18], Batch [891/938], Loss: 0.638515830039978\n",
      "Validation: Epoch [18], Batch [892/938], Loss: 0.7475138902664185\n",
      "Validation: Epoch [18], Batch [893/938], Loss: 0.7553078532218933\n",
      "Validation: Epoch [18], Batch [894/938], Loss: 0.5546650290489197\n",
      "Validation: Epoch [18], Batch [895/938], Loss: 0.7326083183288574\n",
      "Validation: Epoch [18], Batch [896/938], Loss: 0.7480979561805725\n",
      "Validation: Epoch [18], Batch [897/938], Loss: 0.9088946580886841\n",
      "Validation: Epoch [18], Batch [898/938], Loss: 0.547294020652771\n",
      "Validation: Epoch [18], Batch [899/938], Loss: 0.5725046396255493\n",
      "Validation: Epoch [18], Batch [900/938], Loss: 0.5648069977760315\n",
      "Validation: Epoch [18], Batch [901/938], Loss: 0.8145517706871033\n",
      "Validation: Epoch [18], Batch [902/938], Loss: 0.9627645015716553\n",
      "Validation: Epoch [18], Batch [903/938], Loss: 0.6997146010398865\n",
      "Validation: Epoch [18], Batch [904/938], Loss: 0.7291411757469177\n",
      "Validation: Epoch [18], Batch [905/938], Loss: 0.9662219285964966\n",
      "Validation: Epoch [18], Batch [906/938], Loss: 0.8546313643455505\n",
      "Validation: Epoch [18], Batch [907/938], Loss: 0.770675778388977\n",
      "Validation: Epoch [18], Batch [908/938], Loss: 1.2756760120391846\n",
      "Validation: Epoch [18], Batch [909/938], Loss: 0.5294060707092285\n",
      "Validation: Epoch [18], Batch [910/938], Loss: 0.6171557903289795\n",
      "Validation: Epoch [18], Batch [911/938], Loss: 0.7031857371330261\n",
      "Validation: Epoch [18], Batch [912/938], Loss: 0.7186864614486694\n",
      "Validation: Epoch [18], Batch [913/938], Loss: 0.9405795335769653\n",
      "Validation: Epoch [18], Batch [914/938], Loss: 0.6516438126564026\n",
      "Validation: Epoch [18], Batch [915/938], Loss: 0.603969931602478\n",
      "Validation: Epoch [18], Batch [916/938], Loss: 0.7532946467399597\n",
      "Validation: Epoch [18], Batch [917/938], Loss: 0.7552230358123779\n",
      "Validation: Epoch [18], Batch [918/938], Loss: 0.7970482110977173\n",
      "Validation: Epoch [18], Batch [919/938], Loss: 0.6768215298652649\n",
      "Validation: Epoch [18], Batch [920/938], Loss: 0.9175107479095459\n",
      "Validation: Epoch [18], Batch [921/938], Loss: 0.7544322609901428\n",
      "Validation: Epoch [18], Batch [922/938], Loss: 0.9398308992385864\n",
      "Validation: Epoch [18], Batch [923/938], Loss: 0.7641820907592773\n",
      "Validation: Epoch [18], Batch [924/938], Loss: 0.7578999400138855\n",
      "Validation: Epoch [18], Batch [925/938], Loss: 0.7936673164367676\n",
      "Validation: Epoch [18], Batch [926/938], Loss: 0.685529887676239\n",
      "Validation: Epoch [18], Batch [927/938], Loss: 0.6366239786148071\n",
      "Validation: Epoch [18], Batch [928/938], Loss: 0.6403045654296875\n",
      "Validation: Epoch [18], Batch [929/938], Loss: 0.6166296005249023\n",
      "Validation: Epoch [18], Batch [930/938], Loss: 0.6899616718292236\n",
      "Validation: Epoch [18], Batch [931/938], Loss: 0.9022977352142334\n",
      "Validation: Epoch [18], Batch [932/938], Loss: 0.9456557035446167\n",
      "Validation: Epoch [18], Batch [933/938], Loss: 0.8522085547447205\n",
      "Validation: Epoch [18], Batch [934/938], Loss: 0.8279234170913696\n",
      "Validation: Epoch [18], Batch [935/938], Loss: 0.9544118642807007\n",
      "Validation: Epoch [18], Batch [936/938], Loss: 0.7021324634552002\n",
      "Validation: Epoch [18], Batch [937/938], Loss: 0.7981180548667908\n",
      "Validation: Epoch [18], Batch [938/938], Loss: 0.8442299365997314\n",
      "Accuracy of test set: 0.7591833333333333\n",
      "Train: Epoch [19], Batch [1/938], Loss: 0.7015540599822998\n",
      "Train: Epoch [19], Batch [2/938], Loss: 0.6003296375274658\n",
      "Train: Epoch [19], Batch [3/938], Loss: 0.845019519329071\n",
      "Train: Epoch [19], Batch [4/938], Loss: 0.7321443557739258\n",
      "Train: Epoch [19], Batch [5/938], Loss: 0.8528125882148743\n",
      "Train: Epoch [19], Batch [6/938], Loss: 0.7190812230110168\n",
      "Train: Epoch [19], Batch [7/938], Loss: 0.5356849431991577\n",
      "Train: Epoch [19], Batch [8/938], Loss: 0.4911685287952423\n",
      "Train: Epoch [19], Batch [9/938], Loss: 0.6492668390274048\n",
      "Train: Epoch [19], Batch [10/938], Loss: 0.6080876588821411\n",
      "Train: Epoch [19], Batch [11/938], Loss: 0.664298415184021\n",
      "Train: Epoch [19], Batch [12/938], Loss: 0.8102908134460449\n",
      "Train: Epoch [19], Batch [13/938], Loss: 0.5364642143249512\n",
      "Train: Epoch [19], Batch [14/938], Loss: 0.67791748046875\n",
      "Train: Epoch [19], Batch [15/938], Loss: 0.5403642058372498\n",
      "Train: Epoch [19], Batch [16/938], Loss: 0.5507634282112122\n",
      "Train: Epoch [19], Batch [17/938], Loss: 0.523897111415863\n",
      "Train: Epoch [19], Batch [18/938], Loss: 0.6860707998275757\n",
      "Train: Epoch [19], Batch [19/938], Loss: 0.8295429944992065\n",
      "Train: Epoch [19], Batch [20/938], Loss: 0.6215346455574036\n",
      "Train: Epoch [19], Batch [21/938], Loss: 0.5238213539123535\n",
      "Train: Epoch [19], Batch [22/938], Loss: 0.6445208787918091\n",
      "Train: Epoch [19], Batch [23/938], Loss: 0.4716609716415405\n",
      "Train: Epoch [19], Batch [24/938], Loss: 0.674924373626709\n",
      "Train: Epoch [19], Batch [25/938], Loss: 0.8889871835708618\n",
      "Train: Epoch [19], Batch [26/938], Loss: 0.5879840850830078\n",
      "Train: Epoch [19], Batch [27/938], Loss: 0.8483224511146545\n",
      "Train: Epoch [19], Batch [28/938], Loss: 0.6110838651657104\n",
      "Train: Epoch [19], Batch [29/938], Loss: 0.6419789791107178\n",
      "Train: Epoch [19], Batch [30/938], Loss: 0.891861081123352\n",
      "Train: Epoch [19], Batch [31/938], Loss: 0.8115040063858032\n",
      "Train: Epoch [19], Batch [32/938], Loss: 0.6508018970489502\n",
      "Train: Epoch [19], Batch [33/938], Loss: 0.6122886538505554\n",
      "Train: Epoch [19], Batch [34/938], Loss: 0.5817217826843262\n",
      "Train: Epoch [19], Batch [35/938], Loss: 0.8381755352020264\n",
      "Train: Epoch [19], Batch [36/938], Loss: 0.7599070072174072\n",
      "Train: Epoch [19], Batch [37/938], Loss: 0.7340523600578308\n",
      "Train: Epoch [19], Batch [38/938], Loss: 0.5952165126800537\n",
      "Train: Epoch [19], Batch [39/938], Loss: 0.6371124982833862\n",
      "Train: Epoch [19], Batch [40/938], Loss: 0.6657664775848389\n",
      "Train: Epoch [19], Batch [41/938], Loss: 0.6038072109222412\n",
      "Train: Epoch [19], Batch [42/938], Loss: 0.549362063407898\n",
      "Train: Epoch [19], Batch [43/938], Loss: 0.792111337184906\n",
      "Train: Epoch [19], Batch [44/938], Loss: 0.6505837440490723\n",
      "Train: Epoch [19], Batch [45/938], Loss: 0.4465544819831848\n",
      "Train: Epoch [19], Batch [46/938], Loss: 0.6257603168487549\n",
      "Train: Epoch [19], Batch [47/938], Loss: 0.5713189244270325\n",
      "Train: Epoch [19], Batch [48/938], Loss: 0.588152289390564\n",
      "Train: Epoch [19], Batch [49/938], Loss: 0.7119059562683105\n",
      "Train: Epoch [19], Batch [50/938], Loss: 0.6314401626586914\n",
      "Train: Epoch [19], Batch [51/938], Loss: 0.45019930601119995\n",
      "Train: Epoch [19], Batch [52/938], Loss: 0.5404824614524841\n",
      "Train: Epoch [19], Batch [53/938], Loss: 0.6281575560569763\n",
      "Train: Epoch [19], Batch [54/938], Loss: 0.8020569086074829\n",
      "Train: Epoch [19], Batch [55/938], Loss: 0.6355057954788208\n",
      "Train: Epoch [19], Batch [56/938], Loss: 0.7630378603935242\n",
      "Train: Epoch [19], Batch [57/938], Loss: 0.6762619018554688\n",
      "Train: Epoch [19], Batch [58/938], Loss: 0.8084447979927063\n",
      "Train: Epoch [19], Batch [59/938], Loss: 1.0416253805160522\n",
      "Train: Epoch [19], Batch [60/938], Loss: 0.7670662999153137\n",
      "Train: Epoch [19], Batch [61/938], Loss: 0.816565752029419\n",
      "Train: Epoch [19], Batch [62/938], Loss: 0.46652770042419434\n",
      "Train: Epoch [19], Batch [63/938], Loss: 0.6682606935501099\n",
      "Train: Epoch [19], Batch [64/938], Loss: 0.9242070317268372\n",
      "Train: Epoch [19], Batch [65/938], Loss: 0.5361116528511047\n",
      "Train: Epoch [19], Batch [66/938], Loss: 0.6395410299301147\n",
      "Train: Epoch [19], Batch [67/938], Loss: 0.9481982588768005\n",
      "Train: Epoch [19], Batch [68/938], Loss: 0.7272197604179382\n",
      "Train: Epoch [19], Batch [69/938], Loss: 0.7352747917175293\n",
      "Train: Epoch [19], Batch [70/938], Loss: 0.5022966265678406\n",
      "Train: Epoch [19], Batch [71/938], Loss: 0.7510600686073303\n",
      "Train: Epoch [19], Batch [72/938], Loss: 0.9078593850135803\n",
      "Train: Epoch [19], Batch [73/938], Loss: 0.6499747037887573\n",
      "Train: Epoch [19], Batch [74/938], Loss: 0.9066140055656433\n",
      "Train: Epoch [19], Batch [75/938], Loss: 0.8150902390480042\n",
      "Train: Epoch [19], Batch [76/938], Loss: 0.9157735705375671\n",
      "Train: Epoch [19], Batch [77/938], Loss: 0.45033159852027893\n",
      "Train: Epoch [19], Batch [78/938], Loss: 0.5099853873252869\n",
      "Train: Epoch [19], Batch [79/938], Loss: 0.6599476337432861\n",
      "Train: Epoch [19], Batch [80/938], Loss: 0.8488988876342773\n",
      "Train: Epoch [19], Batch [81/938], Loss: 0.89736008644104\n",
      "Train: Epoch [19], Batch [82/938], Loss: 0.9216054677963257\n",
      "Train: Epoch [19], Batch [83/938], Loss: 0.6560785174369812\n",
      "Train: Epoch [19], Batch [84/938], Loss: 0.7113588452339172\n",
      "Train: Epoch [19], Batch [85/938], Loss: 0.5454405546188354\n",
      "Train: Epoch [19], Batch [86/938], Loss: 0.6845780611038208\n",
      "Train: Epoch [19], Batch [87/938], Loss: 0.6191051006317139\n",
      "Train: Epoch [19], Batch [88/938], Loss: 0.8160255551338196\n",
      "Train: Epoch [19], Batch [89/938], Loss: 0.7111164927482605\n",
      "Train: Epoch [19], Batch [90/938], Loss: 1.019513487815857\n",
      "Train: Epoch [19], Batch [91/938], Loss: 0.48420602083206177\n",
      "Train: Epoch [19], Batch [92/938], Loss: 0.4973885715007782\n",
      "Train: Epoch [19], Batch [93/938], Loss: 0.8212685585021973\n",
      "Train: Epoch [19], Batch [94/938], Loss: 0.5845929980278015\n",
      "Train: Epoch [19], Batch [95/938], Loss: 0.723468005657196\n",
      "Train: Epoch [19], Batch [96/938], Loss: 0.6340839266777039\n",
      "Train: Epoch [19], Batch [97/938], Loss: 0.5780510306358337\n",
      "Train: Epoch [19], Batch [98/938], Loss: 0.7116819620132446\n",
      "Train: Epoch [19], Batch [99/938], Loss: 0.6084225177764893\n",
      "Train: Epoch [19], Batch [100/938], Loss: 0.8030335307121277\n",
      "Train: Epoch [19], Batch [101/938], Loss: 0.735731840133667\n",
      "Train: Epoch [19], Batch [102/938], Loss: 0.7297430634498596\n",
      "Train: Epoch [19], Batch [103/938], Loss: 0.6109409332275391\n",
      "Train: Epoch [19], Batch [104/938], Loss: 0.6733033061027527\n",
      "Train: Epoch [19], Batch [105/938], Loss: 0.889533281326294\n",
      "Train: Epoch [19], Batch [106/938], Loss: 0.6869866251945496\n",
      "Train: Epoch [19], Batch [107/938], Loss: 0.7220808267593384\n",
      "Train: Epoch [19], Batch [108/938], Loss: 0.6682554483413696\n",
      "Train: Epoch [19], Batch [109/938], Loss: 0.8855385184288025\n",
      "Train: Epoch [19], Batch [110/938], Loss: 0.7859649062156677\n",
      "Train: Epoch [19], Batch [111/938], Loss: 0.9258908033370972\n",
      "Train: Epoch [19], Batch [112/938], Loss: 0.5242332220077515\n",
      "Train: Epoch [19], Batch [113/938], Loss: 0.7972530126571655\n",
      "Train: Epoch [19], Batch [114/938], Loss: 0.6901937127113342\n",
      "Train: Epoch [19], Batch [115/938], Loss: 0.7091296315193176\n",
      "Train: Epoch [19], Batch [116/938], Loss: 0.7759718894958496\n",
      "Train: Epoch [19], Batch [117/938], Loss: 0.9475799202919006\n",
      "Train: Epoch [19], Batch [118/938], Loss: 0.622678279876709\n",
      "Train: Epoch [19], Batch [119/938], Loss: 0.6500915884971619\n",
      "Train: Epoch [19], Batch [120/938], Loss: 0.67737877368927\n",
      "Train: Epoch [19], Batch [121/938], Loss: 0.5954338908195496\n",
      "Train: Epoch [19], Batch [122/938], Loss: 0.7939555644989014\n",
      "Train: Epoch [19], Batch [123/938], Loss: 0.5465856790542603\n",
      "Train: Epoch [19], Batch [124/938], Loss: 0.4554217457771301\n",
      "Train: Epoch [19], Batch [125/938], Loss: 0.34928590059280396\n",
      "Train: Epoch [19], Batch [126/938], Loss: 0.7934033870697021\n",
      "Train: Epoch [19], Batch [127/938], Loss: 0.5310850143432617\n",
      "Train: Epoch [19], Batch [128/938], Loss: 0.6098968386650085\n",
      "Train: Epoch [19], Batch [129/938], Loss: 0.8762946724891663\n",
      "Train: Epoch [19], Batch [130/938], Loss: 0.9414302110671997\n",
      "Train: Epoch [19], Batch [131/938], Loss: 0.5800549387931824\n",
      "Train: Epoch [19], Batch [132/938], Loss: 0.7706019878387451\n",
      "Train: Epoch [19], Batch [133/938], Loss: 0.5160874724388123\n",
      "Train: Epoch [19], Batch [134/938], Loss: 0.7037127614021301\n",
      "Train: Epoch [19], Batch [135/938], Loss: 0.5305137634277344\n",
      "Train: Epoch [19], Batch [136/938], Loss: 0.6466684937477112\n",
      "Train: Epoch [19], Batch [137/938], Loss: 0.5643319487571716\n",
      "Train: Epoch [19], Batch [138/938], Loss: 0.6371431946754456\n",
      "Train: Epoch [19], Batch [139/938], Loss: 0.7759897112846375\n",
      "Train: Epoch [19], Batch [140/938], Loss: 0.443015992641449\n",
      "Train: Epoch [19], Batch [141/938], Loss: 0.5760102868080139\n",
      "Train: Epoch [19], Batch [142/938], Loss: 0.609210729598999\n",
      "Train: Epoch [19], Batch [143/938], Loss: 0.5743488669395447\n",
      "Train: Epoch [19], Batch [144/938], Loss: 0.8487695455551147\n",
      "Train: Epoch [19], Batch [145/938], Loss: 0.8167873024940491\n",
      "Train: Epoch [19], Batch [146/938], Loss: 0.5430208444595337\n",
      "Train: Epoch [19], Batch [147/938], Loss: 0.6850109100341797\n",
      "Train: Epoch [19], Batch [148/938], Loss: 0.5747348666191101\n",
      "Train: Epoch [19], Batch [149/938], Loss: 0.7228039503097534\n",
      "Train: Epoch [19], Batch [150/938], Loss: 0.6174434423446655\n",
      "Train: Epoch [19], Batch [151/938], Loss: 0.45276007056236267\n",
      "Train: Epoch [19], Batch [152/938], Loss: 0.7526034712791443\n",
      "Train: Epoch [19], Batch [153/938], Loss: 0.6175658702850342\n",
      "Train: Epoch [19], Batch [154/938], Loss: 0.751140296459198\n",
      "Train: Epoch [19], Batch [155/938], Loss: 0.5996179580688477\n",
      "Train: Epoch [19], Batch [156/938], Loss: 0.746437132358551\n",
      "Train: Epoch [19], Batch [157/938], Loss: 0.5427571535110474\n",
      "Train: Epoch [19], Batch [158/938], Loss: 0.5867699384689331\n",
      "Train: Epoch [19], Batch [159/938], Loss: 0.7837154269218445\n",
      "Train: Epoch [19], Batch [160/938], Loss: 0.7523472309112549\n",
      "Train: Epoch [19], Batch [161/938], Loss: 0.44474178552627563\n",
      "Train: Epoch [19], Batch [162/938], Loss: 0.7198799848556519\n",
      "Train: Epoch [19], Batch [163/938], Loss: 0.4830646812915802\n",
      "Train: Epoch [19], Batch [164/938], Loss: 0.4258308708667755\n",
      "Train: Epoch [19], Batch [165/938], Loss: 0.5608930587768555\n",
      "Train: Epoch [19], Batch [166/938], Loss: 0.5224906206130981\n",
      "Train: Epoch [19], Batch [167/938], Loss: 0.6849460005760193\n",
      "Train: Epoch [19], Batch [168/938], Loss: 0.6365950107574463\n",
      "Train: Epoch [19], Batch [169/938], Loss: 0.49301767349243164\n",
      "Train: Epoch [19], Batch [170/938], Loss: 0.8993683457374573\n",
      "Train: Epoch [19], Batch [171/938], Loss: 0.7528455257415771\n",
      "Train: Epoch [19], Batch [172/938], Loss: 0.49795639514923096\n",
      "Train: Epoch [19], Batch [173/938], Loss: 0.8582054376602173\n",
      "Train: Epoch [19], Batch [174/938], Loss: 0.595125138759613\n",
      "Train: Epoch [19], Batch [175/938], Loss: 0.7221677899360657\n",
      "Train: Epoch [19], Batch [176/938], Loss: 0.7383118271827698\n",
      "Train: Epoch [19], Batch [177/938], Loss: 0.8070558309555054\n",
      "Train: Epoch [19], Batch [178/938], Loss: 0.7587158679962158\n",
      "Train: Epoch [19], Batch [179/938], Loss: 0.8193460702896118\n",
      "Train: Epoch [19], Batch [180/938], Loss: 0.6973276138305664\n",
      "Train: Epoch [19], Batch [181/938], Loss: 0.9041970372200012\n",
      "Train: Epoch [19], Batch [182/938], Loss: 0.7440152168273926\n",
      "Train: Epoch [19], Batch [183/938], Loss: 0.7881099581718445\n",
      "Train: Epoch [19], Batch [184/938], Loss: 0.49081405997276306\n",
      "Train: Epoch [19], Batch [185/938], Loss: 0.3963705897331238\n",
      "Train: Epoch [19], Batch [186/938], Loss: 0.4507382810115814\n",
      "Train: Epoch [19], Batch [187/938], Loss: 0.657952070236206\n",
      "Train: Epoch [19], Batch [188/938], Loss: 0.760898768901825\n",
      "Train: Epoch [19], Batch [189/938], Loss: 0.8663085699081421\n",
      "Train: Epoch [19], Batch [190/938], Loss: 0.5434126257896423\n",
      "Train: Epoch [19], Batch [191/938], Loss: 0.5157872438430786\n",
      "Train: Epoch [19], Batch [192/938], Loss: 0.5977149605751038\n",
      "Train: Epoch [19], Batch [193/938], Loss: 0.8239532113075256\n",
      "Train: Epoch [19], Batch [194/938], Loss: 0.6585262417793274\n",
      "Train: Epoch [19], Batch [195/938], Loss: 0.5208706855773926\n",
      "Train: Epoch [19], Batch [196/938], Loss: 0.3931497037410736\n",
      "Train: Epoch [19], Batch [197/938], Loss: 0.6891790628433228\n",
      "Train: Epoch [19], Batch [198/938], Loss: 0.7467257976531982\n",
      "Train: Epoch [19], Batch [199/938], Loss: 0.6779084205627441\n",
      "Train: Epoch [19], Batch [200/938], Loss: 0.5726176500320435\n",
      "Train: Epoch [19], Batch [201/938], Loss: 0.7761605978012085\n",
      "Train: Epoch [19], Batch [202/938], Loss: 0.6837491989135742\n",
      "Train: Epoch [19], Batch [203/938], Loss: 0.722516655921936\n",
      "Train: Epoch [19], Batch [204/938], Loss: 1.1474107503890991\n",
      "Train: Epoch [19], Batch [205/938], Loss: 0.5332545638084412\n",
      "Train: Epoch [19], Batch [206/938], Loss: 0.5201961994171143\n",
      "Train: Epoch [19], Batch [207/938], Loss: 0.5177531242370605\n",
      "Train: Epoch [19], Batch [208/938], Loss: 0.5992692112922668\n",
      "Train: Epoch [19], Batch [209/938], Loss: 0.574254035949707\n",
      "Train: Epoch [19], Batch [210/938], Loss: 0.749923586845398\n",
      "Train: Epoch [19], Batch [211/938], Loss: 0.5217123627662659\n",
      "Train: Epoch [19], Batch [212/938], Loss: 0.6467923521995544\n",
      "Train: Epoch [19], Batch [213/938], Loss: 0.5709977746009827\n",
      "Train: Epoch [19], Batch [214/938], Loss: 0.5791366100311279\n",
      "Train: Epoch [19], Batch [215/938], Loss: 0.5624852776527405\n",
      "Train: Epoch [19], Batch [216/938], Loss: 0.9447903037071228\n",
      "Train: Epoch [19], Batch [217/938], Loss: 0.7037997245788574\n",
      "Train: Epoch [19], Batch [218/938], Loss: 0.595794677734375\n",
      "Train: Epoch [19], Batch [219/938], Loss: 0.5299228429794312\n",
      "Train: Epoch [19], Batch [220/938], Loss: 0.6417387127876282\n",
      "Train: Epoch [19], Batch [221/938], Loss: 0.7380269765853882\n",
      "Train: Epoch [19], Batch [222/938], Loss: 0.6103391647338867\n",
      "Train: Epoch [19], Batch [223/938], Loss: 0.5087756514549255\n",
      "Train: Epoch [19], Batch [224/938], Loss: 0.7552165389060974\n",
      "Train: Epoch [19], Batch [225/938], Loss: 0.6331915855407715\n",
      "Train: Epoch [19], Batch [226/938], Loss: 0.7106000185012817\n",
      "Train: Epoch [19], Batch [227/938], Loss: 0.5764798521995544\n",
      "Train: Epoch [19], Batch [228/938], Loss: 0.9082569479942322\n",
      "Train: Epoch [19], Batch [229/938], Loss: 0.9140291810035706\n",
      "Train: Epoch [19], Batch [230/938], Loss: 0.4559592306613922\n",
      "Train: Epoch [19], Batch [231/938], Loss: 0.9381494522094727\n",
      "Train: Epoch [19], Batch [232/938], Loss: 0.6380702257156372\n",
      "Train: Epoch [19], Batch [233/938], Loss: 0.7158419489860535\n",
      "Train: Epoch [19], Batch [234/938], Loss: 0.7287601828575134\n",
      "Train: Epoch [19], Batch [235/938], Loss: 0.6900089979171753\n",
      "Train: Epoch [19], Batch [236/938], Loss: 0.5248452425003052\n",
      "Train: Epoch [19], Batch [237/938], Loss: 0.5566809773445129\n",
      "Train: Epoch [19], Batch [238/938], Loss: 0.6026034355163574\n",
      "Train: Epoch [19], Batch [239/938], Loss: 0.5322756171226501\n",
      "Train: Epoch [19], Batch [240/938], Loss: 0.6015775799751282\n",
      "Train: Epoch [19], Batch [241/938], Loss: 0.8515208959579468\n",
      "Train: Epoch [19], Batch [242/938], Loss: 0.7389892935752869\n",
      "Train: Epoch [19], Batch [243/938], Loss: 0.5761488080024719\n",
      "Train: Epoch [19], Batch [244/938], Loss: 0.6326498985290527\n",
      "Train: Epoch [19], Batch [245/938], Loss: 0.6180555820465088\n",
      "Train: Epoch [19], Batch [246/938], Loss: 0.5437018275260925\n",
      "Train: Epoch [19], Batch [247/938], Loss: 0.5630513429641724\n",
      "Train: Epoch [19], Batch [248/938], Loss: 0.664239764213562\n",
      "Train: Epoch [19], Batch [249/938], Loss: 0.6086694002151489\n",
      "Train: Epoch [19], Batch [250/938], Loss: 0.5182703733444214\n",
      "Train: Epoch [19], Batch [251/938], Loss: 0.7218447923660278\n",
      "Train: Epoch [19], Batch [252/938], Loss: 0.7827427387237549\n",
      "Train: Epoch [19], Batch [253/938], Loss: 0.7935013771057129\n",
      "Train: Epoch [19], Batch [254/938], Loss: 0.7500432729721069\n",
      "Train: Epoch [19], Batch [255/938], Loss: 0.7363462448120117\n",
      "Train: Epoch [19], Batch [256/938], Loss: 0.6724775433540344\n",
      "Train: Epoch [19], Batch [257/938], Loss: 0.5952678322792053\n",
      "Train: Epoch [19], Batch [258/938], Loss: 0.6703058481216431\n",
      "Train: Epoch [19], Batch [259/938], Loss: 0.6993236541748047\n",
      "Train: Epoch [19], Batch [260/938], Loss: 0.46737200021743774\n",
      "Train: Epoch [19], Batch [261/938], Loss: 0.6962218284606934\n",
      "Train: Epoch [19], Batch [262/938], Loss: 0.74741131067276\n",
      "Train: Epoch [19], Batch [263/938], Loss: 0.5581697225570679\n",
      "Train: Epoch [19], Batch [264/938], Loss: 0.528012752532959\n",
      "Train: Epoch [19], Batch [265/938], Loss: 0.47633257508277893\n",
      "Train: Epoch [19], Batch [266/938], Loss: 0.6797348260879517\n",
      "Train: Epoch [19], Batch [267/938], Loss: 0.778132975101471\n",
      "Train: Epoch [19], Batch [268/938], Loss: 0.7511583566665649\n",
      "Train: Epoch [19], Batch [269/938], Loss: 0.6475991606712341\n",
      "Train: Epoch [19], Batch [270/938], Loss: 0.6463872194290161\n",
      "Train: Epoch [19], Batch [271/938], Loss: 0.5611974596977234\n",
      "Train: Epoch [19], Batch [272/938], Loss: 0.7716963291168213\n",
      "Train: Epoch [19], Batch [273/938], Loss: 0.7263764142990112\n",
      "Train: Epoch [19], Batch [274/938], Loss: 0.6418606042861938\n",
      "Train: Epoch [19], Batch [275/938], Loss: 0.4978390634059906\n",
      "Train: Epoch [19], Batch [276/938], Loss: 0.6223065853118896\n",
      "Train: Epoch [19], Batch [277/938], Loss: 0.6471691727638245\n",
      "Train: Epoch [19], Batch [278/938], Loss: 0.552225649356842\n",
      "Train: Epoch [19], Batch [279/938], Loss: 0.46127843856811523\n",
      "Train: Epoch [19], Batch [280/938], Loss: 0.6087958812713623\n",
      "Train: Epoch [19], Batch [281/938], Loss: 0.7512421607971191\n",
      "Train: Epoch [19], Batch [282/938], Loss: 0.6527289152145386\n",
      "Train: Epoch [19], Batch [283/938], Loss: 0.684135377407074\n",
      "Train: Epoch [19], Batch [284/938], Loss: 0.8423758745193481\n",
      "Train: Epoch [19], Batch [285/938], Loss: 0.6277195811271667\n",
      "Train: Epoch [19], Batch [286/938], Loss: 0.619900107383728\n",
      "Train: Epoch [19], Batch [287/938], Loss: 0.6386458277702332\n",
      "Train: Epoch [19], Batch [288/938], Loss: 0.5844016671180725\n",
      "Train: Epoch [19], Batch [289/938], Loss: 0.5286323428153992\n",
      "Train: Epoch [19], Batch [290/938], Loss: 0.5311886072158813\n",
      "Train: Epoch [19], Batch [291/938], Loss: 0.744817852973938\n",
      "Train: Epoch [19], Batch [292/938], Loss: 0.6968603134155273\n",
      "Train: Epoch [19], Batch [293/938], Loss: 0.5691978335380554\n",
      "Train: Epoch [19], Batch [294/938], Loss: 0.669319212436676\n",
      "Train: Epoch [19], Batch [295/938], Loss: 0.5999309420585632\n",
      "Train: Epoch [19], Batch [296/938], Loss: 0.6765623688697815\n",
      "Train: Epoch [19], Batch [297/938], Loss: 0.7833222150802612\n",
      "Train: Epoch [19], Batch [298/938], Loss: 0.6284339427947998\n",
      "Train: Epoch [19], Batch [299/938], Loss: 0.8563721776008606\n",
      "Train: Epoch [19], Batch [300/938], Loss: 0.34907156229019165\n",
      "Train: Epoch [19], Batch [301/938], Loss: 0.7394727468490601\n",
      "Train: Epoch [19], Batch [302/938], Loss: 0.5742024779319763\n",
      "Train: Epoch [19], Batch [303/938], Loss: 0.719837486743927\n",
      "Train: Epoch [19], Batch [304/938], Loss: 0.671946108341217\n",
      "Train: Epoch [19], Batch [305/938], Loss: 0.4486704170703888\n",
      "Train: Epoch [19], Batch [306/938], Loss: 0.8789829015731812\n",
      "Train: Epoch [19], Batch [307/938], Loss: 0.49517151713371277\n",
      "Train: Epoch [19], Batch [308/938], Loss: 0.38034898042678833\n",
      "Train: Epoch [19], Batch [309/938], Loss: 0.938004732131958\n",
      "Train: Epoch [19], Batch [310/938], Loss: 0.5259470343589783\n",
      "Train: Epoch [19], Batch [311/938], Loss: 0.5826470255851746\n",
      "Train: Epoch [19], Batch [312/938], Loss: 0.6328794956207275\n",
      "Train: Epoch [19], Batch [313/938], Loss: 0.5344389081001282\n",
      "Train: Epoch [19], Batch [314/938], Loss: 0.6700422763824463\n",
      "Train: Epoch [19], Batch [315/938], Loss: 0.628388524055481\n",
      "Train: Epoch [19], Batch [316/938], Loss: 0.6606835722923279\n",
      "Train: Epoch [19], Batch [317/938], Loss: 0.7691552042961121\n",
      "Train: Epoch [19], Batch [318/938], Loss: 0.6661796569824219\n",
      "Train: Epoch [19], Batch [319/938], Loss: 0.6507409811019897\n",
      "Train: Epoch [19], Batch [320/938], Loss: 0.6451055407524109\n",
      "Train: Epoch [19], Batch [321/938], Loss: 0.5714761018753052\n",
      "Train: Epoch [19], Batch [322/938], Loss: 0.6204143762588501\n",
      "Train: Epoch [19], Batch [323/938], Loss: 0.7185245156288147\n",
      "Train: Epoch [19], Batch [324/938], Loss: 0.7180529832839966\n",
      "Train: Epoch [19], Batch [325/938], Loss: 0.7464362978935242\n",
      "Train: Epoch [19], Batch [326/938], Loss: 0.8856961131095886\n",
      "Train: Epoch [19], Batch [327/938], Loss: 0.6826677322387695\n",
      "Train: Epoch [19], Batch [328/938], Loss: 0.6899967193603516\n",
      "Train: Epoch [19], Batch [329/938], Loss: 0.5569666624069214\n",
      "Train: Epoch [19], Batch [330/938], Loss: 0.6740543246269226\n",
      "Train: Epoch [19], Batch [331/938], Loss: 0.5255308151245117\n",
      "Train: Epoch [19], Batch [332/938], Loss: 0.9588147401809692\n",
      "Train: Epoch [19], Batch [333/938], Loss: 0.4418400228023529\n",
      "Train: Epoch [19], Batch [334/938], Loss: 0.719402551651001\n",
      "Train: Epoch [19], Batch [335/938], Loss: 0.895174503326416\n",
      "Train: Epoch [19], Batch [336/938], Loss: 0.722879946231842\n",
      "Train: Epoch [19], Batch [337/938], Loss: 0.6870353817939758\n",
      "Train: Epoch [19], Batch [338/938], Loss: 0.5455073118209839\n",
      "Train: Epoch [19], Batch [339/938], Loss: 0.5918006300926208\n",
      "Train: Epoch [19], Batch [340/938], Loss: 0.5820200443267822\n",
      "Train: Epoch [19], Batch [341/938], Loss: 0.4090030789375305\n",
      "Train: Epoch [19], Batch [342/938], Loss: 0.48121827840805054\n",
      "Train: Epoch [19], Batch [343/938], Loss: 0.5074720978736877\n",
      "Train: Epoch [19], Batch [344/938], Loss: 0.5938926339149475\n",
      "Train: Epoch [19], Batch [345/938], Loss: 0.768686830997467\n",
      "Train: Epoch [19], Batch [346/938], Loss: 0.594058096408844\n",
      "Train: Epoch [19], Batch [347/938], Loss: 0.7169432640075684\n",
      "Train: Epoch [19], Batch [348/938], Loss: 0.5084841251373291\n",
      "Train: Epoch [19], Batch [349/938], Loss: 0.43553659319877625\n",
      "Train: Epoch [19], Batch [350/938], Loss: 0.5898634195327759\n",
      "Train: Epoch [19], Batch [351/938], Loss: 0.49969443678855896\n",
      "Train: Epoch [19], Batch [352/938], Loss: 0.5039591193199158\n",
      "Train: Epoch [19], Batch [353/938], Loss: 0.48547589778900146\n",
      "Train: Epoch [19], Batch [354/938], Loss: 0.4258016049861908\n",
      "Train: Epoch [19], Batch [355/938], Loss: 0.33415281772613525\n",
      "Train: Epoch [19], Batch [356/938], Loss: 0.5163347721099854\n",
      "Train: Epoch [19], Batch [357/938], Loss: 0.7117988467216492\n",
      "Train: Epoch [19], Batch [358/938], Loss: 0.42757701873779297\n",
      "Train: Epoch [19], Batch [359/938], Loss: 0.5734536647796631\n",
      "Train: Epoch [19], Batch [360/938], Loss: 0.31102845072746277\n",
      "Train: Epoch [19], Batch [361/938], Loss: 0.37445542216300964\n",
      "Train: Epoch [19], Batch [362/938], Loss: 0.5391889214515686\n",
      "Train: Epoch [19], Batch [363/938], Loss: 0.3334720730781555\n",
      "Train: Epoch [19], Batch [364/938], Loss: 0.614377498626709\n",
      "Train: Epoch [19], Batch [365/938], Loss: 0.3081563413143158\n",
      "Train: Epoch [19], Batch [366/938], Loss: 0.4022694230079651\n",
      "Train: Epoch [19], Batch [367/938], Loss: 0.42648956179618835\n",
      "Train: Epoch [19], Batch [368/938], Loss: 0.6733725070953369\n",
      "Train: Epoch [19], Batch [369/938], Loss: 0.46587133407592773\n",
      "Train: Epoch [19], Batch [370/938], Loss: 0.4911240339279175\n",
      "Train: Epoch [19], Batch [371/938], Loss: 0.4454493224620819\n",
      "Train: Epoch [19], Batch [372/938], Loss: 0.7749705910682678\n",
      "Train: Epoch [19], Batch [373/938], Loss: 0.3557356297969818\n",
      "Train: Epoch [19], Batch [374/938], Loss: 0.7199309468269348\n",
      "Train: Epoch [19], Batch [375/938], Loss: 0.5477887988090515\n",
      "Train: Epoch [19], Batch [376/938], Loss: 0.4782204031944275\n",
      "Train: Epoch [19], Batch [377/938], Loss: 0.8173405528068542\n",
      "Train: Epoch [19], Batch [378/938], Loss: 0.4583370089530945\n",
      "Train: Epoch [19], Batch [379/938], Loss: 0.5004372000694275\n",
      "Train: Epoch [19], Batch [380/938], Loss: 0.4341282248497009\n",
      "Train: Epoch [19], Batch [381/938], Loss: 0.4460979700088501\n",
      "Train: Epoch [19], Batch [382/938], Loss: 0.7245333790779114\n",
      "Train: Epoch [19], Batch [383/938], Loss: 0.5804687738418579\n",
      "Train: Epoch [19], Batch [384/938], Loss: 0.5034595727920532\n",
      "Train: Epoch [19], Batch [385/938], Loss: 0.4806235730648041\n",
      "Train: Epoch [19], Batch [386/938], Loss: 0.40281859040260315\n",
      "Train: Epoch [19], Batch [387/938], Loss: 0.7483481764793396\n",
      "Train: Epoch [19], Batch [388/938], Loss: 0.46906793117523193\n",
      "Train: Epoch [19], Batch [389/938], Loss: 0.520607590675354\n",
      "Train: Epoch [19], Batch [390/938], Loss: 0.590165376663208\n",
      "Train: Epoch [19], Batch [391/938], Loss: 0.28332844376564026\n",
      "Train: Epoch [19], Batch [392/938], Loss: 0.6617553234100342\n",
      "Train: Epoch [19], Batch [393/938], Loss: 0.4197191596031189\n",
      "Train: Epoch [19], Batch [394/938], Loss: 0.5041943788528442\n",
      "Train: Epoch [19], Batch [395/938], Loss: 0.5427780747413635\n",
      "Train: Epoch [19], Batch [396/938], Loss: 0.6171751022338867\n",
      "Train: Epoch [19], Batch [397/938], Loss: 0.40115711092948914\n",
      "Train: Epoch [19], Batch [398/938], Loss: 0.6102458834648132\n",
      "Train: Epoch [19], Batch [399/938], Loss: 0.4773637652397156\n",
      "Train: Epoch [19], Batch [400/938], Loss: 0.4546838402748108\n",
      "Train: Epoch [19], Batch [401/938], Loss: 0.47889000177383423\n",
      "Train: Epoch [19], Batch [402/938], Loss: 0.4156244099140167\n",
      "Train: Epoch [19], Batch [403/938], Loss: 0.42934513092041016\n",
      "Train: Epoch [19], Batch [404/938], Loss: 0.4049880802631378\n",
      "Train: Epoch [19], Batch [405/938], Loss: 0.67313551902771\n",
      "Train: Epoch [19], Batch [406/938], Loss: 0.45495015382766724\n",
      "Train: Epoch [19], Batch [407/938], Loss: 0.4158398509025574\n",
      "Train: Epoch [19], Batch [408/938], Loss: 0.6833760142326355\n",
      "Train: Epoch [19], Batch [409/938], Loss: 0.5774980783462524\n",
      "Train: Epoch [19], Batch [410/938], Loss: 0.5524318218231201\n",
      "Train: Epoch [19], Batch [411/938], Loss: 0.43702152371406555\n",
      "Train: Epoch [19], Batch [412/938], Loss: 0.44037070870399475\n",
      "Train: Epoch [19], Batch [413/938], Loss: 0.4180023968219757\n",
      "Train: Epoch [19], Batch [414/938], Loss: 0.4440464377403259\n",
      "Train: Epoch [19], Batch [415/938], Loss: 0.5790075659751892\n",
      "Train: Epoch [19], Batch [416/938], Loss: 0.3712909519672394\n",
      "Train: Epoch [19], Batch [417/938], Loss: 0.44106525182724\n",
      "Train: Epoch [19], Batch [418/938], Loss: 0.40835773944854736\n",
      "Train: Epoch [19], Batch [419/938], Loss: 0.5958875417709351\n",
      "Train: Epoch [19], Batch [420/938], Loss: 0.6154357194900513\n",
      "Train: Epoch [19], Batch [421/938], Loss: 0.5471890568733215\n",
      "Train: Epoch [19], Batch [422/938], Loss: 0.3089793026447296\n",
      "Train: Epoch [19], Batch [423/938], Loss: 0.5792655944824219\n",
      "Train: Epoch [19], Batch [424/938], Loss: 0.5730653405189514\n",
      "Train: Epoch [19], Batch [425/938], Loss: 0.3776741921901703\n",
      "Train: Epoch [19], Batch [426/938], Loss: 0.35912787914276123\n",
      "Train: Epoch [19], Batch [427/938], Loss: 0.660265326499939\n",
      "Train: Epoch [19], Batch [428/938], Loss: 0.4757447838783264\n",
      "Train: Epoch [19], Batch [429/938], Loss: 0.6852081418037415\n",
      "Train: Epoch [19], Batch [430/938], Loss: 0.5780650973320007\n",
      "Train: Epoch [19], Batch [431/938], Loss: 0.6404626369476318\n",
      "Train: Epoch [19], Batch [432/938], Loss: 0.7877516746520996\n",
      "Train: Epoch [19], Batch [433/938], Loss: 0.5380526781082153\n",
      "Train: Epoch [19], Batch [434/938], Loss: 0.576119601726532\n",
      "Train: Epoch [19], Batch [435/938], Loss: 0.5040515065193176\n",
      "Train: Epoch [19], Batch [436/938], Loss: 0.3119167983531952\n",
      "Train: Epoch [19], Batch [437/938], Loss: 0.47577789425849915\n",
      "Train: Epoch [19], Batch [438/938], Loss: 0.4159952700138092\n",
      "Train: Epoch [19], Batch [439/938], Loss: 0.6555068492889404\n",
      "Train: Epoch [19], Batch [440/938], Loss: 0.43029332160949707\n",
      "Train: Epoch [19], Batch [441/938], Loss: 0.45005708932876587\n",
      "Train: Epoch [19], Batch [442/938], Loss: 0.5267121195793152\n",
      "Train: Epoch [19], Batch [443/938], Loss: 0.582277774810791\n",
      "Train: Epoch [19], Batch [444/938], Loss: 0.6977143883705139\n",
      "Train: Epoch [19], Batch [445/938], Loss: 0.29754671454429626\n",
      "Train: Epoch [19], Batch [446/938], Loss: 0.6373212933540344\n",
      "Train: Epoch [19], Batch [447/938], Loss: 0.5945158004760742\n",
      "Train: Epoch [19], Batch [448/938], Loss: 0.23254895210266113\n",
      "Train: Epoch [19], Batch [449/938], Loss: 0.25035613775253296\n",
      "Train: Epoch [19], Batch [450/938], Loss: 0.6383780241012573\n",
      "Train: Epoch [19], Batch [451/938], Loss: 0.6018877625465393\n",
      "Train: Epoch [19], Batch [452/938], Loss: 0.4449222981929779\n",
      "Train: Epoch [19], Batch [453/938], Loss: 0.49907347559928894\n",
      "Train: Epoch [19], Batch [454/938], Loss: 0.8174793720245361\n",
      "Train: Epoch [19], Batch [455/938], Loss: 0.7869911789894104\n",
      "Train: Epoch [19], Batch [456/938], Loss: 0.638978123664856\n",
      "Train: Epoch [19], Batch [457/938], Loss: 0.7058374285697937\n",
      "Train: Epoch [19], Batch [458/938], Loss: 0.42032527923583984\n",
      "Train: Epoch [19], Batch [459/938], Loss: 0.5235705375671387\n",
      "Train: Epoch [19], Batch [460/938], Loss: 0.44320008158683777\n",
      "Train: Epoch [19], Batch [461/938], Loss: 0.5606961250305176\n",
      "Train: Epoch [19], Batch [462/938], Loss: 0.5122885704040527\n",
      "Train: Epoch [19], Batch [463/938], Loss: 0.5394074320793152\n",
      "Train: Epoch [19], Batch [464/938], Loss: 0.5564576387405396\n",
      "Train: Epoch [19], Batch [465/938], Loss: 0.46926695108413696\n",
      "Train: Epoch [19], Batch [466/938], Loss: 0.3926028907299042\n",
      "Train: Epoch [19], Batch [467/938], Loss: 0.4183817505836487\n",
      "Train: Epoch [19], Batch [468/938], Loss: 0.5858089923858643\n",
      "Train: Epoch [19], Batch [469/938], Loss: 0.37792861461639404\n",
      "Train: Epoch [19], Batch [470/938], Loss: 0.4641871750354767\n",
      "Train: Epoch [19], Batch [471/938], Loss: 0.7353379130363464\n",
      "Train: Epoch [19], Batch [472/938], Loss: 0.5619347095489502\n",
      "Train: Epoch [19], Batch [473/938], Loss: 0.47212010622024536\n",
      "Train: Epoch [19], Batch [474/938], Loss: 0.4987204968929291\n",
      "Train: Epoch [19], Batch [475/938], Loss: 0.420870840549469\n",
      "Train: Epoch [19], Batch [476/938], Loss: 0.46298760175704956\n",
      "Train: Epoch [19], Batch [477/938], Loss: 0.5780316591262817\n",
      "Train: Epoch [19], Batch [478/938], Loss: 0.5355615615844727\n",
      "Train: Epoch [19], Batch [479/938], Loss: 0.5741701126098633\n",
      "Train: Epoch [19], Batch [480/938], Loss: 0.3794596493244171\n",
      "Train: Epoch [19], Batch [481/938], Loss: 0.731160581111908\n",
      "Train: Epoch [19], Batch [482/938], Loss: 0.4163111448287964\n",
      "Train: Epoch [19], Batch [483/938], Loss: 0.4505827724933624\n",
      "Train: Epoch [19], Batch [484/938], Loss: 0.5590241551399231\n",
      "Train: Epoch [19], Batch [485/938], Loss: 0.6363685131072998\n",
      "Train: Epoch [19], Batch [486/938], Loss: 0.5109107494354248\n",
      "Train: Epoch [19], Batch [487/938], Loss: 0.4362827241420746\n",
      "Train: Epoch [19], Batch [488/938], Loss: 0.5681189298629761\n",
      "Train: Epoch [19], Batch [489/938], Loss: 0.5355760455131531\n",
      "Train: Epoch [19], Batch [490/938], Loss: 0.39160287380218506\n",
      "Train: Epoch [19], Batch [491/938], Loss: 0.3686029314994812\n",
      "Train: Epoch [19], Batch [492/938], Loss: 0.5216826796531677\n",
      "Train: Epoch [19], Batch [493/938], Loss: 0.48881882429122925\n",
      "Train: Epoch [19], Batch [494/938], Loss: 0.355712354183197\n",
      "Train: Epoch [19], Batch [495/938], Loss: 0.37915316224098206\n",
      "Train: Epoch [19], Batch [496/938], Loss: 0.6457216143608093\n",
      "Train: Epoch [19], Batch [497/938], Loss: 0.4928746819496155\n",
      "Train: Epoch [19], Batch [498/938], Loss: 0.5839503407478333\n",
      "Train: Epoch [19], Batch [499/938], Loss: 0.6303841471672058\n",
      "Train: Epoch [19], Batch [500/938], Loss: 0.4090513288974762\n",
      "Train: Epoch [19], Batch [501/938], Loss: 0.5388269424438477\n",
      "Train: Epoch [19], Batch [502/938], Loss: 0.32195767760276794\n",
      "Train: Epoch [19], Batch [503/938], Loss: 0.5171216726303101\n",
      "Train: Epoch [19], Batch [504/938], Loss: 0.5146438479423523\n",
      "Train: Epoch [19], Batch [505/938], Loss: 0.2908214330673218\n",
      "Train: Epoch [19], Batch [506/938], Loss: 0.5610147714614868\n",
      "Train: Epoch [19], Batch [507/938], Loss: 0.45715874433517456\n",
      "Train: Epoch [19], Batch [508/938], Loss: 0.27567756175994873\n",
      "Train: Epoch [19], Batch [509/938], Loss: 0.3323274850845337\n",
      "Train: Epoch [19], Batch [510/938], Loss: 0.5770927667617798\n",
      "Train: Epoch [19], Batch [511/938], Loss: 0.3801780939102173\n",
      "Train: Epoch [19], Batch [512/938], Loss: 0.8366813659667969\n",
      "Train: Epoch [19], Batch [513/938], Loss: 0.35462817549705505\n",
      "Train: Epoch [19], Batch [514/938], Loss: 0.5257559418678284\n",
      "Train: Epoch [19], Batch [515/938], Loss: 0.5214852094650269\n",
      "Train: Epoch [19], Batch [516/938], Loss: 0.48326146602630615\n",
      "Train: Epoch [19], Batch [517/938], Loss: 0.5948021411895752\n",
      "Train: Epoch [19], Batch [518/938], Loss: 0.6046184301376343\n",
      "Train: Epoch [19], Batch [519/938], Loss: 0.4804611802101135\n",
      "Train: Epoch [19], Batch [520/938], Loss: 0.584977924823761\n",
      "Train: Epoch [19], Batch [521/938], Loss: 0.48494523763656616\n",
      "Train: Epoch [19], Batch [522/938], Loss: 0.49886178970336914\n",
      "Train: Epoch [19], Batch [523/938], Loss: 0.3729073107242584\n",
      "Train: Epoch [19], Batch [524/938], Loss: 0.4865783452987671\n",
      "Train: Epoch [19], Batch [525/938], Loss: 0.641337513923645\n",
      "Train: Epoch [19], Batch [526/938], Loss: 0.5823022723197937\n",
      "Train: Epoch [19], Batch [527/938], Loss: 0.642864465713501\n",
      "Train: Epoch [19], Batch [528/938], Loss: 0.489473819732666\n",
      "Train: Epoch [19], Batch [529/938], Loss: 0.4393787086009979\n",
      "Train: Epoch [19], Batch [530/938], Loss: 0.5110349059104919\n",
      "Train: Epoch [19], Batch [531/938], Loss: 0.5729038715362549\n",
      "Train: Epoch [19], Batch [532/938], Loss: 0.5607020854949951\n",
      "Train: Epoch [19], Batch [533/938], Loss: 0.36962443590164185\n",
      "Train: Epoch [19], Batch [534/938], Loss: 0.4742690324783325\n",
      "Train: Epoch [19], Batch [535/938], Loss: 0.5267267227172852\n",
      "Train: Epoch [19], Batch [536/938], Loss: 0.5794886350631714\n",
      "Train: Epoch [19], Batch [537/938], Loss: 0.3828754127025604\n",
      "Train: Epoch [19], Batch [538/938], Loss: 0.5422878861427307\n",
      "Train: Epoch [19], Batch [539/938], Loss: 0.2921968102455139\n",
      "Train: Epoch [19], Batch [540/938], Loss: 0.5613947510719299\n",
      "Train: Epoch [19], Batch [541/938], Loss: 0.3336585760116577\n",
      "Train: Epoch [19], Batch [542/938], Loss: 0.40494731068611145\n",
      "Train: Epoch [19], Batch [543/938], Loss: 0.5009663701057434\n",
      "Train: Epoch [19], Batch [544/938], Loss: 0.30601805448532104\n",
      "Train: Epoch [19], Batch [545/938], Loss: 0.4314902424812317\n",
      "Train: Epoch [19], Batch [546/938], Loss: 0.26458123326301575\n",
      "Train: Epoch [19], Batch [547/938], Loss: 0.3151995837688446\n",
      "Train: Epoch [19], Batch [548/938], Loss: 0.518614649772644\n",
      "Train: Epoch [19], Batch [549/938], Loss: 0.4053576588630676\n",
      "Train: Epoch [19], Batch [550/938], Loss: 0.49992305040359497\n",
      "Train: Epoch [19], Batch [551/938], Loss: 0.325349897146225\n",
      "Train: Epoch [19], Batch [552/938], Loss: 0.4719955623149872\n",
      "Train: Epoch [19], Batch [553/938], Loss: 0.6088578701019287\n",
      "Train: Epoch [19], Batch [554/938], Loss: 0.7185500860214233\n",
      "Train: Epoch [19], Batch [555/938], Loss: 0.43806424736976624\n",
      "Train: Epoch [19], Batch [556/938], Loss: 0.5434979796409607\n",
      "Train: Epoch [19], Batch [557/938], Loss: 0.397164523601532\n",
      "Train: Epoch [19], Batch [558/938], Loss: 0.4826383590698242\n",
      "Train: Epoch [19], Batch [559/938], Loss: 0.38750702142715454\n",
      "Train: Epoch [19], Batch [560/938], Loss: 0.5656434297561646\n",
      "Train: Epoch [19], Batch [561/938], Loss: 0.41027623414993286\n",
      "Train: Epoch [19], Batch [562/938], Loss: 0.35315895080566406\n",
      "Train: Epoch [19], Batch [563/938], Loss: 0.36335477232933044\n",
      "Train: Epoch [19], Batch [564/938], Loss: 0.4931066036224365\n",
      "Train: Epoch [19], Batch [565/938], Loss: 0.40937599539756775\n",
      "Train: Epoch [19], Batch [566/938], Loss: 0.6396218538284302\n",
      "Train: Epoch [19], Batch [567/938], Loss: 0.32346683740615845\n",
      "Train: Epoch [19], Batch [568/938], Loss: 0.5185596346855164\n",
      "Train: Epoch [19], Batch [569/938], Loss: 0.45606550574302673\n",
      "Train: Epoch [19], Batch [570/938], Loss: 0.5418708324432373\n",
      "Train: Epoch [19], Batch [571/938], Loss: 0.31721776723861694\n",
      "Train: Epoch [19], Batch [572/938], Loss: 0.6019238829612732\n",
      "Train: Epoch [19], Batch [573/938], Loss: 0.4843684732913971\n",
      "Train: Epoch [19], Batch [574/938], Loss: 0.5220617651939392\n",
      "Train: Epoch [19], Batch [575/938], Loss: 0.5637215971946716\n",
      "Train: Epoch [19], Batch [576/938], Loss: 0.29833176732063293\n",
      "Train: Epoch [19], Batch [577/938], Loss: 0.41062790155410767\n",
      "Train: Epoch [19], Batch [578/938], Loss: 0.6310129165649414\n",
      "Train: Epoch [19], Batch [579/938], Loss: 0.41280466318130493\n",
      "Train: Epoch [19], Batch [580/938], Loss: 0.42395877838134766\n",
      "Train: Epoch [19], Batch [581/938], Loss: 0.4940963387489319\n",
      "Train: Epoch [19], Batch [582/938], Loss: 0.5556182861328125\n",
      "Train: Epoch [19], Batch [583/938], Loss: 0.3817334771156311\n",
      "Train: Epoch [19], Batch [584/938], Loss: 0.4227437376976013\n",
      "Train: Epoch [19], Batch [585/938], Loss: 0.49742868542671204\n",
      "Train: Epoch [19], Batch [586/938], Loss: 0.2543458342552185\n",
      "Train: Epoch [19], Batch [587/938], Loss: 0.272521048784256\n",
      "Train: Epoch [19], Batch [588/938], Loss: 0.4754789471626282\n",
      "Train: Epoch [19], Batch [589/938], Loss: 0.4224212169647217\n",
      "Train: Epoch [19], Batch [590/938], Loss: 0.6462705135345459\n",
      "Train: Epoch [19], Batch [591/938], Loss: 0.5397672653198242\n",
      "Train: Epoch [19], Batch [592/938], Loss: 0.359417587518692\n",
      "Train: Epoch [19], Batch [593/938], Loss: 0.5224608182907104\n",
      "Train: Epoch [19], Batch [594/938], Loss: 0.5782570838928223\n",
      "Train: Epoch [19], Batch [595/938], Loss: 0.5964306592941284\n",
      "Train: Epoch [19], Batch [596/938], Loss: 0.5152005553245544\n",
      "Train: Epoch [19], Batch [597/938], Loss: 0.49325859546661377\n",
      "Train: Epoch [19], Batch [598/938], Loss: 0.4718753397464752\n",
      "Train: Epoch [19], Batch [599/938], Loss: 0.3741990923881531\n",
      "Train: Epoch [19], Batch [600/938], Loss: 0.4241853952407837\n",
      "Train: Epoch [19], Batch [601/938], Loss: 0.3316481411457062\n",
      "Train: Epoch [19], Batch [602/938], Loss: 0.49179506301879883\n",
      "Train: Epoch [19], Batch [603/938], Loss: 0.4560125470161438\n",
      "Train: Epoch [19], Batch [604/938], Loss: 0.5039781332015991\n",
      "Train: Epoch [19], Batch [605/938], Loss: 0.4463890790939331\n",
      "Train: Epoch [19], Batch [606/938], Loss: 0.48461198806762695\n",
      "Train: Epoch [19], Batch [607/938], Loss: 0.34190279245376587\n",
      "Train: Epoch [19], Batch [608/938], Loss: 0.3499182164669037\n",
      "Train: Epoch [19], Batch [609/938], Loss: 0.5601988434791565\n",
      "Train: Epoch [19], Batch [610/938], Loss: 0.3116177022457123\n",
      "Train: Epoch [19], Batch [611/938], Loss: 0.44772544503211975\n",
      "Train: Epoch [19], Batch [612/938], Loss: 0.42126885056495667\n",
      "Train: Epoch [19], Batch [613/938], Loss: 0.4455970525741577\n",
      "Train: Epoch [19], Batch [614/938], Loss: 0.4234568476676941\n",
      "Train: Epoch [19], Batch [615/938], Loss: 0.46368011832237244\n",
      "Train: Epoch [19], Batch [616/938], Loss: 0.6178461909294128\n",
      "Train: Epoch [19], Batch [617/938], Loss: 0.4563885033130646\n",
      "Train: Epoch [19], Batch [618/938], Loss: 0.5591478943824768\n",
      "Train: Epoch [19], Batch [619/938], Loss: 0.4121948778629303\n",
      "Train: Epoch [19], Batch [620/938], Loss: 0.6046327352523804\n",
      "Train: Epoch [19], Batch [621/938], Loss: 0.520921528339386\n",
      "Train: Epoch [19], Batch [622/938], Loss: 0.7263908386230469\n",
      "Train: Epoch [19], Batch [623/938], Loss: 0.51279217004776\n",
      "Train: Epoch [19], Batch [624/938], Loss: 0.5710294246673584\n",
      "Train: Epoch [19], Batch [625/938], Loss: 0.5911301374435425\n",
      "Train: Epoch [19], Batch [626/938], Loss: 0.5292642712593079\n",
      "Train: Epoch [19], Batch [627/938], Loss: 0.6123376488685608\n",
      "Train: Epoch [19], Batch [628/938], Loss: 0.42072513699531555\n",
      "Train: Epoch [19], Batch [629/938], Loss: 0.4557281732559204\n",
      "Train: Epoch [19], Batch [630/938], Loss: 0.6402786374092102\n",
      "Train: Epoch [19], Batch [631/938], Loss: 0.4833303689956665\n",
      "Train: Epoch [19], Batch [632/938], Loss: 0.4827273488044739\n",
      "Train: Epoch [19], Batch [633/938], Loss: 0.6277849674224854\n",
      "Train: Epoch [19], Batch [634/938], Loss: 0.48193803429603577\n",
      "Train: Epoch [19], Batch [635/938], Loss: 0.4262862801551819\n",
      "Train: Epoch [19], Batch [636/938], Loss: 0.5109057426452637\n",
      "Train: Epoch [19], Batch [637/938], Loss: 0.3836679458618164\n",
      "Train: Epoch [19], Batch [638/938], Loss: 0.39288562536239624\n",
      "Train: Epoch [19], Batch [639/938], Loss: 0.6625840663909912\n",
      "Train: Epoch [19], Batch [640/938], Loss: 0.3972805142402649\n",
      "Train: Epoch [19], Batch [641/938], Loss: 0.6384304761886597\n",
      "Train: Epoch [19], Batch [642/938], Loss: 0.3596862554550171\n",
      "Train: Epoch [19], Batch [643/938], Loss: 0.580100417137146\n",
      "Train: Epoch [19], Batch [644/938], Loss: 0.5941561460494995\n",
      "Train: Epoch [19], Batch [645/938], Loss: 0.4549460709095001\n",
      "Train: Epoch [19], Batch [646/938], Loss: 0.39158934354782104\n",
      "Train: Epoch [19], Batch [647/938], Loss: 0.47394174337387085\n",
      "Train: Epoch [19], Batch [648/938], Loss: 0.8286905288696289\n",
      "Train: Epoch [19], Batch [649/938], Loss: 0.4092254042625427\n",
      "Train: Epoch [19], Batch [650/938], Loss: 0.47567665576934814\n",
      "Train: Epoch [19], Batch [651/938], Loss: 0.41448071599006653\n",
      "Train: Epoch [19], Batch [652/938], Loss: 0.8256738185882568\n",
      "Train: Epoch [19], Batch [653/938], Loss: 0.4533386826515198\n",
      "Train: Epoch [19], Batch [654/938], Loss: 0.42566975951194763\n",
      "Train: Epoch [19], Batch [655/938], Loss: 0.5821142196655273\n",
      "Train: Epoch [19], Batch [656/938], Loss: 0.42669180035591125\n",
      "Train: Epoch [19], Batch [657/938], Loss: 0.3915921449661255\n",
      "Train: Epoch [19], Batch [658/938], Loss: 0.4331851899623871\n",
      "Train: Epoch [19], Batch [659/938], Loss: 0.4860822558403015\n",
      "Train: Epoch [19], Batch [660/938], Loss: 0.47916537523269653\n",
      "Train: Epoch [19], Batch [661/938], Loss: 0.4119162857532501\n",
      "Train: Epoch [19], Batch [662/938], Loss: 0.24876677989959717\n",
      "Train: Epoch [19], Batch [663/938], Loss: 0.5962891578674316\n",
      "Train: Epoch [19], Batch [664/938], Loss: 0.43651285767555237\n",
      "Train: Epoch [19], Batch [665/938], Loss: 0.42069941759109497\n",
      "Train: Epoch [19], Batch [666/938], Loss: 0.43091216683387756\n",
      "Train: Epoch [19], Batch [667/938], Loss: 0.5584286451339722\n",
      "Train: Epoch [19], Batch [668/938], Loss: 0.5652283430099487\n",
      "Train: Epoch [19], Batch [669/938], Loss: 0.41953104734420776\n",
      "Train: Epoch [19], Batch [670/938], Loss: 0.42069804668426514\n",
      "Train: Epoch [19], Batch [671/938], Loss: 0.40163329243659973\n",
      "Train: Epoch [19], Batch [672/938], Loss: 0.5259799957275391\n",
      "Train: Epoch [19], Batch [673/938], Loss: 0.34517303109169006\n",
      "Train: Epoch [19], Batch [674/938], Loss: 0.6217182278633118\n",
      "Train: Epoch [19], Batch [675/938], Loss: 0.5626842975616455\n",
      "Train: Epoch [19], Batch [676/938], Loss: 0.342761367559433\n",
      "Train: Epoch [19], Batch [677/938], Loss: 0.3961106538772583\n",
      "Train: Epoch [19], Batch [678/938], Loss: 0.6299416422843933\n",
      "Train: Epoch [19], Batch [679/938], Loss: 0.4792774021625519\n",
      "Train: Epoch [19], Batch [680/938], Loss: 0.4815710186958313\n",
      "Train: Epoch [19], Batch [681/938], Loss: 0.503413736820221\n",
      "Train: Epoch [19], Batch [682/938], Loss: 0.41301748156547546\n",
      "Train: Epoch [19], Batch [683/938], Loss: 0.5640689730644226\n",
      "Train: Epoch [19], Batch [684/938], Loss: 0.5157598853111267\n",
      "Train: Epoch [19], Batch [685/938], Loss: 0.48675066232681274\n",
      "Train: Epoch [19], Batch [686/938], Loss: 0.7354999780654907\n",
      "Train: Epoch [19], Batch [687/938], Loss: 0.6158859729766846\n",
      "Train: Epoch [19], Batch [688/938], Loss: 0.37811264395713806\n",
      "Train: Epoch [19], Batch [689/938], Loss: 0.5300000905990601\n",
      "Train: Epoch [19], Batch [690/938], Loss: 0.524402916431427\n",
      "Train: Epoch [19], Batch [691/938], Loss: 0.46968722343444824\n",
      "Train: Epoch [19], Batch [692/938], Loss: 0.3835974335670471\n",
      "Train: Epoch [19], Batch [693/938], Loss: 0.45635077357292175\n",
      "Train: Epoch [19], Batch [694/938], Loss: 0.5963797569274902\n",
      "Train: Epoch [19], Batch [695/938], Loss: 0.39097920060157776\n",
      "Train: Epoch [19], Batch [696/938], Loss: 0.4042835831642151\n",
      "Train: Epoch [19], Batch [697/938], Loss: 0.3489818871021271\n",
      "Train: Epoch [19], Batch [698/938], Loss: 0.5634773969650269\n",
      "Train: Epoch [19], Batch [699/938], Loss: 0.37660396099090576\n",
      "Train: Epoch [19], Batch [700/938], Loss: 0.39389148354530334\n",
      "Train: Epoch [19], Batch [701/938], Loss: 0.4401310682296753\n",
      "Train: Epoch [19], Batch [702/938], Loss: 0.4245566427707672\n",
      "Train: Epoch [19], Batch [703/938], Loss: 0.5024760961532593\n",
      "Train: Epoch [19], Batch [704/938], Loss: 0.3357607126235962\n",
      "Train: Epoch [19], Batch [705/938], Loss: 0.5159314870834351\n",
      "Train: Epoch [19], Batch [706/938], Loss: 0.24413590133190155\n",
      "Train: Epoch [19], Batch [707/938], Loss: 0.38692212104797363\n",
      "Train: Epoch [19], Batch [708/938], Loss: 0.6047276854515076\n",
      "Train: Epoch [19], Batch [709/938], Loss: 0.4161795377731323\n",
      "Train: Epoch [19], Batch [710/938], Loss: 0.574502170085907\n",
      "Train: Epoch [19], Batch [711/938], Loss: 0.5400247573852539\n",
      "Train: Epoch [19], Batch [712/938], Loss: 0.5227990746498108\n",
      "Train: Epoch [19], Batch [713/938], Loss: 0.4101792871952057\n",
      "Train: Epoch [19], Batch [714/938], Loss: 0.45215529203414917\n",
      "Train: Epoch [19], Batch [715/938], Loss: 0.6103690266609192\n",
      "Train: Epoch [19], Batch [716/938], Loss: 0.6312949061393738\n",
      "Train: Epoch [19], Batch [717/938], Loss: 0.6085667610168457\n",
      "Train: Epoch [19], Batch [718/938], Loss: 0.4370664954185486\n",
      "Train: Epoch [19], Batch [719/938], Loss: 0.5803287625312805\n",
      "Train: Epoch [19], Batch [720/938], Loss: 0.4029657244682312\n",
      "Train: Epoch [19], Batch [721/938], Loss: 0.2971629202365875\n",
      "Train: Epoch [19], Batch [722/938], Loss: 0.6636837124824524\n",
      "Train: Epoch [19], Batch [723/938], Loss: 0.503279983997345\n",
      "Train: Epoch [19], Batch [724/938], Loss: 0.47307124733924866\n",
      "Train: Epoch [19], Batch [725/938], Loss: 0.35042619705200195\n",
      "Train: Epoch [19], Batch [726/938], Loss: 0.3477613627910614\n",
      "Train: Epoch [19], Batch [727/938], Loss: 0.49178457260131836\n",
      "Train: Epoch [19], Batch [728/938], Loss: 0.45179393887519836\n",
      "Train: Epoch [19], Batch [729/938], Loss: 0.5084524154663086\n",
      "Train: Epoch [19], Batch [730/938], Loss: 0.5484012365341187\n",
      "Train: Epoch [19], Batch [731/938], Loss: 0.37716352939605713\n",
      "Train: Epoch [19], Batch [732/938], Loss: 0.4602121114730835\n",
      "Train: Epoch [19], Batch [733/938], Loss: 0.40801945328712463\n",
      "Train: Epoch [19], Batch [734/938], Loss: 0.4915177524089813\n",
      "Train: Epoch [19], Batch [735/938], Loss: 0.46056127548217773\n",
      "Train: Epoch [19], Batch [736/938], Loss: 0.5069306492805481\n",
      "Train: Epoch [19], Batch [737/938], Loss: 0.6604304313659668\n",
      "Train: Epoch [19], Batch [738/938], Loss: 0.640373945236206\n",
      "Train: Epoch [19], Batch [739/938], Loss: 0.6007170081138611\n",
      "Train: Epoch [19], Batch [740/938], Loss: 0.5851783752441406\n",
      "Train: Epoch [19], Batch [741/938], Loss: 0.45013362169265747\n",
      "Train: Epoch [19], Batch [742/938], Loss: 0.5891212821006775\n",
      "Train: Epoch [19], Batch [743/938], Loss: 0.5459814071655273\n",
      "Train: Epoch [19], Batch [744/938], Loss: 0.5260603427886963\n",
      "Train: Epoch [19], Batch [745/938], Loss: 0.3377159833908081\n",
      "Train: Epoch [19], Batch [746/938], Loss: 0.45368483662605286\n",
      "Train: Epoch [19], Batch [747/938], Loss: 0.2875382900238037\n",
      "Train: Epoch [19], Batch [748/938], Loss: 0.4783167541027069\n",
      "Train: Epoch [19], Batch [749/938], Loss: 0.5037577152252197\n",
      "Train: Epoch [19], Batch [750/938], Loss: 0.5453964471817017\n",
      "Train: Epoch [19], Batch [751/938], Loss: 0.46385300159454346\n",
      "Train: Epoch [19], Batch [752/938], Loss: 0.4798150956630707\n",
      "Train: Epoch [19], Batch [753/938], Loss: 0.313250333070755\n",
      "Train: Epoch [19], Batch [754/938], Loss: 0.4014062285423279\n",
      "Train: Epoch [19], Batch [755/938], Loss: 0.37422969937324524\n",
      "Train: Epoch [19], Batch [756/938], Loss: 0.7137220501899719\n",
      "Train: Epoch [19], Batch [757/938], Loss: 0.30657631158828735\n",
      "Train: Epoch [19], Batch [758/938], Loss: 0.4765458405017853\n",
      "Train: Epoch [19], Batch [759/938], Loss: 0.42921194434165955\n",
      "Train: Epoch [19], Batch [760/938], Loss: 0.4433586001396179\n",
      "Train: Epoch [19], Batch [761/938], Loss: 0.5695640444755554\n",
      "Train: Epoch [19], Batch [762/938], Loss: 0.6092398762702942\n",
      "Train: Epoch [19], Batch [763/938], Loss: 0.4145452082157135\n",
      "Train: Epoch [19], Batch [764/938], Loss: 0.4977233409881592\n",
      "Train: Epoch [19], Batch [765/938], Loss: 0.537679135799408\n",
      "Train: Epoch [19], Batch [766/938], Loss: 0.545746386051178\n",
      "Train: Epoch [19], Batch [767/938], Loss: 0.49068307876586914\n",
      "Train: Epoch [19], Batch [768/938], Loss: 0.5501465797424316\n",
      "Train: Epoch [19], Batch [769/938], Loss: 0.6052407026290894\n",
      "Train: Epoch [19], Batch [770/938], Loss: 0.5006493926048279\n",
      "Train: Epoch [19], Batch [771/938], Loss: 0.41210997104644775\n",
      "Train: Epoch [19], Batch [772/938], Loss: 0.5198774337768555\n",
      "Train: Epoch [19], Batch [773/938], Loss: 0.41402363777160645\n",
      "Train: Epoch [19], Batch [774/938], Loss: 0.7359355688095093\n",
      "Train: Epoch [19], Batch [775/938], Loss: 0.43426650762557983\n",
      "Train: Epoch [19], Batch [776/938], Loss: 0.46553829312324524\n",
      "Train: Epoch [19], Batch [777/938], Loss: 0.5364875793457031\n",
      "Train: Epoch [19], Batch [778/938], Loss: 0.5162533521652222\n",
      "Train: Epoch [19], Batch [779/938], Loss: 0.5437834858894348\n",
      "Train: Epoch [19], Batch [780/938], Loss: 0.5549411773681641\n",
      "Train: Epoch [19], Batch [781/938], Loss: 0.49613937735557556\n",
      "Train: Epoch [19], Batch [782/938], Loss: 0.6730555891990662\n",
      "Train: Epoch [19], Batch [783/938], Loss: 0.5120976567268372\n",
      "Train: Epoch [19], Batch [784/938], Loss: 0.37265607714653015\n",
      "Train: Epoch [19], Batch [785/938], Loss: 0.5703913569450378\n",
      "Train: Epoch [19], Batch [786/938], Loss: 0.4574873745441437\n",
      "Train: Epoch [19], Batch [787/938], Loss: 0.5386767387390137\n",
      "Train: Epoch [19], Batch [788/938], Loss: 0.50755375623703\n",
      "Train: Epoch [19], Batch [789/938], Loss: 0.5740196704864502\n",
      "Train: Epoch [19], Batch [790/938], Loss: 0.587130606174469\n",
      "Train: Epoch [19], Batch [791/938], Loss: 0.5818874835968018\n",
      "Train: Epoch [19], Batch [792/938], Loss: 0.31183433532714844\n",
      "Train: Epoch [19], Batch [793/938], Loss: 0.29980534315109253\n",
      "Train: Epoch [19], Batch [794/938], Loss: 0.5110654830932617\n",
      "Train: Epoch [19], Batch [795/938], Loss: 0.42212235927581787\n",
      "Train: Epoch [19], Batch [796/938], Loss: 0.7108220458030701\n",
      "Train: Epoch [19], Batch [797/938], Loss: 0.46296828985214233\n",
      "Train: Epoch [19], Batch [798/938], Loss: 0.34282857179641724\n",
      "Train: Epoch [19], Batch [799/938], Loss: 0.675251841545105\n",
      "Train: Epoch [19], Batch [800/938], Loss: 0.5360226631164551\n",
      "Train: Epoch [19], Batch [801/938], Loss: 0.5268791913986206\n",
      "Train: Epoch [19], Batch [802/938], Loss: 0.6954345107078552\n",
      "Train: Epoch [19], Batch [803/938], Loss: 0.28884440660476685\n",
      "Train: Epoch [19], Batch [804/938], Loss: 0.5571558475494385\n",
      "Train: Epoch [19], Batch [805/938], Loss: 0.45808863639831543\n",
      "Train: Epoch [19], Batch [806/938], Loss: 0.4870564043521881\n",
      "Train: Epoch [19], Batch [807/938], Loss: 0.515296459197998\n",
      "Train: Epoch [19], Batch [808/938], Loss: 0.4278039336204529\n",
      "Train: Epoch [19], Batch [809/938], Loss: 0.40598905086517334\n",
      "Train: Epoch [19], Batch [810/938], Loss: 0.46853917837142944\n",
      "Train: Epoch [19], Batch [811/938], Loss: 0.3681979179382324\n",
      "Train: Epoch [19], Batch [812/938], Loss: 0.3882930874824524\n",
      "Train: Epoch [19], Batch [813/938], Loss: 0.42462414503097534\n",
      "Train: Epoch [19], Batch [814/938], Loss: 0.48652172088623047\n",
      "Train: Epoch [19], Batch [815/938], Loss: 0.3789667785167694\n",
      "Train: Epoch [19], Batch [816/938], Loss: 0.3630164861679077\n",
      "Train: Epoch [19], Batch [817/938], Loss: 0.4580483138561249\n",
      "Train: Epoch [19], Batch [818/938], Loss: 0.4470287561416626\n",
      "Train: Epoch [19], Batch [819/938], Loss: 0.5746380686759949\n",
      "Train: Epoch [19], Batch [820/938], Loss: 0.751700758934021\n",
      "Train: Epoch [19], Batch [821/938], Loss: 0.5132683515548706\n",
      "Train: Epoch [19], Batch [822/938], Loss: 0.39203521609306335\n",
      "Train: Epoch [19], Batch [823/938], Loss: 0.4451240599155426\n",
      "Train: Epoch [19], Batch [824/938], Loss: 0.6153554320335388\n",
      "Train: Epoch [19], Batch [825/938], Loss: 0.5916674137115479\n",
      "Train: Epoch [19], Batch [826/938], Loss: 0.29460781812667847\n",
      "Train: Epoch [19], Batch [827/938], Loss: 0.337227463722229\n",
      "Train: Epoch [19], Batch [828/938], Loss: 0.6272308230400085\n",
      "Train: Epoch [19], Batch [829/938], Loss: 0.4773297607898712\n",
      "Train: Epoch [19], Batch [830/938], Loss: 0.6911183595657349\n",
      "Train: Epoch [19], Batch [831/938], Loss: 0.5161267518997192\n",
      "Train: Epoch [19], Batch [832/938], Loss: 0.390198677778244\n",
      "Train: Epoch [19], Batch [833/938], Loss: 0.4640268087387085\n",
      "Train: Epoch [19], Batch [834/938], Loss: 0.573336124420166\n",
      "Train: Epoch [19], Batch [835/938], Loss: 0.7019693851470947\n",
      "Train: Epoch [19], Batch [836/938], Loss: 0.5775665640830994\n",
      "Train: Epoch [19], Batch [837/938], Loss: 0.4414801597595215\n",
      "Train: Epoch [19], Batch [838/938], Loss: 0.41838711500167847\n",
      "Train: Epoch [19], Batch [839/938], Loss: 0.32245391607284546\n",
      "Train: Epoch [19], Batch [840/938], Loss: 0.2686624825000763\n",
      "Train: Epoch [19], Batch [841/938], Loss: 0.613323450088501\n",
      "Train: Epoch [19], Batch [842/938], Loss: 0.5001266002655029\n",
      "Train: Epoch [19], Batch [843/938], Loss: 0.5370609760284424\n",
      "Train: Epoch [19], Batch [844/938], Loss: 0.36405858397483826\n",
      "Train: Epoch [19], Batch [845/938], Loss: 0.2476264238357544\n",
      "Train: Epoch [19], Batch [846/938], Loss: 0.505082368850708\n",
      "Train: Epoch [19], Batch [847/938], Loss: 0.5861678123474121\n",
      "Train: Epoch [19], Batch [848/938], Loss: 0.44161200523376465\n",
      "Train: Epoch [19], Batch [849/938], Loss: 0.4828135371208191\n",
      "Train: Epoch [19], Batch [850/938], Loss: 0.47239142656326294\n",
      "Train: Epoch [19], Batch [851/938], Loss: 0.3994103670120239\n",
      "Train: Epoch [19], Batch [852/938], Loss: 0.48557865619659424\n",
      "Train: Epoch [19], Batch [853/938], Loss: 0.5610272288322449\n",
      "Train: Epoch [19], Batch [854/938], Loss: 0.610038161277771\n",
      "Train: Epoch [19], Batch [855/938], Loss: 0.6096343398094177\n",
      "Train: Epoch [19], Batch [856/938], Loss: 0.47676903009414673\n",
      "Train: Epoch [19], Batch [857/938], Loss: 0.2300138771533966\n",
      "Train: Epoch [19], Batch [858/938], Loss: 0.7775257229804993\n",
      "Train: Epoch [19], Batch [859/938], Loss: 0.5463305711746216\n",
      "Train: Epoch [19], Batch [860/938], Loss: 0.42284661531448364\n",
      "Train: Epoch [19], Batch [861/938], Loss: 0.6195007562637329\n",
      "Train: Epoch [19], Batch [862/938], Loss: 0.507439374923706\n",
      "Train: Epoch [19], Batch [863/938], Loss: 0.46066802740097046\n",
      "Train: Epoch [19], Batch [864/938], Loss: 0.71452796459198\n",
      "Train: Epoch [19], Batch [865/938], Loss: 0.31207647919654846\n",
      "Train: Epoch [19], Batch [866/938], Loss: 0.5122252702713013\n",
      "Train: Epoch [19], Batch [867/938], Loss: 0.49653393030166626\n",
      "Train: Epoch [19], Batch [868/938], Loss: 0.525281548500061\n",
      "Train: Epoch [19], Batch [869/938], Loss: 0.5929763317108154\n",
      "Train: Epoch [19], Batch [870/938], Loss: 0.34246882796287537\n",
      "Train: Epoch [19], Batch [871/938], Loss: 0.5034202933311462\n",
      "Train: Epoch [19], Batch [872/938], Loss: 0.4573819637298584\n",
      "Train: Epoch [19], Batch [873/938], Loss: 0.3111450672149658\n",
      "Train: Epoch [19], Batch [874/938], Loss: 0.2560231685638428\n",
      "Train: Epoch [19], Batch [875/938], Loss: 0.4850669503211975\n",
      "Train: Epoch [19], Batch [876/938], Loss: 0.4046347737312317\n",
      "Train: Epoch [19], Batch [877/938], Loss: 0.7063014507293701\n",
      "Train: Epoch [19], Batch [878/938], Loss: 0.685017466545105\n",
      "Train: Epoch [19], Batch [879/938], Loss: 0.5360141396522522\n",
      "Train: Epoch [19], Batch [880/938], Loss: 0.466240793466568\n",
      "Train: Epoch [19], Batch [881/938], Loss: 0.6680076122283936\n",
      "Train: Epoch [19], Batch [882/938], Loss: 0.4344625473022461\n",
      "Train: Epoch [19], Batch [883/938], Loss: 0.5154581665992737\n",
      "Train: Epoch [19], Batch [884/938], Loss: 0.50057452917099\n",
      "Train: Epoch [19], Batch [885/938], Loss: 0.5647162795066833\n",
      "Train: Epoch [19], Batch [886/938], Loss: 0.6281193494796753\n",
      "Train: Epoch [19], Batch [887/938], Loss: 0.5622032880783081\n",
      "Train: Epoch [19], Batch [888/938], Loss: 0.6083944439888\n",
      "Train: Epoch [19], Batch [889/938], Loss: 0.3411640226840973\n",
      "Train: Epoch [19], Batch [890/938], Loss: 0.8225228786468506\n",
      "Train: Epoch [19], Batch [891/938], Loss: 0.6263384819030762\n",
      "Train: Epoch [19], Batch [892/938], Loss: 0.4854303002357483\n",
      "Train: Epoch [19], Batch [893/938], Loss: 0.5411006212234497\n",
      "Train: Epoch [19], Batch [894/938], Loss: 0.36731696128845215\n",
      "Train: Epoch [19], Batch [895/938], Loss: 0.5414096117019653\n",
      "Train: Epoch [19], Batch [896/938], Loss: 0.4907548427581787\n",
      "Train: Epoch [19], Batch [897/938], Loss: 0.5606784224510193\n",
      "Train: Epoch [19], Batch [898/938], Loss: 0.46892985701560974\n",
      "Train: Epoch [19], Batch [899/938], Loss: 0.404111385345459\n",
      "Train: Epoch [19], Batch [900/938], Loss: 0.3379564881324768\n",
      "Train: Epoch [19], Batch [901/938], Loss: 0.420326828956604\n",
      "Train: Epoch [19], Batch [902/938], Loss: 0.49146324396133423\n",
      "Train: Epoch [19], Batch [903/938], Loss: 0.3741503953933716\n",
      "Train: Epoch [19], Batch [904/938], Loss: 0.418300986289978\n",
      "Train: Epoch [19], Batch [905/938], Loss: 0.4946822226047516\n",
      "Train: Epoch [19], Batch [906/938], Loss: 0.4072210490703583\n",
      "Train: Epoch [19], Batch [907/938], Loss: 0.4607749283313751\n",
      "Train: Epoch [19], Batch [908/938], Loss: 0.5067532062530518\n",
      "Train: Epoch [19], Batch [909/938], Loss: 0.28911274671554565\n",
      "Train: Epoch [19], Batch [910/938], Loss: 0.5680620074272156\n",
      "Train: Epoch [19], Batch [911/938], Loss: 0.429115355014801\n",
      "Train: Epoch [19], Batch [912/938], Loss: 0.5135825276374817\n",
      "Train: Epoch [19], Batch [913/938], Loss: 0.3625187873840332\n",
      "Train: Epoch [19], Batch [914/938], Loss: 0.4849434792995453\n",
      "Train: Epoch [19], Batch [915/938], Loss: 0.3855721056461334\n",
      "Train: Epoch [19], Batch [916/938], Loss: 0.37885579466819763\n",
      "Train: Epoch [19], Batch [917/938], Loss: 0.47245532274246216\n",
      "Train: Epoch [19], Batch [918/938], Loss: 0.5034582614898682\n",
      "Train: Epoch [19], Batch [919/938], Loss: 0.3482057452201843\n",
      "Train: Epoch [19], Batch [920/938], Loss: 0.8828293085098267\n",
      "Train: Epoch [19], Batch [921/938], Loss: 0.4301995635032654\n",
      "Train: Epoch [19], Batch [922/938], Loss: 0.39667633175849915\n",
      "Train: Epoch [19], Batch [923/938], Loss: 0.5496392250061035\n",
      "Train: Epoch [19], Batch [924/938], Loss: 0.46938014030456543\n",
      "Train: Epoch [19], Batch [925/938], Loss: 0.3214442729949951\n",
      "Train: Epoch [19], Batch [926/938], Loss: 0.3918737769126892\n",
      "Train: Epoch [19], Batch [927/938], Loss: 0.4861287772655487\n",
      "Train: Epoch [19], Batch [928/938], Loss: 0.6043941378593445\n",
      "Train: Epoch [19], Batch [929/938], Loss: 0.40358346700668335\n",
      "Train: Epoch [19], Batch [930/938], Loss: 0.3736666440963745\n",
      "Train: Epoch [19], Batch [931/938], Loss: 0.3657020628452301\n",
      "Train: Epoch [19], Batch [932/938], Loss: 0.41681620478630066\n",
      "Train: Epoch [19], Batch [933/938], Loss: 0.4668514132499695\n",
      "Train: Epoch [19], Batch [934/938], Loss: 0.4191076159477234\n",
      "Train: Epoch [19], Batch [935/938], Loss: 0.6164739727973938\n",
      "Train: Epoch [19], Batch [936/938], Loss: 0.3736102879047394\n",
      "Train: Epoch [19], Batch [937/938], Loss: 0.5880363583564758\n",
      "Train: Epoch [19], Batch [938/938], Loss: 0.9723038673400879\n",
      "Accuracy of train set: 0.8222\n",
      "Validation: Epoch [19], Batch [1/938], Loss: 0.5251467227935791\n",
      "Validation: Epoch [19], Batch [2/938], Loss: 0.49026811122894287\n",
      "Validation: Epoch [19], Batch [3/938], Loss: 0.4909886121749878\n",
      "Validation: Epoch [19], Batch [4/938], Loss: 0.3880099356174469\n",
      "Validation: Epoch [19], Batch [5/938], Loss: 0.5026360154151917\n",
      "Validation: Epoch [19], Batch [6/938], Loss: 0.7654790282249451\n",
      "Validation: Epoch [19], Batch [7/938], Loss: 0.6041534543037415\n",
      "Validation: Epoch [19], Batch [8/938], Loss: 0.44302240014076233\n",
      "Validation: Epoch [19], Batch [9/938], Loss: 0.5221686363220215\n",
      "Validation: Epoch [19], Batch [10/938], Loss: 0.9903823733329773\n",
      "Validation: Epoch [19], Batch [11/938], Loss: 0.540337860584259\n",
      "Validation: Epoch [19], Batch [12/938], Loss: 0.600207507610321\n",
      "Validation: Epoch [19], Batch [13/938], Loss: 0.8965499401092529\n",
      "Validation: Epoch [19], Batch [14/938], Loss: 0.4852845370769501\n",
      "Validation: Epoch [19], Batch [15/938], Loss: 0.5670661926269531\n",
      "Validation: Epoch [19], Batch [16/938], Loss: 0.4423573911190033\n",
      "Validation: Epoch [19], Batch [17/938], Loss: 0.6267231702804565\n",
      "Validation: Epoch [19], Batch [18/938], Loss: 0.591835081577301\n",
      "Validation: Epoch [19], Batch [19/938], Loss: 0.38468098640441895\n",
      "Validation: Epoch [19], Batch [20/938], Loss: 0.5344693064689636\n",
      "Validation: Epoch [19], Batch [21/938], Loss: 0.5502873063087463\n",
      "Validation: Epoch [19], Batch [22/938], Loss: 0.7089783549308777\n",
      "Validation: Epoch [19], Batch [23/938], Loss: 0.6609898209571838\n",
      "Validation: Epoch [19], Batch [24/938], Loss: 0.8363287448883057\n",
      "Validation: Epoch [19], Batch [25/938], Loss: 0.4776432514190674\n",
      "Validation: Epoch [19], Batch [26/938], Loss: 0.6431010365486145\n",
      "Validation: Epoch [19], Batch [27/938], Loss: 0.5440542697906494\n",
      "Validation: Epoch [19], Batch [28/938], Loss: 0.5913172960281372\n",
      "Validation: Epoch [19], Batch [29/938], Loss: 0.2860705256462097\n",
      "Validation: Epoch [19], Batch [30/938], Loss: 0.846407413482666\n",
      "Validation: Epoch [19], Batch [31/938], Loss: 0.631001889705658\n",
      "Validation: Epoch [19], Batch [32/938], Loss: 0.5513733625411987\n",
      "Validation: Epoch [19], Batch [33/938], Loss: 0.6535062789916992\n",
      "Validation: Epoch [19], Batch [34/938], Loss: 0.662116527557373\n",
      "Validation: Epoch [19], Batch [35/938], Loss: 0.5261108875274658\n",
      "Validation: Epoch [19], Batch [36/938], Loss: 0.774407684803009\n",
      "Validation: Epoch [19], Batch [37/938], Loss: 0.622313916683197\n",
      "Validation: Epoch [19], Batch [38/938], Loss: 0.5239277482032776\n",
      "Validation: Epoch [19], Batch [39/938], Loss: 0.7495158910751343\n",
      "Validation: Epoch [19], Batch [40/938], Loss: 0.6076660752296448\n",
      "Validation: Epoch [19], Batch [41/938], Loss: 0.6061092615127563\n",
      "Validation: Epoch [19], Batch [42/938], Loss: 0.42484933137893677\n",
      "Validation: Epoch [19], Batch [43/938], Loss: 0.7324115037918091\n",
      "Validation: Epoch [19], Batch [44/938], Loss: 0.6682471036911011\n",
      "Validation: Epoch [19], Batch [45/938], Loss: 0.6208993792533875\n",
      "Validation: Epoch [19], Batch [46/938], Loss: 0.5713383555412292\n",
      "Validation: Epoch [19], Batch [47/938], Loss: 0.7295464277267456\n",
      "Validation: Epoch [19], Batch [48/938], Loss: 0.5877283215522766\n",
      "Validation: Epoch [19], Batch [49/938], Loss: 0.9975863695144653\n",
      "Validation: Epoch [19], Batch [50/938], Loss: 0.5272811651229858\n",
      "Validation: Epoch [19], Batch [51/938], Loss: 0.511547327041626\n",
      "Validation: Epoch [19], Batch [52/938], Loss: 0.8366224765777588\n",
      "Validation: Epoch [19], Batch [53/938], Loss: 0.4129135310649872\n",
      "Validation: Epoch [19], Batch [54/938], Loss: 0.49004846811294556\n",
      "Validation: Epoch [19], Batch [55/938], Loss: 0.5854067802429199\n",
      "Validation: Epoch [19], Batch [56/938], Loss: 0.5279149413108826\n",
      "Validation: Epoch [19], Batch [57/938], Loss: 0.5293447375297546\n",
      "Validation: Epoch [19], Batch [58/938], Loss: 0.6025048494338989\n",
      "Validation: Epoch [19], Batch [59/938], Loss: 0.5492329597473145\n",
      "Validation: Epoch [19], Batch [60/938], Loss: 0.5729629993438721\n",
      "Validation: Epoch [19], Batch [61/938], Loss: 0.7061134576797485\n",
      "Validation: Epoch [19], Batch [62/938], Loss: 0.6498228907585144\n",
      "Validation: Epoch [19], Batch [63/938], Loss: 0.5622120499610901\n",
      "Validation: Epoch [19], Batch [64/938], Loss: 0.41455602645874023\n",
      "Validation: Epoch [19], Batch [65/938], Loss: 0.46693694591522217\n",
      "Validation: Epoch [19], Batch [66/938], Loss: 0.6579840183258057\n",
      "Validation: Epoch [19], Batch [67/938], Loss: 0.47001513838768005\n",
      "Validation: Epoch [19], Batch [68/938], Loss: 0.6915175914764404\n",
      "Validation: Epoch [19], Batch [69/938], Loss: 0.43510571122169495\n",
      "Validation: Epoch [19], Batch [70/938], Loss: 0.577680766582489\n",
      "Validation: Epoch [19], Batch [71/938], Loss: 0.5723159313201904\n",
      "Validation: Epoch [19], Batch [72/938], Loss: 0.4653415381908417\n",
      "Validation: Epoch [19], Batch [73/938], Loss: 0.7132270336151123\n",
      "Validation: Epoch [19], Batch [74/938], Loss: 0.6469440460205078\n",
      "Validation: Epoch [19], Batch [75/938], Loss: 0.5145775675773621\n",
      "Validation: Epoch [19], Batch [76/938], Loss: 0.7341793179512024\n",
      "Validation: Epoch [19], Batch [77/938], Loss: 0.6524984240531921\n",
      "Validation: Epoch [19], Batch [78/938], Loss: 0.5494217872619629\n",
      "Validation: Epoch [19], Batch [79/938], Loss: 0.6614987254142761\n",
      "Validation: Epoch [19], Batch [80/938], Loss: 0.45609360933303833\n",
      "Validation: Epoch [19], Batch [81/938], Loss: 0.6336177587509155\n",
      "Validation: Epoch [19], Batch [82/938], Loss: 0.5197463035583496\n",
      "Validation: Epoch [19], Batch [83/938], Loss: 0.7719139456748962\n",
      "Validation: Epoch [19], Batch [84/938], Loss: 0.587925910949707\n",
      "Validation: Epoch [19], Batch [85/938], Loss: 0.4964926242828369\n",
      "Validation: Epoch [19], Batch [86/938], Loss: 0.4802294969558716\n",
      "Validation: Epoch [19], Batch [87/938], Loss: 0.5197291374206543\n",
      "Validation: Epoch [19], Batch [88/938], Loss: 0.5481916069984436\n",
      "Validation: Epoch [19], Batch [89/938], Loss: 0.6312488913536072\n",
      "Validation: Epoch [19], Batch [90/938], Loss: 0.5246027708053589\n",
      "Validation: Epoch [19], Batch [91/938], Loss: 0.4687855839729309\n",
      "Validation: Epoch [19], Batch [92/938], Loss: 0.5056983232498169\n",
      "Validation: Epoch [19], Batch [93/938], Loss: 0.4452899694442749\n",
      "Validation: Epoch [19], Batch [94/938], Loss: 0.6706350445747375\n",
      "Validation: Epoch [19], Batch [95/938], Loss: 0.6109285950660706\n",
      "Validation: Epoch [19], Batch [96/938], Loss: 0.5818437337875366\n",
      "Validation: Epoch [19], Batch [97/938], Loss: 0.6389744281768799\n",
      "Validation: Epoch [19], Batch [98/938], Loss: 0.7816494703292847\n",
      "Validation: Epoch [19], Batch [99/938], Loss: 0.7967112064361572\n",
      "Validation: Epoch [19], Batch [100/938], Loss: 0.6352716684341431\n",
      "Validation: Epoch [19], Batch [101/938], Loss: 0.5600103735923767\n",
      "Validation: Epoch [19], Batch [102/938], Loss: 0.5594062805175781\n",
      "Validation: Epoch [19], Batch [103/938], Loss: 0.46484407782554626\n",
      "Validation: Epoch [19], Batch [104/938], Loss: 0.47884365916252136\n",
      "Validation: Epoch [19], Batch [105/938], Loss: 0.7823362350463867\n",
      "Validation: Epoch [19], Batch [106/938], Loss: 0.6770578622817993\n",
      "Validation: Epoch [19], Batch [107/938], Loss: 0.4582486152648926\n",
      "Validation: Epoch [19], Batch [108/938], Loss: 0.47914430499076843\n",
      "Validation: Epoch [19], Batch [109/938], Loss: 0.519205629825592\n",
      "Validation: Epoch [19], Batch [110/938], Loss: 0.5449302196502686\n",
      "Validation: Epoch [19], Batch [111/938], Loss: 0.7255542278289795\n",
      "Validation: Epoch [19], Batch [112/938], Loss: 0.5306816101074219\n",
      "Validation: Epoch [19], Batch [113/938], Loss: 0.6440565586090088\n",
      "Validation: Epoch [19], Batch [114/938], Loss: 0.8962276577949524\n",
      "Validation: Epoch [19], Batch [115/938], Loss: 0.6294040679931641\n",
      "Validation: Epoch [19], Batch [116/938], Loss: 0.545322060585022\n",
      "Validation: Epoch [19], Batch [117/938], Loss: 0.7091658115386963\n",
      "Validation: Epoch [19], Batch [118/938], Loss: 0.48566365242004395\n",
      "Validation: Epoch [19], Batch [119/938], Loss: 0.6920575499534607\n",
      "Validation: Epoch [19], Batch [120/938], Loss: 0.7109733819961548\n",
      "Validation: Epoch [19], Batch [121/938], Loss: 0.6596788167953491\n",
      "Validation: Epoch [19], Batch [122/938], Loss: 0.6709321141242981\n",
      "Validation: Epoch [19], Batch [123/938], Loss: 0.5688267350196838\n",
      "Validation: Epoch [19], Batch [124/938], Loss: 0.5876075029373169\n",
      "Validation: Epoch [19], Batch [125/938], Loss: 0.4657585024833679\n",
      "Validation: Epoch [19], Batch [126/938], Loss: 0.7458493709564209\n",
      "Validation: Epoch [19], Batch [127/938], Loss: 0.5478295087814331\n",
      "Validation: Epoch [19], Batch [128/938], Loss: 0.5639288425445557\n",
      "Validation: Epoch [19], Batch [129/938], Loss: 0.37104034423828125\n",
      "Validation: Epoch [19], Batch [130/938], Loss: 0.6262258291244507\n",
      "Validation: Epoch [19], Batch [131/938], Loss: 0.4138951599597931\n",
      "Validation: Epoch [19], Batch [132/938], Loss: 0.734099805355072\n",
      "Validation: Epoch [19], Batch [133/938], Loss: 0.814584493637085\n",
      "Validation: Epoch [19], Batch [134/938], Loss: 0.6137738227844238\n",
      "Validation: Epoch [19], Batch [135/938], Loss: 0.3305337131023407\n",
      "Validation: Epoch [19], Batch [136/938], Loss: 0.5521748661994934\n",
      "Validation: Epoch [19], Batch [137/938], Loss: 0.5411858558654785\n",
      "Validation: Epoch [19], Batch [138/938], Loss: 0.7167941927909851\n",
      "Validation: Epoch [19], Batch [139/938], Loss: 0.6880264282226562\n",
      "Validation: Epoch [19], Batch [140/938], Loss: 0.5488646626472473\n",
      "Validation: Epoch [19], Batch [141/938], Loss: 0.6990611553192139\n",
      "Validation: Epoch [19], Batch [142/938], Loss: 0.5157855749130249\n",
      "Validation: Epoch [19], Batch [143/938], Loss: 0.6458380222320557\n",
      "Validation: Epoch [19], Batch [144/938], Loss: 0.8080548048019409\n",
      "Validation: Epoch [19], Batch [145/938], Loss: 0.7463168501853943\n",
      "Validation: Epoch [19], Batch [146/938], Loss: 0.5104215741157532\n",
      "Validation: Epoch [19], Batch [147/938], Loss: 0.46940991282463074\n",
      "Validation: Epoch [19], Batch [148/938], Loss: 0.43454355001449585\n",
      "Validation: Epoch [19], Batch [149/938], Loss: 0.7220337986946106\n",
      "Validation: Epoch [19], Batch [150/938], Loss: 0.795072078704834\n",
      "Validation: Epoch [19], Batch [151/938], Loss: 0.6808803081512451\n",
      "Validation: Epoch [19], Batch [152/938], Loss: 0.4658023715019226\n",
      "Validation: Epoch [19], Batch [153/938], Loss: 0.5760116577148438\n",
      "Validation: Epoch [19], Batch [154/938], Loss: 0.5792405605316162\n",
      "Validation: Epoch [19], Batch [155/938], Loss: 0.7062379121780396\n",
      "Validation: Epoch [19], Batch [156/938], Loss: 0.6820336580276489\n",
      "Validation: Epoch [19], Batch [157/938], Loss: 0.6407994031906128\n",
      "Validation: Epoch [19], Batch [158/938], Loss: 0.7424108386039734\n",
      "Validation: Epoch [19], Batch [159/938], Loss: 0.796305775642395\n",
      "Validation: Epoch [19], Batch [160/938], Loss: 0.6252721548080444\n",
      "Validation: Epoch [19], Batch [161/938], Loss: 0.7140728235244751\n",
      "Validation: Epoch [19], Batch [162/938], Loss: 0.4391588270664215\n",
      "Validation: Epoch [19], Batch [163/938], Loss: 0.5474991798400879\n",
      "Validation: Epoch [19], Batch [164/938], Loss: 0.4976823925971985\n",
      "Validation: Epoch [19], Batch [165/938], Loss: 0.6532420516014099\n",
      "Validation: Epoch [19], Batch [166/938], Loss: 0.4943837821483612\n",
      "Validation: Epoch [19], Batch [167/938], Loss: 0.40241745114326477\n",
      "Validation: Epoch [19], Batch [168/938], Loss: 0.6032242774963379\n",
      "Validation: Epoch [19], Batch [169/938], Loss: 0.6541403532028198\n",
      "Validation: Epoch [19], Batch [170/938], Loss: 0.4275357723236084\n",
      "Validation: Epoch [19], Batch [171/938], Loss: 0.8966764211654663\n",
      "Validation: Epoch [19], Batch [172/938], Loss: 0.6436302661895752\n",
      "Validation: Epoch [19], Batch [173/938], Loss: 0.6267311573028564\n",
      "Validation: Epoch [19], Batch [174/938], Loss: 0.34338825941085815\n",
      "Validation: Epoch [19], Batch [175/938], Loss: 0.703801155090332\n",
      "Validation: Epoch [19], Batch [176/938], Loss: 0.6804903745651245\n",
      "Validation: Epoch [19], Batch [177/938], Loss: 0.5226865410804749\n",
      "Validation: Epoch [19], Batch [178/938], Loss: 0.49866241216659546\n",
      "Validation: Epoch [19], Batch [179/938], Loss: 0.5652968287467957\n",
      "Validation: Epoch [19], Batch [180/938], Loss: 0.6184878349304199\n",
      "Validation: Epoch [19], Batch [181/938], Loss: 0.7880560159683228\n",
      "Validation: Epoch [19], Batch [182/938], Loss: 0.6783208847045898\n",
      "Validation: Epoch [19], Batch [183/938], Loss: 0.4342884421348572\n",
      "Validation: Epoch [19], Batch [184/938], Loss: 0.4504293203353882\n",
      "Validation: Epoch [19], Batch [185/938], Loss: 0.6727246046066284\n",
      "Validation: Epoch [19], Batch [186/938], Loss: 0.6982578039169312\n",
      "Validation: Epoch [19], Batch [187/938], Loss: 0.604498028755188\n",
      "Validation: Epoch [19], Batch [188/938], Loss: 0.6524614691734314\n",
      "Validation: Epoch [19], Batch [189/938], Loss: 0.41640961170196533\n",
      "Validation: Epoch [19], Batch [190/938], Loss: 0.8134803175926208\n",
      "Validation: Epoch [19], Batch [191/938], Loss: 0.5768522620201111\n",
      "Validation: Epoch [19], Batch [192/938], Loss: 0.5304415822029114\n",
      "Validation: Epoch [19], Batch [193/938], Loss: 0.49570679664611816\n",
      "Validation: Epoch [19], Batch [194/938], Loss: 0.7443608641624451\n",
      "Validation: Epoch [19], Batch [195/938], Loss: 0.3890205919742584\n",
      "Validation: Epoch [19], Batch [196/938], Loss: 0.5721738934516907\n",
      "Validation: Epoch [19], Batch [197/938], Loss: 0.46458151936531067\n",
      "Validation: Epoch [19], Batch [198/938], Loss: 0.6791918873786926\n",
      "Validation: Epoch [19], Batch [199/938], Loss: 0.4645223021507263\n",
      "Validation: Epoch [19], Batch [200/938], Loss: 0.6129851341247559\n",
      "Validation: Epoch [19], Batch [201/938], Loss: 0.44515112042427063\n",
      "Validation: Epoch [19], Batch [202/938], Loss: 0.7509570717811584\n",
      "Validation: Epoch [19], Batch [203/938], Loss: 0.3864249885082245\n",
      "Validation: Epoch [19], Batch [204/938], Loss: 0.5139944553375244\n",
      "Validation: Epoch [19], Batch [205/938], Loss: 0.7142437696456909\n",
      "Validation: Epoch [19], Batch [206/938], Loss: 0.4863627552986145\n",
      "Validation: Epoch [19], Batch [207/938], Loss: 0.4141794443130493\n",
      "Validation: Epoch [19], Batch [208/938], Loss: 0.44633281230926514\n",
      "Validation: Epoch [19], Batch [209/938], Loss: 0.4846963882446289\n",
      "Validation: Epoch [19], Batch [210/938], Loss: 0.6844420433044434\n",
      "Validation: Epoch [19], Batch [211/938], Loss: 0.5569719672203064\n",
      "Validation: Epoch [19], Batch [212/938], Loss: 0.8394822478294373\n",
      "Validation: Epoch [19], Batch [213/938], Loss: 0.6377886533737183\n",
      "Validation: Epoch [19], Batch [214/938], Loss: 0.7115715742111206\n",
      "Validation: Epoch [19], Batch [215/938], Loss: 0.48483458161354065\n",
      "Validation: Epoch [19], Batch [216/938], Loss: 0.3842964768409729\n",
      "Validation: Epoch [19], Batch [217/938], Loss: 0.666413426399231\n",
      "Validation: Epoch [19], Batch [218/938], Loss: 0.7589980959892273\n",
      "Validation: Epoch [19], Batch [219/938], Loss: 0.4773503839969635\n",
      "Validation: Epoch [19], Batch [220/938], Loss: 0.4984114170074463\n",
      "Validation: Epoch [19], Batch [221/938], Loss: 0.613444983959198\n",
      "Validation: Epoch [19], Batch [222/938], Loss: 0.5889146327972412\n",
      "Validation: Epoch [19], Batch [223/938], Loss: 0.41515251994132996\n",
      "Validation: Epoch [19], Batch [224/938], Loss: 0.4527418613433838\n",
      "Validation: Epoch [19], Batch [225/938], Loss: 0.6377826929092407\n",
      "Validation: Epoch [19], Batch [226/938], Loss: 0.4991430640220642\n",
      "Validation: Epoch [19], Batch [227/938], Loss: 0.6319928765296936\n",
      "Validation: Epoch [19], Batch [228/938], Loss: 0.463207483291626\n",
      "Validation: Epoch [19], Batch [229/938], Loss: 0.7031688690185547\n",
      "Validation: Epoch [19], Batch [230/938], Loss: 0.5622702240943909\n",
      "Validation: Epoch [19], Batch [231/938], Loss: 0.46327608823776245\n",
      "Validation: Epoch [19], Batch [232/938], Loss: 0.6115522980690002\n",
      "Validation: Epoch [19], Batch [233/938], Loss: 0.5113329291343689\n",
      "Validation: Epoch [19], Batch [234/938], Loss: 0.419405996799469\n",
      "Validation: Epoch [19], Batch [235/938], Loss: 0.6558734774589539\n",
      "Validation: Epoch [19], Batch [236/938], Loss: 0.5208806395530701\n",
      "Validation: Epoch [19], Batch [237/938], Loss: 0.6717827916145325\n",
      "Validation: Epoch [19], Batch [238/938], Loss: 0.807741641998291\n",
      "Validation: Epoch [19], Batch [239/938], Loss: 0.6522160768508911\n",
      "Validation: Epoch [19], Batch [240/938], Loss: 0.41964006423950195\n",
      "Validation: Epoch [19], Batch [241/938], Loss: 0.5735734105110168\n",
      "Validation: Epoch [19], Batch [242/938], Loss: 0.6245169639587402\n",
      "Validation: Epoch [19], Batch [243/938], Loss: 0.6777976751327515\n",
      "Validation: Epoch [19], Batch [244/938], Loss: 0.6137602925300598\n",
      "Validation: Epoch [19], Batch [245/938], Loss: 0.4563385546207428\n",
      "Validation: Epoch [19], Batch [246/938], Loss: 0.7667006850242615\n",
      "Validation: Epoch [19], Batch [247/938], Loss: 0.4881165325641632\n",
      "Validation: Epoch [19], Batch [248/938], Loss: 0.6406088471412659\n",
      "Validation: Epoch [19], Batch [249/938], Loss: 0.37075620889663696\n",
      "Validation: Epoch [19], Batch [250/938], Loss: 0.5492202639579773\n",
      "Validation: Epoch [19], Batch [251/938], Loss: 0.7611595392227173\n",
      "Validation: Epoch [19], Batch [252/938], Loss: 0.451657235622406\n",
      "Validation: Epoch [19], Batch [253/938], Loss: 0.7996475696563721\n",
      "Validation: Epoch [19], Batch [254/938], Loss: 0.36237847805023193\n",
      "Validation: Epoch [19], Batch [255/938], Loss: 0.6251016855239868\n",
      "Validation: Epoch [19], Batch [256/938], Loss: 0.6486345529556274\n",
      "Validation: Epoch [19], Batch [257/938], Loss: 0.6988052129745483\n",
      "Validation: Epoch [19], Batch [258/938], Loss: 0.3845357596874237\n",
      "Validation: Epoch [19], Batch [259/938], Loss: 0.432480126619339\n",
      "Validation: Epoch [19], Batch [260/938], Loss: 0.7605229616165161\n",
      "Validation: Epoch [19], Batch [261/938], Loss: 0.6880760192871094\n",
      "Validation: Epoch [19], Batch [262/938], Loss: 0.5261703729629517\n",
      "Validation: Epoch [19], Batch [263/938], Loss: 0.55675208568573\n",
      "Validation: Epoch [19], Batch [264/938], Loss: 0.6765305399894714\n",
      "Validation: Epoch [19], Batch [265/938], Loss: 0.7519007921218872\n",
      "Validation: Epoch [19], Batch [266/938], Loss: 0.6586969494819641\n",
      "Validation: Epoch [19], Batch [267/938], Loss: 0.5557129979133606\n",
      "Validation: Epoch [19], Batch [268/938], Loss: 0.4682025909423828\n",
      "Validation: Epoch [19], Batch [269/938], Loss: 0.43796446919441223\n",
      "Validation: Epoch [19], Batch [270/938], Loss: 0.489670068025589\n",
      "Validation: Epoch [19], Batch [271/938], Loss: 0.7123805284500122\n",
      "Validation: Epoch [19], Batch [272/938], Loss: 0.5287989974021912\n",
      "Validation: Epoch [19], Batch [273/938], Loss: 0.7697575092315674\n",
      "Validation: Epoch [19], Batch [274/938], Loss: 0.6611070036888123\n",
      "Validation: Epoch [19], Batch [275/938], Loss: 0.6967700719833374\n",
      "Validation: Epoch [19], Batch [276/938], Loss: 0.5600849986076355\n",
      "Validation: Epoch [19], Batch [277/938], Loss: 0.5161274075508118\n",
      "Validation: Epoch [19], Batch [278/938], Loss: 0.5190207362174988\n",
      "Validation: Epoch [19], Batch [279/938], Loss: 0.5590922236442566\n",
      "Validation: Epoch [19], Batch [280/938], Loss: 0.495756596326828\n",
      "Validation: Epoch [19], Batch [281/938], Loss: 0.534304141998291\n",
      "Validation: Epoch [19], Batch [282/938], Loss: 0.8368488550186157\n",
      "Validation: Epoch [19], Batch [283/938], Loss: 0.47335267066955566\n",
      "Validation: Epoch [19], Batch [284/938], Loss: 0.8026508092880249\n",
      "Validation: Epoch [19], Batch [285/938], Loss: 0.6016009449958801\n",
      "Validation: Epoch [19], Batch [286/938], Loss: 0.3701134920120239\n",
      "Validation: Epoch [19], Batch [287/938], Loss: 0.4645378291606903\n",
      "Validation: Epoch [19], Batch [288/938], Loss: 0.550234854221344\n",
      "Validation: Epoch [19], Batch [289/938], Loss: 0.373354434967041\n",
      "Validation: Epoch [19], Batch [290/938], Loss: 0.45944541692733765\n",
      "Validation: Epoch [19], Batch [291/938], Loss: 0.5281593203544617\n",
      "Validation: Epoch [19], Batch [292/938], Loss: 0.48557648062705994\n",
      "Validation: Epoch [19], Batch [293/938], Loss: 0.5788087844848633\n",
      "Validation: Epoch [19], Batch [294/938], Loss: 0.5552695989608765\n",
      "Validation: Epoch [19], Batch [295/938], Loss: 0.516698956489563\n",
      "Validation: Epoch [19], Batch [296/938], Loss: 0.7889455556869507\n",
      "Validation: Epoch [19], Batch [297/938], Loss: 0.5421333909034729\n",
      "Validation: Epoch [19], Batch [298/938], Loss: 0.4262121319770813\n",
      "Validation: Epoch [19], Batch [299/938], Loss: 0.5210462212562561\n",
      "Validation: Epoch [19], Batch [300/938], Loss: 0.6150602102279663\n",
      "Validation: Epoch [19], Batch [301/938], Loss: 0.5713210105895996\n",
      "Validation: Epoch [19], Batch [302/938], Loss: 0.630062460899353\n",
      "Validation: Epoch [19], Batch [303/938], Loss: 0.7186456322669983\n",
      "Validation: Epoch [19], Batch [304/938], Loss: 0.5660619735717773\n",
      "Validation: Epoch [19], Batch [305/938], Loss: 0.8275019526481628\n",
      "Validation: Epoch [19], Batch [306/938], Loss: 0.42989540100097656\n",
      "Validation: Epoch [19], Batch [307/938], Loss: 0.5230230093002319\n",
      "Validation: Epoch [19], Batch [308/938], Loss: 0.7772352695465088\n",
      "Validation: Epoch [19], Batch [309/938], Loss: 0.7527480721473694\n",
      "Validation: Epoch [19], Batch [310/938], Loss: 0.5615854263305664\n",
      "Validation: Epoch [19], Batch [311/938], Loss: 0.811812162399292\n",
      "Validation: Epoch [19], Batch [312/938], Loss: 0.6381939649581909\n",
      "Validation: Epoch [19], Batch [313/938], Loss: 0.5124937295913696\n",
      "Validation: Epoch [19], Batch [314/938], Loss: 0.6366416215896606\n",
      "Validation: Epoch [19], Batch [315/938], Loss: 0.625924825668335\n",
      "Validation: Epoch [19], Batch [316/938], Loss: 0.5568872094154358\n",
      "Validation: Epoch [19], Batch [317/938], Loss: 0.6744211316108704\n",
      "Validation: Epoch [19], Batch [318/938], Loss: 0.36720648407936096\n",
      "Validation: Epoch [19], Batch [319/938], Loss: 0.4644849896430969\n",
      "Validation: Epoch [19], Batch [320/938], Loss: 0.5085285305976868\n",
      "Validation: Epoch [19], Batch [321/938], Loss: 0.47554606199264526\n",
      "Validation: Epoch [19], Batch [322/938], Loss: 0.6392889022827148\n",
      "Validation: Epoch [19], Batch [323/938], Loss: 0.5600790977478027\n",
      "Validation: Epoch [19], Batch [324/938], Loss: 0.6500262022018433\n",
      "Validation: Epoch [19], Batch [325/938], Loss: 0.762727677822113\n",
      "Validation: Epoch [19], Batch [326/938], Loss: 0.7108635306358337\n",
      "Validation: Epoch [19], Batch [327/938], Loss: 0.36333924531936646\n",
      "Validation: Epoch [19], Batch [328/938], Loss: 0.5785317420959473\n",
      "Validation: Epoch [19], Batch [329/938], Loss: 0.5752074718475342\n",
      "Validation: Epoch [19], Batch [330/938], Loss: 0.498975932598114\n",
      "Validation: Epoch [19], Batch [331/938], Loss: 0.6736341118812561\n",
      "Validation: Epoch [19], Batch [332/938], Loss: 0.5267844200134277\n",
      "Validation: Epoch [19], Batch [333/938], Loss: 0.5134768486022949\n",
      "Validation: Epoch [19], Batch [334/938], Loss: 0.5387805700302124\n",
      "Validation: Epoch [19], Batch [335/938], Loss: 0.6132616996765137\n",
      "Validation: Epoch [19], Batch [336/938], Loss: 0.5532666444778442\n",
      "Validation: Epoch [19], Batch [337/938], Loss: 0.48817873001098633\n",
      "Validation: Epoch [19], Batch [338/938], Loss: 0.47955968976020813\n",
      "Validation: Epoch [19], Batch [339/938], Loss: 0.5860523581504822\n",
      "Validation: Epoch [19], Batch [340/938], Loss: 0.41878607869148254\n",
      "Validation: Epoch [19], Batch [341/938], Loss: 0.7773451209068298\n",
      "Validation: Epoch [19], Batch [342/938], Loss: 0.4340306222438812\n",
      "Validation: Epoch [19], Batch [343/938], Loss: 0.624846875667572\n",
      "Validation: Epoch [19], Batch [344/938], Loss: 0.6822351813316345\n",
      "Validation: Epoch [19], Batch [345/938], Loss: 0.36238035559654236\n",
      "Validation: Epoch [19], Batch [346/938], Loss: 0.36817121505737305\n",
      "Validation: Epoch [19], Batch [347/938], Loss: 0.45187699794769287\n",
      "Validation: Epoch [19], Batch [348/938], Loss: 0.5666617751121521\n",
      "Validation: Epoch [19], Batch [349/938], Loss: 0.42043933272361755\n",
      "Validation: Epoch [19], Batch [350/938], Loss: 0.9013206958770752\n",
      "Validation: Epoch [19], Batch [351/938], Loss: 0.807102382183075\n",
      "Validation: Epoch [19], Batch [352/938], Loss: 0.7146334648132324\n",
      "Validation: Epoch [19], Batch [353/938], Loss: 0.7675353288650513\n",
      "Validation: Epoch [19], Batch [354/938], Loss: 0.4667191803455353\n",
      "Validation: Epoch [19], Batch [355/938], Loss: 0.42059022188186646\n",
      "Validation: Epoch [19], Batch [356/938], Loss: 0.5849566459655762\n",
      "Validation: Epoch [19], Batch [357/938], Loss: 0.48489677906036377\n",
      "Validation: Epoch [19], Batch [358/938], Loss: 0.7249464392662048\n",
      "Validation: Epoch [19], Batch [359/938], Loss: 0.4627215564250946\n",
      "Validation: Epoch [19], Batch [360/938], Loss: 0.6875777244567871\n",
      "Validation: Epoch [19], Batch [361/938], Loss: 0.7231078743934631\n",
      "Validation: Epoch [19], Batch [362/938], Loss: 0.8347838521003723\n",
      "Validation: Epoch [19], Batch [363/938], Loss: 0.4830480217933655\n",
      "Validation: Epoch [19], Batch [364/938], Loss: 0.7130668759346008\n",
      "Validation: Epoch [19], Batch [365/938], Loss: 0.6339285373687744\n",
      "Validation: Epoch [19], Batch [366/938], Loss: 0.6831534504890442\n",
      "Validation: Epoch [19], Batch [367/938], Loss: 0.5969870090484619\n",
      "Validation: Epoch [19], Batch [368/938], Loss: 0.6297722458839417\n",
      "Validation: Epoch [19], Batch [369/938], Loss: 0.645784318447113\n",
      "Validation: Epoch [19], Batch [370/938], Loss: 0.5087016224861145\n",
      "Validation: Epoch [19], Batch [371/938], Loss: 0.6147552728652954\n",
      "Validation: Epoch [19], Batch [372/938], Loss: 0.5966158509254456\n",
      "Validation: Epoch [19], Batch [373/938], Loss: 0.6069189310073853\n",
      "Validation: Epoch [19], Batch [374/938], Loss: 0.8700175881385803\n",
      "Validation: Epoch [19], Batch [375/938], Loss: 0.7574852108955383\n",
      "Validation: Epoch [19], Batch [376/938], Loss: 0.660675048828125\n",
      "Validation: Epoch [19], Batch [377/938], Loss: 0.5861779451370239\n",
      "Validation: Epoch [19], Batch [378/938], Loss: 0.509697437286377\n",
      "Validation: Epoch [19], Batch [379/938], Loss: 0.44084566831588745\n",
      "Validation: Epoch [19], Batch [380/938], Loss: 0.5408629179000854\n",
      "Validation: Epoch [19], Batch [381/938], Loss: 0.5123816132545471\n",
      "Validation: Epoch [19], Batch [382/938], Loss: 0.45976755023002625\n",
      "Validation: Epoch [19], Batch [383/938], Loss: 0.3321938216686249\n",
      "Validation: Epoch [19], Batch [384/938], Loss: 0.5774005055427551\n",
      "Validation: Epoch [19], Batch [385/938], Loss: 0.6818399429321289\n",
      "Validation: Epoch [19], Batch [386/938], Loss: 0.5716681480407715\n",
      "Validation: Epoch [19], Batch [387/938], Loss: 0.6227238178253174\n",
      "Validation: Epoch [19], Batch [388/938], Loss: 0.4106062352657318\n",
      "Validation: Epoch [19], Batch [389/938], Loss: 0.5801456570625305\n",
      "Validation: Epoch [19], Batch [390/938], Loss: 0.42876067757606506\n",
      "Validation: Epoch [19], Batch [391/938], Loss: 0.4750651717185974\n",
      "Validation: Epoch [19], Batch [392/938], Loss: 0.6095676422119141\n",
      "Validation: Epoch [19], Batch [393/938], Loss: 0.8548595905303955\n",
      "Validation: Epoch [19], Batch [394/938], Loss: 0.4354572296142578\n",
      "Validation: Epoch [19], Batch [395/938], Loss: 0.570979118347168\n",
      "Validation: Epoch [19], Batch [396/938], Loss: 0.4671906530857086\n",
      "Validation: Epoch [19], Batch [397/938], Loss: 0.5783841013908386\n",
      "Validation: Epoch [19], Batch [398/938], Loss: 0.7165480852127075\n",
      "Validation: Epoch [19], Batch [399/938], Loss: 0.6774321794509888\n",
      "Validation: Epoch [19], Batch [400/938], Loss: 0.6312182545661926\n",
      "Validation: Epoch [19], Batch [401/938], Loss: 0.6748312711715698\n",
      "Validation: Epoch [19], Batch [402/938], Loss: 0.3051697015762329\n",
      "Validation: Epoch [19], Batch [403/938], Loss: 0.7428783774375916\n",
      "Validation: Epoch [19], Batch [404/938], Loss: 0.576862633228302\n",
      "Validation: Epoch [19], Batch [405/938], Loss: 0.597658097743988\n",
      "Validation: Epoch [19], Batch [406/938], Loss: 0.584963321685791\n",
      "Validation: Epoch [19], Batch [407/938], Loss: 0.7026634812355042\n",
      "Validation: Epoch [19], Batch [408/938], Loss: 0.47014346718788147\n",
      "Validation: Epoch [19], Batch [409/938], Loss: 0.7125094532966614\n",
      "Validation: Epoch [19], Batch [410/938], Loss: 0.563014805316925\n",
      "Validation: Epoch [19], Batch [411/938], Loss: 0.47827035188674927\n",
      "Validation: Epoch [19], Batch [412/938], Loss: 0.5330408215522766\n",
      "Validation: Epoch [19], Batch [413/938], Loss: 0.4720498323440552\n",
      "Validation: Epoch [19], Batch [414/938], Loss: 0.7936846017837524\n",
      "Validation: Epoch [19], Batch [415/938], Loss: 0.45682036876678467\n",
      "Validation: Epoch [19], Batch [416/938], Loss: 0.6129152178764343\n",
      "Validation: Epoch [19], Batch [417/938], Loss: 0.6218331456184387\n",
      "Validation: Epoch [19], Batch [418/938], Loss: 0.5871784090995789\n",
      "Validation: Epoch [19], Batch [419/938], Loss: 0.7082003355026245\n",
      "Validation: Epoch [19], Batch [420/938], Loss: 0.7769953608512878\n",
      "Validation: Epoch [19], Batch [421/938], Loss: 0.38993051648139954\n",
      "Validation: Epoch [19], Batch [422/938], Loss: 1.013073444366455\n",
      "Validation: Epoch [19], Batch [423/938], Loss: 0.771625280380249\n",
      "Validation: Epoch [19], Batch [424/938], Loss: 0.5129640102386475\n",
      "Validation: Epoch [19], Batch [425/938], Loss: 0.712696373462677\n",
      "Validation: Epoch [19], Batch [426/938], Loss: 0.4319450259208679\n",
      "Validation: Epoch [19], Batch [427/938], Loss: 0.4839656352996826\n",
      "Validation: Epoch [19], Batch [428/938], Loss: 0.630577027797699\n",
      "Validation: Epoch [19], Batch [429/938], Loss: 0.5896162986755371\n",
      "Validation: Epoch [19], Batch [430/938], Loss: 1.104479432106018\n",
      "Validation: Epoch [19], Batch [431/938], Loss: 0.6706235408782959\n",
      "Validation: Epoch [19], Batch [432/938], Loss: 0.5085437297821045\n",
      "Validation: Epoch [19], Batch [433/938], Loss: 0.6087160110473633\n",
      "Validation: Epoch [19], Batch [434/938], Loss: 0.6446424722671509\n",
      "Validation: Epoch [19], Batch [435/938], Loss: 0.47445881366729736\n",
      "Validation: Epoch [19], Batch [436/938], Loss: 0.6443197727203369\n",
      "Validation: Epoch [19], Batch [437/938], Loss: 0.535620391368866\n",
      "Validation: Epoch [19], Batch [438/938], Loss: 0.4151582717895508\n",
      "Validation: Epoch [19], Batch [439/938], Loss: 0.5228957533836365\n",
      "Validation: Epoch [19], Batch [440/938], Loss: 0.6288979053497314\n",
      "Validation: Epoch [19], Batch [441/938], Loss: 0.4097561836242676\n",
      "Validation: Epoch [19], Batch [442/938], Loss: 0.5815274119377136\n",
      "Validation: Epoch [19], Batch [443/938], Loss: 0.49346113204956055\n",
      "Validation: Epoch [19], Batch [444/938], Loss: 0.538402795791626\n",
      "Validation: Epoch [19], Batch [445/938], Loss: 0.6172846555709839\n",
      "Validation: Epoch [19], Batch [446/938], Loss: 0.905297040939331\n",
      "Validation: Epoch [19], Batch [447/938], Loss: 0.6565515995025635\n",
      "Validation: Epoch [19], Batch [448/938], Loss: 0.43049144744873047\n",
      "Validation: Epoch [19], Batch [449/938], Loss: 0.6761231422424316\n",
      "Validation: Epoch [19], Batch [450/938], Loss: 0.48670321702957153\n",
      "Validation: Epoch [19], Batch [451/938], Loss: 0.8886968493461609\n",
      "Validation: Epoch [19], Batch [452/938], Loss: 0.7047364115715027\n",
      "Validation: Epoch [19], Batch [453/938], Loss: 0.4938279986381531\n",
      "Validation: Epoch [19], Batch [454/938], Loss: 0.8584155440330505\n",
      "Validation: Epoch [19], Batch [455/938], Loss: 0.5717682838439941\n",
      "Validation: Epoch [19], Batch [456/938], Loss: 0.5590804815292358\n",
      "Validation: Epoch [19], Batch [457/938], Loss: 0.6374271512031555\n",
      "Validation: Epoch [19], Batch [458/938], Loss: 0.6381621956825256\n",
      "Validation: Epoch [19], Batch [459/938], Loss: 0.42827466130256653\n",
      "Validation: Epoch [19], Batch [460/938], Loss: 0.7536419630050659\n",
      "Validation: Epoch [19], Batch [461/938], Loss: 0.5378054976463318\n",
      "Validation: Epoch [19], Batch [462/938], Loss: 0.5040300488471985\n",
      "Validation: Epoch [19], Batch [463/938], Loss: 0.466288685798645\n",
      "Validation: Epoch [19], Batch [464/938], Loss: 0.41341710090637207\n",
      "Validation: Epoch [19], Batch [465/938], Loss: 0.8116805553436279\n",
      "Validation: Epoch [19], Batch [466/938], Loss: 0.5073275566101074\n",
      "Validation: Epoch [19], Batch [467/938], Loss: 0.4690607190132141\n",
      "Validation: Epoch [19], Batch [468/938], Loss: 0.9172748327255249\n",
      "Validation: Epoch [19], Batch [469/938], Loss: 0.6229385137557983\n",
      "Validation: Epoch [19], Batch [470/938], Loss: 0.6332257390022278\n",
      "Validation: Epoch [19], Batch [471/938], Loss: 0.8053345084190369\n",
      "Validation: Epoch [19], Batch [472/938], Loss: 0.43059808015823364\n",
      "Validation: Epoch [19], Batch [473/938], Loss: 0.5311921834945679\n",
      "Validation: Epoch [19], Batch [474/938], Loss: 0.3698364794254303\n",
      "Validation: Epoch [19], Batch [475/938], Loss: 0.4335322082042694\n",
      "Validation: Epoch [19], Batch [476/938], Loss: 0.5779320001602173\n",
      "Validation: Epoch [19], Batch [477/938], Loss: 0.5048882961273193\n",
      "Validation: Epoch [19], Batch [478/938], Loss: 0.487092524766922\n",
      "Validation: Epoch [19], Batch [479/938], Loss: 0.4729832708835602\n",
      "Validation: Epoch [19], Batch [480/938], Loss: 0.8379594683647156\n",
      "Validation: Epoch [19], Batch [481/938], Loss: 0.4496035575866699\n",
      "Validation: Epoch [19], Batch [482/938], Loss: 0.7433100342750549\n",
      "Validation: Epoch [19], Batch [483/938], Loss: 0.6089342832565308\n",
      "Validation: Epoch [19], Batch [484/938], Loss: 0.39858338236808777\n",
      "Validation: Epoch [19], Batch [485/938], Loss: 0.43765389919281006\n",
      "Validation: Epoch [19], Batch [486/938], Loss: 0.47533154487609863\n",
      "Validation: Epoch [19], Batch [487/938], Loss: 0.5032720565795898\n",
      "Validation: Epoch [19], Batch [488/938], Loss: 0.46821725368499756\n",
      "Validation: Epoch [19], Batch [489/938], Loss: 0.6675228476524353\n",
      "Validation: Epoch [19], Batch [490/938], Loss: 0.7107436656951904\n",
      "Validation: Epoch [19], Batch [491/938], Loss: 0.6417766213417053\n",
      "Validation: Epoch [19], Batch [492/938], Loss: 0.5929419994354248\n",
      "Validation: Epoch [19], Batch [493/938], Loss: 0.6744649410247803\n",
      "Validation: Epoch [19], Batch [494/938], Loss: 0.34673556685447693\n",
      "Validation: Epoch [19], Batch [495/938], Loss: 0.5088058114051819\n",
      "Validation: Epoch [19], Batch [496/938], Loss: 0.5518664121627808\n",
      "Validation: Epoch [19], Batch [497/938], Loss: 0.4552544951438904\n",
      "Validation: Epoch [19], Batch [498/938], Loss: 0.44443655014038086\n",
      "Validation: Epoch [19], Batch [499/938], Loss: 0.3474186956882477\n",
      "Validation: Epoch [19], Batch [500/938], Loss: 0.38678887486457825\n",
      "Validation: Epoch [19], Batch [501/938], Loss: 0.5516840219497681\n",
      "Validation: Epoch [19], Batch [502/938], Loss: 0.6316760182380676\n",
      "Validation: Epoch [19], Batch [503/938], Loss: 0.5201469659805298\n",
      "Validation: Epoch [19], Batch [504/938], Loss: 0.763271689414978\n",
      "Validation: Epoch [19], Batch [505/938], Loss: 0.4727301299571991\n",
      "Validation: Epoch [19], Batch [506/938], Loss: 0.4607446491718292\n",
      "Validation: Epoch [19], Batch [507/938], Loss: 0.5247660279273987\n",
      "Validation: Epoch [19], Batch [508/938], Loss: 0.7078770995140076\n",
      "Validation: Epoch [19], Batch [509/938], Loss: 0.5871150493621826\n",
      "Validation: Epoch [19], Batch [510/938], Loss: 0.6858530640602112\n",
      "Validation: Epoch [19], Batch [511/938], Loss: 0.5374510884284973\n",
      "Validation: Epoch [19], Batch [512/938], Loss: 0.9738516807556152\n",
      "Validation: Epoch [19], Batch [513/938], Loss: 0.5654131174087524\n",
      "Validation: Epoch [19], Batch [514/938], Loss: 0.5991565585136414\n",
      "Validation: Epoch [19], Batch [515/938], Loss: 0.47793111205101013\n",
      "Validation: Epoch [19], Batch [516/938], Loss: 0.6249287128448486\n",
      "Validation: Epoch [19], Batch [517/938], Loss: 0.4518427550792694\n",
      "Validation: Epoch [19], Batch [518/938], Loss: 0.3872469365596771\n",
      "Validation: Epoch [19], Batch [519/938], Loss: 0.5964783430099487\n",
      "Validation: Epoch [19], Batch [520/938], Loss: 0.3228561282157898\n",
      "Validation: Epoch [19], Batch [521/938], Loss: 0.5848708152770996\n",
      "Validation: Epoch [19], Batch [522/938], Loss: 0.4953981935977936\n",
      "Validation: Epoch [19], Batch [523/938], Loss: 0.8326433897018433\n",
      "Validation: Epoch [19], Batch [524/938], Loss: 0.48442506790161133\n",
      "Validation: Epoch [19], Batch [525/938], Loss: 0.7832725048065186\n",
      "Validation: Epoch [19], Batch [526/938], Loss: 0.3927934765815735\n",
      "Validation: Epoch [19], Batch [527/938], Loss: 0.5108484625816345\n",
      "Validation: Epoch [19], Batch [528/938], Loss: 0.4881012439727783\n",
      "Validation: Epoch [19], Batch [529/938], Loss: 0.673829972743988\n",
      "Validation: Epoch [19], Batch [530/938], Loss: 0.4412625730037689\n",
      "Validation: Epoch [19], Batch [531/938], Loss: 0.5861775875091553\n",
      "Validation: Epoch [19], Batch [532/938], Loss: 0.41995441913604736\n",
      "Validation: Epoch [19], Batch [533/938], Loss: 0.7523094415664673\n",
      "Validation: Epoch [19], Batch [534/938], Loss: 0.7007163763046265\n",
      "Validation: Epoch [19], Batch [535/938], Loss: 0.5829308032989502\n",
      "Validation: Epoch [19], Batch [536/938], Loss: 0.7112426161766052\n",
      "Validation: Epoch [19], Batch [537/938], Loss: 0.6323553323745728\n",
      "Validation: Epoch [19], Batch [538/938], Loss: 0.41544580459594727\n",
      "Validation: Epoch [19], Batch [539/938], Loss: 0.4610097408294678\n",
      "Validation: Epoch [19], Batch [540/938], Loss: 0.6035327911376953\n",
      "Validation: Epoch [19], Batch [541/938], Loss: 0.6792454123497009\n",
      "Validation: Epoch [19], Batch [542/938], Loss: 0.5892709493637085\n",
      "Validation: Epoch [19], Batch [543/938], Loss: 0.4271027147769928\n",
      "Validation: Epoch [19], Batch [544/938], Loss: 0.6030893921852112\n",
      "Validation: Epoch [19], Batch [545/938], Loss: 0.43276819586753845\n",
      "Validation: Epoch [19], Batch [546/938], Loss: 0.43688422441482544\n",
      "Validation: Epoch [19], Batch [547/938], Loss: 0.6857500672340393\n",
      "Validation: Epoch [19], Batch [548/938], Loss: 0.39368152618408203\n",
      "Validation: Epoch [19], Batch [549/938], Loss: 0.6246932148933411\n",
      "Validation: Epoch [19], Batch [550/938], Loss: 0.7035827040672302\n",
      "Validation: Epoch [19], Batch [551/938], Loss: 0.4270615875720978\n",
      "Validation: Epoch [19], Batch [552/938], Loss: 0.44292399287223816\n",
      "Validation: Epoch [19], Batch [553/938], Loss: 0.6241887807846069\n",
      "Validation: Epoch [19], Batch [554/938], Loss: 0.6812543272972107\n",
      "Validation: Epoch [19], Batch [555/938], Loss: 0.8356784582138062\n",
      "Validation: Epoch [19], Batch [556/938], Loss: 0.46723446249961853\n",
      "Validation: Epoch [19], Batch [557/938], Loss: 0.6046044826507568\n",
      "Validation: Epoch [19], Batch [558/938], Loss: 0.5927319526672363\n",
      "Validation: Epoch [19], Batch [559/938], Loss: 0.5835421085357666\n",
      "Validation: Epoch [19], Batch [560/938], Loss: 0.6339893937110901\n",
      "Validation: Epoch [19], Batch [561/938], Loss: 0.5257711410522461\n",
      "Validation: Epoch [19], Batch [562/938], Loss: 0.38047370314598083\n",
      "Validation: Epoch [19], Batch [563/938], Loss: 0.4549333453178406\n",
      "Validation: Epoch [19], Batch [564/938], Loss: 0.39753732085227966\n",
      "Validation: Epoch [19], Batch [565/938], Loss: 0.7959954738616943\n",
      "Validation: Epoch [19], Batch [566/938], Loss: 0.5336806178092957\n",
      "Validation: Epoch [19], Batch [567/938], Loss: 0.950305700302124\n",
      "Validation: Epoch [19], Batch [568/938], Loss: 0.5282834768295288\n",
      "Validation: Epoch [19], Batch [569/938], Loss: 0.6103515028953552\n",
      "Validation: Epoch [19], Batch [570/938], Loss: 0.6313090324401855\n",
      "Validation: Epoch [19], Batch [571/938], Loss: 0.7408388257026672\n",
      "Validation: Epoch [19], Batch [572/938], Loss: 0.4753683805465698\n",
      "Validation: Epoch [19], Batch [573/938], Loss: 0.4456767439842224\n",
      "Validation: Epoch [19], Batch [574/938], Loss: 0.4624166488647461\n",
      "Validation: Epoch [19], Batch [575/938], Loss: 0.5429438352584839\n",
      "Validation: Epoch [19], Batch [576/938], Loss: 0.5981348156929016\n",
      "Validation: Epoch [19], Batch [577/938], Loss: 0.5574400424957275\n",
      "Validation: Epoch [19], Batch [578/938], Loss: 0.6073076725006104\n",
      "Validation: Epoch [19], Batch [579/938], Loss: 0.5647658705711365\n",
      "Validation: Epoch [19], Batch [580/938], Loss: 0.5645735263824463\n",
      "Validation: Epoch [19], Batch [581/938], Loss: 0.6108760237693787\n",
      "Validation: Epoch [19], Batch [582/938], Loss: 0.6313832402229309\n",
      "Validation: Epoch [19], Batch [583/938], Loss: 0.4042704999446869\n",
      "Validation: Epoch [19], Batch [584/938], Loss: 0.6300349235534668\n",
      "Validation: Epoch [19], Batch [585/938], Loss: 0.7427462339401245\n",
      "Validation: Epoch [19], Batch [586/938], Loss: 0.8146777153015137\n",
      "Validation: Epoch [19], Batch [587/938], Loss: 0.5857921838760376\n",
      "Validation: Epoch [19], Batch [588/938], Loss: 0.6318813562393188\n",
      "Validation: Epoch [19], Batch [589/938], Loss: 0.669476330280304\n",
      "Validation: Epoch [19], Batch [590/938], Loss: 0.6006230115890503\n",
      "Validation: Epoch [19], Batch [591/938], Loss: 0.3608577251434326\n",
      "Validation: Epoch [19], Batch [592/938], Loss: 0.64680016040802\n",
      "Validation: Epoch [19], Batch [593/938], Loss: 0.5311170816421509\n",
      "Validation: Epoch [19], Batch [594/938], Loss: 0.5682384371757507\n",
      "Validation: Epoch [19], Batch [595/938], Loss: 0.6153055429458618\n",
      "Validation: Epoch [19], Batch [596/938], Loss: 0.4421488642692566\n",
      "Validation: Epoch [19], Batch [597/938], Loss: 0.7337775230407715\n",
      "Validation: Epoch [19], Batch [598/938], Loss: 0.8390955924987793\n",
      "Validation: Epoch [19], Batch [599/938], Loss: 0.3926321864128113\n",
      "Validation: Epoch [19], Batch [600/938], Loss: 0.6591395139694214\n",
      "Validation: Epoch [19], Batch [601/938], Loss: 0.45315316319465637\n",
      "Validation: Epoch [19], Batch [602/938], Loss: 0.4584709405899048\n",
      "Validation: Epoch [19], Batch [603/938], Loss: 0.7263220548629761\n",
      "Validation: Epoch [19], Batch [604/938], Loss: 0.5397111177444458\n",
      "Validation: Epoch [19], Batch [605/938], Loss: 0.8311549425125122\n",
      "Validation: Epoch [19], Batch [606/938], Loss: 0.6493973731994629\n",
      "Validation: Epoch [19], Batch [607/938], Loss: 0.47980543971061707\n",
      "Validation: Epoch [19], Batch [608/938], Loss: 0.5117441415786743\n",
      "Validation: Epoch [19], Batch [609/938], Loss: 0.6608790159225464\n",
      "Validation: Epoch [19], Batch [610/938], Loss: 0.42738425731658936\n",
      "Validation: Epoch [19], Batch [611/938], Loss: 0.5496504306793213\n",
      "Validation: Epoch [19], Batch [612/938], Loss: 0.6600298285484314\n",
      "Validation: Epoch [19], Batch [613/938], Loss: 0.5825365781784058\n",
      "Validation: Epoch [19], Batch [614/938], Loss: 0.7355527877807617\n",
      "Validation: Epoch [19], Batch [615/938], Loss: 0.6598459482192993\n",
      "Validation: Epoch [19], Batch [616/938], Loss: 0.8645310997962952\n",
      "Validation: Epoch [19], Batch [617/938], Loss: 0.5247272849082947\n",
      "Validation: Epoch [19], Batch [618/938], Loss: 0.5676377415657043\n",
      "Validation: Epoch [19], Batch [619/938], Loss: 0.6984238624572754\n",
      "Validation: Epoch [19], Batch [620/938], Loss: 0.3868885040283203\n",
      "Validation: Epoch [19], Batch [621/938], Loss: 0.632964551448822\n",
      "Validation: Epoch [19], Batch [622/938], Loss: 0.5571051836013794\n",
      "Validation: Epoch [19], Batch [623/938], Loss: 0.6054456830024719\n",
      "Validation: Epoch [19], Batch [624/938], Loss: 0.5094911456108093\n",
      "Validation: Epoch [19], Batch [625/938], Loss: 0.6674262285232544\n",
      "Validation: Epoch [19], Batch [626/938], Loss: 0.6958085298538208\n",
      "Validation: Epoch [19], Batch [627/938], Loss: 0.6269021034240723\n",
      "Validation: Epoch [19], Batch [628/938], Loss: 0.588549017906189\n",
      "Validation: Epoch [19], Batch [629/938], Loss: 0.8079027533531189\n",
      "Validation: Epoch [19], Batch [630/938], Loss: 0.7747434973716736\n",
      "Validation: Epoch [19], Batch [631/938], Loss: 0.6254240870475769\n",
      "Validation: Epoch [19], Batch [632/938], Loss: 0.6081833839416504\n",
      "Validation: Epoch [19], Batch [633/938], Loss: 0.5726486444473267\n",
      "Validation: Epoch [19], Batch [634/938], Loss: 0.9178403615951538\n",
      "Validation: Epoch [19], Batch [635/938], Loss: 0.536876916885376\n",
      "Validation: Epoch [19], Batch [636/938], Loss: 0.44471460580825806\n",
      "Validation: Epoch [19], Batch [637/938], Loss: 0.30640092492103577\n",
      "Validation: Epoch [19], Batch [638/938], Loss: 0.6301489472389221\n",
      "Validation: Epoch [19], Batch [639/938], Loss: 0.4684370160102844\n",
      "Validation: Epoch [19], Batch [640/938], Loss: 0.7760586142539978\n",
      "Validation: Epoch [19], Batch [641/938], Loss: 0.3855682909488678\n",
      "Validation: Epoch [19], Batch [642/938], Loss: 0.38430193066596985\n",
      "Validation: Epoch [19], Batch [643/938], Loss: 0.7566824555397034\n",
      "Validation: Epoch [19], Batch [644/938], Loss: 0.42199698090553284\n",
      "Validation: Epoch [19], Batch [645/938], Loss: 0.47232794761657715\n",
      "Validation: Epoch [19], Batch [646/938], Loss: 0.2997525930404663\n",
      "Validation: Epoch [19], Batch [647/938], Loss: 0.6846700310707092\n",
      "Validation: Epoch [19], Batch [648/938], Loss: 0.7135003805160522\n",
      "Validation: Epoch [19], Batch [649/938], Loss: 0.6510281562805176\n",
      "Validation: Epoch [19], Batch [650/938], Loss: 0.45498448610305786\n",
      "Validation: Epoch [19], Batch [651/938], Loss: 0.5698282122612\n",
      "Validation: Epoch [19], Batch [652/938], Loss: 0.6206547021865845\n",
      "Validation: Epoch [19], Batch [653/938], Loss: 0.5731673836708069\n",
      "Validation: Epoch [19], Batch [654/938], Loss: 0.41433659195899963\n",
      "Validation: Epoch [19], Batch [655/938], Loss: 0.6038529872894287\n",
      "Validation: Epoch [19], Batch [656/938], Loss: 0.4741293489933014\n",
      "Validation: Epoch [19], Batch [657/938], Loss: 0.7524231672286987\n",
      "Validation: Epoch [19], Batch [658/938], Loss: 0.5067000985145569\n",
      "Validation: Epoch [19], Batch [659/938], Loss: 0.7089097499847412\n",
      "Validation: Epoch [19], Batch [660/938], Loss: 0.5211964249610901\n",
      "Validation: Epoch [19], Batch [661/938], Loss: 0.8269444704055786\n",
      "Validation: Epoch [19], Batch [662/938], Loss: 0.466208815574646\n",
      "Validation: Epoch [19], Batch [663/938], Loss: 0.4799748957157135\n",
      "Validation: Epoch [19], Batch [664/938], Loss: 0.8129726648330688\n",
      "Validation: Epoch [19], Batch [665/938], Loss: 0.7411516904830933\n",
      "Validation: Epoch [19], Batch [666/938], Loss: 0.406623512506485\n",
      "Validation: Epoch [19], Batch [667/938], Loss: 0.496050089597702\n",
      "Validation: Epoch [19], Batch [668/938], Loss: 0.6522892713546753\n",
      "Validation: Epoch [19], Batch [669/938], Loss: 0.561967134475708\n",
      "Validation: Epoch [19], Batch [670/938], Loss: 0.5497342944145203\n",
      "Validation: Epoch [19], Batch [671/938], Loss: 0.30142638087272644\n",
      "Validation: Epoch [19], Batch [672/938], Loss: 0.5540672540664673\n",
      "Validation: Epoch [19], Batch [673/938], Loss: 0.4741038680076599\n",
      "Validation: Epoch [19], Batch [674/938], Loss: 0.6726458072662354\n",
      "Validation: Epoch [19], Batch [675/938], Loss: 0.5698879361152649\n",
      "Validation: Epoch [19], Batch [676/938], Loss: 0.38035908341407776\n",
      "Validation: Epoch [19], Batch [677/938], Loss: 0.7529280185699463\n",
      "Validation: Epoch [19], Batch [678/938], Loss: 0.6155388951301575\n",
      "Validation: Epoch [19], Batch [679/938], Loss: 0.42822927236557007\n",
      "Validation: Epoch [19], Batch [680/938], Loss: 0.4936599135398865\n",
      "Validation: Epoch [19], Batch [681/938], Loss: 0.35278767347335815\n",
      "Validation: Epoch [19], Batch [682/938], Loss: 0.680109441280365\n",
      "Validation: Epoch [19], Batch [683/938], Loss: 0.6145827770233154\n",
      "Validation: Epoch [19], Batch [684/938], Loss: 0.5611409544944763\n",
      "Validation: Epoch [19], Batch [685/938], Loss: 0.5660839676856995\n",
      "Validation: Epoch [19], Batch [686/938], Loss: 0.5173070430755615\n",
      "Validation: Epoch [19], Batch [687/938], Loss: 0.4967743754386902\n",
      "Validation: Epoch [19], Batch [688/938], Loss: 0.4933239221572876\n",
      "Validation: Epoch [19], Batch [689/938], Loss: 0.5263386964797974\n",
      "Validation: Epoch [19], Batch [690/938], Loss: 0.8000466227531433\n",
      "Validation: Epoch [19], Batch [691/938], Loss: 0.6462479829788208\n",
      "Validation: Epoch [19], Batch [692/938], Loss: 0.3819561302661896\n",
      "Validation: Epoch [19], Batch [693/938], Loss: 0.815784215927124\n",
      "Validation: Epoch [19], Batch [694/938], Loss: 0.7746289372444153\n",
      "Validation: Epoch [19], Batch [695/938], Loss: 0.5339791774749756\n",
      "Validation: Epoch [19], Batch [696/938], Loss: 0.7403556108474731\n",
      "Validation: Epoch [19], Batch [697/938], Loss: 0.5537415742874146\n",
      "Validation: Epoch [19], Batch [698/938], Loss: 0.35738664865493774\n",
      "Validation: Epoch [19], Batch [699/938], Loss: 0.6981310844421387\n",
      "Validation: Epoch [19], Batch [700/938], Loss: 0.5901690721511841\n",
      "Validation: Epoch [19], Batch [701/938], Loss: 0.6660304665565491\n",
      "Validation: Epoch [19], Batch [702/938], Loss: 0.3564787805080414\n",
      "Validation: Epoch [19], Batch [703/938], Loss: 0.6099569797515869\n",
      "Validation: Epoch [19], Batch [704/938], Loss: 0.5487259030342102\n",
      "Validation: Epoch [19], Batch [705/938], Loss: 0.5550270676612854\n",
      "Validation: Epoch [19], Batch [706/938], Loss: 0.5720597505569458\n",
      "Validation: Epoch [19], Batch [707/938], Loss: 0.6179828643798828\n",
      "Validation: Epoch [19], Batch [708/938], Loss: 0.5530264377593994\n",
      "Validation: Epoch [19], Batch [709/938], Loss: 0.5829225182533264\n",
      "Validation: Epoch [19], Batch [710/938], Loss: 0.5572806596755981\n",
      "Validation: Epoch [19], Batch [711/938], Loss: 0.5862753391265869\n",
      "Validation: Epoch [19], Batch [712/938], Loss: 0.5260781645774841\n",
      "Validation: Epoch [19], Batch [713/938], Loss: 0.6141979694366455\n",
      "Validation: Epoch [19], Batch [714/938], Loss: 0.46568796038627625\n",
      "Validation: Epoch [19], Batch [715/938], Loss: 0.42249059677124023\n",
      "Validation: Epoch [19], Batch [716/938], Loss: 0.6911827325820923\n",
      "Validation: Epoch [19], Batch [717/938], Loss: 0.6716725826263428\n",
      "Validation: Epoch [19], Batch [718/938], Loss: 0.6476041078567505\n",
      "Validation: Epoch [19], Batch [719/938], Loss: 0.48733043670654297\n",
      "Validation: Epoch [19], Batch [720/938], Loss: 0.8655290603637695\n",
      "Validation: Epoch [19], Batch [721/938], Loss: 0.5217894911766052\n",
      "Validation: Epoch [19], Batch [722/938], Loss: 0.5931671857833862\n",
      "Validation: Epoch [19], Batch [723/938], Loss: 0.5349549055099487\n",
      "Validation: Epoch [19], Batch [724/938], Loss: 0.736857533454895\n",
      "Validation: Epoch [19], Batch [725/938], Loss: 0.5179988145828247\n",
      "Validation: Epoch [19], Batch [726/938], Loss: 0.37829241156578064\n",
      "Validation: Epoch [19], Batch [727/938], Loss: 0.5285829305648804\n",
      "Validation: Epoch [19], Batch [728/938], Loss: 0.3260621130466461\n",
      "Validation: Epoch [19], Batch [729/938], Loss: 0.549785852432251\n",
      "Validation: Epoch [19], Batch [730/938], Loss: 0.9465319514274597\n",
      "Validation: Epoch [19], Batch [731/938], Loss: 0.3338759243488312\n",
      "Validation: Epoch [19], Batch [732/938], Loss: 0.6209214925765991\n",
      "Validation: Epoch [19], Batch [733/938], Loss: 0.4982760548591614\n",
      "Validation: Epoch [19], Batch [734/938], Loss: 0.7795067429542542\n",
      "Validation: Epoch [19], Batch [735/938], Loss: 0.6199507117271423\n",
      "Validation: Epoch [19], Batch [736/938], Loss: 0.3480307459831238\n",
      "Validation: Epoch [19], Batch [737/938], Loss: 0.5114673972129822\n",
      "Validation: Epoch [19], Batch [738/938], Loss: 0.5406954288482666\n",
      "Validation: Epoch [19], Batch [739/938], Loss: 0.3685913383960724\n",
      "Validation: Epoch [19], Batch [740/938], Loss: 0.36807841062545776\n",
      "Validation: Epoch [19], Batch [741/938], Loss: 0.5685181617736816\n",
      "Validation: Epoch [19], Batch [742/938], Loss: 0.6078860759735107\n",
      "Validation: Epoch [19], Batch [743/938], Loss: 0.8210322260856628\n",
      "Validation: Epoch [19], Batch [744/938], Loss: 0.690529465675354\n",
      "Validation: Epoch [19], Batch [745/938], Loss: 0.5195521116256714\n",
      "Validation: Epoch [19], Batch [746/938], Loss: 0.42717424035072327\n",
      "Validation: Epoch [19], Batch [747/938], Loss: 0.8360037207603455\n",
      "Validation: Epoch [19], Batch [748/938], Loss: 0.7324732542037964\n",
      "Validation: Epoch [19], Batch [749/938], Loss: 0.6286109089851379\n",
      "Validation: Epoch [19], Batch [750/938], Loss: 0.9547590613365173\n",
      "Validation: Epoch [19], Batch [751/938], Loss: 0.504528284072876\n",
      "Validation: Epoch [19], Batch [752/938], Loss: 0.7229740619659424\n",
      "Validation: Epoch [19], Batch [753/938], Loss: 0.46572253108024597\n",
      "Validation: Epoch [19], Batch [754/938], Loss: 0.6282350420951843\n",
      "Validation: Epoch [19], Batch [755/938], Loss: 0.509082555770874\n",
      "Validation: Epoch [19], Batch [756/938], Loss: 0.7005435228347778\n",
      "Validation: Epoch [19], Batch [757/938], Loss: 0.5709719061851501\n",
      "Validation: Epoch [19], Batch [758/938], Loss: 0.8128926753997803\n",
      "Validation: Epoch [19], Batch [759/938], Loss: 0.5231679081916809\n",
      "Validation: Epoch [19], Batch [760/938], Loss: 0.6009784936904907\n",
      "Validation: Epoch [19], Batch [761/938], Loss: 0.6758255958557129\n",
      "Validation: Epoch [19], Batch [762/938], Loss: 0.45274618268013\n",
      "Validation: Epoch [19], Batch [763/938], Loss: 0.6311855316162109\n",
      "Validation: Epoch [19], Batch [764/938], Loss: 0.6915658712387085\n",
      "Validation: Epoch [19], Batch [765/938], Loss: 0.4929483234882355\n",
      "Validation: Epoch [19], Batch [766/938], Loss: 0.6445967555046082\n",
      "Validation: Epoch [19], Batch [767/938], Loss: 0.5325344800949097\n",
      "Validation: Epoch [19], Batch [768/938], Loss: 0.5477002859115601\n",
      "Validation: Epoch [19], Batch [769/938], Loss: 0.5698824524879456\n",
      "Validation: Epoch [19], Batch [770/938], Loss: 0.48790442943573\n",
      "Validation: Epoch [19], Batch [771/938], Loss: 0.594927191734314\n",
      "Validation: Epoch [19], Batch [772/938], Loss: 0.8212217092514038\n",
      "Validation: Epoch [19], Batch [773/938], Loss: 0.7229303121566772\n",
      "Validation: Epoch [19], Batch [774/938], Loss: 0.5741429924964905\n",
      "Validation: Epoch [19], Batch [775/938], Loss: 0.5958038568496704\n",
      "Validation: Epoch [19], Batch [776/938], Loss: 0.5116263031959534\n",
      "Validation: Epoch [19], Batch [777/938], Loss: 0.33922278881073\n",
      "Validation: Epoch [19], Batch [778/938], Loss: 0.6712380647659302\n",
      "Validation: Epoch [19], Batch [779/938], Loss: 0.6676673889160156\n",
      "Validation: Epoch [19], Batch [780/938], Loss: 0.37528476119041443\n",
      "Validation: Epoch [19], Batch [781/938], Loss: 0.6149824261665344\n",
      "Validation: Epoch [19], Batch [782/938], Loss: 0.6095790863037109\n",
      "Validation: Epoch [19], Batch [783/938], Loss: 0.6461750268936157\n",
      "Validation: Epoch [19], Batch [784/938], Loss: 0.5206981301307678\n",
      "Validation: Epoch [19], Batch [785/938], Loss: 0.885729968547821\n",
      "Validation: Epoch [19], Batch [786/938], Loss: 0.5777125954627991\n",
      "Validation: Epoch [19], Batch [787/938], Loss: 0.7446593046188354\n",
      "Validation: Epoch [19], Batch [788/938], Loss: 0.5135542154312134\n",
      "Validation: Epoch [19], Batch [789/938], Loss: 0.5181554555892944\n",
      "Validation: Epoch [19], Batch [790/938], Loss: 0.4519396424293518\n",
      "Validation: Epoch [19], Batch [791/938], Loss: 0.5939018726348877\n",
      "Validation: Epoch [19], Batch [792/938], Loss: 0.7163511514663696\n",
      "Validation: Epoch [19], Batch [793/938], Loss: 0.7397375106811523\n",
      "Validation: Epoch [19], Batch [794/938], Loss: 0.7000221610069275\n",
      "Validation: Epoch [19], Batch [795/938], Loss: 0.61528080701828\n",
      "Validation: Epoch [19], Batch [796/938], Loss: 0.76779705286026\n",
      "Validation: Epoch [19], Batch [797/938], Loss: 0.5167620778083801\n",
      "Validation: Epoch [19], Batch [798/938], Loss: 0.7263199687004089\n",
      "Validation: Epoch [19], Batch [799/938], Loss: 0.5172332525253296\n",
      "Validation: Epoch [19], Batch [800/938], Loss: 0.5657519698143005\n",
      "Validation: Epoch [19], Batch [801/938], Loss: 0.7587928771972656\n",
      "Validation: Epoch [19], Batch [802/938], Loss: 0.5676775574684143\n",
      "Validation: Epoch [19], Batch [803/938], Loss: 0.5245909690856934\n",
      "Validation: Epoch [19], Batch [804/938], Loss: 0.5577957630157471\n",
      "Validation: Epoch [19], Batch [805/938], Loss: 0.6815438270568848\n",
      "Validation: Epoch [19], Batch [806/938], Loss: 0.41539889574050903\n",
      "Validation: Epoch [19], Batch [807/938], Loss: 0.5839284658432007\n",
      "Validation: Epoch [19], Batch [808/938], Loss: 0.7316579818725586\n",
      "Validation: Epoch [19], Batch [809/938], Loss: 0.49572843313217163\n",
      "Validation: Epoch [19], Batch [810/938], Loss: 0.5267696976661682\n",
      "Validation: Epoch [19], Batch [811/938], Loss: 0.4333524703979492\n",
      "Validation: Epoch [19], Batch [812/938], Loss: 0.6020567417144775\n",
      "Validation: Epoch [19], Batch [813/938], Loss: 0.6742359399795532\n",
      "Validation: Epoch [19], Batch [814/938], Loss: 0.6438290476799011\n",
      "Validation: Epoch [19], Batch [815/938], Loss: 0.4327579736709595\n",
      "Validation: Epoch [19], Batch [816/938], Loss: 0.5245561599731445\n",
      "Validation: Epoch [19], Batch [817/938], Loss: 0.7754579782485962\n",
      "Validation: Epoch [19], Batch [818/938], Loss: 0.3174820840358734\n",
      "Validation: Epoch [19], Batch [819/938], Loss: 0.6344527006149292\n",
      "Validation: Epoch [19], Batch [820/938], Loss: 0.6481868624687195\n",
      "Validation: Epoch [19], Batch [821/938], Loss: 0.6793620586395264\n",
      "Validation: Epoch [19], Batch [822/938], Loss: 0.46087783575057983\n",
      "Validation: Epoch [19], Batch [823/938], Loss: 0.650701642036438\n",
      "Validation: Epoch [19], Batch [824/938], Loss: 0.5168720483779907\n",
      "Validation: Epoch [19], Batch [825/938], Loss: 0.726599395275116\n",
      "Validation: Epoch [19], Batch [826/938], Loss: 0.6277102828025818\n",
      "Validation: Epoch [19], Batch [827/938], Loss: 0.716286838054657\n",
      "Validation: Epoch [19], Batch [828/938], Loss: 0.40770450234413147\n",
      "Validation: Epoch [19], Batch [829/938], Loss: 0.40929552912712097\n",
      "Validation: Epoch [19], Batch [830/938], Loss: 0.718640148639679\n",
      "Validation: Epoch [19], Batch [831/938], Loss: 0.39912882447242737\n",
      "Validation: Epoch [19], Batch [832/938], Loss: 0.41018643975257874\n",
      "Validation: Epoch [19], Batch [833/938], Loss: 0.5040520429611206\n",
      "Validation: Epoch [19], Batch [834/938], Loss: 0.5311390161514282\n",
      "Validation: Epoch [19], Batch [835/938], Loss: 0.5954734683036804\n",
      "Validation: Epoch [19], Batch [836/938], Loss: 0.3804795444011688\n",
      "Validation: Epoch [19], Batch [837/938], Loss: 0.6294338703155518\n",
      "Validation: Epoch [19], Batch [838/938], Loss: 0.5080478191375732\n",
      "Validation: Epoch [19], Batch [839/938], Loss: 0.5549910068511963\n",
      "Validation: Epoch [19], Batch [840/938], Loss: 0.5411117076873779\n",
      "Validation: Epoch [19], Batch [841/938], Loss: 0.5644702315330505\n",
      "Validation: Epoch [19], Batch [842/938], Loss: 0.6211334466934204\n",
      "Validation: Epoch [19], Batch [843/938], Loss: 0.7147477865219116\n",
      "Validation: Epoch [19], Batch [844/938], Loss: 0.41420015692710876\n",
      "Validation: Epoch [19], Batch [845/938], Loss: 0.7970898747444153\n",
      "Validation: Epoch [19], Batch [846/938], Loss: 0.5470585227012634\n",
      "Validation: Epoch [19], Batch [847/938], Loss: 0.6360953450202942\n",
      "Validation: Epoch [19], Batch [848/938], Loss: 0.6266200542449951\n",
      "Validation: Epoch [19], Batch [849/938], Loss: 0.5756375193595886\n",
      "Validation: Epoch [19], Batch [850/938], Loss: 0.5029661655426025\n",
      "Validation: Epoch [19], Batch [851/938], Loss: 0.48799464106559753\n",
      "Validation: Epoch [19], Batch [852/938], Loss: 0.4836331605911255\n",
      "Validation: Epoch [19], Batch [853/938], Loss: 0.43332639336586\n",
      "Validation: Epoch [19], Batch [854/938], Loss: 0.49126049876213074\n",
      "Validation: Epoch [19], Batch [855/938], Loss: 0.6811692118644714\n",
      "Validation: Epoch [19], Batch [856/938], Loss: 0.4557788372039795\n",
      "Validation: Epoch [19], Batch [857/938], Loss: 0.836904764175415\n",
      "Validation: Epoch [19], Batch [858/938], Loss: 0.6076351404190063\n",
      "Validation: Epoch [19], Batch [859/938], Loss: 0.5110056400299072\n",
      "Validation: Epoch [19], Batch [860/938], Loss: 0.41621989011764526\n",
      "Validation: Epoch [19], Batch [861/938], Loss: 0.8567958474159241\n",
      "Validation: Epoch [19], Batch [862/938], Loss: 0.5170174837112427\n",
      "Validation: Epoch [19], Batch [863/938], Loss: 0.5390588045120239\n",
      "Validation: Epoch [19], Batch [864/938], Loss: 0.6032295823097229\n",
      "Validation: Epoch [19], Batch [865/938], Loss: 0.4503897428512573\n",
      "Validation: Epoch [19], Batch [866/938], Loss: 0.4700655937194824\n",
      "Validation: Epoch [19], Batch [867/938], Loss: 0.632733941078186\n",
      "Validation: Epoch [19], Batch [868/938], Loss: 0.5459007620811462\n",
      "Validation: Epoch [19], Batch [869/938], Loss: 0.8551392555236816\n",
      "Validation: Epoch [19], Batch [870/938], Loss: 0.6237654685974121\n",
      "Validation: Epoch [19], Batch [871/938], Loss: 0.5417683720588684\n",
      "Validation: Epoch [19], Batch [872/938], Loss: 0.6132678389549255\n",
      "Validation: Epoch [19], Batch [873/938], Loss: 0.4092377722263336\n",
      "Validation: Epoch [19], Batch [874/938], Loss: 0.6100735068321228\n",
      "Validation: Epoch [19], Batch [875/938], Loss: 0.6502543091773987\n",
      "Validation: Epoch [19], Batch [876/938], Loss: 0.8295900821685791\n",
      "Validation: Epoch [19], Batch [877/938], Loss: 0.7025427222251892\n",
      "Validation: Epoch [19], Batch [878/938], Loss: 0.5402075052261353\n",
      "Validation: Epoch [19], Batch [879/938], Loss: 0.6019099950790405\n",
      "Validation: Epoch [19], Batch [880/938], Loss: 0.6320200562477112\n",
      "Validation: Epoch [19], Batch [881/938], Loss: 0.5231047868728638\n",
      "Validation: Epoch [19], Batch [882/938], Loss: 0.5463241934776306\n",
      "Validation: Epoch [19], Batch [883/938], Loss: 0.7628189921379089\n",
      "Validation: Epoch [19], Batch [884/938], Loss: 0.3512246310710907\n",
      "Validation: Epoch [19], Batch [885/938], Loss: 0.6430309414863586\n",
      "Validation: Epoch [19], Batch [886/938], Loss: 0.44934165477752686\n",
      "Validation: Epoch [19], Batch [887/938], Loss: 0.6667696833610535\n",
      "Validation: Epoch [19], Batch [888/938], Loss: 0.6515490412712097\n",
      "Validation: Epoch [19], Batch [889/938], Loss: 0.6294758319854736\n",
      "Validation: Epoch [19], Batch [890/938], Loss: 0.606330394744873\n",
      "Validation: Epoch [19], Batch [891/938], Loss: 0.5835549831390381\n",
      "Validation: Epoch [19], Batch [892/938], Loss: 0.41959142684936523\n",
      "Validation: Epoch [19], Batch [893/938], Loss: 0.3778860867023468\n",
      "Validation: Epoch [19], Batch [894/938], Loss: 0.5024577379226685\n",
      "Validation: Epoch [19], Batch [895/938], Loss: 0.6561564207077026\n",
      "Validation: Epoch [19], Batch [896/938], Loss: 0.5278404951095581\n",
      "Validation: Epoch [19], Batch [897/938], Loss: 0.7604550123214722\n",
      "Validation: Epoch [19], Batch [898/938], Loss: 0.6186581254005432\n",
      "Validation: Epoch [19], Batch [899/938], Loss: 0.7726635932922363\n",
      "Validation: Epoch [19], Batch [900/938], Loss: 0.7058928608894348\n",
      "Validation: Epoch [19], Batch [901/938], Loss: 0.5739789009094238\n",
      "Validation: Epoch [19], Batch [902/938], Loss: 0.35125911235809326\n",
      "Validation: Epoch [19], Batch [903/938], Loss: 0.36313915252685547\n",
      "Validation: Epoch [19], Batch [904/938], Loss: 0.8432016372680664\n",
      "Validation: Epoch [19], Batch [905/938], Loss: 0.47466960549354553\n",
      "Validation: Epoch [19], Batch [906/938], Loss: 0.46793603897094727\n",
      "Validation: Epoch [19], Batch [907/938], Loss: 0.8802316188812256\n",
      "Validation: Epoch [19], Batch [908/938], Loss: 0.62650066614151\n",
      "Validation: Epoch [19], Batch [909/938], Loss: 0.6006865501403809\n",
      "Validation: Epoch [19], Batch [910/938], Loss: 0.5086274147033691\n",
      "Validation: Epoch [19], Batch [911/938], Loss: 0.6537666320800781\n",
      "Validation: Epoch [19], Batch [912/938], Loss: 0.5092790126800537\n",
      "Validation: Epoch [19], Batch [913/938], Loss: 0.8689013123512268\n",
      "Validation: Epoch [19], Batch [914/938], Loss: 0.7491747736930847\n",
      "Validation: Epoch [19], Batch [915/938], Loss: 0.5132721662521362\n",
      "Validation: Epoch [19], Batch [916/938], Loss: 0.619389533996582\n",
      "Validation: Epoch [19], Batch [917/938], Loss: 0.6280633211135864\n",
      "Validation: Epoch [19], Batch [918/938], Loss: 0.7793731689453125\n",
      "Validation: Epoch [19], Batch [919/938], Loss: 0.7317141890525818\n",
      "Validation: Epoch [19], Batch [920/938], Loss: 0.5190688967704773\n",
      "Validation: Epoch [19], Batch [921/938], Loss: 0.8691087365150452\n",
      "Validation: Epoch [19], Batch [922/938], Loss: 0.8293444514274597\n",
      "Validation: Epoch [19], Batch [923/938], Loss: 0.4496016502380371\n",
      "Validation: Epoch [19], Batch [924/938], Loss: 0.6932141184806824\n",
      "Validation: Epoch [19], Batch [925/938], Loss: 0.7095137238502502\n",
      "Validation: Epoch [19], Batch [926/938], Loss: 0.6221034526824951\n",
      "Validation: Epoch [19], Batch [927/938], Loss: 0.43522098660469055\n",
      "Validation: Epoch [19], Batch [928/938], Loss: 0.5448014736175537\n",
      "Validation: Epoch [19], Batch [929/938], Loss: 0.8103560209274292\n",
      "Validation: Epoch [19], Batch [930/938], Loss: 0.5929253101348877\n",
      "Validation: Epoch [19], Batch [931/938], Loss: 0.8315092921257019\n",
      "Validation: Epoch [19], Batch [932/938], Loss: 0.43811437487602234\n",
      "Validation: Epoch [19], Batch [933/938], Loss: 0.6370726227760315\n",
      "Validation: Epoch [19], Batch [934/938], Loss: 0.8157612681388855\n",
      "Validation: Epoch [19], Batch [935/938], Loss: 0.4988864064216614\n",
      "Validation: Epoch [19], Batch [936/938], Loss: 0.5196803212165833\n",
      "Validation: Epoch [19], Batch [937/938], Loss: 0.6480150818824768\n",
      "Validation: Epoch [19], Batch [938/938], Loss: 0.46368446946144104\n",
      "Accuracy of test set: 0.78345\n",
      "Train: Epoch [20], Batch [1/938], Loss: 0.6285356879234314\n",
      "Train: Epoch [20], Batch [2/938], Loss: 0.49554815888404846\n",
      "Train: Epoch [20], Batch [3/938], Loss: 0.5584602952003479\n",
      "Train: Epoch [20], Batch [4/938], Loss: 0.4029717445373535\n",
      "Train: Epoch [20], Batch [5/938], Loss: 0.4919103980064392\n",
      "Train: Epoch [20], Batch [6/938], Loss: 0.5475554466247559\n",
      "Train: Epoch [20], Batch [7/938], Loss: 0.42950862646102905\n",
      "Train: Epoch [20], Batch [8/938], Loss: 0.3582824468612671\n",
      "Train: Epoch [20], Batch [9/938], Loss: 0.5249560475349426\n",
      "Train: Epoch [20], Batch [10/938], Loss: 0.44274112582206726\n",
      "Train: Epoch [20], Batch [11/938], Loss: 0.4953811764717102\n",
      "Train: Epoch [20], Batch [12/938], Loss: 0.4322226345539093\n",
      "Train: Epoch [20], Batch [13/938], Loss: 0.5873435735702515\n",
      "Train: Epoch [20], Batch [14/938], Loss: 0.6676474809646606\n",
      "Train: Epoch [20], Batch [15/938], Loss: 0.5293442010879517\n",
      "Train: Epoch [20], Batch [16/938], Loss: 0.5808226466178894\n",
      "Train: Epoch [20], Batch [17/938], Loss: 0.4253532290458679\n",
      "Train: Epoch [20], Batch [18/938], Loss: 0.4043061137199402\n",
      "Train: Epoch [20], Batch [19/938], Loss: 0.39590510725975037\n",
      "Train: Epoch [20], Batch [20/938], Loss: 0.4791625738143921\n",
      "Train: Epoch [20], Batch [21/938], Loss: 0.5404084324836731\n",
      "Train: Epoch [20], Batch [22/938], Loss: 0.5345194339752197\n",
      "Train: Epoch [20], Batch [23/938], Loss: 0.3481449782848358\n",
      "Train: Epoch [20], Batch [24/938], Loss: 0.3753013610839844\n",
      "Train: Epoch [20], Batch [25/938], Loss: 0.3807913362979889\n",
      "Train: Epoch [20], Batch [26/938], Loss: 0.4017688035964966\n",
      "Train: Epoch [20], Batch [27/938], Loss: 0.4981245994567871\n",
      "Train: Epoch [20], Batch [28/938], Loss: 0.5652394890785217\n",
      "Train: Epoch [20], Batch [29/938], Loss: 0.348328173160553\n",
      "Train: Epoch [20], Batch [30/938], Loss: 0.443123996257782\n",
      "Train: Epoch [20], Batch [31/938], Loss: 0.4078257381916046\n",
      "Train: Epoch [20], Batch [32/938], Loss: 0.4229239225387573\n",
      "Train: Epoch [20], Batch [33/938], Loss: 0.5974772572517395\n",
      "Train: Epoch [20], Batch [34/938], Loss: 0.5699354410171509\n",
      "Train: Epoch [20], Batch [35/938], Loss: 0.4718013107776642\n",
      "Train: Epoch [20], Batch [36/938], Loss: 0.39637672901153564\n",
      "Train: Epoch [20], Batch [37/938], Loss: 0.4044190049171448\n",
      "Train: Epoch [20], Batch [38/938], Loss: 0.36044538021087646\n",
      "Train: Epoch [20], Batch [39/938], Loss: 0.39707136154174805\n",
      "Train: Epoch [20], Batch [40/938], Loss: 0.42336809635162354\n",
      "Train: Epoch [20], Batch [41/938], Loss: 0.7341067790985107\n",
      "Train: Epoch [20], Batch [42/938], Loss: 0.5967788100242615\n",
      "Train: Epoch [20], Batch [43/938], Loss: 0.5900604724884033\n",
      "Train: Epoch [20], Batch [44/938], Loss: 0.42262721061706543\n",
      "Train: Epoch [20], Batch [45/938], Loss: 0.4145113527774811\n",
      "Train: Epoch [20], Batch [46/938], Loss: 0.40339916944503784\n",
      "Train: Epoch [20], Batch [47/938], Loss: 0.42660656571388245\n",
      "Train: Epoch [20], Batch [48/938], Loss: 0.503433346748352\n",
      "Train: Epoch [20], Batch [49/938], Loss: 0.4848634898662567\n",
      "Train: Epoch [20], Batch [50/938], Loss: 0.49761587381362915\n",
      "Train: Epoch [20], Batch [51/938], Loss: 0.3669281005859375\n",
      "Train: Epoch [20], Batch [52/938], Loss: 0.4400182366371155\n",
      "Train: Epoch [20], Batch [53/938], Loss: 0.4759303033351898\n",
      "Train: Epoch [20], Batch [54/938], Loss: 0.3042759895324707\n",
      "Train: Epoch [20], Batch [55/938], Loss: 0.525357186794281\n",
      "Train: Epoch [20], Batch [56/938], Loss: 0.5114129781723022\n",
      "Train: Epoch [20], Batch [57/938], Loss: 0.40526363253593445\n",
      "Train: Epoch [20], Batch [58/938], Loss: 0.9782599806785583\n",
      "Train: Epoch [20], Batch [59/938], Loss: 0.42531919479370117\n",
      "Train: Epoch [20], Batch [60/938], Loss: 0.483643114566803\n",
      "Train: Epoch [20], Batch [61/938], Loss: 0.5179189443588257\n",
      "Train: Epoch [20], Batch [62/938], Loss: 0.6321811079978943\n",
      "Train: Epoch [20], Batch [63/938], Loss: 0.5268329977989197\n",
      "Train: Epoch [20], Batch [64/938], Loss: 0.4542311131954193\n",
      "Train: Epoch [20], Batch [65/938], Loss: 0.5174799561500549\n",
      "Train: Epoch [20], Batch [66/938], Loss: 0.6008734703063965\n",
      "Train: Epoch [20], Batch [67/938], Loss: 0.34833845496177673\n",
      "Train: Epoch [20], Batch [68/938], Loss: 0.3467235267162323\n",
      "Train: Epoch [20], Batch [69/938], Loss: 0.5342669486999512\n",
      "Train: Epoch [20], Batch [70/938], Loss: 0.4249361753463745\n",
      "Train: Epoch [20], Batch [71/938], Loss: 0.7248810529708862\n",
      "Train: Epoch [20], Batch [72/938], Loss: 0.31087633967399597\n",
      "Train: Epoch [20], Batch [73/938], Loss: 0.4449341893196106\n",
      "Train: Epoch [20], Batch [74/938], Loss: 0.4047715365886688\n",
      "Train: Epoch [20], Batch [75/938], Loss: 0.4444772005081177\n",
      "Train: Epoch [20], Batch [76/938], Loss: 0.30281370878219604\n",
      "Train: Epoch [20], Batch [77/938], Loss: 0.3356766402721405\n",
      "Train: Epoch [20], Batch [78/938], Loss: 0.4942217469215393\n",
      "Train: Epoch [20], Batch [79/938], Loss: 0.26929736137390137\n",
      "Train: Epoch [20], Batch [80/938], Loss: 0.4284829795360565\n",
      "Train: Epoch [20], Batch [81/938], Loss: 0.3113863170146942\n",
      "Train: Epoch [20], Batch [82/938], Loss: 0.3108375072479248\n",
      "Train: Epoch [20], Batch [83/938], Loss: 0.6541896462440491\n",
      "Train: Epoch [20], Batch [84/938], Loss: 0.4633552134037018\n",
      "Train: Epoch [20], Batch [85/938], Loss: 0.38829073309898376\n",
      "Train: Epoch [20], Batch [86/938], Loss: 0.4852650761604309\n",
      "Train: Epoch [20], Batch [87/938], Loss: 0.27535828948020935\n",
      "Train: Epoch [20], Batch [88/938], Loss: 0.346994549036026\n",
      "Train: Epoch [20], Batch [89/938], Loss: 0.4373873174190521\n",
      "Train: Epoch [20], Batch [90/938], Loss: 0.5884120464324951\n",
      "Train: Epoch [20], Batch [91/938], Loss: 0.5315842628479004\n",
      "Train: Epoch [20], Batch [92/938], Loss: 0.39643365144729614\n",
      "Train: Epoch [20], Batch [93/938], Loss: 0.48817119002342224\n",
      "Train: Epoch [20], Batch [94/938], Loss: 0.5407639741897583\n",
      "Train: Epoch [20], Batch [95/938], Loss: 0.5273922681808472\n",
      "Train: Epoch [20], Batch [96/938], Loss: 0.3642845153808594\n",
      "Train: Epoch [20], Batch [97/938], Loss: 0.468942791223526\n",
      "Train: Epoch [20], Batch [98/938], Loss: 0.5769681930541992\n",
      "Train: Epoch [20], Batch [99/938], Loss: 0.5188416838645935\n",
      "Train: Epoch [20], Batch [100/938], Loss: 0.3943888247013092\n",
      "Train: Epoch [20], Batch [101/938], Loss: 0.5626347064971924\n",
      "Train: Epoch [20], Batch [102/938], Loss: 0.5570369958877563\n",
      "Train: Epoch [20], Batch [103/938], Loss: 0.37995633482933044\n",
      "Train: Epoch [20], Batch [104/938], Loss: 0.20093494653701782\n",
      "Train: Epoch [20], Batch [105/938], Loss: 0.6361228823661804\n",
      "Train: Epoch [20], Batch [106/938], Loss: 0.3817242383956909\n",
      "Train: Epoch [20], Batch [107/938], Loss: 0.43266457319259644\n",
      "Train: Epoch [20], Batch [108/938], Loss: 0.24380964040756226\n",
      "Train: Epoch [20], Batch [109/938], Loss: 0.489827036857605\n",
      "Train: Epoch [20], Batch [110/938], Loss: 0.45069605112075806\n",
      "Train: Epoch [20], Batch [111/938], Loss: 0.5937106013298035\n",
      "Train: Epoch [20], Batch [112/938], Loss: 0.5594842433929443\n",
      "Train: Epoch [20], Batch [113/938], Loss: 0.6382841467857361\n",
      "Train: Epoch [20], Batch [114/938], Loss: 0.3834032118320465\n",
      "Train: Epoch [20], Batch [115/938], Loss: 0.36744266748428345\n",
      "Train: Epoch [20], Batch [116/938], Loss: 0.48124751448631287\n",
      "Train: Epoch [20], Batch [117/938], Loss: 0.41846930980682373\n",
      "Train: Epoch [20], Batch [118/938], Loss: 0.40989458560943604\n",
      "Train: Epoch [20], Batch [119/938], Loss: 0.6647831797599792\n",
      "Train: Epoch [20], Batch [120/938], Loss: 0.5782539248466492\n",
      "Train: Epoch [20], Batch [121/938], Loss: 0.3309544324874878\n",
      "Train: Epoch [20], Batch [122/938], Loss: 0.5287295579910278\n",
      "Train: Epoch [20], Batch [123/938], Loss: 0.3520563542842865\n",
      "Train: Epoch [20], Batch [124/938], Loss: 0.5257024168968201\n",
      "Train: Epoch [20], Batch [125/938], Loss: 0.6928179860115051\n",
      "Train: Epoch [20], Batch [126/938], Loss: 0.5742883682250977\n",
      "Train: Epoch [20], Batch [127/938], Loss: 0.437391072511673\n",
      "Train: Epoch [20], Batch [128/938], Loss: 0.4622496962547302\n",
      "Train: Epoch [20], Batch [129/938], Loss: 0.8189877867698669\n",
      "Train: Epoch [20], Batch [130/938], Loss: 0.7028429508209229\n",
      "Train: Epoch [20], Batch [131/938], Loss: 0.5197574496269226\n",
      "Train: Epoch [20], Batch [132/938], Loss: 0.49017032980918884\n",
      "Train: Epoch [20], Batch [133/938], Loss: 0.45725545287132263\n",
      "Train: Epoch [20], Batch [134/938], Loss: 0.4669523239135742\n",
      "Train: Epoch [20], Batch [135/938], Loss: 0.4671604037284851\n",
      "Train: Epoch [20], Batch [136/938], Loss: 0.3789333999156952\n",
      "Train: Epoch [20], Batch [137/938], Loss: 0.5689964294433594\n",
      "Train: Epoch [20], Batch [138/938], Loss: 0.5792539715766907\n",
      "Train: Epoch [20], Batch [139/938], Loss: 0.5272350907325745\n",
      "Train: Epoch [20], Batch [140/938], Loss: 0.3386748433113098\n",
      "Train: Epoch [20], Batch [141/938], Loss: 0.537014365196228\n",
      "Train: Epoch [20], Batch [142/938], Loss: 0.3888286352157593\n",
      "Train: Epoch [20], Batch [143/938], Loss: 0.4961163401603699\n",
      "Train: Epoch [20], Batch [144/938], Loss: 0.39356741309165955\n",
      "Train: Epoch [20], Batch [145/938], Loss: 0.5710214972496033\n",
      "Train: Epoch [20], Batch [146/938], Loss: 0.2940085232257843\n",
      "Train: Epoch [20], Batch [147/938], Loss: 0.4647231698036194\n",
      "Train: Epoch [20], Batch [148/938], Loss: 0.2985873818397522\n",
      "Train: Epoch [20], Batch [149/938], Loss: 0.3933655917644501\n",
      "Train: Epoch [20], Batch [150/938], Loss: 0.34373944997787476\n",
      "Train: Epoch [20], Batch [151/938], Loss: 0.4138811230659485\n",
      "Train: Epoch [20], Batch [152/938], Loss: 0.5087032914161682\n",
      "Train: Epoch [20], Batch [153/938], Loss: 0.495845764875412\n",
      "Train: Epoch [20], Batch [154/938], Loss: 0.5074142813682556\n",
      "Train: Epoch [20], Batch [155/938], Loss: 0.7143165469169617\n",
      "Train: Epoch [20], Batch [156/938], Loss: 0.5285451412200928\n",
      "Train: Epoch [20], Batch [157/938], Loss: 0.4980471134185791\n",
      "Train: Epoch [20], Batch [158/938], Loss: 0.604792594909668\n",
      "Train: Epoch [20], Batch [159/938], Loss: 0.4091810882091522\n",
      "Train: Epoch [20], Batch [160/938], Loss: 0.38874319195747375\n",
      "Train: Epoch [20], Batch [161/938], Loss: 0.4916120171546936\n",
      "Train: Epoch [20], Batch [162/938], Loss: 0.5497773885726929\n",
      "Train: Epoch [20], Batch [163/938], Loss: 0.5421197414398193\n",
      "Train: Epoch [20], Batch [164/938], Loss: 0.7234869003295898\n",
      "Train: Epoch [20], Batch [165/938], Loss: 0.4426165223121643\n",
      "Train: Epoch [20], Batch [166/938], Loss: 0.4292464256286621\n",
      "Train: Epoch [20], Batch [167/938], Loss: 0.5171548128128052\n",
      "Train: Epoch [20], Batch [168/938], Loss: 0.6361973285675049\n",
      "Train: Epoch [20], Batch [169/938], Loss: 0.5985962748527527\n",
      "Train: Epoch [20], Batch [170/938], Loss: 0.7582810521125793\n",
      "Train: Epoch [20], Batch [171/938], Loss: 0.5359295010566711\n",
      "Train: Epoch [20], Batch [172/938], Loss: 0.5157955884933472\n",
      "Train: Epoch [20], Batch [173/938], Loss: 0.4832994043827057\n",
      "Train: Epoch [20], Batch [174/938], Loss: 0.491859495639801\n",
      "Train: Epoch [20], Batch [175/938], Loss: 0.6090194582939148\n",
      "Train: Epoch [20], Batch [176/938], Loss: 0.392205148935318\n",
      "Train: Epoch [20], Batch [177/938], Loss: 0.5466541051864624\n",
      "Train: Epoch [20], Batch [178/938], Loss: 0.46653813123703003\n",
      "Train: Epoch [20], Batch [179/938], Loss: 0.432524174451828\n",
      "Train: Epoch [20], Batch [180/938], Loss: 0.311915785074234\n",
      "Train: Epoch [20], Batch [181/938], Loss: 0.5046861171722412\n",
      "Train: Epoch [20], Batch [182/938], Loss: 0.46830880641937256\n",
      "Train: Epoch [20], Batch [183/938], Loss: 0.4279501736164093\n",
      "Train: Epoch [20], Batch [184/938], Loss: 0.49999260902404785\n",
      "Train: Epoch [20], Batch [185/938], Loss: 0.48541873693466187\n",
      "Train: Epoch [20], Batch [186/938], Loss: 0.5116961002349854\n",
      "Train: Epoch [20], Batch [187/938], Loss: 0.48968902230262756\n",
      "Train: Epoch [20], Batch [188/938], Loss: 0.6347635388374329\n",
      "Train: Epoch [20], Batch [189/938], Loss: 0.5008687376976013\n",
      "Train: Epoch [20], Batch [190/938], Loss: 0.5850507020950317\n",
      "Train: Epoch [20], Batch [191/938], Loss: 0.5885951519012451\n",
      "Train: Epoch [20], Batch [192/938], Loss: 0.5776261687278748\n",
      "Train: Epoch [20], Batch [193/938], Loss: 0.3237995505332947\n",
      "Train: Epoch [20], Batch [194/938], Loss: 0.6458190679550171\n",
      "Train: Epoch [20], Batch [195/938], Loss: 0.4229544699192047\n",
      "Train: Epoch [20], Batch [196/938], Loss: 0.37373632192611694\n",
      "Train: Epoch [20], Batch [197/938], Loss: 0.5031466484069824\n",
      "Train: Epoch [20], Batch [198/938], Loss: 0.7585504055023193\n",
      "Train: Epoch [20], Batch [199/938], Loss: 0.2832065522670746\n",
      "Train: Epoch [20], Batch [200/938], Loss: 0.39609038829803467\n",
      "Train: Epoch [20], Batch [201/938], Loss: 0.4398718774318695\n",
      "Train: Epoch [20], Batch [202/938], Loss: 0.4813317656517029\n",
      "Train: Epoch [20], Batch [203/938], Loss: 0.20496580004692078\n",
      "Train: Epoch [20], Batch [204/938], Loss: 0.6186645030975342\n",
      "Train: Epoch [20], Batch [205/938], Loss: 0.5110467672348022\n",
      "Train: Epoch [20], Batch [206/938], Loss: 0.5029321312904358\n",
      "Train: Epoch [20], Batch [207/938], Loss: 0.4282224774360657\n",
      "Train: Epoch [20], Batch [208/938], Loss: 0.43279024958610535\n",
      "Train: Epoch [20], Batch [209/938], Loss: 0.5582373142242432\n",
      "Train: Epoch [20], Batch [210/938], Loss: 0.5466110110282898\n",
      "Train: Epoch [20], Batch [211/938], Loss: 0.7224147915840149\n",
      "Train: Epoch [20], Batch [212/938], Loss: 0.42839315533638\n",
      "Train: Epoch [20], Batch [213/938], Loss: 0.22455456852912903\n",
      "Train: Epoch [20], Batch [214/938], Loss: 0.42583441734313965\n",
      "Train: Epoch [20], Batch [215/938], Loss: 0.3364790976047516\n",
      "Train: Epoch [20], Batch [216/938], Loss: 0.30265510082244873\n",
      "Train: Epoch [20], Batch [217/938], Loss: 0.3716074526309967\n",
      "Train: Epoch [20], Batch [218/938], Loss: 0.37579166889190674\n",
      "Train: Epoch [20], Batch [219/938], Loss: 0.430797815322876\n",
      "Train: Epoch [20], Batch [220/938], Loss: 0.38163483142852783\n",
      "Train: Epoch [20], Batch [221/938], Loss: 0.6741549372673035\n",
      "Train: Epoch [20], Batch [222/938], Loss: 0.47786980867385864\n",
      "Train: Epoch [20], Batch [223/938], Loss: 0.2695002257823944\n",
      "Train: Epoch [20], Batch [224/938], Loss: 0.37792620062828064\n",
      "Train: Epoch [20], Batch [225/938], Loss: 0.47375205159187317\n",
      "Train: Epoch [20], Batch [226/938], Loss: 0.6351187229156494\n",
      "Train: Epoch [20], Batch [227/938], Loss: 0.415473997592926\n",
      "Train: Epoch [20], Batch [228/938], Loss: 0.5837520956993103\n",
      "Train: Epoch [20], Batch [229/938], Loss: 0.5412086248397827\n",
      "Train: Epoch [20], Batch [230/938], Loss: 0.3331799805164337\n",
      "Train: Epoch [20], Batch [231/938], Loss: 0.4715542495250702\n",
      "Train: Epoch [20], Batch [232/938], Loss: 0.4164447784423828\n",
      "Train: Epoch [20], Batch [233/938], Loss: 0.4321196377277374\n",
      "Train: Epoch [20], Batch [234/938], Loss: 0.3591669797897339\n",
      "Train: Epoch [20], Batch [235/938], Loss: 0.6476252675056458\n",
      "Train: Epoch [20], Batch [236/938], Loss: 0.4169076979160309\n",
      "Train: Epoch [20], Batch [237/938], Loss: 0.6400067806243896\n",
      "Train: Epoch [20], Batch [238/938], Loss: 0.4015111029148102\n",
      "Train: Epoch [20], Batch [239/938], Loss: 0.5144925713539124\n",
      "Train: Epoch [20], Batch [240/938], Loss: 0.5518026947975159\n",
      "Train: Epoch [20], Batch [241/938], Loss: 0.3975553810596466\n",
      "Train: Epoch [20], Batch [242/938], Loss: 0.536472499370575\n",
      "Train: Epoch [20], Batch [243/938], Loss: 0.5171341300010681\n",
      "Train: Epoch [20], Batch [244/938], Loss: 0.696959376335144\n",
      "Train: Epoch [20], Batch [245/938], Loss: 0.4287909269332886\n",
      "Train: Epoch [20], Batch [246/938], Loss: 0.599074125289917\n",
      "Train: Epoch [20], Batch [247/938], Loss: 0.4228205978870392\n",
      "Train: Epoch [20], Batch [248/938], Loss: 0.5061883926391602\n",
      "Train: Epoch [20], Batch [249/938], Loss: 0.4775073826313019\n",
      "Train: Epoch [20], Batch [250/938], Loss: 0.3560100793838501\n",
      "Train: Epoch [20], Batch [251/938], Loss: 0.6357948780059814\n",
      "Train: Epoch [20], Batch [252/938], Loss: 0.4613422751426697\n",
      "Train: Epoch [20], Batch [253/938], Loss: 0.732231616973877\n",
      "Train: Epoch [20], Batch [254/938], Loss: 0.5036970973014832\n",
      "Train: Epoch [20], Batch [255/938], Loss: 0.46555566787719727\n",
      "Train: Epoch [20], Batch [256/938], Loss: 0.5547782182693481\n",
      "Train: Epoch [20], Batch [257/938], Loss: 0.43893730640411377\n",
      "Train: Epoch [20], Batch [258/938], Loss: 0.6759328842163086\n",
      "Train: Epoch [20], Batch [259/938], Loss: 0.6114458441734314\n",
      "Train: Epoch [20], Batch [260/938], Loss: 0.7749375104904175\n",
      "Train: Epoch [20], Batch [261/938], Loss: 0.32195568084716797\n",
      "Train: Epoch [20], Batch [262/938], Loss: 0.4841338098049164\n",
      "Train: Epoch [20], Batch [263/938], Loss: 0.3340706527233124\n",
      "Train: Epoch [20], Batch [264/938], Loss: 0.4430272579193115\n",
      "Train: Epoch [20], Batch [265/938], Loss: 0.6117345690727234\n",
      "Train: Epoch [20], Batch [266/938], Loss: 0.4416864216327667\n",
      "Train: Epoch [20], Batch [267/938], Loss: 0.39167484641075134\n",
      "Train: Epoch [20], Batch [268/938], Loss: 0.30738604068756104\n",
      "Train: Epoch [20], Batch [269/938], Loss: 0.3804531395435333\n",
      "Train: Epoch [20], Batch [270/938], Loss: 0.4574919641017914\n",
      "Train: Epoch [20], Batch [271/938], Loss: 0.3662129044532776\n",
      "Train: Epoch [20], Batch [272/938], Loss: 0.3331504762172699\n",
      "Train: Epoch [20], Batch [273/938], Loss: 0.4562316834926605\n",
      "Train: Epoch [20], Batch [274/938], Loss: 0.5848612189292908\n",
      "Train: Epoch [20], Batch [275/938], Loss: 0.3574894368648529\n",
      "Train: Epoch [20], Batch [276/938], Loss: 0.39366650581359863\n",
      "Train: Epoch [20], Batch [277/938], Loss: 0.3961274325847626\n",
      "Train: Epoch [20], Batch [278/938], Loss: 0.7001233696937561\n",
      "Train: Epoch [20], Batch [279/938], Loss: 0.42694586515426636\n",
      "Train: Epoch [20], Batch [280/938], Loss: 0.6572908759117126\n",
      "Train: Epoch [20], Batch [281/938], Loss: 0.3261794149875641\n",
      "Train: Epoch [20], Batch [282/938], Loss: 0.5580276250839233\n",
      "Train: Epoch [20], Batch [283/938], Loss: 0.6036294102668762\n",
      "Train: Epoch [20], Batch [284/938], Loss: 0.3248852491378784\n",
      "Train: Epoch [20], Batch [285/938], Loss: 0.4058159291744232\n",
      "Train: Epoch [20], Batch [286/938], Loss: 0.30084696412086487\n",
      "Train: Epoch [20], Batch [287/938], Loss: 0.5953978300094604\n",
      "Train: Epoch [20], Batch [288/938], Loss: 0.6074854135513306\n",
      "Train: Epoch [20], Batch [289/938], Loss: 0.595632791519165\n",
      "Train: Epoch [20], Batch [290/938], Loss: 0.4348938465118408\n",
      "Train: Epoch [20], Batch [291/938], Loss: 0.5082569122314453\n",
      "Train: Epoch [20], Batch [292/938], Loss: 0.369747519493103\n",
      "Train: Epoch [20], Batch [293/938], Loss: 0.48118361830711365\n",
      "Train: Epoch [20], Batch [294/938], Loss: 0.40508753061294556\n",
      "Train: Epoch [20], Batch [295/938], Loss: 0.803988516330719\n",
      "Train: Epoch [20], Batch [296/938], Loss: 0.36093369126319885\n",
      "Train: Epoch [20], Batch [297/938], Loss: 0.4420924782752991\n",
      "Train: Epoch [20], Batch [298/938], Loss: 0.7068397402763367\n",
      "Train: Epoch [20], Batch [299/938], Loss: 0.4753561019897461\n",
      "Train: Epoch [20], Batch [300/938], Loss: 0.543302595615387\n",
      "Train: Epoch [20], Batch [301/938], Loss: 0.5511581897735596\n",
      "Train: Epoch [20], Batch [302/938], Loss: 0.5611773133277893\n",
      "Train: Epoch [20], Batch [303/938], Loss: 0.3863581120967865\n",
      "Train: Epoch [20], Batch [304/938], Loss: 0.6564574241638184\n",
      "Train: Epoch [20], Batch [305/938], Loss: 0.4473269283771515\n",
      "Train: Epoch [20], Batch [306/938], Loss: 0.5931551456451416\n",
      "Train: Epoch [20], Batch [307/938], Loss: 0.5900499820709229\n",
      "Train: Epoch [20], Batch [308/938], Loss: 0.4871020019054413\n",
      "Train: Epoch [20], Batch [309/938], Loss: 0.5432213544845581\n",
      "Train: Epoch [20], Batch [310/938], Loss: 0.7565553784370422\n",
      "Train: Epoch [20], Batch [311/938], Loss: 0.4452131390571594\n",
      "Train: Epoch [20], Batch [312/938], Loss: 0.41405346989631653\n",
      "Train: Epoch [20], Batch [313/938], Loss: 0.43528175354003906\n",
      "Train: Epoch [20], Batch [314/938], Loss: 0.6646972894668579\n",
      "Train: Epoch [20], Batch [315/938], Loss: 0.5019612908363342\n",
      "Train: Epoch [20], Batch [316/938], Loss: 0.49242687225341797\n",
      "Train: Epoch [20], Batch [317/938], Loss: 0.4685482680797577\n",
      "Train: Epoch [20], Batch [318/938], Loss: 0.49130210280418396\n",
      "Train: Epoch [20], Batch [319/938], Loss: 0.6376029253005981\n",
      "Train: Epoch [20], Batch [320/938], Loss: 0.41497987508773804\n",
      "Train: Epoch [20], Batch [321/938], Loss: 0.6568871140480042\n",
      "Train: Epoch [20], Batch [322/938], Loss: 0.6041938066482544\n",
      "Train: Epoch [20], Batch [323/938], Loss: 0.4697027802467346\n",
      "Train: Epoch [20], Batch [324/938], Loss: 0.5971510410308838\n",
      "Train: Epoch [20], Batch [325/938], Loss: 0.4186672568321228\n",
      "Train: Epoch [20], Batch [326/938], Loss: 0.4234549105167389\n",
      "Train: Epoch [20], Batch [327/938], Loss: 0.5134037137031555\n",
      "Train: Epoch [20], Batch [328/938], Loss: 0.4573083817958832\n",
      "Train: Epoch [20], Batch [329/938], Loss: 0.5673555135726929\n",
      "Train: Epoch [20], Batch [330/938], Loss: 0.4724363386631012\n",
      "Train: Epoch [20], Batch [331/938], Loss: 0.6488609910011292\n",
      "Train: Epoch [20], Batch [332/938], Loss: 0.34886181354522705\n",
      "Train: Epoch [20], Batch [333/938], Loss: 0.7105017900466919\n",
      "Train: Epoch [20], Batch [334/938], Loss: 0.3572912812232971\n",
      "Train: Epoch [20], Batch [335/938], Loss: 0.6530869007110596\n",
      "Train: Epoch [20], Batch [336/938], Loss: 0.5528490543365479\n",
      "Train: Epoch [20], Batch [337/938], Loss: 0.4136081635951996\n",
      "Train: Epoch [20], Batch [338/938], Loss: 0.3596976101398468\n",
      "Train: Epoch [20], Batch [339/938], Loss: 0.4637686014175415\n",
      "Train: Epoch [20], Batch [340/938], Loss: 0.6288667917251587\n",
      "Train: Epoch [20], Batch [341/938], Loss: 0.5492846369743347\n",
      "Train: Epoch [20], Batch [342/938], Loss: 0.6504181027412415\n",
      "Train: Epoch [20], Batch [343/938], Loss: 0.5161001682281494\n",
      "Train: Epoch [20], Batch [344/938], Loss: 0.48427918553352356\n",
      "Train: Epoch [20], Batch [345/938], Loss: 0.5555431246757507\n",
      "Train: Epoch [20], Batch [346/938], Loss: 0.42994359135627747\n",
      "Train: Epoch [20], Batch [347/938], Loss: 0.4448959231376648\n",
      "Train: Epoch [20], Batch [348/938], Loss: 0.3620426058769226\n",
      "Train: Epoch [20], Batch [349/938], Loss: 0.4586491584777832\n",
      "Train: Epoch [20], Batch [350/938], Loss: 0.5122209787368774\n",
      "Train: Epoch [20], Batch [351/938], Loss: 0.41169875860214233\n",
      "Train: Epoch [20], Batch [352/938], Loss: 0.3351649045944214\n",
      "Train: Epoch [20], Batch [353/938], Loss: 0.38565391302108765\n",
      "Train: Epoch [20], Batch [354/938], Loss: 0.44921162724494934\n",
      "Train: Epoch [20], Batch [355/938], Loss: 0.6766120195388794\n",
      "Train: Epoch [20], Batch [356/938], Loss: 0.9005513787269592\n",
      "Train: Epoch [20], Batch [357/938], Loss: 0.4051230549812317\n",
      "Train: Epoch [20], Batch [358/938], Loss: 0.609555184841156\n",
      "Train: Epoch [20], Batch [359/938], Loss: 0.5232025384902954\n",
      "Train: Epoch [20], Batch [360/938], Loss: 0.5928991436958313\n",
      "Train: Epoch [20], Batch [361/938], Loss: 0.41274213790893555\n",
      "Train: Epoch [20], Batch [362/938], Loss: 0.608515739440918\n",
      "Train: Epoch [20], Batch [363/938], Loss: 0.39302515983581543\n",
      "Train: Epoch [20], Batch [364/938], Loss: 0.42015981674194336\n",
      "Train: Epoch [20], Batch [365/938], Loss: 0.6247497200965881\n",
      "Train: Epoch [20], Batch [366/938], Loss: 0.630733847618103\n",
      "Train: Epoch [20], Batch [367/938], Loss: 0.5103868246078491\n",
      "Train: Epoch [20], Batch [368/938], Loss: 0.5687806010246277\n",
      "Train: Epoch [20], Batch [369/938], Loss: 0.5744369626045227\n",
      "Train: Epoch [20], Batch [370/938], Loss: 0.4377129077911377\n",
      "Train: Epoch [20], Batch [371/938], Loss: 0.6480075120925903\n",
      "Train: Epoch [20], Batch [372/938], Loss: 0.37473997473716736\n",
      "Train: Epoch [20], Batch [373/938], Loss: 0.4505852460861206\n",
      "Train: Epoch [20], Batch [374/938], Loss: 0.58202064037323\n",
      "Train: Epoch [20], Batch [375/938], Loss: 0.47160404920578003\n",
      "Train: Epoch [20], Batch [376/938], Loss: 0.5635576248168945\n",
      "Train: Epoch [20], Batch [377/938], Loss: 0.4150860905647278\n",
      "Train: Epoch [20], Batch [378/938], Loss: 0.45093488693237305\n",
      "Train: Epoch [20], Batch [379/938], Loss: 0.45253655314445496\n",
      "Train: Epoch [20], Batch [380/938], Loss: 0.4040965437889099\n",
      "Train: Epoch [20], Batch [381/938], Loss: 0.5123888254165649\n",
      "Train: Epoch [20], Batch [382/938], Loss: 0.39836227893829346\n",
      "Train: Epoch [20], Batch [383/938], Loss: 0.46394139528274536\n",
      "Train: Epoch [20], Batch [384/938], Loss: 0.2580484449863434\n",
      "Train: Epoch [20], Batch [385/938], Loss: 0.4060797393321991\n",
      "Train: Epoch [20], Batch [386/938], Loss: 0.4572604298591614\n",
      "Train: Epoch [20], Batch [387/938], Loss: 0.47163787484169006\n",
      "Train: Epoch [20], Batch [388/938], Loss: 0.4030191898345947\n",
      "Train: Epoch [20], Batch [389/938], Loss: 0.5108954310417175\n",
      "Train: Epoch [20], Batch [390/938], Loss: 0.31772664189338684\n",
      "Train: Epoch [20], Batch [391/938], Loss: 0.3960433006286621\n",
      "Train: Epoch [20], Batch [392/938], Loss: 0.4258720576763153\n",
      "Train: Epoch [20], Batch [393/938], Loss: 0.4125409424304962\n",
      "Train: Epoch [20], Batch [394/938], Loss: 0.4080136716365814\n",
      "Train: Epoch [20], Batch [395/938], Loss: 0.3560379147529602\n",
      "Train: Epoch [20], Batch [396/938], Loss: 0.6133544445037842\n",
      "Train: Epoch [20], Batch [397/938], Loss: 0.3166946768760681\n",
      "Train: Epoch [20], Batch [398/938], Loss: 0.5716995000839233\n",
      "Train: Epoch [20], Batch [399/938], Loss: 0.45135802030563354\n",
      "Train: Epoch [20], Batch [400/938], Loss: 0.5617698431015015\n",
      "Train: Epoch [20], Batch [401/938], Loss: 0.5621299147605896\n",
      "Train: Epoch [20], Batch [402/938], Loss: 0.6295230388641357\n",
      "Train: Epoch [20], Batch [403/938], Loss: 0.5798592567443848\n",
      "Train: Epoch [20], Batch [404/938], Loss: 0.4311230480670929\n",
      "Train: Epoch [20], Batch [405/938], Loss: 0.32685455679893494\n",
      "Train: Epoch [20], Batch [406/938], Loss: 0.323905885219574\n",
      "Train: Epoch [20], Batch [407/938], Loss: 0.6553103923797607\n",
      "Train: Epoch [20], Batch [408/938], Loss: 0.41807615756988525\n",
      "Train: Epoch [20], Batch [409/938], Loss: 0.44125819206237793\n",
      "Train: Epoch [20], Batch [410/938], Loss: 0.567237913608551\n",
      "Train: Epoch [20], Batch [411/938], Loss: 0.34782183170318604\n",
      "Train: Epoch [20], Batch [412/938], Loss: 0.3757963180541992\n",
      "Train: Epoch [20], Batch [413/938], Loss: 0.38847142457962036\n",
      "Train: Epoch [20], Batch [414/938], Loss: 0.5641827583312988\n",
      "Train: Epoch [20], Batch [415/938], Loss: 0.4160025119781494\n",
      "Train: Epoch [20], Batch [416/938], Loss: 0.514845609664917\n",
      "Train: Epoch [20], Batch [417/938], Loss: 0.6194654703140259\n",
      "Train: Epoch [20], Batch [418/938], Loss: 0.6919954419136047\n",
      "Train: Epoch [20], Batch [419/938], Loss: 0.24733641743659973\n",
      "Train: Epoch [20], Batch [420/938], Loss: 0.4540729820728302\n",
      "Train: Epoch [20], Batch [421/938], Loss: 0.452374666929245\n",
      "Train: Epoch [20], Batch [422/938], Loss: 0.5812264680862427\n",
      "Train: Epoch [20], Batch [423/938], Loss: 0.28835704922676086\n",
      "Train: Epoch [20], Batch [424/938], Loss: 0.509659469127655\n",
      "Train: Epoch [20], Batch [425/938], Loss: 0.3503677546977997\n",
      "Train: Epoch [20], Batch [426/938], Loss: 0.36551159620285034\n",
      "Train: Epoch [20], Batch [427/938], Loss: 0.3442629873752594\n",
      "Train: Epoch [20], Batch [428/938], Loss: 0.4653288722038269\n",
      "Train: Epoch [20], Batch [429/938], Loss: 0.4084870517253876\n",
      "Train: Epoch [20], Batch [430/938], Loss: 0.5446452498435974\n",
      "Train: Epoch [20], Batch [431/938], Loss: 0.6955671906471252\n",
      "Train: Epoch [20], Batch [432/938], Loss: 0.5208742022514343\n",
      "Train: Epoch [20], Batch [433/938], Loss: 0.2871941030025482\n",
      "Train: Epoch [20], Batch [434/938], Loss: 0.5430777668952942\n",
      "Train: Epoch [20], Batch [435/938], Loss: 0.6532787680625916\n",
      "Train: Epoch [20], Batch [436/938], Loss: 0.3435889780521393\n",
      "Train: Epoch [20], Batch [437/938], Loss: 0.4102095067501068\n",
      "Train: Epoch [20], Batch [438/938], Loss: 0.6234903931617737\n",
      "Train: Epoch [20], Batch [439/938], Loss: 0.3461472988128662\n",
      "Train: Epoch [20], Batch [440/938], Loss: 0.584269106388092\n",
      "Train: Epoch [20], Batch [441/938], Loss: 0.3311413824558258\n",
      "Train: Epoch [20], Batch [442/938], Loss: 0.49896180629730225\n",
      "Train: Epoch [20], Batch [443/938], Loss: 0.7011799812316895\n",
      "Train: Epoch [20], Batch [444/938], Loss: 0.4226536750793457\n",
      "Train: Epoch [20], Batch [445/938], Loss: 0.6043229103088379\n",
      "Train: Epoch [20], Batch [446/938], Loss: 0.3757614195346832\n",
      "Train: Epoch [20], Batch [447/938], Loss: 0.638388454914093\n",
      "Train: Epoch [20], Batch [448/938], Loss: 0.4980762302875519\n",
      "Train: Epoch [20], Batch [449/938], Loss: 0.7757858037948608\n",
      "Train: Epoch [20], Batch [450/938], Loss: 0.5559932589530945\n",
      "Train: Epoch [20], Batch [451/938], Loss: 0.4503060579299927\n",
      "Train: Epoch [20], Batch [452/938], Loss: 0.4885406494140625\n",
      "Train: Epoch [20], Batch [453/938], Loss: 0.5189028382301331\n",
      "Train: Epoch [20], Batch [454/938], Loss: 0.5042535662651062\n",
      "Train: Epoch [20], Batch [455/938], Loss: 0.37036949396133423\n",
      "Train: Epoch [20], Batch [456/938], Loss: 0.535628080368042\n",
      "Train: Epoch [20], Batch [457/938], Loss: 0.6448652744293213\n",
      "Train: Epoch [20], Batch [458/938], Loss: 0.30013012886047363\n",
      "Train: Epoch [20], Batch [459/938], Loss: 0.33763378858566284\n",
      "Train: Epoch [20], Batch [460/938], Loss: 0.6086758971214294\n",
      "Train: Epoch [20], Batch [461/938], Loss: 0.5966141223907471\n",
      "Train: Epoch [20], Batch [462/938], Loss: 0.29338738322257996\n",
      "Train: Epoch [20], Batch [463/938], Loss: 0.47614502906799316\n",
      "Train: Epoch [20], Batch [464/938], Loss: 0.6565305590629578\n",
      "Train: Epoch [20], Batch [465/938], Loss: 0.43632903695106506\n",
      "Train: Epoch [20], Batch [466/938], Loss: 0.6635461449623108\n",
      "Train: Epoch [20], Batch [467/938], Loss: 0.3537995517253876\n",
      "Train: Epoch [20], Batch [468/938], Loss: 0.34042200446128845\n",
      "Train: Epoch [20], Batch [469/938], Loss: 0.5104771852493286\n",
      "Train: Epoch [20], Batch [470/938], Loss: 0.30856725573539734\n",
      "Train: Epoch [20], Batch [471/938], Loss: 0.405496746301651\n",
      "Train: Epoch [20], Batch [472/938], Loss: 0.6293845772743225\n",
      "Train: Epoch [20], Batch [473/938], Loss: 0.5264136791229248\n",
      "Train: Epoch [20], Batch [474/938], Loss: 0.36913105845451355\n",
      "Train: Epoch [20], Batch [475/938], Loss: 0.38329875469207764\n",
      "Train: Epoch [20], Batch [476/938], Loss: 0.3479248881340027\n",
      "Train: Epoch [20], Batch [477/938], Loss: 0.4472888708114624\n",
      "Train: Epoch [20], Batch [478/938], Loss: 0.5286949872970581\n",
      "Train: Epoch [20], Batch [479/938], Loss: 0.3193308711051941\n",
      "Train: Epoch [20], Batch [480/938], Loss: 0.3543580174446106\n",
      "Train: Epoch [20], Batch [481/938], Loss: 0.4462739825248718\n",
      "Train: Epoch [20], Batch [482/938], Loss: 0.34146562218666077\n",
      "Train: Epoch [20], Batch [483/938], Loss: 0.46099916100502014\n",
      "Train: Epoch [20], Batch [484/938], Loss: 0.3747207224369049\n",
      "Train: Epoch [20], Batch [485/938], Loss: 0.35905835032463074\n",
      "Train: Epoch [20], Batch [486/938], Loss: 0.7436994910240173\n",
      "Train: Epoch [20], Batch [487/938], Loss: 0.4378834366798401\n",
      "Train: Epoch [20], Batch [488/938], Loss: 0.42453882098197937\n",
      "Train: Epoch [20], Batch [489/938], Loss: 0.3098401129245758\n",
      "Train: Epoch [20], Batch [490/938], Loss: 0.47522562742233276\n",
      "Train: Epoch [20], Batch [491/938], Loss: 0.45673924684524536\n",
      "Train: Epoch [20], Batch [492/938], Loss: 0.32994070649147034\n",
      "Train: Epoch [20], Batch [493/938], Loss: 0.44733405113220215\n",
      "Train: Epoch [20], Batch [494/938], Loss: 0.2701801061630249\n",
      "Train: Epoch [20], Batch [495/938], Loss: 0.44152531027793884\n",
      "Train: Epoch [20], Batch [496/938], Loss: 0.3572043776512146\n",
      "Train: Epoch [20], Batch [497/938], Loss: 0.425896018743515\n",
      "Train: Epoch [20], Batch [498/938], Loss: 0.47786077857017517\n",
      "Train: Epoch [20], Batch [499/938], Loss: 0.5134493708610535\n",
      "Train: Epoch [20], Batch [500/938], Loss: 0.3584156036376953\n",
      "Train: Epoch [20], Batch [501/938], Loss: 0.5254104733467102\n",
      "Train: Epoch [20], Batch [502/938], Loss: 0.44594255089759827\n",
      "Train: Epoch [20], Batch [503/938], Loss: 0.34649837017059326\n",
      "Train: Epoch [20], Batch [504/938], Loss: 0.5756627321243286\n",
      "Train: Epoch [20], Batch [505/938], Loss: 0.6289889216423035\n",
      "Train: Epoch [20], Batch [506/938], Loss: 0.30469438433647156\n",
      "Train: Epoch [20], Batch [507/938], Loss: 0.397331565618515\n",
      "Train: Epoch [20], Batch [508/938], Loss: 0.36503246426582336\n",
      "Train: Epoch [20], Batch [509/938], Loss: 0.5339832305908203\n",
      "Train: Epoch [20], Batch [510/938], Loss: 0.6530030965805054\n",
      "Train: Epoch [20], Batch [511/938], Loss: 0.49059200286865234\n",
      "Train: Epoch [20], Batch [512/938], Loss: 0.37832096219062805\n",
      "Train: Epoch [20], Batch [513/938], Loss: 0.39410901069641113\n",
      "Train: Epoch [20], Batch [514/938], Loss: 0.33603635430336\n",
      "Train: Epoch [20], Batch [515/938], Loss: 0.40861642360687256\n",
      "Train: Epoch [20], Batch [516/938], Loss: 0.4663744270801544\n",
      "Train: Epoch [20], Batch [517/938], Loss: 0.4469648003578186\n",
      "Train: Epoch [20], Batch [518/938], Loss: 0.5509058237075806\n",
      "Train: Epoch [20], Batch [519/938], Loss: 0.5090187788009644\n",
      "Train: Epoch [20], Batch [520/938], Loss: 0.24447599053382874\n",
      "Train: Epoch [20], Batch [521/938], Loss: 0.6179629564285278\n",
      "Train: Epoch [20], Batch [522/938], Loss: 0.5028606653213501\n",
      "Train: Epoch [20], Batch [523/938], Loss: 0.648734986782074\n",
      "Train: Epoch [20], Batch [524/938], Loss: 0.5626260042190552\n",
      "Train: Epoch [20], Batch [525/938], Loss: 0.5527607202529907\n",
      "Train: Epoch [20], Batch [526/938], Loss: 0.6273316144943237\n",
      "Train: Epoch [20], Batch [527/938], Loss: 0.6214030981063843\n",
      "Train: Epoch [20], Batch [528/938], Loss: 0.4463008940219879\n",
      "Train: Epoch [20], Batch [529/938], Loss: 0.6279898881912231\n",
      "Train: Epoch [20], Batch [530/938], Loss: 0.4637661576271057\n",
      "Train: Epoch [20], Batch [531/938], Loss: 0.471699595451355\n",
      "Train: Epoch [20], Batch [532/938], Loss: 0.5808840990066528\n",
      "Train: Epoch [20], Batch [533/938], Loss: 0.6345720887184143\n",
      "Train: Epoch [20], Batch [534/938], Loss: 0.5518617630004883\n",
      "Train: Epoch [20], Batch [535/938], Loss: 0.4458184540271759\n",
      "Train: Epoch [20], Batch [536/938], Loss: 0.3530116081237793\n",
      "Train: Epoch [20], Batch [537/938], Loss: 0.5109513998031616\n",
      "Train: Epoch [20], Batch [538/938], Loss: 0.3919086754322052\n",
      "Train: Epoch [20], Batch [539/938], Loss: 0.5554625988006592\n",
      "Train: Epoch [20], Batch [540/938], Loss: 0.6591822504997253\n",
      "Train: Epoch [20], Batch [541/938], Loss: 0.4324032664299011\n",
      "Train: Epoch [20], Batch [542/938], Loss: 0.5987803936004639\n",
      "Train: Epoch [20], Batch [543/938], Loss: 0.3209990859031677\n",
      "Train: Epoch [20], Batch [544/938], Loss: 0.3251985013484955\n",
      "Train: Epoch [20], Batch [545/938], Loss: 0.64571613073349\n",
      "Train: Epoch [20], Batch [546/938], Loss: 0.4749412536621094\n",
      "Train: Epoch [20], Batch [547/938], Loss: 0.49279364943504333\n",
      "Train: Epoch [20], Batch [548/938], Loss: 0.46621066331863403\n",
      "Train: Epoch [20], Batch [549/938], Loss: 0.4033201336860657\n",
      "Train: Epoch [20], Batch [550/938], Loss: 0.5447179675102234\n",
      "Train: Epoch [20], Batch [551/938], Loss: 0.431741327047348\n",
      "Train: Epoch [20], Batch [552/938], Loss: 0.39211559295654297\n",
      "Train: Epoch [20], Batch [553/938], Loss: 0.24464461207389832\n",
      "Train: Epoch [20], Batch [554/938], Loss: 0.33058440685272217\n",
      "Train: Epoch [20], Batch [555/938], Loss: 0.436478853225708\n",
      "Train: Epoch [20], Batch [556/938], Loss: 0.6808353066444397\n",
      "Train: Epoch [20], Batch [557/938], Loss: 0.3629535436630249\n",
      "Train: Epoch [20], Batch [558/938], Loss: 0.4323357045650482\n",
      "Train: Epoch [20], Batch [559/938], Loss: 0.6450709700584412\n",
      "Train: Epoch [20], Batch [560/938], Loss: 0.39936450123786926\n",
      "Train: Epoch [20], Batch [561/938], Loss: 0.37476611137390137\n",
      "Train: Epoch [20], Batch [562/938], Loss: 0.37015247344970703\n",
      "Train: Epoch [20], Batch [563/938], Loss: 0.5331805944442749\n",
      "Train: Epoch [20], Batch [564/938], Loss: 0.2649967670440674\n",
      "Train: Epoch [20], Batch [565/938], Loss: 0.2872958183288574\n",
      "Train: Epoch [20], Batch [566/938], Loss: 0.4556174874305725\n",
      "Train: Epoch [20], Batch [567/938], Loss: 0.4000949561595917\n",
      "Train: Epoch [20], Batch [568/938], Loss: 0.3749678134918213\n",
      "Train: Epoch [20], Batch [569/938], Loss: 0.3321913778781891\n",
      "Train: Epoch [20], Batch [570/938], Loss: 0.3438814878463745\n",
      "Train: Epoch [20], Batch [571/938], Loss: 0.34146493673324585\n",
      "Train: Epoch [20], Batch [572/938], Loss: 0.48609334230422974\n",
      "Train: Epoch [20], Batch [573/938], Loss: 0.43611884117126465\n",
      "Train: Epoch [20], Batch [574/938], Loss: 0.40704596042633057\n",
      "Train: Epoch [20], Batch [575/938], Loss: 0.2737172544002533\n",
      "Train: Epoch [20], Batch [576/938], Loss: 0.5141525268554688\n",
      "Train: Epoch [20], Batch [577/938], Loss: 0.42102929949760437\n",
      "Train: Epoch [20], Batch [578/938], Loss: 0.4411855936050415\n",
      "Train: Epoch [20], Batch [579/938], Loss: 0.3278505206108093\n",
      "Train: Epoch [20], Batch [580/938], Loss: 0.31012752652168274\n",
      "Train: Epoch [20], Batch [581/938], Loss: 0.37099316716194153\n",
      "Train: Epoch [20], Batch [582/938], Loss: 0.4847760498523712\n",
      "Train: Epoch [20], Batch [583/938], Loss: 0.3229078948497772\n",
      "Train: Epoch [20], Batch [584/938], Loss: 0.5857558250427246\n",
      "Train: Epoch [20], Batch [585/938], Loss: 0.5218626260757446\n",
      "Train: Epoch [20], Batch [586/938], Loss: 0.5352420210838318\n",
      "Train: Epoch [20], Batch [587/938], Loss: 0.5604127049446106\n",
      "Train: Epoch [20], Batch [588/938], Loss: 0.5776395797729492\n",
      "Train: Epoch [20], Batch [589/938], Loss: 0.39673352241516113\n",
      "Train: Epoch [20], Batch [590/938], Loss: 0.4627223014831543\n",
      "Train: Epoch [20], Batch [591/938], Loss: 0.5213862061500549\n",
      "Train: Epoch [20], Batch [592/938], Loss: 0.4980446696281433\n",
      "Train: Epoch [20], Batch [593/938], Loss: 0.316609650850296\n",
      "Train: Epoch [20], Batch [594/938], Loss: 0.4387120306491852\n",
      "Train: Epoch [20], Batch [595/938], Loss: 0.501709520816803\n",
      "Train: Epoch [20], Batch [596/938], Loss: 0.5345492959022522\n",
      "Train: Epoch [20], Batch [597/938], Loss: 0.4719275236129761\n",
      "Train: Epoch [20], Batch [598/938], Loss: 0.48954397439956665\n",
      "Train: Epoch [20], Batch [599/938], Loss: 0.616619348526001\n",
      "Train: Epoch [20], Batch [600/938], Loss: 0.5833960771560669\n",
      "Train: Epoch [20], Batch [601/938], Loss: 0.6499693989753723\n",
      "Train: Epoch [20], Batch [602/938], Loss: 0.44550976157188416\n",
      "Train: Epoch [20], Batch [603/938], Loss: 0.5481586456298828\n",
      "Train: Epoch [20], Batch [604/938], Loss: 0.4691041111946106\n",
      "Train: Epoch [20], Batch [605/938], Loss: 0.5259405970573425\n",
      "Train: Epoch [20], Batch [606/938], Loss: 0.6169797778129578\n",
      "Train: Epoch [20], Batch [607/938], Loss: 0.4734099507331848\n",
      "Train: Epoch [20], Batch [608/938], Loss: 0.3296220004558563\n",
      "Train: Epoch [20], Batch [609/938], Loss: 0.5048894882202148\n",
      "Train: Epoch [20], Batch [610/938], Loss: 0.39885300397872925\n",
      "Train: Epoch [20], Batch [611/938], Loss: 0.4403694272041321\n",
      "Train: Epoch [20], Batch [612/938], Loss: 0.3033742904663086\n",
      "Train: Epoch [20], Batch [613/938], Loss: 0.41366177797317505\n",
      "Train: Epoch [20], Batch [614/938], Loss: 0.49071282148361206\n",
      "Train: Epoch [20], Batch [615/938], Loss: 0.5189417004585266\n",
      "Train: Epoch [20], Batch [616/938], Loss: 0.40626081824302673\n",
      "Train: Epoch [20], Batch [617/938], Loss: 0.31580212712287903\n",
      "Train: Epoch [20], Batch [618/938], Loss: 0.5978659391403198\n",
      "Train: Epoch [20], Batch [619/938], Loss: 0.3767343759536743\n",
      "Train: Epoch [20], Batch [620/938], Loss: 0.34702369570732117\n",
      "Train: Epoch [20], Batch [621/938], Loss: 0.42992398142814636\n",
      "Train: Epoch [20], Batch [622/938], Loss: 0.3676687180995941\n",
      "Train: Epoch [20], Batch [623/938], Loss: 0.49460986256599426\n",
      "Train: Epoch [20], Batch [624/938], Loss: 0.32346582412719727\n",
      "Train: Epoch [20], Batch [625/938], Loss: 0.5060218572616577\n",
      "Train: Epoch [20], Batch [626/938], Loss: 0.4868095815181732\n",
      "Train: Epoch [20], Batch [627/938], Loss: 0.586322546005249\n",
      "Train: Epoch [20], Batch [628/938], Loss: 0.3940154016017914\n",
      "Train: Epoch [20], Batch [629/938], Loss: 0.38444942235946655\n",
      "Train: Epoch [20], Batch [630/938], Loss: 0.43316560983657837\n",
      "Train: Epoch [20], Batch [631/938], Loss: 0.20385366678237915\n",
      "Train: Epoch [20], Batch [632/938], Loss: 0.4790111184120178\n",
      "Train: Epoch [20], Batch [633/938], Loss: 0.4331303536891937\n",
      "Train: Epoch [20], Batch [634/938], Loss: 0.5543921589851379\n",
      "Train: Epoch [20], Batch [635/938], Loss: 0.5492272973060608\n",
      "Train: Epoch [20], Batch [636/938], Loss: 0.5106435418128967\n",
      "Train: Epoch [20], Batch [637/938], Loss: 0.4502716660499573\n",
      "Train: Epoch [20], Batch [638/938], Loss: 0.5174544453620911\n",
      "Train: Epoch [20], Batch [639/938], Loss: 0.4835078716278076\n",
      "Train: Epoch [20], Batch [640/938], Loss: 0.5604224801063538\n",
      "Train: Epoch [20], Batch [641/938], Loss: 0.42284053564071655\n",
      "Train: Epoch [20], Batch [642/938], Loss: 0.39220237731933594\n",
      "Train: Epoch [20], Batch [643/938], Loss: 0.4596269726753235\n",
      "Train: Epoch [20], Batch [644/938], Loss: 0.4139552116394043\n",
      "Train: Epoch [20], Batch [645/938], Loss: 0.6144156455993652\n",
      "Train: Epoch [20], Batch [646/938], Loss: 0.3691326975822449\n",
      "Train: Epoch [20], Batch [647/938], Loss: 0.7147285342216492\n",
      "Train: Epoch [20], Batch [648/938], Loss: 0.4846511781215668\n",
      "Train: Epoch [20], Batch [649/938], Loss: 0.24950021505355835\n",
      "Train: Epoch [20], Batch [650/938], Loss: 0.45749956369400024\n",
      "Train: Epoch [20], Batch [651/938], Loss: 0.6761555671691895\n",
      "Train: Epoch [20], Batch [652/938], Loss: 0.3854202330112457\n",
      "Train: Epoch [20], Batch [653/938], Loss: 0.6220259666442871\n",
      "Train: Epoch [20], Batch [654/938], Loss: 0.3272438049316406\n",
      "Train: Epoch [20], Batch [655/938], Loss: 0.5054528713226318\n",
      "Train: Epoch [20], Batch [656/938], Loss: 0.48701775074005127\n",
      "Train: Epoch [20], Batch [657/938], Loss: 0.5625713467597961\n",
      "Train: Epoch [20], Batch [658/938], Loss: 0.5885844826698303\n",
      "Train: Epoch [20], Batch [659/938], Loss: 0.4789983332157135\n",
      "Train: Epoch [20], Batch [660/938], Loss: 0.36719101667404175\n",
      "Train: Epoch [20], Batch [661/938], Loss: 0.5311184525489807\n",
      "Train: Epoch [20], Batch [662/938], Loss: 0.39140886068344116\n",
      "Train: Epoch [20], Batch [663/938], Loss: 0.4269565939903259\n",
      "Train: Epoch [20], Batch [664/938], Loss: 0.5679290294647217\n",
      "Train: Epoch [20], Batch [665/938], Loss: 0.4923657774925232\n",
      "Train: Epoch [20], Batch [666/938], Loss: 0.36761340498924255\n",
      "Train: Epoch [20], Batch [667/938], Loss: 0.4271535873413086\n",
      "Train: Epoch [20], Batch [668/938], Loss: 0.36108359694480896\n",
      "Train: Epoch [20], Batch [669/938], Loss: 0.45307791233062744\n",
      "Train: Epoch [20], Batch [670/938], Loss: 0.40304744243621826\n",
      "Train: Epoch [20], Batch [671/938], Loss: 0.36965009570121765\n",
      "Train: Epoch [20], Batch [672/938], Loss: 0.5536249279975891\n",
      "Train: Epoch [20], Batch [673/938], Loss: 0.40912309288978577\n",
      "Train: Epoch [20], Batch [674/938], Loss: 0.39885061979293823\n",
      "Train: Epoch [20], Batch [675/938], Loss: 0.5378704071044922\n",
      "Train: Epoch [20], Batch [676/938], Loss: 0.4045376777648926\n",
      "Train: Epoch [20], Batch [677/938], Loss: 0.5105436444282532\n",
      "Train: Epoch [20], Batch [678/938], Loss: 0.47776782512664795\n",
      "Train: Epoch [20], Batch [679/938], Loss: 0.8060554265975952\n",
      "Train: Epoch [20], Batch [680/938], Loss: 0.5944581031799316\n",
      "Train: Epoch [20], Batch [681/938], Loss: 0.32284313440322876\n",
      "Train: Epoch [20], Batch [682/938], Loss: 0.5522512793540955\n",
      "Train: Epoch [20], Batch [683/938], Loss: 0.38005492091178894\n",
      "Train: Epoch [20], Batch [684/938], Loss: 0.4192717671394348\n",
      "Train: Epoch [20], Batch [685/938], Loss: 0.5333371162414551\n",
      "Train: Epoch [20], Batch [686/938], Loss: 0.6634302139282227\n",
      "Train: Epoch [20], Batch [687/938], Loss: 0.4357982575893402\n",
      "Train: Epoch [20], Batch [688/938], Loss: 0.5739309787750244\n",
      "Train: Epoch [20], Batch [689/938], Loss: 0.41141781210899353\n",
      "Train: Epoch [20], Batch [690/938], Loss: 0.42667707800865173\n",
      "Train: Epoch [20], Batch [691/938], Loss: 0.5067459940910339\n",
      "Train: Epoch [20], Batch [692/938], Loss: 0.4549224376678467\n",
      "Train: Epoch [20], Batch [693/938], Loss: 0.4412940740585327\n",
      "Train: Epoch [20], Batch [694/938], Loss: 0.34279537200927734\n",
      "Train: Epoch [20], Batch [695/938], Loss: 0.3066868185997009\n",
      "Train: Epoch [20], Batch [696/938], Loss: 0.25539571046829224\n",
      "Train: Epoch [20], Batch [697/938], Loss: 0.6144706606864929\n",
      "Train: Epoch [20], Batch [698/938], Loss: 0.378167062997818\n",
      "Train: Epoch [20], Batch [699/938], Loss: 0.4812004566192627\n",
      "Train: Epoch [20], Batch [700/938], Loss: 0.411830872297287\n",
      "Train: Epoch [20], Batch [701/938], Loss: 0.42453455924987793\n",
      "Train: Epoch [20], Batch [702/938], Loss: 0.44066107273101807\n",
      "Train: Epoch [20], Batch [703/938], Loss: 0.5141464471817017\n",
      "Train: Epoch [20], Batch [704/938], Loss: 0.5282037258148193\n",
      "Train: Epoch [20], Batch [705/938], Loss: 0.3903389573097229\n",
      "Train: Epoch [20], Batch [706/938], Loss: 0.5595716834068298\n",
      "Train: Epoch [20], Batch [707/938], Loss: 0.47348135709762573\n",
      "Train: Epoch [20], Batch [708/938], Loss: 0.5281375050544739\n",
      "Train: Epoch [20], Batch [709/938], Loss: 0.3084583878517151\n",
      "Train: Epoch [20], Batch [710/938], Loss: 0.37137371301651\n",
      "Train: Epoch [20], Batch [711/938], Loss: 0.6428778767585754\n",
      "Train: Epoch [20], Batch [712/938], Loss: 0.41077157855033875\n",
      "Train: Epoch [20], Batch [713/938], Loss: 0.4851607382297516\n",
      "Train: Epoch [20], Batch [714/938], Loss: 0.7137029767036438\n",
      "Train: Epoch [20], Batch [715/938], Loss: 0.4914596676826477\n",
      "Train: Epoch [20], Batch [716/938], Loss: 0.5572972893714905\n",
      "Train: Epoch [20], Batch [717/938], Loss: 0.39768877625465393\n",
      "Train: Epoch [20], Batch [718/938], Loss: 0.6332513093948364\n",
      "Train: Epoch [20], Batch [719/938], Loss: 0.5218573808670044\n",
      "Train: Epoch [20], Batch [720/938], Loss: 0.32482147216796875\n",
      "Train: Epoch [20], Batch [721/938], Loss: 0.3798205256462097\n",
      "Train: Epoch [20], Batch [722/938], Loss: 0.663055419921875\n",
      "Train: Epoch [20], Batch [723/938], Loss: 0.34900951385498047\n",
      "Train: Epoch [20], Batch [724/938], Loss: 0.30603981018066406\n",
      "Train: Epoch [20], Batch [725/938], Loss: 0.4519999027252197\n",
      "Train: Epoch [20], Batch [726/938], Loss: 0.4051184356212616\n",
      "Train: Epoch [20], Batch [727/938], Loss: 0.28577691316604614\n",
      "Train: Epoch [20], Batch [728/938], Loss: 0.3128742575645447\n",
      "Train: Epoch [20], Batch [729/938], Loss: 0.47134673595428467\n",
      "Train: Epoch [20], Batch [730/938], Loss: 0.39649295806884766\n",
      "Train: Epoch [20], Batch [731/938], Loss: 0.578602135181427\n",
      "Train: Epoch [20], Batch [732/938], Loss: 0.43755900859832764\n",
      "Train: Epoch [20], Batch [733/938], Loss: 0.40996965765953064\n",
      "Train: Epoch [20], Batch [734/938], Loss: 0.5487139225006104\n",
      "Train: Epoch [20], Batch [735/938], Loss: 0.4405139088630676\n",
      "Train: Epoch [20], Batch [736/938], Loss: 0.2857646942138672\n",
      "Train: Epoch [20], Batch [737/938], Loss: 0.4976159334182739\n",
      "Train: Epoch [20], Batch [738/938], Loss: 0.27220702171325684\n",
      "Train: Epoch [20], Batch [739/938], Loss: 0.3505779504776001\n",
      "Train: Epoch [20], Batch [740/938], Loss: 0.3719005584716797\n",
      "Train: Epoch [20], Batch [741/938], Loss: 0.4211934506893158\n",
      "Train: Epoch [20], Batch [742/938], Loss: 0.44187259674072266\n",
      "Train: Epoch [20], Batch [743/938], Loss: 0.3988282084465027\n",
      "Train: Epoch [20], Batch [744/938], Loss: 0.5148674845695496\n",
      "Train: Epoch [20], Batch [745/938], Loss: 0.3767489790916443\n",
      "Train: Epoch [20], Batch [746/938], Loss: 0.47215452790260315\n",
      "Train: Epoch [20], Batch [747/938], Loss: 0.5203031897544861\n",
      "Train: Epoch [20], Batch [748/938], Loss: 0.3248646855354309\n",
      "Train: Epoch [20], Batch [749/938], Loss: 0.36978983879089355\n",
      "Train: Epoch [20], Batch [750/938], Loss: 0.3490517735481262\n",
      "Train: Epoch [20], Batch [751/938], Loss: 0.3778732419013977\n",
      "Train: Epoch [20], Batch [752/938], Loss: 0.41376304626464844\n",
      "Train: Epoch [20], Batch [753/938], Loss: 0.3819216191768646\n",
      "Train: Epoch [20], Batch [754/938], Loss: 0.44653019309043884\n",
      "Train: Epoch [20], Batch [755/938], Loss: 0.4707062840461731\n",
      "Train: Epoch [20], Batch [756/938], Loss: 0.6065844893455505\n",
      "Train: Epoch [20], Batch [757/938], Loss: 0.37626180052757263\n",
      "Train: Epoch [20], Batch [758/938], Loss: 0.531646192073822\n",
      "Train: Epoch [20], Batch [759/938], Loss: 0.4557437300682068\n",
      "Train: Epoch [20], Batch [760/938], Loss: 0.39683136343955994\n",
      "Train: Epoch [20], Batch [761/938], Loss: 0.4020916223526001\n",
      "Train: Epoch [20], Batch [762/938], Loss: 0.31276145577430725\n",
      "Train: Epoch [20], Batch [763/938], Loss: 0.693406879901886\n",
      "Train: Epoch [20], Batch [764/938], Loss: 0.547574520111084\n",
      "Train: Epoch [20], Batch [765/938], Loss: 0.5835511684417725\n",
      "Train: Epoch [20], Batch [766/938], Loss: 0.4941643476486206\n",
      "Train: Epoch [20], Batch [767/938], Loss: 0.3567717671394348\n",
      "Train: Epoch [20], Batch [768/938], Loss: 0.4794910252094269\n",
      "Train: Epoch [20], Batch [769/938], Loss: 0.33643838763237\n",
      "Train: Epoch [20], Batch [770/938], Loss: 0.48437121510505676\n",
      "Train: Epoch [20], Batch [771/938], Loss: 0.5139281153678894\n",
      "Train: Epoch [20], Batch [772/938], Loss: 0.495259165763855\n",
      "Train: Epoch [20], Batch [773/938], Loss: 0.34788134694099426\n",
      "Train: Epoch [20], Batch [774/938], Loss: 0.5992534756660461\n",
      "Train: Epoch [20], Batch [775/938], Loss: 0.4480731785297394\n",
      "Train: Epoch [20], Batch [776/938], Loss: 0.5361830592155457\n",
      "Train: Epoch [20], Batch [777/938], Loss: 0.5719902515411377\n",
      "Train: Epoch [20], Batch [778/938], Loss: 0.4518705904483795\n",
      "Train: Epoch [20], Batch [779/938], Loss: 0.3498619496822357\n",
      "Train: Epoch [20], Batch [780/938], Loss: 0.4895257353782654\n",
      "Train: Epoch [20], Batch [781/938], Loss: 0.3813084065914154\n",
      "Train: Epoch [20], Batch [782/938], Loss: 0.4811496436595917\n",
      "Train: Epoch [20], Batch [783/938], Loss: 0.38013947010040283\n",
      "Train: Epoch [20], Batch [784/938], Loss: 0.5048342347145081\n",
      "Train: Epoch [20], Batch [785/938], Loss: 0.4294450581073761\n",
      "Train: Epoch [20], Batch [786/938], Loss: 0.3791285455226898\n",
      "Train: Epoch [20], Batch [787/938], Loss: 0.3260931372642517\n",
      "Train: Epoch [20], Batch [788/938], Loss: 0.4115522503852844\n",
      "Train: Epoch [20], Batch [789/938], Loss: 0.46754032373428345\n",
      "Train: Epoch [20], Batch [790/938], Loss: 0.5746195912361145\n",
      "Train: Epoch [20], Batch [791/938], Loss: 0.5041409134864807\n",
      "Train: Epoch [20], Batch [792/938], Loss: 0.5336446166038513\n",
      "Train: Epoch [20], Batch [793/938], Loss: 0.4914385676383972\n",
      "Train: Epoch [20], Batch [794/938], Loss: 0.3519759476184845\n",
      "Train: Epoch [20], Batch [795/938], Loss: 0.6080630421638489\n",
      "Train: Epoch [20], Batch [796/938], Loss: 0.3980884253978729\n",
      "Train: Epoch [20], Batch [797/938], Loss: 0.4434148073196411\n",
      "Train: Epoch [20], Batch [798/938], Loss: 0.476873517036438\n",
      "Train: Epoch [20], Batch [799/938], Loss: 0.4172573983669281\n",
      "Train: Epoch [20], Batch [800/938], Loss: 0.6054329872131348\n",
      "Train: Epoch [20], Batch [801/938], Loss: 0.47994017601013184\n",
      "Train: Epoch [20], Batch [802/938], Loss: 0.39029642939567566\n",
      "Train: Epoch [20], Batch [803/938], Loss: 0.5467889904975891\n",
      "Train: Epoch [20], Batch [804/938], Loss: 0.5823854207992554\n",
      "Train: Epoch [20], Batch [805/938], Loss: 0.593140721321106\n",
      "Train: Epoch [20], Batch [806/938], Loss: 0.46758610010147095\n",
      "Train: Epoch [20], Batch [807/938], Loss: 0.7483720183372498\n",
      "Train: Epoch [20], Batch [808/938], Loss: 0.2662722170352936\n",
      "Train: Epoch [20], Batch [809/938], Loss: 0.3226914405822754\n",
      "Train: Epoch [20], Batch [810/938], Loss: 0.5630696415901184\n",
      "Train: Epoch [20], Batch [811/938], Loss: 0.4374111592769623\n",
      "Train: Epoch [20], Batch [812/938], Loss: 0.43508225679397583\n",
      "Train: Epoch [20], Batch [813/938], Loss: 0.408285915851593\n",
      "Train: Epoch [20], Batch [814/938], Loss: 0.5798056125640869\n",
      "Train: Epoch [20], Batch [815/938], Loss: 0.6167477965354919\n",
      "Train: Epoch [20], Batch [816/938], Loss: 0.4487285614013672\n",
      "Train: Epoch [20], Batch [817/938], Loss: 0.3073865473270416\n",
      "Train: Epoch [20], Batch [818/938], Loss: 0.4312223196029663\n",
      "Train: Epoch [20], Batch [819/938], Loss: 0.4154302775859833\n",
      "Train: Epoch [20], Batch [820/938], Loss: 0.52815181016922\n",
      "Train: Epoch [20], Batch [821/938], Loss: 0.6026631593704224\n",
      "Train: Epoch [20], Batch [822/938], Loss: 0.6027876138687134\n",
      "Train: Epoch [20], Batch [823/938], Loss: 0.6895054578781128\n",
      "Train: Epoch [20], Batch [824/938], Loss: 0.4109262228012085\n",
      "Train: Epoch [20], Batch [825/938], Loss: 0.5876001715660095\n",
      "Train: Epoch [20], Batch [826/938], Loss: 0.5004865527153015\n",
      "Train: Epoch [20], Batch [827/938], Loss: 0.32570645213127136\n",
      "Train: Epoch [20], Batch [828/938], Loss: 0.46532002091407776\n",
      "Train: Epoch [20], Batch [829/938], Loss: 0.562401533126831\n",
      "Train: Epoch [20], Batch [830/938], Loss: 0.38743826746940613\n",
      "Train: Epoch [20], Batch [831/938], Loss: 0.7682302594184875\n",
      "Train: Epoch [20], Batch [832/938], Loss: 0.5110416412353516\n",
      "Train: Epoch [20], Batch [833/938], Loss: 0.6263551712036133\n",
      "Train: Epoch [20], Batch [834/938], Loss: 0.641776442527771\n",
      "Train: Epoch [20], Batch [835/938], Loss: 0.37330362200737\n",
      "Train: Epoch [20], Batch [836/938], Loss: 0.4235996901988983\n",
      "Train: Epoch [20], Batch [837/938], Loss: 0.46065109968185425\n",
      "Train: Epoch [20], Batch [838/938], Loss: 0.3961353003978729\n",
      "Train: Epoch [20], Batch [839/938], Loss: 0.5664103627204895\n",
      "Train: Epoch [20], Batch [840/938], Loss: 0.3933223485946655\n",
      "Train: Epoch [20], Batch [841/938], Loss: 0.5027749538421631\n",
      "Train: Epoch [20], Batch [842/938], Loss: 0.5029186606407166\n",
      "Train: Epoch [20], Batch [843/938], Loss: 0.27164459228515625\n",
      "Train: Epoch [20], Batch [844/938], Loss: 0.36855047941207886\n",
      "Train: Epoch [20], Batch [845/938], Loss: 0.4282793402671814\n",
      "Train: Epoch [20], Batch [846/938], Loss: 0.597547173500061\n",
      "Train: Epoch [20], Batch [847/938], Loss: 0.4888489544391632\n",
      "Train: Epoch [20], Batch [848/938], Loss: 0.40091171860694885\n",
      "Train: Epoch [20], Batch [849/938], Loss: 0.44127631187438965\n",
      "Train: Epoch [20], Batch [850/938], Loss: 0.2997693717479706\n",
      "Train: Epoch [20], Batch [851/938], Loss: 0.4375574290752411\n",
      "Train: Epoch [20], Batch [852/938], Loss: 0.6257407665252686\n",
      "Train: Epoch [20], Batch [853/938], Loss: 0.3498179316520691\n",
      "Train: Epoch [20], Batch [854/938], Loss: 0.3968973159790039\n",
      "Train: Epoch [20], Batch [855/938], Loss: 0.6929281949996948\n",
      "Train: Epoch [20], Batch [856/938], Loss: 0.5312069654464722\n",
      "Train: Epoch [20], Batch [857/938], Loss: 0.5696841478347778\n",
      "Train: Epoch [20], Batch [858/938], Loss: 0.5601791739463806\n",
      "Train: Epoch [20], Batch [859/938], Loss: 0.4061928689479828\n",
      "Train: Epoch [20], Batch [860/938], Loss: 0.2445315569639206\n",
      "Train: Epoch [20], Batch [861/938], Loss: 0.35565924644470215\n",
      "Train: Epoch [20], Batch [862/938], Loss: 0.5173143744468689\n",
      "Train: Epoch [20], Batch [863/938], Loss: 0.3721539080142975\n",
      "Train: Epoch [20], Batch [864/938], Loss: 0.38947850465774536\n",
      "Train: Epoch [20], Batch [865/938], Loss: 0.411744624376297\n",
      "Train: Epoch [20], Batch [866/938], Loss: 0.2788873612880707\n",
      "Train: Epoch [20], Batch [867/938], Loss: 0.4337010383605957\n",
      "Train: Epoch [20], Batch [868/938], Loss: 0.4210149049758911\n",
      "Train: Epoch [20], Batch [869/938], Loss: 0.3704372048377991\n",
      "Train: Epoch [20], Batch [870/938], Loss: 0.6280047297477722\n",
      "Train: Epoch [20], Batch [871/938], Loss: 0.45914894342422485\n",
      "Train: Epoch [20], Batch [872/938], Loss: 0.3442539870738983\n",
      "Train: Epoch [20], Batch [873/938], Loss: 0.7104317545890808\n",
      "Train: Epoch [20], Batch [874/938], Loss: 0.4684411585330963\n",
      "Train: Epoch [20], Batch [875/938], Loss: 0.6428922414779663\n",
      "Train: Epoch [20], Batch [876/938], Loss: 0.29522356390953064\n",
      "Train: Epoch [20], Batch [877/938], Loss: 0.4159142076969147\n",
      "Train: Epoch [20], Batch [878/938], Loss: 0.37612617015838623\n",
      "Train: Epoch [20], Batch [879/938], Loss: 0.44575512409210205\n",
      "Train: Epoch [20], Batch [880/938], Loss: 0.39093202352523804\n",
      "Train: Epoch [20], Batch [881/938], Loss: 0.476138710975647\n",
      "Train: Epoch [20], Batch [882/938], Loss: 0.585664689540863\n",
      "Train: Epoch [20], Batch [883/938], Loss: 0.4075269401073456\n",
      "Train: Epoch [20], Batch [884/938], Loss: 0.5063074827194214\n",
      "Train: Epoch [20], Batch [885/938], Loss: 0.5335646271705627\n",
      "Train: Epoch [20], Batch [886/938], Loss: 0.44462838768959045\n",
      "Train: Epoch [20], Batch [887/938], Loss: 0.40893423557281494\n",
      "Train: Epoch [20], Batch [888/938], Loss: 0.4544438123703003\n",
      "Train: Epoch [20], Batch [889/938], Loss: 0.43233323097229004\n",
      "Train: Epoch [20], Batch [890/938], Loss: 0.4960193336009979\n",
      "Train: Epoch [20], Batch [891/938], Loss: 0.5149462223052979\n",
      "Train: Epoch [20], Batch [892/938], Loss: 0.49988651275634766\n",
      "Train: Epoch [20], Batch [893/938], Loss: 0.47858306765556335\n",
      "Train: Epoch [20], Batch [894/938], Loss: 0.30673596262931824\n",
      "Train: Epoch [20], Batch [895/938], Loss: 0.36996132135391235\n",
      "Train: Epoch [20], Batch [896/938], Loss: 0.37411707639694214\n",
      "Train: Epoch [20], Batch [897/938], Loss: 0.37862151861190796\n",
      "Train: Epoch [20], Batch [898/938], Loss: 0.4271738827228546\n",
      "Train: Epoch [20], Batch [899/938], Loss: 0.5906167030334473\n",
      "Train: Epoch [20], Batch [900/938], Loss: 0.5711778998374939\n",
      "Train: Epoch [20], Batch [901/938], Loss: 0.6820593476295471\n",
      "Train: Epoch [20], Batch [902/938], Loss: 0.47843506932258606\n",
      "Train: Epoch [20], Batch [903/938], Loss: 0.3798257112503052\n",
      "Train: Epoch [20], Batch [904/938], Loss: 0.30700182914733887\n",
      "Train: Epoch [20], Batch [905/938], Loss: 0.31731927394866943\n",
      "Train: Epoch [20], Batch [906/938], Loss: 0.36850759387016296\n",
      "Train: Epoch [20], Batch [907/938], Loss: 0.48877912759780884\n",
      "Train: Epoch [20], Batch [908/938], Loss: 0.5589166283607483\n",
      "Train: Epoch [20], Batch [909/938], Loss: 0.1956382542848587\n",
      "Train: Epoch [20], Batch [910/938], Loss: 0.4674951136112213\n",
      "Train: Epoch [20], Batch [911/938], Loss: 0.5958570241928101\n",
      "Train: Epoch [20], Batch [912/938], Loss: 0.5013266801834106\n",
      "Train: Epoch [20], Batch [913/938], Loss: 0.3581242263317108\n",
      "Train: Epoch [20], Batch [914/938], Loss: 0.5813279747962952\n",
      "Train: Epoch [20], Batch [915/938], Loss: 0.6197077631950378\n",
      "Train: Epoch [20], Batch [916/938], Loss: 0.4481729567050934\n",
      "Train: Epoch [20], Batch [917/938], Loss: 0.40541213750839233\n",
      "Train: Epoch [20], Batch [918/938], Loss: 0.40880799293518066\n",
      "Train: Epoch [20], Batch [919/938], Loss: 0.5484341382980347\n",
      "Train: Epoch [20], Batch [920/938], Loss: 0.5098744034767151\n",
      "Train: Epoch [20], Batch [921/938], Loss: 0.649008572101593\n",
      "Train: Epoch [20], Batch [922/938], Loss: 0.5649620294570923\n",
      "Train: Epoch [20], Batch [923/938], Loss: 0.3669090270996094\n",
      "Train: Epoch [20], Batch [924/938], Loss: 0.6090397834777832\n",
      "Train: Epoch [20], Batch [925/938], Loss: 0.3207404613494873\n",
      "Train: Epoch [20], Batch [926/938], Loss: 0.41121935844421387\n",
      "Train: Epoch [20], Batch [927/938], Loss: 0.56501305103302\n",
      "Train: Epoch [20], Batch [928/938], Loss: 0.304908812046051\n",
      "Train: Epoch [20], Batch [929/938], Loss: 0.4239213466644287\n",
      "Train: Epoch [20], Batch [930/938], Loss: 0.339199423789978\n",
      "Train: Epoch [20], Batch [931/938], Loss: 0.39734017848968506\n",
      "Train: Epoch [20], Batch [932/938], Loss: 0.3191843628883362\n",
      "Train: Epoch [20], Batch [933/938], Loss: 0.4167362451553345\n",
      "Train: Epoch [20], Batch [934/938], Loss: 0.6368284821510315\n",
      "Train: Epoch [20], Batch [935/938], Loss: 0.629817008972168\n",
      "Train: Epoch [20], Batch [936/938], Loss: 0.521243155002594\n",
      "Train: Epoch [20], Batch [937/938], Loss: 0.3350571393966675\n",
      "Train: Epoch [20], Batch [938/938], Loss: 0.4402896463871002\n",
      "Accuracy of train set: 0.8384333333333334\n",
      "Validation: Epoch [20], Batch [1/938], Loss: 0.44817930459976196\n",
      "Validation: Epoch [20], Batch [2/938], Loss: 0.47655290365219116\n",
      "Validation: Epoch [20], Batch [3/938], Loss: 0.4006613492965698\n",
      "Validation: Epoch [20], Batch [4/938], Loss: 0.7728302478790283\n",
      "Validation: Epoch [20], Batch [5/938], Loss: 0.3595658540725708\n",
      "Validation: Epoch [20], Batch [6/938], Loss: 0.4178442358970642\n",
      "Validation: Epoch [20], Batch [7/938], Loss: 0.6086480021476746\n",
      "Validation: Epoch [20], Batch [8/938], Loss: 0.39367911219596863\n",
      "Validation: Epoch [20], Batch [9/938], Loss: 0.3893887996673584\n",
      "Validation: Epoch [20], Batch [10/938], Loss: 0.30711570382118225\n",
      "Validation: Epoch [20], Batch [11/938], Loss: 0.4452488124370575\n",
      "Validation: Epoch [20], Batch [12/938], Loss: 0.49775949120521545\n",
      "Validation: Epoch [20], Batch [13/938], Loss: 0.36615025997161865\n",
      "Validation: Epoch [20], Batch [14/938], Loss: 0.3662688434123993\n",
      "Validation: Epoch [20], Batch [15/938], Loss: 0.3317860960960388\n",
      "Validation: Epoch [20], Batch [16/938], Loss: 0.4757815897464752\n",
      "Validation: Epoch [20], Batch [17/938], Loss: 0.35036802291870117\n",
      "Validation: Epoch [20], Batch [18/938], Loss: 0.4239884614944458\n",
      "Validation: Epoch [20], Batch [19/938], Loss: 0.3853175938129425\n",
      "Validation: Epoch [20], Batch [20/938], Loss: 0.43245184421539307\n",
      "Validation: Epoch [20], Batch [21/938], Loss: 0.572871208190918\n",
      "Validation: Epoch [20], Batch [22/938], Loss: 0.43623000383377075\n",
      "Validation: Epoch [20], Batch [23/938], Loss: 0.5681129097938538\n",
      "Validation: Epoch [20], Batch [24/938], Loss: 0.5224046111106873\n",
      "Validation: Epoch [20], Batch [25/938], Loss: 0.3770814836025238\n",
      "Validation: Epoch [20], Batch [26/938], Loss: 0.5232478380203247\n",
      "Validation: Epoch [20], Batch [27/938], Loss: 0.3039015233516693\n",
      "Validation: Epoch [20], Batch [28/938], Loss: 0.2858389914035797\n",
      "Validation: Epoch [20], Batch [29/938], Loss: 0.3427797853946686\n",
      "Validation: Epoch [20], Batch [30/938], Loss: 0.3427625894546509\n",
      "Validation: Epoch [20], Batch [31/938], Loss: 0.9501424431800842\n",
      "Validation: Epoch [20], Batch [32/938], Loss: 0.4420984983444214\n",
      "Validation: Epoch [20], Batch [33/938], Loss: 0.3769100308418274\n",
      "Validation: Epoch [20], Batch [34/938], Loss: 0.44297340512275696\n",
      "Validation: Epoch [20], Batch [35/938], Loss: 0.36980652809143066\n",
      "Validation: Epoch [20], Batch [36/938], Loss: 0.4340450167655945\n",
      "Validation: Epoch [20], Batch [37/938], Loss: 0.4683147966861725\n",
      "Validation: Epoch [20], Batch [38/938], Loss: 0.6022557020187378\n",
      "Validation: Epoch [20], Batch [39/938], Loss: 0.5967844724655151\n",
      "Validation: Epoch [20], Batch [40/938], Loss: 0.5356910228729248\n",
      "Validation: Epoch [20], Batch [41/938], Loss: 0.30932241678237915\n",
      "Validation: Epoch [20], Batch [42/938], Loss: 0.5323769450187683\n",
      "Validation: Epoch [20], Batch [43/938], Loss: 0.5161154866218567\n",
      "Validation: Epoch [20], Batch [44/938], Loss: 0.5910307168960571\n",
      "Validation: Epoch [20], Batch [45/938], Loss: 0.558313250541687\n",
      "Validation: Epoch [20], Batch [46/938], Loss: 0.47954368591308594\n",
      "Validation: Epoch [20], Batch [47/938], Loss: 0.3071044683456421\n",
      "Validation: Epoch [20], Batch [48/938], Loss: 0.596953809261322\n",
      "Validation: Epoch [20], Batch [49/938], Loss: 0.6042476892471313\n",
      "Validation: Epoch [20], Batch [50/938], Loss: 0.6051751971244812\n",
      "Validation: Epoch [20], Batch [51/938], Loss: 0.5025768876075745\n",
      "Validation: Epoch [20], Batch [52/938], Loss: 0.605331540107727\n",
      "Validation: Epoch [20], Batch [53/938], Loss: 0.5895993113517761\n",
      "Validation: Epoch [20], Batch [54/938], Loss: 0.35092130303382874\n",
      "Validation: Epoch [20], Batch [55/938], Loss: 0.6000882387161255\n",
      "Validation: Epoch [20], Batch [56/938], Loss: 0.49767568707466125\n",
      "Validation: Epoch [20], Batch [57/938], Loss: 0.5201924443244934\n",
      "Validation: Epoch [20], Batch [58/938], Loss: 0.5304614901542664\n",
      "Validation: Epoch [20], Batch [59/938], Loss: 0.4691132605075836\n",
      "Validation: Epoch [20], Batch [60/938], Loss: 0.40094658732414246\n",
      "Validation: Epoch [20], Batch [61/938], Loss: 0.49806854128837585\n",
      "Validation: Epoch [20], Batch [62/938], Loss: 0.5227734446525574\n",
      "Validation: Epoch [20], Batch [63/938], Loss: 0.5109213590621948\n",
      "Validation: Epoch [20], Batch [64/938], Loss: 0.4123486280441284\n",
      "Validation: Epoch [20], Batch [65/938], Loss: 0.5082184076309204\n",
      "Validation: Epoch [20], Batch [66/938], Loss: 0.3390437364578247\n",
      "Validation: Epoch [20], Batch [67/938], Loss: 0.3993839919567108\n",
      "Validation: Epoch [20], Batch [68/938], Loss: 0.40978237986564636\n",
      "Validation: Epoch [20], Batch [69/938], Loss: 0.48857414722442627\n",
      "Validation: Epoch [20], Batch [70/938], Loss: 0.4061455726623535\n",
      "Validation: Epoch [20], Batch [71/938], Loss: 0.44831663370132446\n",
      "Validation: Epoch [20], Batch [72/938], Loss: 0.3786752223968506\n",
      "Validation: Epoch [20], Batch [73/938], Loss: 0.40499719977378845\n",
      "Validation: Epoch [20], Batch [74/938], Loss: 0.7577583193778992\n",
      "Validation: Epoch [20], Batch [75/938], Loss: 0.4700353741645813\n",
      "Validation: Epoch [20], Batch [76/938], Loss: 0.4351443946361542\n",
      "Validation: Epoch [20], Batch [77/938], Loss: 0.26643890142440796\n",
      "Validation: Epoch [20], Batch [78/938], Loss: 0.4643091559410095\n",
      "Validation: Epoch [20], Batch [79/938], Loss: 0.6010348796844482\n",
      "Validation: Epoch [20], Batch [80/938], Loss: 0.4001297354698181\n",
      "Validation: Epoch [20], Batch [81/938], Loss: 0.3194745182991028\n",
      "Validation: Epoch [20], Batch [82/938], Loss: 0.530069887638092\n",
      "Validation: Epoch [20], Batch [83/938], Loss: 0.43889421224594116\n",
      "Validation: Epoch [20], Batch [84/938], Loss: 0.5002454519271851\n",
      "Validation: Epoch [20], Batch [85/938], Loss: 0.7294308543205261\n",
      "Validation: Epoch [20], Batch [86/938], Loss: 0.39112135767936707\n",
      "Validation: Epoch [20], Batch [87/938], Loss: 0.6699875593185425\n",
      "Validation: Epoch [20], Batch [88/938], Loss: 0.30360040068626404\n",
      "Validation: Epoch [20], Batch [89/938], Loss: 0.5633881688117981\n",
      "Validation: Epoch [20], Batch [90/938], Loss: 0.46467718482017517\n",
      "Validation: Epoch [20], Batch [91/938], Loss: 0.3219282627105713\n",
      "Validation: Epoch [20], Batch [92/938], Loss: 0.3207584023475647\n",
      "Validation: Epoch [20], Batch [93/938], Loss: 0.4642620086669922\n",
      "Validation: Epoch [20], Batch [94/938], Loss: 0.38117337226867676\n",
      "Validation: Epoch [20], Batch [95/938], Loss: 0.5361359715461731\n",
      "Validation: Epoch [20], Batch [96/938], Loss: 0.41109082102775574\n",
      "Validation: Epoch [20], Batch [97/938], Loss: 0.3336467742919922\n",
      "Validation: Epoch [20], Batch [98/938], Loss: 0.43285220861434937\n",
      "Validation: Epoch [20], Batch [99/938], Loss: 0.6122485995292664\n",
      "Validation: Epoch [20], Batch [100/938], Loss: 0.7457578778266907\n",
      "Validation: Epoch [20], Batch [101/938], Loss: 0.5638061165809631\n",
      "Validation: Epoch [20], Batch [102/938], Loss: 0.66587233543396\n",
      "Validation: Epoch [20], Batch [103/938], Loss: 0.38641008734703064\n",
      "Validation: Epoch [20], Batch [104/938], Loss: 0.5990944504737854\n",
      "Validation: Epoch [20], Batch [105/938], Loss: 0.352986216545105\n",
      "Validation: Epoch [20], Batch [106/938], Loss: 0.28343328833580017\n",
      "Validation: Epoch [20], Batch [107/938], Loss: 0.38447922468185425\n",
      "Validation: Epoch [20], Batch [108/938], Loss: 0.4433753490447998\n",
      "Validation: Epoch [20], Batch [109/938], Loss: 0.2343798130750656\n",
      "Validation: Epoch [20], Batch [110/938], Loss: 0.629305899143219\n",
      "Validation: Epoch [20], Batch [111/938], Loss: 0.4087839424610138\n",
      "Validation: Epoch [20], Batch [112/938], Loss: 0.45166081190109253\n",
      "Validation: Epoch [20], Batch [113/938], Loss: 0.7298824787139893\n",
      "Validation: Epoch [20], Batch [114/938], Loss: 0.3955436646938324\n",
      "Validation: Epoch [20], Batch [115/938], Loss: 0.43533480167388916\n",
      "Validation: Epoch [20], Batch [116/938], Loss: 0.7577975392341614\n",
      "Validation: Epoch [20], Batch [117/938], Loss: 0.40763282775878906\n",
      "Validation: Epoch [20], Batch [118/938], Loss: 0.6515858173370361\n",
      "Validation: Epoch [20], Batch [119/938], Loss: 0.6302409172058105\n",
      "Validation: Epoch [20], Batch [120/938], Loss: 0.5256819725036621\n",
      "Validation: Epoch [20], Batch [121/938], Loss: 0.3111433982849121\n",
      "Validation: Epoch [20], Batch [122/938], Loss: 0.36397987604141235\n",
      "Validation: Epoch [20], Batch [123/938], Loss: 0.4141756296157837\n",
      "Validation: Epoch [20], Batch [124/938], Loss: 0.3433015048503876\n",
      "Validation: Epoch [20], Batch [125/938], Loss: 0.6939305067062378\n",
      "Validation: Epoch [20], Batch [126/938], Loss: 0.6494914293289185\n",
      "Validation: Epoch [20], Batch [127/938], Loss: 0.43821102380752563\n",
      "Validation: Epoch [20], Batch [128/938], Loss: 0.6539661288261414\n",
      "Validation: Epoch [20], Batch [129/938], Loss: 0.390966534614563\n",
      "Validation: Epoch [20], Batch [130/938], Loss: 0.4245954155921936\n",
      "Validation: Epoch [20], Batch [131/938], Loss: 0.3966061472892761\n",
      "Validation: Epoch [20], Batch [132/938], Loss: 0.37503430247306824\n",
      "Validation: Epoch [20], Batch [133/938], Loss: 0.4018579125404358\n",
      "Validation: Epoch [20], Batch [134/938], Loss: 0.32007524371147156\n",
      "Validation: Epoch [20], Batch [135/938], Loss: 0.315696120262146\n",
      "Validation: Epoch [20], Batch [136/938], Loss: 0.31502407789230347\n",
      "Validation: Epoch [20], Batch [137/938], Loss: 0.5520731210708618\n",
      "Validation: Epoch [20], Batch [138/938], Loss: 0.5348946452140808\n",
      "Validation: Epoch [20], Batch [139/938], Loss: 0.6062151789665222\n",
      "Validation: Epoch [20], Batch [140/938], Loss: 0.9319584369659424\n",
      "Validation: Epoch [20], Batch [141/938], Loss: 0.2740482985973358\n",
      "Validation: Epoch [20], Batch [142/938], Loss: 0.5383051037788391\n",
      "Validation: Epoch [20], Batch [143/938], Loss: 0.3585650324821472\n",
      "Validation: Epoch [20], Batch [144/938], Loss: 0.5402937531471252\n",
      "Validation: Epoch [20], Batch [145/938], Loss: 0.34844282269477844\n",
      "Validation: Epoch [20], Batch [146/938], Loss: 0.654533326625824\n",
      "Validation: Epoch [20], Batch [147/938], Loss: 0.6767410635948181\n",
      "Validation: Epoch [20], Batch [148/938], Loss: 0.46124041080474854\n",
      "Validation: Epoch [20], Batch [149/938], Loss: 0.30287986993789673\n",
      "Validation: Epoch [20], Batch [150/938], Loss: 0.5491851568222046\n",
      "Validation: Epoch [20], Batch [151/938], Loss: 0.44903624057769775\n",
      "Validation: Epoch [20], Batch [152/938], Loss: 0.3467281460762024\n",
      "Validation: Epoch [20], Batch [153/938], Loss: 0.40503478050231934\n",
      "Validation: Epoch [20], Batch [154/938], Loss: 0.5100889801979065\n",
      "Validation: Epoch [20], Batch [155/938], Loss: 0.24215508997440338\n",
      "Validation: Epoch [20], Batch [156/938], Loss: 0.6714758276939392\n",
      "Validation: Epoch [20], Batch [157/938], Loss: 0.7339392304420471\n",
      "Validation: Epoch [20], Batch [158/938], Loss: 0.7220297455787659\n",
      "Validation: Epoch [20], Batch [159/938], Loss: 0.3644537627696991\n",
      "Validation: Epoch [20], Batch [160/938], Loss: 0.4251735210418701\n",
      "Validation: Epoch [20], Batch [161/938], Loss: 0.35453134775161743\n",
      "Validation: Epoch [20], Batch [162/938], Loss: 0.6959784030914307\n",
      "Validation: Epoch [20], Batch [163/938], Loss: 0.4338090717792511\n",
      "Validation: Epoch [20], Batch [164/938], Loss: 0.35330212116241455\n",
      "Validation: Epoch [20], Batch [165/938], Loss: 0.31180936098098755\n",
      "Validation: Epoch [20], Batch [166/938], Loss: 0.33526620268821716\n",
      "Validation: Epoch [20], Batch [167/938], Loss: 0.17948687076568604\n",
      "Validation: Epoch [20], Batch [168/938], Loss: 0.37662971019744873\n",
      "Validation: Epoch [20], Batch [169/938], Loss: 0.47216957807540894\n",
      "Validation: Epoch [20], Batch [170/938], Loss: 0.6725309491157532\n",
      "Validation: Epoch [20], Batch [171/938], Loss: 0.5355059504508972\n",
      "Validation: Epoch [20], Batch [172/938], Loss: 0.3920593559741974\n",
      "Validation: Epoch [20], Batch [173/938], Loss: 0.4719986617565155\n",
      "Validation: Epoch [20], Batch [174/938], Loss: 0.3182659447193146\n",
      "Validation: Epoch [20], Batch [175/938], Loss: 0.41597050428390503\n",
      "Validation: Epoch [20], Batch [176/938], Loss: 0.42379316687583923\n",
      "Validation: Epoch [20], Batch [177/938], Loss: 0.6169909238815308\n",
      "Validation: Epoch [20], Batch [178/938], Loss: 0.27689695358276367\n",
      "Validation: Epoch [20], Batch [179/938], Loss: 0.5691148042678833\n",
      "Validation: Epoch [20], Batch [180/938], Loss: 0.4430960416793823\n",
      "Validation: Epoch [20], Batch [181/938], Loss: 0.6399856209754944\n",
      "Validation: Epoch [20], Batch [182/938], Loss: 0.363319456577301\n",
      "Validation: Epoch [20], Batch [183/938], Loss: 0.5166536569595337\n",
      "Validation: Epoch [20], Batch [184/938], Loss: 0.4637705385684967\n",
      "Validation: Epoch [20], Batch [185/938], Loss: 0.5692185163497925\n",
      "Validation: Epoch [20], Batch [186/938], Loss: 0.6572176814079285\n",
      "Validation: Epoch [20], Batch [187/938], Loss: 0.339306503534317\n",
      "Validation: Epoch [20], Batch [188/938], Loss: 0.7605665326118469\n",
      "Validation: Epoch [20], Batch [189/938], Loss: 0.30329203605651855\n",
      "Validation: Epoch [20], Batch [190/938], Loss: 0.4676000773906708\n",
      "Validation: Epoch [20], Batch [191/938], Loss: 0.36049044132232666\n",
      "Validation: Epoch [20], Batch [192/938], Loss: 0.3268582820892334\n",
      "Validation: Epoch [20], Batch [193/938], Loss: 0.3912144899368286\n",
      "Validation: Epoch [20], Batch [194/938], Loss: 0.2882188558578491\n",
      "Validation: Epoch [20], Batch [195/938], Loss: 0.5174931883811951\n",
      "Validation: Epoch [20], Batch [196/938], Loss: 0.6386094093322754\n",
      "Validation: Epoch [20], Batch [197/938], Loss: 0.476627379655838\n",
      "Validation: Epoch [20], Batch [198/938], Loss: 0.4656164050102234\n",
      "Validation: Epoch [20], Batch [199/938], Loss: 0.33122095465660095\n",
      "Validation: Epoch [20], Batch [200/938], Loss: 0.40653643012046814\n",
      "Validation: Epoch [20], Batch [201/938], Loss: 0.5851185321807861\n",
      "Validation: Epoch [20], Batch [202/938], Loss: 0.30830666422843933\n",
      "Validation: Epoch [20], Batch [203/938], Loss: 0.590142548084259\n",
      "Validation: Epoch [20], Batch [204/938], Loss: 0.5242114663124084\n",
      "Validation: Epoch [20], Batch [205/938], Loss: 0.49480676651000977\n",
      "Validation: Epoch [20], Batch [206/938], Loss: 0.7140610218048096\n",
      "Validation: Epoch [20], Batch [207/938], Loss: 0.5181248784065247\n",
      "Validation: Epoch [20], Batch [208/938], Loss: 0.4157203733921051\n",
      "Validation: Epoch [20], Batch [209/938], Loss: 0.33828797936439514\n",
      "Validation: Epoch [20], Batch [210/938], Loss: 0.5051282644271851\n",
      "Validation: Epoch [20], Batch [211/938], Loss: 0.44678935408592224\n",
      "Validation: Epoch [20], Batch [212/938], Loss: 0.4901237487792969\n",
      "Validation: Epoch [20], Batch [213/938], Loss: 0.39615222811698914\n",
      "Validation: Epoch [20], Batch [214/938], Loss: 0.4358351528644562\n",
      "Validation: Epoch [20], Batch [215/938], Loss: 0.4298975467681885\n",
      "Validation: Epoch [20], Batch [216/938], Loss: 0.4613867998123169\n",
      "Validation: Epoch [20], Batch [217/938], Loss: 0.5694698095321655\n",
      "Validation: Epoch [20], Batch [218/938], Loss: 0.3924831748008728\n",
      "Validation: Epoch [20], Batch [219/938], Loss: 0.5115818381309509\n",
      "Validation: Epoch [20], Batch [220/938], Loss: 0.5700717568397522\n",
      "Validation: Epoch [20], Batch [221/938], Loss: 0.40458768606185913\n",
      "Validation: Epoch [20], Batch [222/938], Loss: 0.5966870784759521\n",
      "Validation: Epoch [20], Batch [223/938], Loss: 0.4234897494316101\n",
      "Validation: Epoch [20], Batch [224/938], Loss: 0.4068412780761719\n",
      "Validation: Epoch [20], Batch [225/938], Loss: 0.4770066738128662\n",
      "Validation: Epoch [20], Batch [226/938], Loss: 0.5939036011695862\n",
      "Validation: Epoch [20], Batch [227/938], Loss: 0.27100035548210144\n",
      "Validation: Epoch [20], Batch [228/938], Loss: 0.46436381340026855\n",
      "Validation: Epoch [20], Batch [229/938], Loss: 0.6536842584609985\n",
      "Validation: Epoch [20], Batch [230/938], Loss: 0.43732893466949463\n",
      "Validation: Epoch [20], Batch [231/938], Loss: 0.2796981632709503\n",
      "Validation: Epoch [20], Batch [232/938], Loss: 0.5016506910324097\n",
      "Validation: Epoch [20], Batch [233/938], Loss: 0.435098797082901\n",
      "Validation: Epoch [20], Batch [234/938], Loss: 0.50133216381073\n",
      "Validation: Epoch [20], Batch [235/938], Loss: 0.7734299898147583\n",
      "Validation: Epoch [20], Batch [236/938], Loss: 0.3685269355773926\n",
      "Validation: Epoch [20], Batch [237/938], Loss: 0.39395207166671753\n",
      "Validation: Epoch [20], Batch [238/938], Loss: 0.3811922073364258\n",
      "Validation: Epoch [20], Batch [239/938], Loss: 0.39271119236946106\n",
      "Validation: Epoch [20], Batch [240/938], Loss: 0.5674535632133484\n",
      "Validation: Epoch [20], Batch [241/938], Loss: 0.37038058042526245\n",
      "Validation: Epoch [20], Batch [242/938], Loss: 0.5122750401496887\n",
      "Validation: Epoch [20], Batch [243/938], Loss: 0.3286646008491516\n",
      "Validation: Epoch [20], Batch [244/938], Loss: 0.42157506942749023\n",
      "Validation: Epoch [20], Batch [245/938], Loss: 0.45890045166015625\n",
      "Validation: Epoch [20], Batch [246/938], Loss: 0.5551177263259888\n",
      "Validation: Epoch [20], Batch [247/938], Loss: 0.49807050824165344\n",
      "Validation: Epoch [20], Batch [248/938], Loss: 0.7028912305831909\n",
      "Validation: Epoch [20], Batch [249/938], Loss: 0.4588645100593567\n",
      "Validation: Epoch [20], Batch [250/938], Loss: 0.41333040595054626\n",
      "Validation: Epoch [20], Batch [251/938], Loss: 0.3748156428337097\n",
      "Validation: Epoch [20], Batch [252/938], Loss: 0.4279544949531555\n",
      "Validation: Epoch [20], Batch [253/938], Loss: 0.48507189750671387\n",
      "Validation: Epoch [20], Batch [254/938], Loss: 0.4199337363243103\n",
      "Validation: Epoch [20], Batch [255/938], Loss: 0.44765016436576843\n",
      "Validation: Epoch [20], Batch [256/938], Loss: 0.3723270297050476\n",
      "Validation: Epoch [20], Batch [257/938], Loss: 0.47628965973854065\n",
      "Validation: Epoch [20], Batch [258/938], Loss: 0.5938180685043335\n",
      "Validation: Epoch [20], Batch [259/938], Loss: 0.25932884216308594\n",
      "Validation: Epoch [20], Batch [260/938], Loss: 0.6281769275665283\n",
      "Validation: Epoch [20], Batch [261/938], Loss: 0.5460389852523804\n",
      "Validation: Epoch [20], Batch [262/938], Loss: 0.4103226661682129\n",
      "Validation: Epoch [20], Batch [263/938], Loss: 0.3753827214241028\n",
      "Validation: Epoch [20], Batch [264/938], Loss: 0.40153199434280396\n",
      "Validation: Epoch [20], Batch [265/938], Loss: 0.504331111907959\n",
      "Validation: Epoch [20], Batch [266/938], Loss: 0.5203937292098999\n",
      "Validation: Epoch [20], Batch [267/938], Loss: 0.5222753286361694\n",
      "Validation: Epoch [20], Batch [268/938], Loss: 0.48286303877830505\n",
      "Validation: Epoch [20], Batch [269/938], Loss: 0.320385217666626\n",
      "Validation: Epoch [20], Batch [270/938], Loss: 0.7634285688400269\n",
      "Validation: Epoch [20], Batch [271/938], Loss: 0.38958147168159485\n",
      "Validation: Epoch [20], Batch [272/938], Loss: 0.5636658072471619\n",
      "Validation: Epoch [20], Batch [273/938], Loss: 0.5770244598388672\n",
      "Validation: Epoch [20], Batch [274/938], Loss: 0.22717589139938354\n",
      "Validation: Epoch [20], Batch [275/938], Loss: 0.4223138988018036\n",
      "Validation: Epoch [20], Batch [276/938], Loss: 0.6768750548362732\n",
      "Validation: Epoch [20], Batch [277/938], Loss: 0.622732400894165\n",
      "Validation: Epoch [20], Batch [278/938], Loss: 0.5537112951278687\n",
      "Validation: Epoch [20], Batch [279/938], Loss: 0.6687109470367432\n",
      "Validation: Epoch [20], Batch [280/938], Loss: 0.585796594619751\n",
      "Validation: Epoch [20], Batch [281/938], Loss: 0.3558874726295471\n",
      "Validation: Epoch [20], Batch [282/938], Loss: 0.6484493613243103\n",
      "Validation: Epoch [20], Batch [283/938], Loss: 0.5727074146270752\n",
      "Validation: Epoch [20], Batch [284/938], Loss: 0.3542361855506897\n",
      "Validation: Epoch [20], Batch [285/938], Loss: 0.512691080570221\n",
      "Validation: Epoch [20], Batch [286/938], Loss: 0.3904932141304016\n",
      "Validation: Epoch [20], Batch [287/938], Loss: 0.2551229000091553\n",
      "Validation: Epoch [20], Batch [288/938], Loss: 0.518170177936554\n",
      "Validation: Epoch [20], Batch [289/938], Loss: 0.2596660554409027\n",
      "Validation: Epoch [20], Batch [290/938], Loss: 0.4696041941642761\n",
      "Validation: Epoch [20], Batch [291/938], Loss: 0.49479517340660095\n",
      "Validation: Epoch [20], Batch [292/938], Loss: 0.2682558298110962\n",
      "Validation: Epoch [20], Batch [293/938], Loss: 0.5191361308097839\n",
      "Validation: Epoch [20], Batch [294/938], Loss: 0.47899213433265686\n",
      "Validation: Epoch [20], Batch [295/938], Loss: 0.4634774625301361\n",
      "Validation: Epoch [20], Batch [296/938], Loss: 0.39399927854537964\n",
      "Validation: Epoch [20], Batch [297/938], Loss: 0.5245126485824585\n",
      "Validation: Epoch [20], Batch [298/938], Loss: 0.33956748247146606\n",
      "Validation: Epoch [20], Batch [299/938], Loss: 0.3892863988876343\n",
      "Validation: Epoch [20], Batch [300/938], Loss: 0.43601250648498535\n",
      "Validation: Epoch [20], Batch [301/938], Loss: 0.36330947279930115\n",
      "Validation: Epoch [20], Batch [302/938], Loss: 0.3639688491821289\n",
      "Validation: Epoch [20], Batch [303/938], Loss: 0.5802230834960938\n",
      "Validation: Epoch [20], Batch [304/938], Loss: 0.39673009514808655\n",
      "Validation: Epoch [20], Batch [305/938], Loss: 0.3620489239692688\n",
      "Validation: Epoch [20], Batch [306/938], Loss: 0.2888634502887726\n",
      "Validation: Epoch [20], Batch [307/938], Loss: 0.4715798795223236\n",
      "Validation: Epoch [20], Batch [308/938], Loss: 0.3694797158241272\n",
      "Validation: Epoch [20], Batch [309/938], Loss: 0.3425785303115845\n",
      "Validation: Epoch [20], Batch [310/938], Loss: 0.4048049747943878\n",
      "Validation: Epoch [20], Batch [311/938], Loss: 0.5798457860946655\n",
      "Validation: Epoch [20], Batch [312/938], Loss: 0.47758060693740845\n",
      "Validation: Epoch [20], Batch [313/938], Loss: 0.4827185869216919\n",
      "Validation: Epoch [20], Batch [314/938], Loss: 0.3773619830608368\n",
      "Validation: Epoch [20], Batch [315/938], Loss: 0.2927131950855255\n",
      "Validation: Epoch [20], Batch [316/938], Loss: 0.3515227138996124\n",
      "Validation: Epoch [20], Batch [317/938], Loss: 0.4997401833534241\n",
      "Validation: Epoch [20], Batch [318/938], Loss: 0.5130715370178223\n",
      "Validation: Epoch [20], Batch [319/938], Loss: 0.3097931444644928\n",
      "Validation: Epoch [20], Batch [320/938], Loss: 0.6473403573036194\n",
      "Validation: Epoch [20], Batch [321/938], Loss: 0.4460546374320984\n",
      "Validation: Epoch [20], Batch [322/938], Loss: 0.38651934266090393\n",
      "Validation: Epoch [20], Batch [323/938], Loss: 0.35863059759140015\n",
      "Validation: Epoch [20], Batch [324/938], Loss: 0.42970627546310425\n",
      "Validation: Epoch [20], Batch [325/938], Loss: 0.3642517030239105\n",
      "Validation: Epoch [20], Batch [326/938], Loss: 0.40190911293029785\n",
      "Validation: Epoch [20], Batch [327/938], Loss: 0.5126701593399048\n",
      "Validation: Epoch [20], Batch [328/938], Loss: 0.27761945128440857\n",
      "Validation: Epoch [20], Batch [329/938], Loss: 0.4909088611602783\n",
      "Validation: Epoch [20], Batch [330/938], Loss: 0.4867088198661804\n",
      "Validation: Epoch [20], Batch [331/938], Loss: 0.44247332215309143\n",
      "Validation: Epoch [20], Batch [332/938], Loss: 0.3784646987915039\n",
      "Validation: Epoch [20], Batch [333/938], Loss: 0.36361223459243774\n",
      "Validation: Epoch [20], Batch [334/938], Loss: 0.4977356493473053\n",
      "Validation: Epoch [20], Batch [335/938], Loss: 0.4502894878387451\n",
      "Validation: Epoch [20], Batch [336/938], Loss: 0.4951687157154083\n",
      "Validation: Epoch [20], Batch [337/938], Loss: 0.3675312101840973\n",
      "Validation: Epoch [20], Batch [338/938], Loss: 0.5181031227111816\n",
      "Validation: Epoch [20], Batch [339/938], Loss: 0.5871545672416687\n",
      "Validation: Epoch [20], Batch [340/938], Loss: 0.5242449641227722\n",
      "Validation: Epoch [20], Batch [341/938], Loss: 0.3359646797180176\n",
      "Validation: Epoch [20], Batch [342/938], Loss: 0.594107985496521\n",
      "Validation: Epoch [20], Batch [343/938], Loss: 0.4312664270401001\n",
      "Validation: Epoch [20], Batch [344/938], Loss: 0.6700289845466614\n",
      "Validation: Epoch [20], Batch [345/938], Loss: 0.4732944369316101\n",
      "Validation: Epoch [20], Batch [346/938], Loss: 0.3643653392791748\n",
      "Validation: Epoch [20], Batch [347/938], Loss: 0.39767417311668396\n",
      "Validation: Epoch [20], Batch [348/938], Loss: 0.591680109500885\n",
      "Validation: Epoch [20], Batch [349/938], Loss: 0.36677661538124084\n",
      "Validation: Epoch [20], Batch [350/938], Loss: 0.339722216129303\n",
      "Validation: Epoch [20], Batch [351/938], Loss: 0.30458781123161316\n",
      "Validation: Epoch [20], Batch [352/938], Loss: 0.6000133156776428\n",
      "Validation: Epoch [20], Batch [353/938], Loss: 0.3385322093963623\n",
      "Validation: Epoch [20], Batch [354/938], Loss: 0.4335557520389557\n",
      "Validation: Epoch [20], Batch [355/938], Loss: 0.6037967801094055\n",
      "Validation: Epoch [20], Batch [356/938], Loss: 0.5476716756820679\n",
      "Validation: Epoch [20], Batch [357/938], Loss: 0.42908167839050293\n",
      "Validation: Epoch [20], Batch [358/938], Loss: 0.39699575304985046\n",
      "Validation: Epoch [20], Batch [359/938], Loss: 0.4722095727920532\n",
      "Validation: Epoch [20], Batch [360/938], Loss: 0.5566969513893127\n",
      "Validation: Epoch [20], Batch [361/938], Loss: 0.32457560300827026\n",
      "Validation: Epoch [20], Batch [362/938], Loss: 0.511120617389679\n",
      "Validation: Epoch [20], Batch [363/938], Loss: 0.4616362154483795\n",
      "Validation: Epoch [20], Batch [364/938], Loss: 0.37679505348205566\n",
      "Validation: Epoch [20], Batch [365/938], Loss: 0.6686961650848389\n",
      "Validation: Epoch [20], Batch [366/938], Loss: 0.5129799246788025\n",
      "Validation: Epoch [20], Batch [367/938], Loss: 0.2860718071460724\n",
      "Validation: Epoch [20], Batch [368/938], Loss: 0.5460690855979919\n",
      "Validation: Epoch [20], Batch [369/938], Loss: 0.4726133644580841\n",
      "Validation: Epoch [20], Batch [370/938], Loss: 0.5780611038208008\n",
      "Validation: Epoch [20], Batch [371/938], Loss: 0.506449282169342\n",
      "Validation: Epoch [20], Batch [372/938], Loss: 0.3278772532939911\n",
      "Validation: Epoch [20], Batch [373/938], Loss: 0.45367804169654846\n",
      "Validation: Epoch [20], Batch [374/938], Loss: 0.3875340521335602\n",
      "Validation: Epoch [20], Batch [375/938], Loss: 0.6327896118164062\n",
      "Validation: Epoch [20], Batch [376/938], Loss: 0.4442167282104492\n",
      "Validation: Epoch [20], Batch [377/938], Loss: 0.4536774158477783\n",
      "Validation: Epoch [20], Batch [378/938], Loss: 0.4189762473106384\n",
      "Validation: Epoch [20], Batch [379/938], Loss: 0.44354262948036194\n",
      "Validation: Epoch [20], Batch [380/938], Loss: 0.5259717702865601\n",
      "Validation: Epoch [20], Batch [381/938], Loss: 0.48249000310897827\n",
      "Validation: Epoch [20], Batch [382/938], Loss: 0.4049002230167389\n",
      "Validation: Epoch [20], Batch [383/938], Loss: 0.3350721299648285\n",
      "Validation: Epoch [20], Batch [384/938], Loss: 0.3524759113788605\n",
      "Validation: Epoch [20], Batch [385/938], Loss: 0.3674221932888031\n",
      "Validation: Epoch [20], Batch [386/938], Loss: 0.4525092542171478\n",
      "Validation: Epoch [20], Batch [387/938], Loss: 0.6368933320045471\n",
      "Validation: Epoch [20], Batch [388/938], Loss: 0.4460718333721161\n",
      "Validation: Epoch [20], Batch [389/938], Loss: 0.35591256618499756\n",
      "Validation: Epoch [20], Batch [390/938], Loss: 0.42176419496536255\n",
      "Validation: Epoch [20], Batch [391/938], Loss: 0.524753212928772\n",
      "Validation: Epoch [20], Batch [392/938], Loss: 0.3082491159439087\n",
      "Validation: Epoch [20], Batch [393/938], Loss: 0.5304638147354126\n",
      "Validation: Epoch [20], Batch [394/938], Loss: 0.4647926688194275\n",
      "Validation: Epoch [20], Batch [395/938], Loss: 0.4631527364253998\n",
      "Validation: Epoch [20], Batch [396/938], Loss: 0.4084215760231018\n",
      "Validation: Epoch [20], Batch [397/938], Loss: 0.6823352575302124\n",
      "Validation: Epoch [20], Batch [398/938], Loss: 0.470880389213562\n",
      "Validation: Epoch [20], Batch [399/938], Loss: 0.31942668557167053\n",
      "Validation: Epoch [20], Batch [400/938], Loss: 0.7245708703994751\n",
      "Validation: Epoch [20], Batch [401/938], Loss: 0.35926735401153564\n",
      "Validation: Epoch [20], Batch [402/938], Loss: 0.30734753608703613\n",
      "Validation: Epoch [20], Batch [403/938], Loss: 0.42362669110298157\n",
      "Validation: Epoch [20], Batch [404/938], Loss: 0.35536980628967285\n",
      "Validation: Epoch [20], Batch [405/938], Loss: 0.3731318712234497\n",
      "Validation: Epoch [20], Batch [406/938], Loss: 0.265239953994751\n",
      "Validation: Epoch [20], Batch [407/938], Loss: 0.4244997501373291\n",
      "Validation: Epoch [20], Batch [408/938], Loss: 0.4893471300601959\n",
      "Validation: Epoch [20], Batch [409/938], Loss: 0.4087216556072235\n",
      "Validation: Epoch [20], Batch [410/938], Loss: 0.3325793743133545\n",
      "Validation: Epoch [20], Batch [411/938], Loss: 0.5490196347236633\n",
      "Validation: Epoch [20], Batch [412/938], Loss: 0.3842574954032898\n",
      "Validation: Epoch [20], Batch [413/938], Loss: 0.3188963830471039\n",
      "Validation: Epoch [20], Batch [414/938], Loss: 0.40264153480529785\n",
      "Validation: Epoch [20], Batch [415/938], Loss: 0.42816704511642456\n",
      "Validation: Epoch [20], Batch [416/938], Loss: 0.7283427715301514\n",
      "Validation: Epoch [20], Batch [417/938], Loss: 0.4134203791618347\n",
      "Validation: Epoch [20], Batch [418/938], Loss: 0.2658405005931854\n",
      "Validation: Epoch [20], Batch [419/938], Loss: 0.3643345832824707\n",
      "Validation: Epoch [20], Batch [420/938], Loss: 0.4162452220916748\n",
      "Validation: Epoch [20], Batch [421/938], Loss: 0.36616721749305725\n",
      "Validation: Epoch [20], Batch [422/938], Loss: 0.7014820575714111\n",
      "Validation: Epoch [20], Batch [423/938], Loss: 0.5346224308013916\n",
      "Validation: Epoch [20], Batch [424/938], Loss: 0.5021635890007019\n",
      "Validation: Epoch [20], Batch [425/938], Loss: 0.5768750905990601\n",
      "Validation: Epoch [20], Batch [426/938], Loss: 0.446638822555542\n",
      "Validation: Epoch [20], Batch [427/938], Loss: 0.50074303150177\n",
      "Validation: Epoch [20], Batch [428/938], Loss: 0.4027365446090698\n",
      "Validation: Epoch [20], Batch [429/938], Loss: 0.3337358832359314\n",
      "Validation: Epoch [20], Batch [430/938], Loss: 0.34952232241630554\n",
      "Validation: Epoch [20], Batch [431/938], Loss: 0.3684391379356384\n",
      "Validation: Epoch [20], Batch [432/938], Loss: 0.4852677583694458\n",
      "Validation: Epoch [20], Batch [433/938], Loss: 0.6173407435417175\n",
      "Validation: Epoch [20], Batch [434/938], Loss: 0.39913371205329895\n",
      "Validation: Epoch [20], Batch [435/938], Loss: 0.474054217338562\n",
      "Validation: Epoch [20], Batch [436/938], Loss: 0.44994068145751953\n",
      "Validation: Epoch [20], Batch [437/938], Loss: 0.5202486515045166\n",
      "Validation: Epoch [20], Batch [438/938], Loss: 0.4522591531276703\n",
      "Validation: Epoch [20], Batch [439/938], Loss: 0.5443716645240784\n",
      "Validation: Epoch [20], Batch [440/938], Loss: 0.6150512099266052\n",
      "Validation: Epoch [20], Batch [441/938], Loss: 0.352567583322525\n",
      "Validation: Epoch [20], Batch [442/938], Loss: 0.4703606367111206\n",
      "Validation: Epoch [20], Batch [443/938], Loss: 0.5974406003952026\n",
      "Validation: Epoch [20], Batch [444/938], Loss: 0.46250808238983154\n",
      "Validation: Epoch [20], Batch [445/938], Loss: 0.39461570978164673\n",
      "Validation: Epoch [20], Batch [446/938], Loss: 0.35195645689964294\n",
      "Validation: Epoch [20], Batch [447/938], Loss: 0.2723172903060913\n",
      "Validation: Epoch [20], Batch [448/938], Loss: 0.5792162418365479\n",
      "Validation: Epoch [20], Batch [449/938], Loss: 0.27420127391815186\n",
      "Validation: Epoch [20], Batch [450/938], Loss: 0.5221956372261047\n",
      "Validation: Epoch [20], Batch [451/938], Loss: 0.6431127786636353\n",
      "Validation: Epoch [20], Batch [452/938], Loss: 0.26664814352989197\n",
      "Validation: Epoch [20], Batch [453/938], Loss: 0.46798476576805115\n",
      "Validation: Epoch [20], Batch [454/938], Loss: 0.4139535129070282\n",
      "Validation: Epoch [20], Batch [455/938], Loss: 0.3371407091617584\n",
      "Validation: Epoch [20], Batch [456/938], Loss: 0.47121092677116394\n",
      "Validation: Epoch [20], Batch [457/938], Loss: 0.37405169010162354\n",
      "Validation: Epoch [20], Batch [458/938], Loss: 0.28710445761680603\n",
      "Validation: Epoch [20], Batch [459/938], Loss: 0.4657018184661865\n",
      "Validation: Epoch [20], Batch [460/938], Loss: 0.7372940182685852\n",
      "Validation: Epoch [20], Batch [461/938], Loss: 0.42011865973472595\n",
      "Validation: Epoch [20], Batch [462/938], Loss: 0.5093629956245422\n",
      "Validation: Epoch [20], Batch [463/938], Loss: 0.5528137683868408\n",
      "Validation: Epoch [20], Batch [464/938], Loss: 0.4547290503978729\n",
      "Validation: Epoch [20], Batch [465/938], Loss: 0.45958051085472107\n",
      "Validation: Epoch [20], Batch [466/938], Loss: 0.6520764231681824\n",
      "Validation: Epoch [20], Batch [467/938], Loss: 0.4861326813697815\n",
      "Validation: Epoch [20], Batch [468/938], Loss: 0.3364163041114807\n",
      "Validation: Epoch [20], Batch [469/938], Loss: 0.5204244256019592\n",
      "Validation: Epoch [20], Batch [470/938], Loss: 0.6563632488250732\n",
      "Validation: Epoch [20], Batch [471/938], Loss: 0.5427084565162659\n",
      "Validation: Epoch [20], Batch [472/938], Loss: 0.6209723353385925\n",
      "Validation: Epoch [20], Batch [473/938], Loss: 0.38090038299560547\n",
      "Validation: Epoch [20], Batch [474/938], Loss: 0.4924168586730957\n",
      "Validation: Epoch [20], Batch [475/938], Loss: 0.5647003650665283\n",
      "Validation: Epoch [20], Batch [476/938], Loss: 0.41812384128570557\n",
      "Validation: Epoch [20], Batch [477/938], Loss: 0.4714733064174652\n",
      "Validation: Epoch [20], Batch [478/938], Loss: 0.31677454710006714\n",
      "Validation: Epoch [20], Batch [479/938], Loss: 0.36710214614868164\n",
      "Validation: Epoch [20], Batch [480/938], Loss: 0.3324899673461914\n",
      "Validation: Epoch [20], Batch [481/938], Loss: 0.421610027551651\n",
      "Validation: Epoch [20], Batch [482/938], Loss: 0.44784849882125854\n",
      "Validation: Epoch [20], Batch [483/938], Loss: 0.48591476678848267\n",
      "Validation: Epoch [20], Batch [484/938], Loss: 0.32706642150878906\n",
      "Validation: Epoch [20], Batch [485/938], Loss: 0.5156987309455872\n",
      "Validation: Epoch [20], Batch [486/938], Loss: 0.534869372844696\n",
      "Validation: Epoch [20], Batch [487/938], Loss: 0.6578139066696167\n",
      "Validation: Epoch [20], Batch [488/938], Loss: 0.30723610520362854\n",
      "Validation: Epoch [20], Batch [489/938], Loss: 0.5158430337905884\n",
      "Validation: Epoch [20], Batch [490/938], Loss: 0.565313458442688\n",
      "Validation: Epoch [20], Batch [491/938], Loss: 0.6303608417510986\n",
      "Validation: Epoch [20], Batch [492/938], Loss: 0.5277014970779419\n",
      "Validation: Epoch [20], Batch [493/938], Loss: 0.39260759949684143\n",
      "Validation: Epoch [20], Batch [494/938], Loss: 0.3536272943019867\n",
      "Validation: Epoch [20], Batch [495/938], Loss: 0.39512190222740173\n",
      "Validation: Epoch [20], Batch [496/938], Loss: 0.4626469910144806\n",
      "Validation: Epoch [20], Batch [497/938], Loss: 0.37224194407463074\n",
      "Validation: Epoch [20], Batch [498/938], Loss: 0.3528171181678772\n",
      "Validation: Epoch [20], Batch [499/938], Loss: 0.45067811012268066\n",
      "Validation: Epoch [20], Batch [500/938], Loss: 0.42236119508743286\n",
      "Validation: Epoch [20], Batch [501/938], Loss: 0.5286325812339783\n",
      "Validation: Epoch [20], Batch [502/938], Loss: 0.4685755968093872\n",
      "Validation: Epoch [20], Batch [503/938], Loss: 0.36956721544265747\n",
      "Validation: Epoch [20], Batch [504/938], Loss: 0.5431404113769531\n",
      "Validation: Epoch [20], Batch [505/938], Loss: 0.7262546420097351\n",
      "Validation: Epoch [20], Batch [506/938], Loss: 0.45866522192955017\n",
      "Validation: Epoch [20], Batch [507/938], Loss: 0.297505259513855\n",
      "Validation: Epoch [20], Batch [508/938], Loss: 0.5360904932022095\n",
      "Validation: Epoch [20], Batch [509/938], Loss: 0.24452534317970276\n",
      "Validation: Epoch [20], Batch [510/938], Loss: 0.3666648864746094\n",
      "Validation: Epoch [20], Batch [511/938], Loss: 0.6200512647628784\n",
      "Validation: Epoch [20], Batch [512/938], Loss: 0.6995672583580017\n",
      "Validation: Epoch [20], Batch [513/938], Loss: 0.5534533858299255\n",
      "Validation: Epoch [20], Batch [514/938], Loss: 0.5381704568862915\n",
      "Validation: Epoch [20], Batch [515/938], Loss: 0.6236331462860107\n",
      "Validation: Epoch [20], Batch [516/938], Loss: 0.4708824157714844\n",
      "Validation: Epoch [20], Batch [517/938], Loss: 0.3233645260334015\n",
      "Validation: Epoch [20], Batch [518/938], Loss: 0.5394718647003174\n",
      "Validation: Epoch [20], Batch [519/938], Loss: 0.5529025793075562\n",
      "Validation: Epoch [20], Batch [520/938], Loss: 0.44014373421669006\n",
      "Validation: Epoch [20], Batch [521/938], Loss: 0.48679855465888977\n",
      "Validation: Epoch [20], Batch [522/938], Loss: 0.4830385744571686\n",
      "Validation: Epoch [20], Batch [523/938], Loss: 0.4507159888744354\n",
      "Validation: Epoch [20], Batch [524/938], Loss: 0.3886216878890991\n",
      "Validation: Epoch [20], Batch [525/938], Loss: 0.5069878697395325\n",
      "Validation: Epoch [20], Batch [526/938], Loss: 0.3555852472782135\n",
      "Validation: Epoch [20], Batch [527/938], Loss: 0.44372162222862244\n",
      "Validation: Epoch [20], Batch [528/938], Loss: 0.6151079535484314\n",
      "Validation: Epoch [20], Batch [529/938], Loss: 0.524106502532959\n",
      "Validation: Epoch [20], Batch [530/938], Loss: 0.2555428445339203\n",
      "Validation: Epoch [20], Batch [531/938], Loss: 0.4145927429199219\n",
      "Validation: Epoch [20], Batch [532/938], Loss: 0.24504181742668152\n",
      "Validation: Epoch [20], Batch [533/938], Loss: 0.3420131206512451\n",
      "Validation: Epoch [20], Batch [534/938], Loss: 0.44730013608932495\n",
      "Validation: Epoch [20], Batch [535/938], Loss: 0.5145158767700195\n",
      "Validation: Epoch [20], Batch [536/938], Loss: 0.5324826836585999\n",
      "Validation: Epoch [20], Batch [537/938], Loss: 0.37765878438949585\n",
      "Validation: Epoch [20], Batch [538/938], Loss: 0.37564337253570557\n",
      "Validation: Epoch [20], Batch [539/938], Loss: 0.3809215724468231\n",
      "Validation: Epoch [20], Batch [540/938], Loss: 0.5458073019981384\n",
      "Validation: Epoch [20], Batch [541/938], Loss: 0.3718601167201996\n",
      "Validation: Epoch [20], Batch [542/938], Loss: 0.38903653621673584\n",
      "Validation: Epoch [20], Batch [543/938], Loss: 0.45348167419433594\n",
      "Validation: Epoch [20], Batch [544/938], Loss: 0.526175856590271\n",
      "Validation: Epoch [20], Batch [545/938], Loss: 0.24041779339313507\n",
      "Validation: Epoch [20], Batch [546/938], Loss: 0.4615071415901184\n",
      "Validation: Epoch [20], Batch [547/938], Loss: 0.713778018951416\n",
      "Validation: Epoch [20], Batch [548/938], Loss: 0.5145001411437988\n",
      "Validation: Epoch [20], Batch [549/938], Loss: 0.410209983587265\n",
      "Validation: Epoch [20], Batch [550/938], Loss: 0.3582117557525635\n",
      "Validation: Epoch [20], Batch [551/938], Loss: 0.4750090539455414\n",
      "Validation: Epoch [20], Batch [552/938], Loss: 0.3052937090396881\n",
      "Validation: Epoch [20], Batch [553/938], Loss: 0.4233115613460541\n",
      "Validation: Epoch [20], Batch [554/938], Loss: 0.7150234580039978\n",
      "Validation: Epoch [20], Batch [555/938], Loss: 0.3135865330696106\n",
      "Validation: Epoch [20], Batch [556/938], Loss: 0.7562662959098816\n",
      "Validation: Epoch [20], Batch [557/938], Loss: 0.5526342988014221\n",
      "Validation: Epoch [20], Batch [558/938], Loss: 0.4593716263771057\n",
      "Validation: Epoch [20], Batch [559/938], Loss: 0.4398657977581024\n",
      "Validation: Epoch [20], Batch [560/938], Loss: 0.3391396403312683\n",
      "Validation: Epoch [20], Batch [561/938], Loss: 0.256305068731308\n",
      "Validation: Epoch [20], Batch [562/938], Loss: 0.5196398496627808\n",
      "Validation: Epoch [20], Batch [563/938], Loss: 0.4185672998428345\n",
      "Validation: Epoch [20], Batch [564/938], Loss: 0.583579421043396\n",
      "Validation: Epoch [20], Batch [565/938], Loss: 0.4166792035102844\n",
      "Validation: Epoch [20], Batch [566/938], Loss: 0.41220423579216003\n",
      "Validation: Epoch [20], Batch [567/938], Loss: 0.25612056255340576\n",
      "Validation: Epoch [20], Batch [568/938], Loss: 0.36781662702560425\n",
      "Validation: Epoch [20], Batch [569/938], Loss: 0.43113285303115845\n",
      "Validation: Epoch [20], Batch [570/938], Loss: 0.5432397723197937\n",
      "Validation: Epoch [20], Batch [571/938], Loss: 0.418422669172287\n",
      "Validation: Epoch [20], Batch [572/938], Loss: 0.33708739280700684\n",
      "Validation: Epoch [20], Batch [573/938], Loss: 0.4799831211566925\n",
      "Validation: Epoch [20], Batch [574/938], Loss: 0.42185521125793457\n",
      "Validation: Epoch [20], Batch [575/938], Loss: 0.3758428692817688\n",
      "Validation: Epoch [20], Batch [576/938], Loss: 0.5072584748268127\n",
      "Validation: Epoch [20], Batch [577/938], Loss: 0.2420579493045807\n",
      "Validation: Epoch [20], Batch [578/938], Loss: 0.32816973328590393\n",
      "Validation: Epoch [20], Batch [579/938], Loss: 0.530004620552063\n",
      "Validation: Epoch [20], Batch [580/938], Loss: 0.4398781657218933\n",
      "Validation: Epoch [20], Batch [581/938], Loss: 0.45204776525497437\n",
      "Validation: Epoch [20], Batch [582/938], Loss: 0.30793413519859314\n",
      "Validation: Epoch [20], Batch [583/938], Loss: 0.4754791855812073\n",
      "Validation: Epoch [20], Batch [584/938], Loss: 0.6422017812728882\n",
      "Validation: Epoch [20], Batch [585/938], Loss: 0.3080236613750458\n",
      "Validation: Epoch [20], Batch [586/938], Loss: 0.634721577167511\n",
      "Validation: Epoch [20], Batch [587/938], Loss: 0.4505060017108917\n",
      "Validation: Epoch [20], Batch [588/938], Loss: 0.37047114968299866\n",
      "Validation: Epoch [20], Batch [589/938], Loss: 0.5274831652641296\n",
      "Validation: Epoch [20], Batch [590/938], Loss: 0.2637213170528412\n",
      "Validation: Epoch [20], Batch [591/938], Loss: 0.4518986940383911\n",
      "Validation: Epoch [20], Batch [592/938], Loss: 0.42555132508277893\n",
      "Validation: Epoch [20], Batch [593/938], Loss: 0.5475034713745117\n",
      "Validation: Epoch [20], Batch [594/938], Loss: 0.36227351427078247\n",
      "Validation: Epoch [20], Batch [595/938], Loss: 0.5357912182807922\n",
      "Validation: Epoch [20], Batch [596/938], Loss: 0.39136070013046265\n",
      "Validation: Epoch [20], Batch [597/938], Loss: 0.49247148633003235\n",
      "Validation: Epoch [20], Batch [598/938], Loss: 0.5174968242645264\n",
      "Validation: Epoch [20], Batch [599/938], Loss: 0.3869991600513458\n",
      "Validation: Epoch [20], Batch [600/938], Loss: 0.4719181954860687\n",
      "Validation: Epoch [20], Batch [601/938], Loss: 0.6530555486679077\n",
      "Validation: Epoch [20], Batch [602/938], Loss: 0.42953991889953613\n",
      "Validation: Epoch [20], Batch [603/938], Loss: 0.5419970154762268\n",
      "Validation: Epoch [20], Batch [604/938], Loss: 0.5076746940612793\n",
      "Validation: Epoch [20], Batch [605/938], Loss: 0.3089669942855835\n",
      "Validation: Epoch [20], Batch [606/938], Loss: 0.4019128084182739\n",
      "Validation: Epoch [20], Batch [607/938], Loss: 0.4467422068119049\n",
      "Validation: Epoch [20], Batch [608/938], Loss: 0.22514541447162628\n",
      "Validation: Epoch [20], Batch [609/938], Loss: 0.41205576062202454\n",
      "Validation: Epoch [20], Batch [610/938], Loss: 0.4157518148422241\n",
      "Validation: Epoch [20], Batch [611/938], Loss: 0.44563838839530945\n",
      "Validation: Epoch [20], Batch [612/938], Loss: 0.7018515467643738\n",
      "Validation: Epoch [20], Batch [613/938], Loss: 0.3924826681613922\n",
      "Validation: Epoch [20], Batch [614/938], Loss: 0.516951858997345\n",
      "Validation: Epoch [20], Batch [615/938], Loss: 0.33451324701309204\n",
      "Validation: Epoch [20], Batch [616/938], Loss: 0.372237890958786\n",
      "Validation: Epoch [20], Batch [617/938], Loss: 0.5198327898979187\n",
      "Validation: Epoch [20], Batch [618/938], Loss: 0.4776385426521301\n",
      "Validation: Epoch [20], Batch [619/938], Loss: 0.4294094443321228\n",
      "Validation: Epoch [20], Batch [620/938], Loss: 0.35959094762802124\n",
      "Validation: Epoch [20], Batch [621/938], Loss: 0.4034317135810852\n",
      "Validation: Epoch [20], Batch [622/938], Loss: 0.2937595844268799\n",
      "Validation: Epoch [20], Batch [623/938], Loss: 0.4281385540962219\n",
      "Validation: Epoch [20], Batch [624/938], Loss: 0.3596650958061218\n",
      "Validation: Epoch [20], Batch [625/938], Loss: 0.35819071531295776\n",
      "Validation: Epoch [20], Batch [626/938], Loss: 0.48050862550735474\n",
      "Validation: Epoch [20], Batch [627/938], Loss: 0.46610456705093384\n",
      "Validation: Epoch [20], Batch [628/938], Loss: 0.3996312618255615\n",
      "Validation: Epoch [20], Batch [629/938], Loss: 0.34775614738464355\n",
      "Validation: Epoch [20], Batch [630/938], Loss: 0.7066391110420227\n",
      "Validation: Epoch [20], Batch [631/938], Loss: 0.4423867166042328\n",
      "Validation: Epoch [20], Batch [632/938], Loss: 0.5011916160583496\n",
      "Validation: Epoch [20], Batch [633/938], Loss: 0.45448607206344604\n",
      "Validation: Epoch [20], Batch [634/938], Loss: 0.5873082876205444\n",
      "Validation: Epoch [20], Batch [635/938], Loss: 0.5127776265144348\n",
      "Validation: Epoch [20], Batch [636/938], Loss: 0.5548509359359741\n",
      "Validation: Epoch [20], Batch [637/938], Loss: 0.34129899740219116\n",
      "Validation: Epoch [20], Batch [638/938], Loss: 0.6588008403778076\n",
      "Validation: Epoch [20], Batch [639/938], Loss: 0.415693998336792\n",
      "Validation: Epoch [20], Batch [640/938], Loss: 0.4292133152484894\n",
      "Validation: Epoch [20], Batch [641/938], Loss: 0.4784168004989624\n",
      "Validation: Epoch [20], Batch [642/938], Loss: 0.4075045883655548\n",
      "Validation: Epoch [20], Batch [643/938], Loss: 0.444659560918808\n",
      "Validation: Epoch [20], Batch [644/938], Loss: 0.9378429651260376\n",
      "Validation: Epoch [20], Batch [645/938], Loss: 0.4953228831291199\n",
      "Validation: Epoch [20], Batch [646/938], Loss: 0.6535561680793762\n",
      "Validation: Epoch [20], Batch [647/938], Loss: 0.32168301939964294\n",
      "Validation: Epoch [20], Batch [648/938], Loss: 0.34312841296195984\n",
      "Validation: Epoch [20], Batch [649/938], Loss: 0.2838394045829773\n",
      "Validation: Epoch [20], Batch [650/938], Loss: 0.36106520891189575\n",
      "Validation: Epoch [20], Batch [651/938], Loss: 0.4918604791164398\n",
      "Validation: Epoch [20], Batch [652/938], Loss: 0.4830579161643982\n",
      "Validation: Epoch [20], Batch [653/938], Loss: 0.318916380405426\n",
      "Validation: Epoch [20], Batch [654/938], Loss: 0.3802659511566162\n",
      "Validation: Epoch [20], Batch [655/938], Loss: 0.5819190144538879\n",
      "Validation: Epoch [20], Batch [656/938], Loss: 0.5470111966133118\n",
      "Validation: Epoch [20], Batch [657/938], Loss: 0.4306783974170685\n",
      "Validation: Epoch [20], Batch [658/938], Loss: 0.4256782531738281\n",
      "Validation: Epoch [20], Batch [659/938], Loss: 0.4813602566719055\n",
      "Validation: Epoch [20], Batch [660/938], Loss: 0.3223404884338379\n",
      "Validation: Epoch [20], Batch [661/938], Loss: 0.5957212448120117\n",
      "Validation: Epoch [20], Batch [662/938], Loss: 0.499794602394104\n",
      "Validation: Epoch [20], Batch [663/938], Loss: 0.4174516499042511\n",
      "Validation: Epoch [20], Batch [664/938], Loss: 0.4784935712814331\n",
      "Validation: Epoch [20], Batch [665/938], Loss: 0.34707966446876526\n",
      "Validation: Epoch [20], Batch [666/938], Loss: 0.3025699555873871\n",
      "Validation: Epoch [20], Batch [667/938], Loss: 0.3563639521598816\n",
      "Validation: Epoch [20], Batch [668/938], Loss: 0.4544842541217804\n",
      "Validation: Epoch [20], Batch [669/938], Loss: 0.5120756030082703\n",
      "Validation: Epoch [20], Batch [670/938], Loss: 0.4067474901676178\n",
      "Validation: Epoch [20], Batch [671/938], Loss: 0.5460928678512573\n",
      "Validation: Epoch [20], Batch [672/938], Loss: 0.7048148512840271\n",
      "Validation: Epoch [20], Batch [673/938], Loss: 0.35557234287261963\n",
      "Validation: Epoch [20], Batch [674/938], Loss: 0.40028923749923706\n",
      "Validation: Epoch [20], Batch [675/938], Loss: 0.5690562129020691\n",
      "Validation: Epoch [20], Batch [676/938], Loss: 0.3789139688014984\n",
      "Validation: Epoch [20], Batch [677/938], Loss: 0.5869444608688354\n",
      "Validation: Epoch [20], Batch [678/938], Loss: 0.5253827571868896\n",
      "Validation: Epoch [20], Batch [679/938], Loss: 0.34350818395614624\n",
      "Validation: Epoch [20], Batch [680/938], Loss: 0.4796607494354248\n",
      "Validation: Epoch [20], Batch [681/938], Loss: 0.5167213678359985\n",
      "Validation: Epoch [20], Batch [682/938], Loss: 0.49663245677948\n",
      "Validation: Epoch [20], Batch [683/938], Loss: 0.2818259000778198\n",
      "Validation: Epoch [20], Batch [684/938], Loss: 0.4446335732936859\n",
      "Validation: Epoch [20], Batch [685/938], Loss: 0.4508456587791443\n",
      "Validation: Epoch [20], Batch [686/938], Loss: 0.5387301445007324\n",
      "Validation: Epoch [20], Batch [687/938], Loss: 0.26406338810920715\n",
      "Validation: Epoch [20], Batch [688/938], Loss: 0.46939873695373535\n",
      "Validation: Epoch [20], Batch [689/938], Loss: 0.5002371072769165\n",
      "Validation: Epoch [20], Batch [690/938], Loss: 0.5433448553085327\n",
      "Validation: Epoch [20], Batch [691/938], Loss: 0.2883678674697876\n",
      "Validation: Epoch [20], Batch [692/938], Loss: 0.528459906578064\n",
      "Validation: Epoch [20], Batch [693/938], Loss: 0.4071263372898102\n",
      "Validation: Epoch [20], Batch [694/938], Loss: 0.4196910262107849\n",
      "Validation: Epoch [20], Batch [695/938], Loss: 0.3042025566101074\n",
      "Validation: Epoch [20], Batch [696/938], Loss: 0.47977787256240845\n",
      "Validation: Epoch [20], Batch [697/938], Loss: 0.41099268198013306\n",
      "Validation: Epoch [20], Batch [698/938], Loss: 0.40788576006889343\n",
      "Validation: Epoch [20], Batch [699/938], Loss: 0.7012911438941956\n",
      "Validation: Epoch [20], Batch [700/938], Loss: 0.38952401280403137\n",
      "Validation: Epoch [20], Batch [701/938], Loss: 0.5103093981742859\n",
      "Validation: Epoch [20], Batch [702/938], Loss: 0.3951328694820404\n",
      "Validation: Epoch [20], Batch [703/938], Loss: 0.3411821126937866\n",
      "Validation: Epoch [20], Batch [704/938], Loss: 0.44007956981658936\n",
      "Validation: Epoch [20], Batch [705/938], Loss: 0.6279406547546387\n",
      "Validation: Epoch [20], Batch [706/938], Loss: 0.5480162501335144\n",
      "Validation: Epoch [20], Batch [707/938], Loss: 0.44267353415489197\n",
      "Validation: Epoch [20], Batch [708/938], Loss: 0.4321531057357788\n",
      "Validation: Epoch [20], Batch [709/938], Loss: 0.26492297649383545\n",
      "Validation: Epoch [20], Batch [710/938], Loss: 0.5335673093795776\n",
      "Validation: Epoch [20], Batch [711/938], Loss: 0.5144235491752625\n",
      "Validation: Epoch [20], Batch [712/938], Loss: 0.2895602881908417\n",
      "Validation: Epoch [20], Batch [713/938], Loss: 0.5574811697006226\n",
      "Validation: Epoch [20], Batch [714/938], Loss: 0.44767117500305176\n",
      "Validation: Epoch [20], Batch [715/938], Loss: 0.5225391983985901\n",
      "Validation: Epoch [20], Batch [716/938], Loss: 0.5795966982841492\n",
      "Validation: Epoch [20], Batch [717/938], Loss: 0.5035717487335205\n",
      "Validation: Epoch [20], Batch [718/938], Loss: 0.47003352642059326\n",
      "Validation: Epoch [20], Batch [719/938], Loss: 0.3883386552333832\n",
      "Validation: Epoch [20], Batch [720/938], Loss: 0.6282239556312561\n",
      "Validation: Epoch [20], Batch [721/938], Loss: 0.5232217311859131\n",
      "Validation: Epoch [20], Batch [722/938], Loss: 0.40813198685646057\n",
      "Validation: Epoch [20], Batch [723/938], Loss: 0.4442536234855652\n",
      "Validation: Epoch [20], Batch [724/938], Loss: 0.3868948221206665\n",
      "Validation: Epoch [20], Batch [725/938], Loss: 0.39331209659576416\n",
      "Validation: Epoch [20], Batch [726/938], Loss: 0.4314229190349579\n",
      "Validation: Epoch [20], Batch [727/938], Loss: 0.3196842670440674\n",
      "Validation: Epoch [20], Batch [728/938], Loss: 0.4931073784828186\n",
      "Validation: Epoch [20], Batch [729/938], Loss: 0.5379992127418518\n",
      "Validation: Epoch [20], Batch [730/938], Loss: 0.5336136221885681\n",
      "Validation: Epoch [20], Batch [731/938], Loss: 0.5389844179153442\n",
      "Validation: Epoch [20], Batch [732/938], Loss: 0.37365585565567017\n",
      "Validation: Epoch [20], Batch [733/938], Loss: 0.3651407063007355\n",
      "Validation: Epoch [20], Batch [734/938], Loss: 0.47069647908210754\n",
      "Validation: Epoch [20], Batch [735/938], Loss: 0.29559898376464844\n",
      "Validation: Epoch [20], Batch [736/938], Loss: 0.415914922952652\n",
      "Validation: Epoch [20], Batch [737/938], Loss: 0.40657222270965576\n",
      "Validation: Epoch [20], Batch [738/938], Loss: 0.6220113635063171\n",
      "Validation: Epoch [20], Batch [739/938], Loss: 0.364703506231308\n",
      "Validation: Epoch [20], Batch [740/938], Loss: 0.38156864047050476\n",
      "Validation: Epoch [20], Batch [741/938], Loss: 0.44835686683654785\n",
      "Validation: Epoch [20], Batch [742/938], Loss: 0.31738582253456116\n",
      "Validation: Epoch [20], Batch [743/938], Loss: 0.3531133830547333\n",
      "Validation: Epoch [20], Batch [744/938], Loss: 0.41467875242233276\n",
      "Validation: Epoch [20], Batch [745/938], Loss: 0.5018184185028076\n",
      "Validation: Epoch [20], Batch [746/938], Loss: 0.46488094329833984\n",
      "Validation: Epoch [20], Batch [747/938], Loss: 0.41426947712898254\n",
      "Validation: Epoch [20], Batch [748/938], Loss: 0.574048638343811\n",
      "Validation: Epoch [20], Batch [749/938], Loss: 0.4230983555316925\n",
      "Validation: Epoch [20], Batch [750/938], Loss: 0.3902025818824768\n",
      "Validation: Epoch [20], Batch [751/938], Loss: 0.4577830135822296\n",
      "Validation: Epoch [20], Batch [752/938], Loss: 0.3603377044200897\n",
      "Validation: Epoch [20], Batch [753/938], Loss: 0.3184818625450134\n",
      "Validation: Epoch [20], Batch [754/938], Loss: 0.5387144088745117\n",
      "Validation: Epoch [20], Batch [755/938], Loss: 0.4257674217224121\n",
      "Validation: Epoch [20], Batch [756/938], Loss: 0.482637882232666\n",
      "Validation: Epoch [20], Batch [757/938], Loss: 0.6310955286026001\n",
      "Validation: Epoch [20], Batch [758/938], Loss: 0.4499104619026184\n",
      "Validation: Epoch [20], Batch [759/938], Loss: 0.27054065465927124\n",
      "Validation: Epoch [20], Batch [760/938], Loss: 0.3691270351409912\n",
      "Validation: Epoch [20], Batch [761/938], Loss: 0.4791713356971741\n",
      "Validation: Epoch [20], Batch [762/938], Loss: 0.31984925270080566\n",
      "Validation: Epoch [20], Batch [763/938], Loss: 0.5159100294113159\n",
      "Validation: Epoch [20], Batch [764/938], Loss: 0.40201711654663086\n",
      "Validation: Epoch [20], Batch [765/938], Loss: 0.5032926797866821\n",
      "Validation: Epoch [20], Batch [766/938], Loss: 0.39649316668510437\n",
      "Validation: Epoch [20], Batch [767/938], Loss: 0.46941065788269043\n",
      "Validation: Epoch [20], Batch [768/938], Loss: 0.35568851232528687\n",
      "Validation: Epoch [20], Batch [769/938], Loss: 0.4628934860229492\n",
      "Validation: Epoch [20], Batch [770/938], Loss: 0.6531562209129333\n",
      "Validation: Epoch [20], Batch [771/938], Loss: 0.401956707239151\n",
      "Validation: Epoch [20], Batch [772/938], Loss: 0.3996196985244751\n",
      "Validation: Epoch [20], Batch [773/938], Loss: 0.40897685289382935\n",
      "Validation: Epoch [20], Batch [774/938], Loss: 0.4002988040447235\n",
      "Validation: Epoch [20], Batch [775/938], Loss: 0.36392858624458313\n",
      "Validation: Epoch [20], Batch [776/938], Loss: 0.4872032403945923\n",
      "Validation: Epoch [20], Batch [777/938], Loss: 0.386040061712265\n",
      "Validation: Epoch [20], Batch [778/938], Loss: 0.47983071208000183\n",
      "Validation: Epoch [20], Batch [779/938], Loss: 0.5051378607749939\n",
      "Validation: Epoch [20], Batch [780/938], Loss: 0.4235052168369293\n",
      "Validation: Epoch [20], Batch [781/938], Loss: 0.4598624110221863\n",
      "Validation: Epoch [20], Batch [782/938], Loss: 0.5051286816596985\n",
      "Validation: Epoch [20], Batch [783/938], Loss: 0.4854080080986023\n",
      "Validation: Epoch [20], Batch [784/938], Loss: 0.5754786729812622\n",
      "Validation: Epoch [20], Batch [785/938], Loss: 0.3589854836463928\n",
      "Validation: Epoch [20], Batch [786/938], Loss: 0.34529900550842285\n",
      "Validation: Epoch [20], Batch [787/938], Loss: 0.2814681828022003\n",
      "Validation: Epoch [20], Batch [788/938], Loss: 0.5884062051773071\n",
      "Validation: Epoch [20], Batch [789/938], Loss: 0.47370508313179016\n",
      "Validation: Epoch [20], Batch [790/938], Loss: 0.4978426694869995\n",
      "Validation: Epoch [20], Batch [791/938], Loss: 0.49975842237472534\n",
      "Validation: Epoch [20], Batch [792/938], Loss: 0.4065477252006531\n",
      "Validation: Epoch [20], Batch [793/938], Loss: 0.4102727770805359\n",
      "Validation: Epoch [20], Batch [794/938], Loss: 0.4461025893688202\n",
      "Validation: Epoch [20], Batch [795/938], Loss: 0.5292103886604309\n",
      "Validation: Epoch [20], Batch [796/938], Loss: 0.3758951723575592\n",
      "Validation: Epoch [20], Batch [797/938], Loss: 0.4397170841693878\n",
      "Validation: Epoch [20], Batch [798/938], Loss: 0.25712746381759644\n",
      "Validation: Epoch [20], Batch [799/938], Loss: 0.3495643734931946\n",
      "Validation: Epoch [20], Batch [800/938], Loss: 0.46585753560066223\n",
      "Validation: Epoch [20], Batch [801/938], Loss: 0.3868449926376343\n",
      "Validation: Epoch [20], Batch [802/938], Loss: 0.4054659903049469\n",
      "Validation: Epoch [20], Batch [803/938], Loss: 0.3461079001426697\n",
      "Validation: Epoch [20], Batch [804/938], Loss: 0.4278137683868408\n",
      "Validation: Epoch [20], Batch [805/938], Loss: 0.6934850215911865\n",
      "Validation: Epoch [20], Batch [806/938], Loss: 0.38260090351104736\n",
      "Validation: Epoch [20], Batch [807/938], Loss: 0.38456130027770996\n",
      "Validation: Epoch [20], Batch [808/938], Loss: 0.30986738204956055\n",
      "Validation: Epoch [20], Batch [809/938], Loss: 0.41995030641555786\n",
      "Validation: Epoch [20], Batch [810/938], Loss: 0.9550800323486328\n",
      "Validation: Epoch [20], Batch [811/938], Loss: 0.3837295174598694\n",
      "Validation: Epoch [20], Batch [812/938], Loss: 0.5561882853507996\n",
      "Validation: Epoch [20], Batch [813/938], Loss: 0.3912751078605652\n",
      "Validation: Epoch [20], Batch [814/938], Loss: 0.4375092685222626\n",
      "Validation: Epoch [20], Batch [815/938], Loss: 0.36101606488227844\n",
      "Validation: Epoch [20], Batch [816/938], Loss: 0.42758604884147644\n",
      "Validation: Epoch [20], Batch [817/938], Loss: 0.5810571312904358\n",
      "Validation: Epoch [20], Batch [818/938], Loss: 0.5234246850013733\n",
      "Validation: Epoch [20], Batch [819/938], Loss: 0.6660682559013367\n",
      "Validation: Epoch [20], Batch [820/938], Loss: 0.3763365149497986\n",
      "Validation: Epoch [20], Batch [821/938], Loss: 0.3578045070171356\n",
      "Validation: Epoch [20], Batch [822/938], Loss: 0.4208219349384308\n",
      "Validation: Epoch [20], Batch [823/938], Loss: 0.5651471614837646\n",
      "Validation: Epoch [20], Batch [824/938], Loss: 0.47870922088623047\n",
      "Validation: Epoch [20], Batch [825/938], Loss: 0.6490470767021179\n",
      "Validation: Epoch [20], Batch [826/938], Loss: 0.5890265107154846\n",
      "Validation: Epoch [20], Batch [827/938], Loss: 0.599337637424469\n",
      "Validation: Epoch [20], Batch [828/938], Loss: 0.43874242901802063\n",
      "Validation: Epoch [20], Batch [829/938], Loss: 0.4102625548839569\n",
      "Validation: Epoch [20], Batch [830/938], Loss: 0.5903661251068115\n",
      "Validation: Epoch [20], Batch [831/938], Loss: 0.4873664379119873\n",
      "Validation: Epoch [20], Batch [832/938], Loss: 0.5647643208503723\n",
      "Validation: Epoch [20], Batch [833/938], Loss: 0.4579734206199646\n",
      "Validation: Epoch [20], Batch [834/938], Loss: 0.5890918374061584\n",
      "Validation: Epoch [20], Batch [835/938], Loss: 0.5195244550704956\n",
      "Validation: Epoch [20], Batch [836/938], Loss: 0.5635078549385071\n",
      "Validation: Epoch [20], Batch [837/938], Loss: 0.522420346736908\n",
      "Validation: Epoch [20], Batch [838/938], Loss: 0.4577607214450836\n",
      "Validation: Epoch [20], Batch [839/938], Loss: 0.6428540945053101\n",
      "Validation: Epoch [20], Batch [840/938], Loss: 0.3805721700191498\n",
      "Validation: Epoch [20], Batch [841/938], Loss: 0.40935033559799194\n",
      "Validation: Epoch [20], Batch [842/938], Loss: 0.38198256492614746\n",
      "Validation: Epoch [20], Batch [843/938], Loss: 0.3963049650192261\n",
      "Validation: Epoch [20], Batch [844/938], Loss: 0.38493233919143677\n",
      "Validation: Epoch [20], Batch [845/938], Loss: 0.7109741568565369\n",
      "Validation: Epoch [20], Batch [846/938], Loss: 0.5169934630393982\n",
      "Validation: Epoch [20], Batch [847/938], Loss: 0.48472508788108826\n",
      "Validation: Epoch [20], Batch [848/938], Loss: 0.39822080731391907\n",
      "Validation: Epoch [20], Batch [849/938], Loss: 0.43136459589004517\n",
      "Validation: Epoch [20], Batch [850/938], Loss: 0.4223189651966095\n",
      "Validation: Epoch [20], Batch [851/938], Loss: 0.39666762948036194\n",
      "Validation: Epoch [20], Batch [852/938], Loss: 0.5634898543357849\n",
      "Validation: Epoch [20], Batch [853/938], Loss: 0.463512122631073\n",
      "Validation: Epoch [20], Batch [854/938], Loss: 0.3989684283733368\n",
      "Validation: Epoch [20], Batch [855/938], Loss: 0.43769821524620056\n",
      "Validation: Epoch [20], Batch [856/938], Loss: 0.44019532203674316\n",
      "Validation: Epoch [20], Batch [857/938], Loss: 0.4837742745876312\n",
      "Validation: Epoch [20], Batch [858/938], Loss: 0.37128010392189026\n",
      "Validation: Epoch [20], Batch [859/938], Loss: 0.5474745631217957\n",
      "Validation: Epoch [20], Batch [860/938], Loss: 0.6406463980674744\n",
      "Validation: Epoch [20], Batch [861/938], Loss: 0.3233697712421417\n",
      "Validation: Epoch [20], Batch [862/938], Loss: 0.46404486894607544\n",
      "Validation: Epoch [20], Batch [863/938], Loss: 0.5288456678390503\n",
      "Validation: Epoch [20], Batch [864/938], Loss: 0.41028332710266113\n",
      "Validation: Epoch [20], Batch [865/938], Loss: 0.6911718845367432\n",
      "Validation: Epoch [20], Batch [866/938], Loss: 0.41954267024993896\n",
      "Validation: Epoch [20], Batch [867/938], Loss: 0.38856667280197144\n",
      "Validation: Epoch [20], Batch [868/938], Loss: 0.4094032347202301\n",
      "Validation: Epoch [20], Batch [869/938], Loss: 0.40869712829589844\n",
      "Validation: Epoch [20], Batch [870/938], Loss: 0.4409107565879822\n",
      "Validation: Epoch [20], Batch [871/938], Loss: 0.35145071148872375\n",
      "Validation: Epoch [20], Batch [872/938], Loss: 0.45489898324012756\n",
      "Validation: Epoch [20], Batch [873/938], Loss: 0.42806386947631836\n",
      "Validation: Epoch [20], Batch [874/938], Loss: 0.2922614514827728\n",
      "Validation: Epoch [20], Batch [875/938], Loss: 0.45457831025123596\n",
      "Validation: Epoch [20], Batch [876/938], Loss: 0.6052427887916565\n",
      "Validation: Epoch [20], Batch [877/938], Loss: 0.4842032194137573\n",
      "Validation: Epoch [20], Batch [878/938], Loss: 0.34501898288726807\n",
      "Validation: Epoch [20], Batch [879/938], Loss: 0.4374920129776001\n",
      "Validation: Epoch [20], Batch [880/938], Loss: 0.557937502861023\n",
      "Validation: Epoch [20], Batch [881/938], Loss: 0.503246545791626\n",
      "Validation: Epoch [20], Batch [882/938], Loss: 0.33606305718421936\n",
      "Validation: Epoch [20], Batch [883/938], Loss: 0.3972342014312744\n",
      "Validation: Epoch [20], Batch [884/938], Loss: 0.550687313079834\n",
      "Validation: Epoch [20], Batch [885/938], Loss: 0.4597804546356201\n",
      "Validation: Epoch [20], Batch [886/938], Loss: 0.5486121773719788\n",
      "Validation: Epoch [20], Batch [887/938], Loss: 0.3420989215373993\n",
      "Validation: Epoch [20], Batch [888/938], Loss: 0.419344037771225\n",
      "Validation: Epoch [20], Batch [889/938], Loss: 0.5089418292045593\n",
      "Validation: Epoch [20], Batch [890/938], Loss: 0.6101528406143188\n",
      "Validation: Epoch [20], Batch [891/938], Loss: 0.5908414125442505\n",
      "Validation: Epoch [20], Batch [892/938], Loss: 0.3817382752895355\n",
      "Validation: Epoch [20], Batch [893/938], Loss: 0.37012648582458496\n",
      "Validation: Epoch [20], Batch [894/938], Loss: 0.4824903607368469\n",
      "Validation: Epoch [20], Batch [895/938], Loss: 0.2845498323440552\n",
      "Validation: Epoch [20], Batch [896/938], Loss: 0.5121742486953735\n",
      "Validation: Epoch [20], Batch [897/938], Loss: 0.43558257818222046\n",
      "Validation: Epoch [20], Batch [898/938], Loss: 0.447732150554657\n",
      "Validation: Epoch [20], Batch [899/938], Loss: 0.5709735751152039\n",
      "Validation: Epoch [20], Batch [900/938], Loss: 0.5019809603691101\n",
      "Validation: Epoch [20], Batch [901/938], Loss: 0.43738454580307007\n",
      "Validation: Epoch [20], Batch [902/938], Loss: 0.5133764147758484\n",
      "Validation: Epoch [20], Batch [903/938], Loss: 0.44146788120269775\n",
      "Validation: Epoch [20], Batch [904/938], Loss: 0.4864164888858795\n",
      "Validation: Epoch [20], Batch [905/938], Loss: 0.4090655744075775\n",
      "Validation: Epoch [20], Batch [906/938], Loss: 0.42659908533096313\n",
      "Validation: Epoch [20], Batch [907/938], Loss: 0.546096682548523\n",
      "Validation: Epoch [20], Batch [908/938], Loss: 0.6125587821006775\n",
      "Validation: Epoch [20], Batch [909/938], Loss: 0.5610907077789307\n",
      "Validation: Epoch [20], Batch [910/938], Loss: 0.438875675201416\n",
      "Validation: Epoch [20], Batch [911/938], Loss: 0.5009404420852661\n",
      "Validation: Epoch [20], Batch [912/938], Loss: 0.46822071075439453\n",
      "Validation: Epoch [20], Batch [913/938], Loss: 0.43075138330459595\n",
      "Validation: Epoch [20], Batch [914/938], Loss: 0.28774765133857727\n",
      "Validation: Epoch [20], Batch [915/938], Loss: 0.34474480152130127\n",
      "Validation: Epoch [20], Batch [916/938], Loss: 0.4061105251312256\n",
      "Validation: Epoch [20], Batch [917/938], Loss: 0.3926243782043457\n",
      "Validation: Epoch [20], Batch [918/938], Loss: 0.35162967443466187\n",
      "Validation: Epoch [20], Batch [919/938], Loss: 0.46286898851394653\n",
      "Validation: Epoch [20], Batch [920/938], Loss: 0.47303006052970886\n",
      "Validation: Epoch [20], Batch [921/938], Loss: 0.32628101110458374\n",
      "Validation: Epoch [20], Batch [922/938], Loss: 0.37420153617858887\n",
      "Validation: Epoch [20], Batch [923/938], Loss: 0.5833335518836975\n",
      "Validation: Epoch [20], Batch [924/938], Loss: 0.4542849659919739\n",
      "Validation: Epoch [20], Batch [925/938], Loss: 0.3779083788394928\n",
      "Validation: Epoch [20], Batch [926/938], Loss: 0.3325103521347046\n",
      "Validation: Epoch [20], Batch [927/938], Loss: 0.501756489276886\n",
      "Validation: Epoch [20], Batch [928/938], Loss: 0.3754563331604004\n",
      "Validation: Epoch [20], Batch [929/938], Loss: 0.49890172481536865\n",
      "Validation: Epoch [20], Batch [930/938], Loss: 0.5616658926010132\n",
      "Validation: Epoch [20], Batch [931/938], Loss: 0.3886946141719818\n",
      "Validation: Epoch [20], Batch [932/938], Loss: 0.4373701214790344\n",
      "Validation: Epoch [20], Batch [933/938], Loss: 0.45924219489097595\n",
      "Validation: Epoch [20], Batch [934/938], Loss: 0.5044116377830505\n",
      "Validation: Epoch [20], Batch [935/938], Loss: 0.36902520060539246\n",
      "Validation: Epoch [20], Batch [936/938], Loss: 0.41639044880867004\n",
      "Validation: Epoch [20], Batch [937/938], Loss: 0.45849066972732544\n",
      "Validation: Epoch [20], Batch [938/938], Loss: 0.4240087866783142\n",
      "Accuracy of test set: 0.8433166666666667\n",
      "Train: Epoch [21], Batch [1/938], Loss: 0.6458348631858826\n",
      "Train: Epoch [21], Batch [2/938], Loss: 0.4673582911491394\n",
      "Train: Epoch [21], Batch [3/938], Loss: 0.3915569484233856\n",
      "Train: Epoch [21], Batch [4/938], Loss: 0.18300585448741913\n",
      "Train: Epoch [21], Batch [5/938], Loss: 0.35179030895233154\n",
      "Train: Epoch [21], Batch [6/938], Loss: 0.3351564109325409\n",
      "Train: Epoch [21], Batch [7/938], Loss: 0.5064461827278137\n",
      "Train: Epoch [21], Batch [8/938], Loss: 0.48606806993484497\n",
      "Train: Epoch [21], Batch [9/938], Loss: 0.33219197392463684\n",
      "Train: Epoch [21], Batch [10/938], Loss: 0.46956324577331543\n",
      "Train: Epoch [21], Batch [11/938], Loss: 0.4445191025733948\n",
      "Train: Epoch [21], Batch [12/938], Loss: 0.49893754720687866\n",
      "Train: Epoch [21], Batch [13/938], Loss: 0.5595905184745789\n",
      "Train: Epoch [21], Batch [14/938], Loss: 0.4525212049484253\n",
      "Train: Epoch [21], Batch [15/938], Loss: 0.4672858715057373\n",
      "Train: Epoch [21], Batch [16/938], Loss: 0.3478665351867676\n",
      "Train: Epoch [21], Batch [17/938], Loss: 0.4648280143737793\n",
      "Train: Epoch [21], Batch [18/938], Loss: 0.34195587038993835\n",
      "Train: Epoch [21], Batch [19/938], Loss: 0.3781794309616089\n",
      "Train: Epoch [21], Batch [20/938], Loss: 0.4811803996562958\n",
      "Train: Epoch [21], Batch [21/938], Loss: 0.3869342505931854\n",
      "Train: Epoch [21], Batch [22/938], Loss: 0.5716283321380615\n",
      "Train: Epoch [21], Batch [23/938], Loss: 0.5864160060882568\n",
      "Train: Epoch [21], Batch [24/938], Loss: 0.5157535672187805\n",
      "Train: Epoch [21], Batch [25/938], Loss: 0.6173464059829712\n",
      "Train: Epoch [21], Batch [26/938], Loss: 0.3648401200771332\n",
      "Train: Epoch [21], Batch [27/938], Loss: 0.29224473237991333\n",
      "Train: Epoch [21], Batch [28/938], Loss: 0.46453389525413513\n",
      "Train: Epoch [21], Batch [29/938], Loss: 0.4533601403236389\n",
      "Train: Epoch [21], Batch [30/938], Loss: 0.49986496567726135\n",
      "Train: Epoch [21], Batch [31/938], Loss: 0.3096441328525543\n",
      "Train: Epoch [21], Batch [32/938], Loss: 0.40744099020957947\n",
      "Train: Epoch [21], Batch [33/938], Loss: 0.47185420989990234\n",
      "Train: Epoch [21], Batch [34/938], Loss: 0.49077680706977844\n",
      "Train: Epoch [21], Batch [35/938], Loss: 0.41878417134284973\n",
      "Train: Epoch [21], Batch [36/938], Loss: 0.3818172216415405\n",
      "Train: Epoch [21], Batch [37/938], Loss: 0.39193880558013916\n",
      "Train: Epoch [21], Batch [38/938], Loss: 0.40425094962120056\n",
      "Train: Epoch [21], Batch [39/938], Loss: 0.4621348977088928\n",
      "Train: Epoch [21], Batch [40/938], Loss: 0.6129034757614136\n",
      "Train: Epoch [21], Batch [41/938], Loss: 0.5445890426635742\n",
      "Train: Epoch [21], Batch [42/938], Loss: 0.40379810333251953\n",
      "Train: Epoch [21], Batch [43/938], Loss: 0.4367237091064453\n",
      "Train: Epoch [21], Batch [44/938], Loss: 0.325365275144577\n",
      "Train: Epoch [21], Batch [45/938], Loss: 0.37687039375305176\n",
      "Train: Epoch [21], Batch [46/938], Loss: 0.3411114513874054\n",
      "Train: Epoch [21], Batch [47/938], Loss: 0.6694966554641724\n",
      "Train: Epoch [21], Batch [48/938], Loss: 0.4797402620315552\n",
      "Train: Epoch [21], Batch [49/938], Loss: 0.3043951988220215\n",
      "Train: Epoch [21], Batch [50/938], Loss: 0.38635385036468506\n",
      "Train: Epoch [21], Batch [51/938], Loss: 0.4238724708557129\n",
      "Train: Epoch [21], Batch [52/938], Loss: 0.36704686284065247\n",
      "Train: Epoch [21], Batch [53/938], Loss: 0.43480801582336426\n",
      "Train: Epoch [21], Batch [54/938], Loss: 0.504276692867279\n",
      "Train: Epoch [21], Batch [55/938], Loss: 0.410576730966568\n",
      "Train: Epoch [21], Batch [56/938], Loss: 0.4050014019012451\n",
      "Train: Epoch [21], Batch [57/938], Loss: 0.4512532353401184\n",
      "Train: Epoch [21], Batch [58/938], Loss: 0.4457169771194458\n",
      "Train: Epoch [21], Batch [59/938], Loss: 0.5364094376564026\n",
      "Train: Epoch [21], Batch [60/938], Loss: 0.5919675827026367\n",
      "Train: Epoch [21], Batch [61/938], Loss: 0.6397137641906738\n",
      "Train: Epoch [21], Batch [62/938], Loss: 0.45377811789512634\n",
      "Train: Epoch [21], Batch [63/938], Loss: 0.4322983920574188\n",
      "Train: Epoch [21], Batch [64/938], Loss: 0.3837167024612427\n",
      "Train: Epoch [21], Batch [65/938], Loss: 0.6022136807441711\n",
      "Train: Epoch [21], Batch [66/938], Loss: 0.48899227380752563\n",
      "Train: Epoch [21], Batch [67/938], Loss: 0.5048235654830933\n",
      "Train: Epoch [21], Batch [68/938], Loss: 0.3924826383590698\n",
      "Train: Epoch [21], Batch [69/938], Loss: 0.39130911231040955\n",
      "Train: Epoch [21], Batch [70/938], Loss: 0.3748806118965149\n",
      "Train: Epoch [21], Batch [71/938], Loss: 0.5276017189025879\n",
      "Train: Epoch [21], Batch [72/938], Loss: 0.46375536918640137\n",
      "Train: Epoch [21], Batch [73/938], Loss: 0.5359177589416504\n",
      "Train: Epoch [21], Batch [74/938], Loss: 0.5045813918113708\n",
      "Train: Epoch [21], Batch [75/938], Loss: 0.4159066081047058\n",
      "Train: Epoch [21], Batch [76/938], Loss: 0.4144366681575775\n",
      "Train: Epoch [21], Batch [77/938], Loss: 0.4736172556877136\n",
      "Train: Epoch [21], Batch [78/938], Loss: 0.33906665444374084\n",
      "Train: Epoch [21], Batch [79/938], Loss: 0.42812901735305786\n",
      "Train: Epoch [21], Batch [80/938], Loss: 0.37938952445983887\n",
      "Train: Epoch [21], Batch [81/938], Loss: 0.44311216473579407\n",
      "Train: Epoch [21], Batch [82/938], Loss: 0.38215896487236023\n",
      "Train: Epoch [21], Batch [83/938], Loss: 0.2718796730041504\n",
      "Train: Epoch [21], Batch [84/938], Loss: 0.3462356925010681\n",
      "Train: Epoch [21], Batch [85/938], Loss: 0.512852668762207\n",
      "Train: Epoch [21], Batch [86/938], Loss: 0.5704723596572876\n",
      "Train: Epoch [21], Batch [87/938], Loss: 0.5629738569259644\n",
      "Train: Epoch [21], Batch [88/938], Loss: 0.5167464017868042\n",
      "Train: Epoch [21], Batch [89/938], Loss: 0.39281657338142395\n",
      "Train: Epoch [21], Batch [90/938], Loss: 0.3150475323200226\n",
      "Train: Epoch [21], Batch [91/938], Loss: 0.5591974854469299\n",
      "Train: Epoch [21], Batch [92/938], Loss: 0.33648407459259033\n",
      "Train: Epoch [21], Batch [93/938], Loss: 0.33747631311416626\n",
      "Train: Epoch [21], Batch [94/938], Loss: 0.5545535683631897\n",
      "Train: Epoch [21], Batch [95/938], Loss: 0.48469042778015137\n",
      "Train: Epoch [21], Batch [96/938], Loss: 0.52396559715271\n",
      "Train: Epoch [21], Batch [97/938], Loss: 0.5514487028121948\n",
      "Train: Epoch [21], Batch [98/938], Loss: 0.5359410643577576\n",
      "Train: Epoch [21], Batch [99/938], Loss: 0.7197154760360718\n",
      "Train: Epoch [21], Batch [100/938], Loss: 0.392200767993927\n",
      "Train: Epoch [21], Batch [101/938], Loss: 0.361372709274292\n",
      "Train: Epoch [21], Batch [102/938], Loss: 0.5207577347755432\n",
      "Train: Epoch [21], Batch [103/938], Loss: 0.4655882716178894\n",
      "Train: Epoch [21], Batch [104/938], Loss: 0.4952537715435028\n",
      "Train: Epoch [21], Batch [105/938], Loss: 0.6955464482307434\n",
      "Train: Epoch [21], Batch [106/938], Loss: 0.3255213499069214\n",
      "Train: Epoch [21], Batch [107/938], Loss: 0.31036946177482605\n",
      "Train: Epoch [21], Batch [108/938], Loss: 0.4401882588863373\n",
      "Train: Epoch [21], Batch [109/938], Loss: 0.3159756660461426\n",
      "Train: Epoch [21], Batch [110/938], Loss: 0.349260538816452\n",
      "Train: Epoch [21], Batch [111/938], Loss: 0.49308088421821594\n",
      "Train: Epoch [21], Batch [112/938], Loss: 0.43570753931999207\n",
      "Train: Epoch [21], Batch [113/938], Loss: 0.49115100502967834\n",
      "Train: Epoch [21], Batch [114/938], Loss: 0.504879355430603\n",
      "Train: Epoch [21], Batch [115/938], Loss: 0.5821440815925598\n",
      "Train: Epoch [21], Batch [116/938], Loss: 0.47793543338775635\n",
      "Train: Epoch [21], Batch [117/938], Loss: 0.47973793745040894\n",
      "Train: Epoch [21], Batch [118/938], Loss: 0.5870963931083679\n",
      "Train: Epoch [21], Batch [119/938], Loss: 0.4389384388923645\n",
      "Train: Epoch [21], Batch [120/938], Loss: 0.41883352398872375\n",
      "Train: Epoch [21], Batch [121/938], Loss: 0.558821976184845\n",
      "Train: Epoch [21], Batch [122/938], Loss: 0.38802972435951233\n",
      "Train: Epoch [21], Batch [123/938], Loss: 0.3281075060367584\n",
      "Train: Epoch [21], Batch [124/938], Loss: 0.5231862664222717\n",
      "Train: Epoch [21], Batch [125/938], Loss: 0.5010915994644165\n",
      "Train: Epoch [21], Batch [126/938], Loss: 0.4879867136478424\n",
      "Train: Epoch [21], Batch [127/938], Loss: 0.25701504945755005\n",
      "Train: Epoch [21], Batch [128/938], Loss: 0.5048086643218994\n",
      "Train: Epoch [21], Batch [129/938], Loss: 0.3387672007083893\n",
      "Train: Epoch [21], Batch [130/938], Loss: 0.42659854888916016\n",
      "Train: Epoch [21], Batch [131/938], Loss: 0.5735155940055847\n",
      "Train: Epoch [21], Batch [132/938], Loss: 0.4266154170036316\n",
      "Train: Epoch [21], Batch [133/938], Loss: 0.5342316627502441\n",
      "Train: Epoch [21], Batch [134/938], Loss: 0.5574506521224976\n",
      "Train: Epoch [21], Batch [135/938], Loss: 0.47008973360061646\n",
      "Train: Epoch [21], Batch [136/938], Loss: 0.3229075074195862\n",
      "Train: Epoch [21], Batch [137/938], Loss: 0.383931428194046\n",
      "Train: Epoch [21], Batch [138/938], Loss: 0.492185115814209\n",
      "Train: Epoch [21], Batch [139/938], Loss: 0.37169408798217773\n",
      "Train: Epoch [21], Batch [140/938], Loss: 0.4441944360733032\n",
      "Train: Epoch [21], Batch [141/938], Loss: 0.5555480122566223\n",
      "Train: Epoch [21], Batch [142/938], Loss: 0.5836973190307617\n",
      "Train: Epoch [21], Batch [143/938], Loss: 0.4134097695350647\n",
      "Train: Epoch [21], Batch [144/938], Loss: 0.3873422145843506\n",
      "Train: Epoch [21], Batch [145/938], Loss: 0.3394086956977844\n",
      "Train: Epoch [21], Batch [146/938], Loss: 0.5156558752059937\n",
      "Train: Epoch [21], Batch [147/938], Loss: 0.42403459548950195\n",
      "Train: Epoch [21], Batch [148/938], Loss: 0.48222947120666504\n",
      "Train: Epoch [21], Batch [149/938], Loss: 0.49496930837631226\n",
      "Train: Epoch [21], Batch [150/938], Loss: 0.4406445026397705\n",
      "Train: Epoch [21], Batch [151/938], Loss: 0.5503661036491394\n",
      "Train: Epoch [21], Batch [152/938], Loss: 0.3173115849494934\n",
      "Train: Epoch [21], Batch [153/938], Loss: 0.37527209520339966\n",
      "Train: Epoch [21], Batch [154/938], Loss: 0.43325936794281006\n",
      "Train: Epoch [21], Batch [155/938], Loss: 0.40519827604293823\n",
      "Train: Epoch [21], Batch [156/938], Loss: 0.5117557048797607\n",
      "Train: Epoch [21], Batch [157/938], Loss: 0.4399547576904297\n",
      "Train: Epoch [21], Batch [158/938], Loss: 0.6875096559524536\n",
      "Train: Epoch [21], Batch [159/938], Loss: 0.6671467423439026\n",
      "Train: Epoch [21], Batch [160/938], Loss: 0.4874420166015625\n",
      "Train: Epoch [21], Batch [161/938], Loss: 0.5539770722389221\n",
      "Train: Epoch [21], Batch [162/938], Loss: 0.35388919711112976\n",
      "Train: Epoch [21], Batch [163/938], Loss: 0.30100083351135254\n",
      "Train: Epoch [21], Batch [164/938], Loss: 0.42652979493141174\n",
      "Train: Epoch [21], Batch [165/938], Loss: 0.5985773205757141\n",
      "Train: Epoch [21], Batch [166/938], Loss: 0.37386763095855713\n",
      "Train: Epoch [21], Batch [167/938], Loss: 0.34493356943130493\n",
      "Train: Epoch [21], Batch [168/938], Loss: 0.37565338611602783\n",
      "Train: Epoch [21], Batch [169/938], Loss: 0.45052096247673035\n",
      "Train: Epoch [21], Batch [170/938], Loss: 0.4154907166957855\n",
      "Train: Epoch [21], Batch [171/938], Loss: 0.7531260251998901\n",
      "Train: Epoch [21], Batch [172/938], Loss: 0.45425811409950256\n",
      "Train: Epoch [21], Batch [173/938], Loss: 0.3974500894546509\n",
      "Train: Epoch [21], Batch [174/938], Loss: 0.46052131056785583\n",
      "Train: Epoch [21], Batch [175/938], Loss: 0.34423744678497314\n",
      "Train: Epoch [21], Batch [176/938], Loss: 0.5606876015663147\n",
      "Train: Epoch [21], Batch [177/938], Loss: 0.3537197411060333\n",
      "Train: Epoch [21], Batch [178/938], Loss: 0.5560715794563293\n",
      "Train: Epoch [21], Batch [179/938], Loss: 0.40308305621147156\n",
      "Train: Epoch [21], Batch [180/938], Loss: 0.4827806055545807\n",
      "Train: Epoch [21], Batch [181/938], Loss: 0.5341668128967285\n",
      "Train: Epoch [21], Batch [182/938], Loss: 0.6487675309181213\n",
      "Train: Epoch [21], Batch [183/938], Loss: 0.3256867825984955\n",
      "Train: Epoch [21], Batch [184/938], Loss: 0.48428356647491455\n",
      "Train: Epoch [21], Batch [185/938], Loss: 0.37312909960746765\n",
      "Train: Epoch [21], Batch [186/938], Loss: 0.5622904896736145\n",
      "Train: Epoch [21], Batch [187/938], Loss: 0.5739538073539734\n",
      "Train: Epoch [21], Batch [188/938], Loss: 0.5910198092460632\n",
      "Train: Epoch [21], Batch [189/938], Loss: 0.4537822902202606\n",
      "Train: Epoch [21], Batch [190/938], Loss: 0.28024280071258545\n",
      "Train: Epoch [21], Batch [191/938], Loss: 0.46164464950561523\n",
      "Train: Epoch [21], Batch [192/938], Loss: 0.3795020282268524\n",
      "Train: Epoch [21], Batch [193/938], Loss: 0.26904118061065674\n",
      "Train: Epoch [21], Batch [194/938], Loss: 0.43430736660957336\n",
      "Train: Epoch [21], Batch [195/938], Loss: 0.4747523367404938\n",
      "Train: Epoch [21], Batch [196/938], Loss: 0.41026973724365234\n",
      "Train: Epoch [21], Batch [197/938], Loss: 0.5597457885742188\n",
      "Train: Epoch [21], Batch [198/938], Loss: 0.33456605672836304\n",
      "Train: Epoch [21], Batch [199/938], Loss: 0.36088109016418457\n",
      "Train: Epoch [21], Batch [200/938], Loss: 0.43111473321914673\n",
      "Train: Epoch [21], Batch [201/938], Loss: 0.5972378253936768\n",
      "Train: Epoch [21], Batch [202/938], Loss: 0.4539525806903839\n",
      "Train: Epoch [21], Batch [203/938], Loss: 0.6584432125091553\n",
      "Train: Epoch [21], Batch [204/938], Loss: 0.320631742477417\n",
      "Train: Epoch [21], Batch [205/938], Loss: 0.46274614334106445\n",
      "Train: Epoch [21], Batch [206/938], Loss: 0.656346321105957\n",
      "Train: Epoch [21], Batch [207/938], Loss: 0.43986427783966064\n",
      "Train: Epoch [21], Batch [208/938], Loss: 0.42982667684555054\n",
      "Train: Epoch [21], Batch [209/938], Loss: 0.31406742334365845\n",
      "Train: Epoch [21], Batch [210/938], Loss: 0.5731511116027832\n",
      "Train: Epoch [21], Batch [211/938], Loss: 0.3626213073730469\n",
      "Train: Epoch [21], Batch [212/938], Loss: 0.4718470573425293\n",
      "Train: Epoch [21], Batch [213/938], Loss: 0.36029523611068726\n",
      "Train: Epoch [21], Batch [214/938], Loss: 0.8507930040359497\n",
      "Train: Epoch [21], Batch [215/938], Loss: 0.591151773929596\n",
      "Train: Epoch [21], Batch [216/938], Loss: 0.5468007922172546\n",
      "Train: Epoch [21], Batch [217/938], Loss: 0.30799904465675354\n",
      "Train: Epoch [21], Batch [218/938], Loss: 0.4757610261440277\n",
      "Train: Epoch [21], Batch [219/938], Loss: 0.4575992226600647\n",
      "Train: Epoch [21], Batch [220/938], Loss: 0.4681842029094696\n",
      "Train: Epoch [21], Batch [221/938], Loss: 0.4683731496334076\n",
      "Train: Epoch [21], Batch [222/938], Loss: 0.3208377957344055\n",
      "Train: Epoch [21], Batch [223/938], Loss: 0.45979905128479004\n",
      "Train: Epoch [21], Batch [224/938], Loss: 0.43716785311698914\n",
      "Train: Epoch [21], Batch [225/938], Loss: 0.4278269112110138\n",
      "Train: Epoch [21], Batch [226/938], Loss: 0.526520848274231\n",
      "Train: Epoch [21], Batch [227/938], Loss: 0.5729250311851501\n",
      "Train: Epoch [21], Batch [228/938], Loss: 0.42714419960975647\n",
      "Train: Epoch [21], Batch [229/938], Loss: 0.3762360215187073\n",
      "Train: Epoch [21], Batch [230/938], Loss: 0.3409144878387451\n",
      "Train: Epoch [21], Batch [231/938], Loss: 0.3794631063938141\n",
      "Train: Epoch [21], Batch [232/938], Loss: 0.3392203748226166\n",
      "Train: Epoch [21], Batch [233/938], Loss: 0.44618523120880127\n",
      "Train: Epoch [21], Batch [234/938], Loss: 0.49007925391197205\n",
      "Train: Epoch [21], Batch [235/938], Loss: 0.4040144085884094\n",
      "Train: Epoch [21], Batch [236/938], Loss: 0.6686238050460815\n",
      "Train: Epoch [21], Batch [237/938], Loss: 0.28234773874282837\n",
      "Train: Epoch [21], Batch [238/938], Loss: 0.30495113134384155\n",
      "Train: Epoch [21], Batch [239/938], Loss: 0.3020070493221283\n",
      "Train: Epoch [21], Batch [240/938], Loss: 0.6049045324325562\n",
      "Train: Epoch [21], Batch [241/938], Loss: 0.4606712758541107\n",
      "Train: Epoch [21], Batch [242/938], Loss: 0.42710262537002563\n",
      "Train: Epoch [21], Batch [243/938], Loss: 0.5135558843612671\n",
      "Train: Epoch [21], Batch [244/938], Loss: 0.4579596221446991\n",
      "Train: Epoch [21], Batch [245/938], Loss: 0.46516096591949463\n",
      "Train: Epoch [21], Batch [246/938], Loss: 0.5399917960166931\n",
      "Train: Epoch [21], Batch [247/938], Loss: 0.4111824035644531\n",
      "Train: Epoch [21], Batch [248/938], Loss: 0.5192181468009949\n",
      "Train: Epoch [21], Batch [249/938], Loss: 0.4960179924964905\n",
      "Train: Epoch [21], Batch [250/938], Loss: 0.2911832332611084\n",
      "Train: Epoch [21], Batch [251/938], Loss: 0.36088451743125916\n",
      "Train: Epoch [21], Batch [252/938], Loss: 0.40051308274269104\n",
      "Train: Epoch [21], Batch [253/938], Loss: 0.4999184012413025\n",
      "Train: Epoch [21], Batch [254/938], Loss: 0.41736748814582825\n",
      "Train: Epoch [21], Batch [255/938], Loss: 0.357261598110199\n",
      "Train: Epoch [21], Batch [256/938], Loss: 0.3361010253429413\n",
      "Train: Epoch [21], Batch [257/938], Loss: 0.6499137282371521\n",
      "Train: Epoch [21], Batch [258/938], Loss: 0.4044787287712097\n",
      "Train: Epoch [21], Batch [259/938], Loss: 0.43733593821525574\n",
      "Train: Epoch [21], Batch [260/938], Loss: 0.7142876386642456\n",
      "Train: Epoch [21], Batch [261/938], Loss: 0.6183467507362366\n",
      "Train: Epoch [21], Batch [262/938], Loss: 0.600937008857727\n",
      "Train: Epoch [21], Batch [263/938], Loss: 0.5969544053077698\n",
      "Train: Epoch [21], Batch [264/938], Loss: 0.5836292505264282\n",
      "Train: Epoch [21], Batch [265/938], Loss: 0.29115718603134155\n",
      "Train: Epoch [21], Batch [266/938], Loss: 0.5267078876495361\n",
      "Train: Epoch [21], Batch [267/938], Loss: 0.4515298008918762\n",
      "Train: Epoch [21], Batch [268/938], Loss: 0.3022540509700775\n",
      "Train: Epoch [21], Batch [269/938], Loss: 0.42202264070510864\n",
      "Train: Epoch [21], Batch [270/938], Loss: 0.5919507741928101\n",
      "Train: Epoch [21], Batch [271/938], Loss: 0.5558341145515442\n",
      "Train: Epoch [21], Batch [272/938], Loss: 0.42858558893203735\n",
      "Train: Epoch [21], Batch [273/938], Loss: 0.47674888372421265\n",
      "Train: Epoch [21], Batch [274/938], Loss: 0.764682412147522\n",
      "Train: Epoch [21], Batch [275/938], Loss: 0.44112083315849304\n",
      "Train: Epoch [21], Batch [276/938], Loss: 0.3283425569534302\n",
      "Train: Epoch [21], Batch [277/938], Loss: 0.44389668107032776\n",
      "Train: Epoch [21], Batch [278/938], Loss: 0.6438275575637817\n",
      "Train: Epoch [21], Batch [279/938], Loss: 0.45471128821372986\n",
      "Train: Epoch [21], Batch [280/938], Loss: 0.3562012314796448\n",
      "Train: Epoch [21], Batch [281/938], Loss: 0.6355074644088745\n",
      "Train: Epoch [21], Batch [282/938], Loss: 0.46606695652008057\n",
      "Train: Epoch [21], Batch [283/938], Loss: 0.40630707144737244\n",
      "Train: Epoch [21], Batch [284/938], Loss: 0.41486257314682007\n",
      "Train: Epoch [21], Batch [285/938], Loss: 0.48535284399986267\n",
      "Train: Epoch [21], Batch [286/938], Loss: 0.3991132974624634\n",
      "Train: Epoch [21], Batch [287/938], Loss: 0.5636308789253235\n",
      "Train: Epoch [21], Batch [288/938], Loss: 0.5278988480567932\n",
      "Train: Epoch [21], Batch [289/938], Loss: 0.6440153121948242\n",
      "Train: Epoch [21], Batch [290/938], Loss: 0.35290586948394775\n",
      "Train: Epoch [21], Batch [291/938], Loss: 0.41606295108795166\n",
      "Train: Epoch [21], Batch [292/938], Loss: 0.5620476603507996\n",
      "Train: Epoch [21], Batch [293/938], Loss: 0.5463701486587524\n",
      "Train: Epoch [21], Batch [294/938], Loss: 0.3616636395454407\n",
      "Train: Epoch [21], Batch [295/938], Loss: 0.4574555456638336\n",
      "Train: Epoch [21], Batch [296/938], Loss: 0.30821436643600464\n",
      "Train: Epoch [21], Batch [297/938], Loss: 0.3763847351074219\n",
      "Train: Epoch [21], Batch [298/938], Loss: 0.36521586775779724\n",
      "Train: Epoch [21], Batch [299/938], Loss: 0.3250497579574585\n",
      "Train: Epoch [21], Batch [300/938], Loss: 0.49863505363464355\n",
      "Train: Epoch [21], Batch [301/938], Loss: 0.43399178981781006\n",
      "Train: Epoch [21], Batch [302/938], Loss: 0.3127078413963318\n",
      "Train: Epoch [21], Batch [303/938], Loss: 0.6019904017448425\n",
      "Train: Epoch [21], Batch [304/938], Loss: 0.48422327637672424\n",
      "Train: Epoch [21], Batch [305/938], Loss: 0.3658830523490906\n",
      "Train: Epoch [21], Batch [306/938], Loss: 0.28034597635269165\n",
      "Train: Epoch [21], Batch [307/938], Loss: 0.3341656029224396\n",
      "Train: Epoch [21], Batch [308/938], Loss: 0.48329317569732666\n",
      "Train: Epoch [21], Batch [309/938], Loss: 0.35741403698921204\n",
      "Train: Epoch [21], Batch [310/938], Loss: 0.440642774105072\n",
      "Train: Epoch [21], Batch [311/938], Loss: 0.39084064960479736\n",
      "Train: Epoch [21], Batch [312/938], Loss: 0.44110262393951416\n",
      "Train: Epoch [21], Batch [313/938], Loss: 0.6147623062133789\n",
      "Train: Epoch [21], Batch [314/938], Loss: 0.48459556698799133\n",
      "Train: Epoch [21], Batch [315/938], Loss: 0.4864597022533417\n",
      "Train: Epoch [21], Batch [316/938], Loss: 0.4826575517654419\n",
      "Train: Epoch [21], Batch [317/938], Loss: 0.3752310574054718\n",
      "Train: Epoch [21], Batch [318/938], Loss: 0.5296681523323059\n",
      "Train: Epoch [21], Batch [319/938], Loss: 0.5026820302009583\n",
      "Train: Epoch [21], Batch [320/938], Loss: 0.43446969985961914\n",
      "Train: Epoch [21], Batch [321/938], Loss: 0.5590824484825134\n",
      "Train: Epoch [21], Batch [322/938], Loss: 0.6308519244194031\n",
      "Train: Epoch [21], Batch [323/938], Loss: 0.47411149740219116\n",
      "Train: Epoch [21], Batch [324/938], Loss: 0.4769350588321686\n",
      "Train: Epoch [21], Batch [325/938], Loss: 0.3126353621482849\n",
      "Train: Epoch [21], Batch [326/938], Loss: 0.5419673323631287\n",
      "Train: Epoch [21], Batch [327/938], Loss: 0.4729316532611847\n",
      "Train: Epoch [21], Batch [328/938], Loss: 0.33314281702041626\n",
      "Train: Epoch [21], Batch [329/938], Loss: 0.35256293416023254\n",
      "Train: Epoch [21], Batch [330/938], Loss: 0.4624027609825134\n",
      "Train: Epoch [21], Batch [331/938], Loss: 0.3661753237247467\n",
      "Train: Epoch [21], Batch [332/938], Loss: 0.4357478618621826\n",
      "Train: Epoch [21], Batch [333/938], Loss: 0.3366330564022064\n",
      "Train: Epoch [21], Batch [334/938], Loss: 0.49196892976760864\n",
      "Train: Epoch [21], Batch [335/938], Loss: 0.5023579001426697\n",
      "Train: Epoch [21], Batch [336/938], Loss: 0.497783362865448\n",
      "Train: Epoch [21], Batch [337/938], Loss: 0.5034008026123047\n",
      "Train: Epoch [21], Batch [338/938], Loss: 0.42547523975372314\n",
      "Train: Epoch [21], Batch [339/938], Loss: 0.29022058844566345\n",
      "Train: Epoch [21], Batch [340/938], Loss: 0.4973199665546417\n",
      "Train: Epoch [21], Batch [341/938], Loss: 0.42027807235717773\n",
      "Train: Epoch [21], Batch [342/938], Loss: 0.4271361827850342\n",
      "Train: Epoch [21], Batch [343/938], Loss: 0.5356661677360535\n",
      "Train: Epoch [21], Batch [344/938], Loss: 0.47494861483573914\n",
      "Train: Epoch [21], Batch [345/938], Loss: 0.41455286741256714\n",
      "Train: Epoch [21], Batch [346/938], Loss: 0.31963109970092773\n",
      "Train: Epoch [21], Batch [347/938], Loss: 0.3621140420436859\n",
      "Train: Epoch [21], Batch [348/938], Loss: 0.38767415285110474\n",
      "Train: Epoch [21], Batch [349/938], Loss: 0.4347613751888275\n",
      "Train: Epoch [21], Batch [350/938], Loss: 0.513689398765564\n",
      "Train: Epoch [21], Batch [351/938], Loss: 0.46284419298171997\n",
      "Train: Epoch [21], Batch [352/938], Loss: 0.30762243270874023\n",
      "Train: Epoch [21], Batch [353/938], Loss: 0.45525792241096497\n",
      "Train: Epoch [21], Batch [354/938], Loss: 0.3934190273284912\n",
      "Train: Epoch [21], Batch [355/938], Loss: 0.29007914662361145\n",
      "Train: Epoch [21], Batch [356/938], Loss: 0.5108387470245361\n",
      "Train: Epoch [21], Batch [357/938], Loss: 0.43880876898765564\n",
      "Train: Epoch [21], Batch [358/938], Loss: 0.509427547454834\n",
      "Train: Epoch [21], Batch [359/938], Loss: 0.3612249791622162\n",
      "Train: Epoch [21], Batch [360/938], Loss: 0.35357269644737244\n",
      "Train: Epoch [21], Batch [361/938], Loss: 0.3728530704975128\n",
      "Train: Epoch [21], Batch [362/938], Loss: 0.41626495122909546\n",
      "Train: Epoch [21], Batch [363/938], Loss: 0.430164098739624\n",
      "Train: Epoch [21], Batch [364/938], Loss: 0.4636628329753876\n",
      "Train: Epoch [21], Batch [365/938], Loss: 0.341006338596344\n",
      "Train: Epoch [21], Batch [366/938], Loss: 0.35313090682029724\n",
      "Train: Epoch [21], Batch [367/938], Loss: 0.44163256883621216\n",
      "Train: Epoch [21], Batch [368/938], Loss: 0.5724780559539795\n",
      "Train: Epoch [21], Batch [369/938], Loss: 0.60785311460495\n",
      "Train: Epoch [21], Batch [370/938], Loss: 0.33715373277664185\n",
      "Train: Epoch [21], Batch [371/938], Loss: 0.4132259488105774\n",
      "Train: Epoch [21], Batch [372/938], Loss: 0.7207800149917603\n",
      "Train: Epoch [21], Batch [373/938], Loss: 0.6208844184875488\n",
      "Train: Epoch [21], Batch [374/938], Loss: 0.4745692014694214\n",
      "Train: Epoch [21], Batch [375/938], Loss: 0.5504236221313477\n",
      "Train: Epoch [21], Batch [376/938], Loss: 0.5279316902160645\n",
      "Train: Epoch [21], Batch [377/938], Loss: 0.4701569080352783\n",
      "Train: Epoch [21], Batch [378/938], Loss: 0.43651261925697327\n",
      "Train: Epoch [21], Batch [379/938], Loss: 0.34963613748550415\n",
      "Train: Epoch [21], Batch [380/938], Loss: 0.4022848606109619\n",
      "Train: Epoch [21], Batch [381/938], Loss: 0.5891664028167725\n",
      "Train: Epoch [21], Batch [382/938], Loss: 0.4222854673862457\n",
      "Train: Epoch [21], Batch [383/938], Loss: 0.40295082330703735\n",
      "Train: Epoch [21], Batch [384/938], Loss: 0.43243831396102905\n",
      "Train: Epoch [21], Batch [385/938], Loss: 0.48038893938064575\n",
      "Train: Epoch [21], Batch [386/938], Loss: 0.37443575263023376\n",
      "Train: Epoch [21], Batch [387/938], Loss: 0.3015293478965759\n",
      "Train: Epoch [21], Batch [388/938], Loss: 0.4009973406791687\n",
      "Train: Epoch [21], Batch [389/938], Loss: 0.33994850516319275\n",
      "Train: Epoch [21], Batch [390/938], Loss: 0.5435761213302612\n",
      "Train: Epoch [21], Batch [391/938], Loss: 0.6584676504135132\n",
      "Train: Epoch [21], Batch [392/938], Loss: 0.5298469066619873\n",
      "Train: Epoch [21], Batch [393/938], Loss: 0.23672904074192047\n",
      "Train: Epoch [21], Batch [394/938], Loss: 0.4594363272190094\n",
      "Train: Epoch [21], Batch [395/938], Loss: 0.5398302674293518\n",
      "Train: Epoch [21], Batch [396/938], Loss: 0.4931357502937317\n",
      "Train: Epoch [21], Batch [397/938], Loss: 0.42874377965927124\n",
      "Train: Epoch [21], Batch [398/938], Loss: 0.5903303027153015\n",
      "Train: Epoch [21], Batch [399/938], Loss: 0.6532126069068909\n",
      "Train: Epoch [21], Batch [400/938], Loss: 0.3522491455078125\n",
      "Train: Epoch [21], Batch [401/938], Loss: 0.4289535582065582\n",
      "Train: Epoch [21], Batch [402/938], Loss: 0.3648134469985962\n",
      "Train: Epoch [21], Batch [403/938], Loss: 0.4122719466686249\n",
      "Train: Epoch [21], Batch [404/938], Loss: 0.5985892415046692\n",
      "Train: Epoch [21], Batch [405/938], Loss: 0.727092981338501\n",
      "Train: Epoch [21], Batch [406/938], Loss: 0.4451850652694702\n",
      "Train: Epoch [21], Batch [407/938], Loss: 0.4294974207878113\n",
      "Train: Epoch [21], Batch [408/938], Loss: 0.3993029296398163\n",
      "Train: Epoch [21], Batch [409/938], Loss: 0.36770564317703247\n",
      "Train: Epoch [21], Batch [410/938], Loss: 0.5410670042037964\n",
      "Train: Epoch [21], Batch [411/938], Loss: 0.5158218741416931\n",
      "Train: Epoch [21], Batch [412/938], Loss: 0.5595665574073792\n",
      "Train: Epoch [21], Batch [413/938], Loss: 0.31184303760528564\n",
      "Train: Epoch [21], Batch [414/938], Loss: 0.29438942670822144\n",
      "Train: Epoch [21], Batch [415/938], Loss: 0.36457404494285583\n",
      "Train: Epoch [21], Batch [416/938], Loss: 0.33582040667533875\n",
      "Train: Epoch [21], Batch [417/938], Loss: 0.4315451681613922\n",
      "Train: Epoch [21], Batch [418/938], Loss: 0.4695837199687958\n",
      "Train: Epoch [21], Batch [419/938], Loss: 0.4622746706008911\n",
      "Train: Epoch [21], Batch [420/938], Loss: 0.4883102774620056\n",
      "Train: Epoch [21], Batch [421/938], Loss: 0.4351574778556824\n",
      "Train: Epoch [21], Batch [422/938], Loss: 0.4674570560455322\n",
      "Train: Epoch [21], Batch [423/938], Loss: 0.4090973436832428\n",
      "Train: Epoch [21], Batch [424/938], Loss: 0.38648930191993713\n",
      "Train: Epoch [21], Batch [425/938], Loss: 0.611301839351654\n",
      "Train: Epoch [21], Batch [426/938], Loss: 0.532680332660675\n",
      "Train: Epoch [21], Batch [427/938], Loss: 0.5512254238128662\n",
      "Train: Epoch [21], Batch [428/938], Loss: 0.5517750978469849\n",
      "Train: Epoch [21], Batch [429/938], Loss: 0.4551335871219635\n",
      "Train: Epoch [21], Batch [430/938], Loss: 0.3261408507823944\n",
      "Train: Epoch [21], Batch [431/938], Loss: 0.42435622215270996\n",
      "Train: Epoch [21], Batch [432/938], Loss: 0.6952385306358337\n",
      "Train: Epoch [21], Batch [433/938], Loss: 0.38121524453163147\n",
      "Train: Epoch [21], Batch [434/938], Loss: 0.5397840738296509\n",
      "Train: Epoch [21], Batch [435/938], Loss: 0.39947569370269775\n",
      "Train: Epoch [21], Batch [436/938], Loss: 0.42920026183128357\n",
      "Train: Epoch [21], Batch [437/938], Loss: 0.3704743981361389\n",
      "Train: Epoch [21], Batch [438/938], Loss: 0.34074729681015015\n",
      "Train: Epoch [21], Batch [439/938], Loss: 0.46588826179504395\n",
      "Train: Epoch [21], Batch [440/938], Loss: 0.5405130386352539\n",
      "Train: Epoch [21], Batch [441/938], Loss: 0.3151530623435974\n",
      "Train: Epoch [21], Batch [442/938], Loss: 0.4294177293777466\n",
      "Train: Epoch [21], Batch [443/938], Loss: 0.34779107570648193\n",
      "Train: Epoch [21], Batch [444/938], Loss: 0.327487051486969\n",
      "Train: Epoch [21], Batch [445/938], Loss: 0.4719924330711365\n",
      "Train: Epoch [21], Batch [446/938], Loss: 0.4664534330368042\n",
      "Train: Epoch [21], Batch [447/938], Loss: 0.2851719856262207\n",
      "Train: Epoch [21], Batch [448/938], Loss: 0.2957580089569092\n",
      "Train: Epoch [21], Batch [449/938], Loss: 0.4616185128688812\n",
      "Train: Epoch [21], Batch [450/938], Loss: 0.7894227504730225\n",
      "Train: Epoch [21], Batch [451/938], Loss: 0.4951440989971161\n",
      "Train: Epoch [21], Batch [452/938], Loss: 0.531083345413208\n",
      "Train: Epoch [21], Batch [453/938], Loss: 0.5917651057243347\n",
      "Train: Epoch [21], Batch [454/938], Loss: 0.295076459646225\n",
      "Train: Epoch [21], Batch [455/938], Loss: 0.4931865930557251\n",
      "Train: Epoch [21], Batch [456/938], Loss: 0.433769553899765\n",
      "Train: Epoch [21], Batch [457/938], Loss: 0.3341594636440277\n",
      "Train: Epoch [21], Batch [458/938], Loss: 0.4162214696407318\n",
      "Train: Epoch [21], Batch [459/938], Loss: 0.4796975255012512\n",
      "Train: Epoch [21], Batch [460/938], Loss: 0.3037262558937073\n",
      "Train: Epoch [21], Batch [461/938], Loss: 0.5677551031112671\n",
      "Train: Epoch [21], Batch [462/938], Loss: 0.387465238571167\n",
      "Train: Epoch [21], Batch [463/938], Loss: 0.4128061532974243\n",
      "Train: Epoch [21], Batch [464/938], Loss: 0.6018837690353394\n",
      "Train: Epoch [21], Batch [465/938], Loss: 0.6183537840843201\n",
      "Train: Epoch [21], Batch [466/938], Loss: 0.4469183385372162\n",
      "Train: Epoch [21], Batch [467/938], Loss: 0.57003253698349\n",
      "Train: Epoch [21], Batch [468/938], Loss: 0.4341186583042145\n",
      "Train: Epoch [21], Batch [469/938], Loss: 0.2799238860607147\n",
      "Train: Epoch [21], Batch [470/938], Loss: 0.34903401136398315\n",
      "Train: Epoch [21], Batch [471/938], Loss: 0.46844974160194397\n",
      "Train: Epoch [21], Batch [472/938], Loss: 0.4808109998703003\n",
      "Train: Epoch [21], Batch [473/938], Loss: 0.4812684953212738\n",
      "Train: Epoch [21], Batch [474/938], Loss: 0.7829462885856628\n",
      "Train: Epoch [21], Batch [475/938], Loss: 0.46654027700424194\n",
      "Train: Epoch [21], Batch [476/938], Loss: 0.39246025681495667\n",
      "Train: Epoch [21], Batch [477/938], Loss: 0.5730962753295898\n",
      "Train: Epoch [21], Batch [478/938], Loss: 0.3750278353691101\n",
      "Train: Epoch [21], Batch [479/938], Loss: 0.4835219085216522\n",
      "Train: Epoch [21], Batch [480/938], Loss: 0.4902753233909607\n",
      "Train: Epoch [21], Batch [481/938], Loss: 0.4359046518802643\n",
      "Train: Epoch [21], Batch [482/938], Loss: 0.354698121547699\n",
      "Train: Epoch [21], Batch [483/938], Loss: 0.35206249356269836\n",
      "Train: Epoch [21], Batch [484/938], Loss: 0.3035581409931183\n",
      "Train: Epoch [21], Batch [485/938], Loss: 0.4896162450313568\n",
      "Train: Epoch [21], Batch [486/938], Loss: 0.5414642095565796\n",
      "Train: Epoch [21], Batch [487/938], Loss: 0.605901300907135\n",
      "Train: Epoch [21], Batch [488/938], Loss: 0.30738598108291626\n",
      "Train: Epoch [21], Batch [489/938], Loss: 0.7408806681632996\n",
      "Train: Epoch [21], Batch [490/938], Loss: 0.584953784942627\n",
      "Train: Epoch [21], Batch [491/938], Loss: 0.49511057138442993\n",
      "Train: Epoch [21], Batch [492/938], Loss: 0.49674519896507263\n",
      "Train: Epoch [21], Batch [493/938], Loss: 0.44644007086753845\n",
      "Train: Epoch [21], Batch [494/938], Loss: 0.4911018908023834\n",
      "Train: Epoch [21], Batch [495/938], Loss: 0.6429742574691772\n",
      "Train: Epoch [21], Batch [496/938], Loss: 0.3489820957183838\n",
      "Train: Epoch [21], Batch [497/938], Loss: 0.284932017326355\n",
      "Train: Epoch [21], Batch [498/938], Loss: 0.45600616931915283\n",
      "Train: Epoch [21], Batch [499/938], Loss: 0.3671348989009857\n",
      "Train: Epoch [21], Batch [500/938], Loss: 0.376033753156662\n",
      "Train: Epoch [21], Batch [501/938], Loss: 0.5154564380645752\n",
      "Train: Epoch [21], Batch [502/938], Loss: 0.41358041763305664\n",
      "Train: Epoch [21], Batch [503/938], Loss: 0.4521598219871521\n",
      "Train: Epoch [21], Batch [504/938], Loss: 0.19648218154907227\n",
      "Train: Epoch [21], Batch [505/938], Loss: 0.45751696825027466\n",
      "Train: Epoch [21], Batch [506/938], Loss: 0.3601534068584442\n",
      "Train: Epoch [21], Batch [507/938], Loss: 0.5899814963340759\n",
      "Train: Epoch [21], Batch [508/938], Loss: 0.4965549111366272\n",
      "Train: Epoch [21], Batch [509/938], Loss: 0.46315640211105347\n",
      "Train: Epoch [21], Batch [510/938], Loss: 0.21939398348331451\n",
      "Train: Epoch [21], Batch [511/938], Loss: 0.44612449407577515\n",
      "Train: Epoch [21], Batch [512/938], Loss: 0.4598642885684967\n",
      "Train: Epoch [21], Batch [513/938], Loss: 0.5577037334442139\n",
      "Train: Epoch [21], Batch [514/938], Loss: 0.40268564224243164\n",
      "Train: Epoch [21], Batch [515/938], Loss: 0.29023778438568115\n",
      "Train: Epoch [21], Batch [516/938], Loss: 0.475063681602478\n",
      "Train: Epoch [21], Batch [517/938], Loss: 0.36510321497917175\n",
      "Train: Epoch [21], Batch [518/938], Loss: 0.40174829959869385\n",
      "Train: Epoch [21], Batch [519/938], Loss: 0.5543581247329712\n",
      "Train: Epoch [21], Batch [520/938], Loss: 0.46218356490135193\n",
      "Train: Epoch [21], Batch [521/938], Loss: 0.4995572865009308\n",
      "Train: Epoch [21], Batch [522/938], Loss: 0.3342231512069702\n",
      "Train: Epoch [21], Batch [523/938], Loss: 0.5304509997367859\n",
      "Train: Epoch [21], Batch [524/938], Loss: 0.2848834693431854\n",
      "Train: Epoch [21], Batch [525/938], Loss: 0.6265449523925781\n",
      "Train: Epoch [21], Batch [526/938], Loss: 0.32601991295814514\n",
      "Train: Epoch [21], Batch [527/938], Loss: 0.2823314070701599\n",
      "Train: Epoch [21], Batch [528/938], Loss: 0.568017840385437\n",
      "Train: Epoch [21], Batch [529/938], Loss: 0.2006237655878067\n",
      "Train: Epoch [21], Batch [530/938], Loss: 0.5975817441940308\n",
      "Train: Epoch [21], Batch [531/938], Loss: 0.4353393614292145\n",
      "Train: Epoch [21], Batch [532/938], Loss: 0.7415589094161987\n",
      "Train: Epoch [21], Batch [533/938], Loss: 0.5765371322631836\n",
      "Train: Epoch [21], Batch [534/938], Loss: 0.5928633809089661\n",
      "Train: Epoch [21], Batch [535/938], Loss: 0.47250914573669434\n",
      "Train: Epoch [21], Batch [536/938], Loss: 0.26333940029144287\n",
      "Train: Epoch [21], Batch [537/938], Loss: 0.5751938223838806\n",
      "Train: Epoch [21], Batch [538/938], Loss: 0.3467560410499573\n",
      "Train: Epoch [21], Batch [539/938], Loss: 0.6323549151420593\n",
      "Train: Epoch [21], Batch [540/938], Loss: 0.41854146122932434\n",
      "Train: Epoch [21], Batch [541/938], Loss: 0.6247603893280029\n",
      "Train: Epoch [21], Batch [542/938], Loss: 0.4645650386810303\n",
      "Train: Epoch [21], Batch [543/938], Loss: 0.23771479725837708\n",
      "Train: Epoch [21], Batch [544/938], Loss: 0.41834694147109985\n",
      "Train: Epoch [21], Batch [545/938], Loss: 0.5660229921340942\n",
      "Train: Epoch [21], Batch [546/938], Loss: 0.7165038585662842\n",
      "Train: Epoch [21], Batch [547/938], Loss: 0.6113756895065308\n",
      "Train: Epoch [21], Batch [548/938], Loss: 0.5667382478713989\n",
      "Train: Epoch [21], Batch [549/938], Loss: 0.552166223526001\n",
      "Train: Epoch [21], Batch [550/938], Loss: 0.42876386642456055\n",
      "Train: Epoch [21], Batch [551/938], Loss: 0.4639841616153717\n",
      "Train: Epoch [21], Batch [552/938], Loss: 0.39171239733695984\n",
      "Train: Epoch [21], Batch [553/938], Loss: 0.4313868284225464\n",
      "Train: Epoch [21], Batch [554/938], Loss: 0.5021204948425293\n",
      "Train: Epoch [21], Batch [555/938], Loss: 0.545418381690979\n",
      "Train: Epoch [21], Batch [556/938], Loss: 0.4765223562717438\n",
      "Train: Epoch [21], Batch [557/938], Loss: 0.39218905568122864\n",
      "Train: Epoch [21], Batch [558/938], Loss: 0.29271405935287476\n",
      "Train: Epoch [21], Batch [559/938], Loss: 0.4504469931125641\n",
      "Train: Epoch [21], Batch [560/938], Loss: 0.45492491126060486\n",
      "Train: Epoch [21], Batch [561/938], Loss: 0.5155266523361206\n",
      "Train: Epoch [21], Batch [562/938], Loss: 0.5596367716789246\n",
      "Train: Epoch [21], Batch [563/938], Loss: 0.5583891868591309\n",
      "Train: Epoch [21], Batch [564/938], Loss: 0.4147890508174896\n",
      "Train: Epoch [21], Batch [565/938], Loss: 0.6594914793968201\n",
      "Train: Epoch [21], Batch [566/938], Loss: 0.434169739484787\n",
      "Train: Epoch [21], Batch [567/938], Loss: 0.44001322984695435\n",
      "Train: Epoch [21], Batch [568/938], Loss: 0.7433834075927734\n",
      "Train: Epoch [21], Batch [569/938], Loss: 0.5026991367340088\n",
      "Train: Epoch [21], Batch [570/938], Loss: 0.4080685079097748\n",
      "Train: Epoch [21], Batch [571/938], Loss: 0.3651195168495178\n",
      "Train: Epoch [21], Batch [572/938], Loss: 0.6395344734191895\n",
      "Train: Epoch [21], Batch [573/938], Loss: 0.36079537868499756\n",
      "Train: Epoch [21], Batch [574/938], Loss: 0.5707857012748718\n",
      "Train: Epoch [21], Batch [575/938], Loss: 0.43718278408050537\n",
      "Train: Epoch [21], Batch [576/938], Loss: 0.3357594907283783\n",
      "Train: Epoch [21], Batch [577/938], Loss: 0.323500394821167\n",
      "Train: Epoch [21], Batch [578/938], Loss: 0.33515551686286926\n",
      "Train: Epoch [21], Batch [579/938], Loss: 0.46813350915908813\n",
      "Train: Epoch [21], Batch [580/938], Loss: 0.37899792194366455\n",
      "Train: Epoch [21], Batch [581/938], Loss: 0.4657672047615051\n",
      "Train: Epoch [21], Batch [582/938], Loss: 0.49073272943496704\n",
      "Train: Epoch [21], Batch [583/938], Loss: 0.4821837544441223\n",
      "Train: Epoch [21], Batch [584/938], Loss: 0.5098885893821716\n",
      "Train: Epoch [21], Batch [585/938], Loss: 0.3517136573791504\n",
      "Train: Epoch [21], Batch [586/938], Loss: 0.601394534111023\n",
      "Train: Epoch [21], Batch [587/938], Loss: 0.35018301010131836\n",
      "Train: Epoch [21], Batch [588/938], Loss: 0.3431013822555542\n",
      "Train: Epoch [21], Batch [589/938], Loss: 0.542593777179718\n",
      "Train: Epoch [21], Batch [590/938], Loss: 0.34817278385162354\n",
      "Train: Epoch [21], Batch [591/938], Loss: 0.4737728238105774\n",
      "Train: Epoch [21], Batch [592/938], Loss: 0.49047693610191345\n",
      "Train: Epoch [21], Batch [593/938], Loss: 0.4030954837799072\n",
      "Train: Epoch [21], Batch [594/938], Loss: 0.47518056631088257\n",
      "Train: Epoch [21], Batch [595/938], Loss: 0.43648460507392883\n",
      "Train: Epoch [21], Batch [596/938], Loss: 0.2247156947851181\n",
      "Train: Epoch [21], Batch [597/938], Loss: 0.5966103672981262\n",
      "Train: Epoch [21], Batch [598/938], Loss: 0.4150781035423279\n",
      "Train: Epoch [21], Batch [599/938], Loss: 0.47567111253738403\n",
      "Train: Epoch [21], Batch [600/938], Loss: 0.38278400897979736\n",
      "Train: Epoch [21], Batch [601/938], Loss: 0.41936540603637695\n",
      "Train: Epoch [21], Batch [602/938], Loss: 0.37858858704566956\n",
      "Train: Epoch [21], Batch [603/938], Loss: 0.5007792711257935\n",
      "Train: Epoch [21], Batch [604/938], Loss: 0.5360711216926575\n",
      "Train: Epoch [21], Batch [605/938], Loss: 0.6458954215049744\n",
      "Train: Epoch [21], Batch [606/938], Loss: 0.5591792464256287\n",
      "Train: Epoch [21], Batch [607/938], Loss: 0.48316580057144165\n",
      "Train: Epoch [21], Batch [608/938], Loss: 0.43238911032676697\n",
      "Train: Epoch [21], Batch [609/938], Loss: 0.39777421951293945\n",
      "Train: Epoch [21], Batch [610/938], Loss: 0.5140590667724609\n",
      "Train: Epoch [21], Batch [611/938], Loss: 0.5163110494613647\n",
      "Train: Epoch [21], Batch [612/938], Loss: 0.49770215153694153\n",
      "Train: Epoch [21], Batch [613/938], Loss: 0.5163750648498535\n",
      "Train: Epoch [21], Batch [614/938], Loss: 0.5507240295410156\n",
      "Train: Epoch [21], Batch [615/938], Loss: 0.4156275689601898\n",
      "Train: Epoch [21], Batch [616/938], Loss: 0.27804017066955566\n",
      "Train: Epoch [21], Batch [617/938], Loss: 0.6012618541717529\n",
      "Train: Epoch [21], Batch [618/938], Loss: 0.5087823867797852\n",
      "Train: Epoch [21], Batch [619/938], Loss: 0.6441454291343689\n",
      "Train: Epoch [21], Batch [620/938], Loss: 0.37845170497894287\n",
      "Train: Epoch [21], Batch [621/938], Loss: 0.4641243815422058\n",
      "Train: Epoch [21], Batch [622/938], Loss: 0.9264524579048157\n",
      "Train: Epoch [21], Batch [623/938], Loss: 0.5457630157470703\n",
      "Train: Epoch [21], Batch [624/938], Loss: 0.5070542097091675\n",
      "Train: Epoch [21], Batch [625/938], Loss: 0.4024750888347626\n",
      "Train: Epoch [21], Batch [626/938], Loss: 0.27702632546424866\n",
      "Train: Epoch [21], Batch [627/938], Loss: 0.6063867807388306\n",
      "Train: Epoch [21], Batch [628/938], Loss: 0.3872186243534088\n",
      "Train: Epoch [21], Batch [629/938], Loss: 0.6682202816009521\n",
      "Train: Epoch [21], Batch [630/938], Loss: 0.5569633841514587\n",
      "Train: Epoch [21], Batch [631/938], Loss: 0.3345223367214203\n",
      "Train: Epoch [21], Batch [632/938], Loss: 0.36208319664001465\n",
      "Train: Epoch [21], Batch [633/938], Loss: 0.5757856369018555\n",
      "Train: Epoch [21], Batch [634/938], Loss: 0.44520726799964905\n",
      "Train: Epoch [21], Batch [635/938], Loss: 0.39437615871429443\n",
      "Train: Epoch [21], Batch [636/938], Loss: 0.3469475209712982\n",
      "Train: Epoch [21], Batch [637/938], Loss: 0.4476587176322937\n",
      "Train: Epoch [21], Batch [638/938], Loss: 0.1872866451740265\n",
      "Train: Epoch [21], Batch [639/938], Loss: 0.2979799509048462\n",
      "Train: Epoch [21], Batch [640/938], Loss: 0.39896640181541443\n",
      "Train: Epoch [21], Batch [641/938], Loss: 0.5156331062316895\n",
      "Train: Epoch [21], Batch [642/938], Loss: 0.5622473359107971\n",
      "Train: Epoch [21], Batch [643/938], Loss: 0.48441827297210693\n",
      "Train: Epoch [21], Batch [644/938], Loss: 0.5672804117202759\n",
      "Train: Epoch [21], Batch [645/938], Loss: 0.6107756495475769\n",
      "Train: Epoch [21], Batch [646/938], Loss: 0.3394487202167511\n",
      "Train: Epoch [21], Batch [647/938], Loss: 0.5864554643630981\n",
      "Train: Epoch [21], Batch [648/938], Loss: 0.5554546117782593\n",
      "Train: Epoch [21], Batch [649/938], Loss: 0.46253952383995056\n",
      "Train: Epoch [21], Batch [650/938], Loss: 0.3213070034980774\n",
      "Train: Epoch [21], Batch [651/938], Loss: 0.6088919639587402\n",
      "Train: Epoch [21], Batch [652/938], Loss: 0.5662809610366821\n",
      "Train: Epoch [21], Batch [653/938], Loss: 0.4528387486934662\n",
      "Train: Epoch [21], Batch [654/938], Loss: 0.3750170171260834\n",
      "Train: Epoch [21], Batch [655/938], Loss: 0.6329337358474731\n",
      "Train: Epoch [21], Batch [656/938], Loss: 0.6142820715904236\n",
      "Train: Epoch [21], Batch [657/938], Loss: 0.2865155339241028\n",
      "Train: Epoch [21], Batch [658/938], Loss: 0.3051399290561676\n",
      "Train: Epoch [21], Batch [659/938], Loss: 0.3491024076938629\n",
      "Train: Epoch [21], Batch [660/938], Loss: 0.6237350106239319\n",
      "Train: Epoch [21], Batch [661/938], Loss: 0.4809946119785309\n",
      "Train: Epoch [21], Batch [662/938], Loss: 0.4323192536830902\n",
      "Train: Epoch [21], Batch [663/938], Loss: 0.5187035799026489\n",
      "Train: Epoch [21], Batch [664/938], Loss: 0.5366049408912659\n",
      "Train: Epoch [21], Batch [665/938], Loss: 0.5032235980033875\n",
      "Train: Epoch [21], Batch [666/938], Loss: 0.3415845036506653\n",
      "Train: Epoch [21], Batch [667/938], Loss: 0.39210546016693115\n",
      "Train: Epoch [21], Batch [668/938], Loss: 0.2733561396598816\n",
      "Train: Epoch [21], Batch [669/938], Loss: 0.3560764193534851\n",
      "Train: Epoch [21], Batch [670/938], Loss: 0.5825835466384888\n",
      "Train: Epoch [21], Batch [671/938], Loss: 0.25721749663352966\n",
      "Train: Epoch [21], Batch [672/938], Loss: 0.26847606897354126\n",
      "Train: Epoch [21], Batch [673/938], Loss: 0.6553977727890015\n",
      "Train: Epoch [21], Batch [674/938], Loss: 0.37572625279426575\n",
      "Train: Epoch [21], Batch [675/938], Loss: 0.4204215705394745\n",
      "Train: Epoch [21], Batch [676/938], Loss: 0.2531181275844574\n",
      "Train: Epoch [21], Batch [677/938], Loss: 0.40177369117736816\n",
      "Train: Epoch [21], Batch [678/938], Loss: 0.46966099739074707\n",
      "Train: Epoch [21], Batch [679/938], Loss: 0.32628393173217773\n",
      "Train: Epoch [21], Batch [680/938], Loss: 0.32631078362464905\n",
      "Train: Epoch [21], Batch [681/938], Loss: 0.2858383357524872\n",
      "Train: Epoch [21], Batch [682/938], Loss: 0.534978985786438\n",
      "Train: Epoch [21], Batch [683/938], Loss: 0.4285857081413269\n",
      "Train: Epoch [21], Batch [684/938], Loss: 0.3884514570236206\n",
      "Train: Epoch [21], Batch [685/938], Loss: 0.250639408826828\n",
      "Train: Epoch [21], Batch [686/938], Loss: 0.3827556371688843\n",
      "Train: Epoch [21], Batch [687/938], Loss: 0.4658581614494324\n",
      "Train: Epoch [21], Batch [688/938], Loss: 0.5618807673454285\n",
      "Train: Epoch [21], Batch [689/938], Loss: 0.42071449756622314\n",
      "Train: Epoch [21], Batch [690/938], Loss: 0.38355809450149536\n",
      "Train: Epoch [21], Batch [691/938], Loss: 0.40951085090637207\n",
      "Train: Epoch [21], Batch [692/938], Loss: 0.5532829165458679\n",
      "Train: Epoch [21], Batch [693/938], Loss: 0.40355372428894043\n",
      "Train: Epoch [21], Batch [694/938], Loss: 0.6074024438858032\n",
      "Train: Epoch [21], Batch [695/938], Loss: 0.3755777180194855\n",
      "Train: Epoch [21], Batch [696/938], Loss: 0.21836143732070923\n",
      "Train: Epoch [21], Batch [697/938], Loss: 0.3876965641975403\n",
      "Train: Epoch [21], Batch [698/938], Loss: 0.4310798645019531\n",
      "Train: Epoch [21], Batch [699/938], Loss: 0.2274985909461975\n",
      "Train: Epoch [21], Batch [700/938], Loss: 0.42079785466194153\n",
      "Train: Epoch [21], Batch [701/938], Loss: 0.3594111502170563\n",
      "Train: Epoch [21], Batch [702/938], Loss: 0.5238725543022156\n",
      "Train: Epoch [21], Batch [703/938], Loss: 0.619358479976654\n",
      "Train: Epoch [21], Batch [704/938], Loss: 0.4463813900947571\n",
      "Train: Epoch [21], Batch [705/938], Loss: 0.4338936507701874\n",
      "Train: Epoch [21], Batch [706/938], Loss: 0.30398160219192505\n",
      "Train: Epoch [21], Batch [707/938], Loss: 0.5889902710914612\n",
      "Train: Epoch [21], Batch [708/938], Loss: 0.518640398979187\n",
      "Train: Epoch [21], Batch [709/938], Loss: 0.5980579853057861\n",
      "Train: Epoch [21], Batch [710/938], Loss: 0.5229539275169373\n",
      "Train: Epoch [21], Batch [711/938], Loss: 0.5400612354278564\n",
      "Train: Epoch [21], Batch [712/938], Loss: 0.42236626148223877\n",
      "Train: Epoch [21], Batch [713/938], Loss: 0.42179378867149353\n",
      "Train: Epoch [21], Batch [714/938], Loss: 0.33917880058288574\n",
      "Train: Epoch [21], Batch [715/938], Loss: 0.29086947441101074\n",
      "Train: Epoch [21], Batch [716/938], Loss: 0.3463461399078369\n",
      "Train: Epoch [21], Batch [717/938], Loss: 0.5142568349838257\n",
      "Train: Epoch [21], Batch [718/938], Loss: 0.3477477729320526\n",
      "Train: Epoch [21], Batch [719/938], Loss: 0.560063898563385\n",
      "Train: Epoch [21], Batch [720/938], Loss: 0.40908369421958923\n",
      "Train: Epoch [21], Batch [721/938], Loss: 0.32590240240097046\n",
      "Train: Epoch [21], Batch [722/938], Loss: 0.43294161558151245\n",
      "Train: Epoch [21], Batch [723/938], Loss: 0.46861979365348816\n",
      "Train: Epoch [21], Batch [724/938], Loss: 0.41003140807151794\n",
      "Train: Epoch [21], Batch [725/938], Loss: 0.3046731650829315\n",
      "Train: Epoch [21], Batch [726/938], Loss: 0.4733787477016449\n",
      "Train: Epoch [21], Batch [727/938], Loss: 0.29405587911605835\n",
      "Train: Epoch [21], Batch [728/938], Loss: 0.3825833201408386\n",
      "Train: Epoch [21], Batch [729/938], Loss: 0.5802860260009766\n",
      "Train: Epoch [21], Batch [730/938], Loss: 0.48921656608581543\n",
      "Train: Epoch [21], Batch [731/938], Loss: 0.5490503311157227\n",
      "Train: Epoch [21], Batch [732/938], Loss: 0.6796548366546631\n",
      "Train: Epoch [21], Batch [733/938], Loss: 0.4945450723171234\n",
      "Train: Epoch [21], Batch [734/938], Loss: 0.5136367082595825\n",
      "Train: Epoch [21], Batch [735/938], Loss: 0.45727503299713135\n",
      "Train: Epoch [21], Batch [736/938], Loss: 0.4526049494743347\n",
      "Train: Epoch [21], Batch [737/938], Loss: 0.23633107542991638\n",
      "Train: Epoch [21], Batch [738/938], Loss: 0.2907581925392151\n",
      "Train: Epoch [21], Batch [739/938], Loss: 0.3133869171142578\n",
      "Train: Epoch [21], Batch [740/938], Loss: 0.5898029804229736\n",
      "Train: Epoch [21], Batch [741/938], Loss: 0.39674991369247437\n",
      "Train: Epoch [21], Batch [742/938], Loss: 0.47134849429130554\n",
      "Train: Epoch [21], Batch [743/938], Loss: 0.3236660361289978\n",
      "Train: Epoch [21], Batch [744/938], Loss: 0.6116048097610474\n",
      "Train: Epoch [21], Batch [745/938], Loss: 0.48998215794563293\n",
      "Train: Epoch [21], Batch [746/938], Loss: 0.4165145754814148\n",
      "Train: Epoch [21], Batch [747/938], Loss: 0.36964231729507446\n",
      "Train: Epoch [21], Batch [748/938], Loss: 0.484453022480011\n",
      "Train: Epoch [21], Batch [749/938], Loss: 0.4010191857814789\n",
      "Train: Epoch [21], Batch [750/938], Loss: 0.3171335756778717\n",
      "Train: Epoch [21], Batch [751/938], Loss: 0.30657848715782166\n",
      "Train: Epoch [21], Batch [752/938], Loss: 0.3436284363269806\n",
      "Train: Epoch [21], Batch [753/938], Loss: 0.596650242805481\n",
      "Train: Epoch [21], Batch [754/938], Loss: 0.7090648412704468\n",
      "Train: Epoch [21], Batch [755/938], Loss: 0.39733824133872986\n",
      "Train: Epoch [21], Batch [756/938], Loss: 0.4066128730773926\n",
      "Train: Epoch [21], Batch [757/938], Loss: 0.5469469428062439\n",
      "Train: Epoch [21], Batch [758/938], Loss: 0.4966724216938019\n",
      "Train: Epoch [21], Batch [759/938], Loss: 0.5438642501831055\n",
      "Train: Epoch [21], Batch [760/938], Loss: 0.5088132619857788\n",
      "Train: Epoch [21], Batch [761/938], Loss: 0.551984965801239\n",
      "Train: Epoch [21], Batch [762/938], Loss: 0.46795374155044556\n",
      "Train: Epoch [21], Batch [763/938], Loss: 0.5674384236335754\n",
      "Train: Epoch [21], Batch [764/938], Loss: 0.7895311117172241\n",
      "Train: Epoch [21], Batch [765/938], Loss: 0.5434078574180603\n",
      "Train: Epoch [21], Batch [766/938], Loss: 0.30575427412986755\n",
      "Train: Epoch [21], Batch [767/938], Loss: 0.4994478225708008\n",
      "Train: Epoch [21], Batch [768/938], Loss: 0.36247488856315613\n",
      "Train: Epoch [21], Batch [769/938], Loss: 0.4506281316280365\n",
      "Train: Epoch [21], Batch [770/938], Loss: 0.413261353969574\n",
      "Train: Epoch [21], Batch [771/938], Loss: 0.5727247595787048\n",
      "Train: Epoch [21], Batch [772/938], Loss: 0.3166518211364746\n",
      "Train: Epoch [21], Batch [773/938], Loss: 0.5712622404098511\n",
      "Train: Epoch [21], Batch [774/938], Loss: 0.6359880566596985\n",
      "Train: Epoch [21], Batch [775/938], Loss: 0.46694713830947876\n",
      "Train: Epoch [21], Batch [776/938], Loss: 0.4115811884403229\n",
      "Train: Epoch [21], Batch [777/938], Loss: 0.335997074842453\n",
      "Train: Epoch [21], Batch [778/938], Loss: 0.3922511041164398\n",
      "Train: Epoch [21], Batch [779/938], Loss: 0.35324302315711975\n",
      "Train: Epoch [21], Batch [780/938], Loss: 0.4050309658050537\n",
      "Train: Epoch [21], Batch [781/938], Loss: 0.4121518135070801\n",
      "Train: Epoch [21], Batch [782/938], Loss: 0.5712542533874512\n",
      "Train: Epoch [21], Batch [783/938], Loss: 0.40325498580932617\n",
      "Train: Epoch [21], Batch [784/938], Loss: 0.369213342666626\n",
      "Train: Epoch [21], Batch [785/938], Loss: 0.3864128887653351\n",
      "Train: Epoch [21], Batch [786/938], Loss: 0.560267984867096\n",
      "Train: Epoch [21], Batch [787/938], Loss: 0.5598719716072083\n",
      "Train: Epoch [21], Batch [788/938], Loss: 0.2995336055755615\n",
      "Train: Epoch [21], Batch [789/938], Loss: 0.5867184400558472\n",
      "Train: Epoch [21], Batch [790/938], Loss: 0.3497559726238251\n",
      "Train: Epoch [21], Batch [791/938], Loss: 0.4100133180618286\n",
      "Train: Epoch [21], Batch [792/938], Loss: 0.36938580870628357\n",
      "Train: Epoch [21], Batch [793/938], Loss: 0.43735837936401367\n",
      "Train: Epoch [21], Batch [794/938], Loss: 0.37343814969062805\n",
      "Train: Epoch [21], Batch [795/938], Loss: 0.646580696105957\n",
      "Train: Epoch [21], Batch [796/938], Loss: 0.4386165738105774\n",
      "Train: Epoch [21], Batch [797/938], Loss: 0.31175923347473145\n",
      "Train: Epoch [21], Batch [798/938], Loss: 0.44454309344291687\n",
      "Train: Epoch [21], Batch [799/938], Loss: 0.4608563780784607\n",
      "Train: Epoch [21], Batch [800/938], Loss: 0.5385249853134155\n",
      "Train: Epoch [21], Batch [801/938], Loss: 0.3817685842514038\n",
      "Train: Epoch [21], Batch [802/938], Loss: 0.5615561008453369\n",
      "Train: Epoch [21], Batch [803/938], Loss: 0.6404807567596436\n",
      "Train: Epoch [21], Batch [804/938], Loss: 0.5078873634338379\n",
      "Train: Epoch [21], Batch [805/938], Loss: 0.5322893857955933\n",
      "Train: Epoch [21], Batch [806/938], Loss: 0.382672518491745\n",
      "Train: Epoch [21], Batch [807/938], Loss: 0.19627681374549866\n",
      "Train: Epoch [21], Batch [808/938], Loss: 0.21257179975509644\n",
      "Train: Epoch [21], Batch [809/938], Loss: 0.4962422847747803\n",
      "Train: Epoch [21], Batch [810/938], Loss: 0.32814061641693115\n",
      "Train: Epoch [21], Batch [811/938], Loss: 0.4507688283920288\n",
      "Train: Epoch [21], Batch [812/938], Loss: 0.5175381302833557\n",
      "Train: Epoch [21], Batch [813/938], Loss: 0.5508124828338623\n",
      "Train: Epoch [21], Batch [814/938], Loss: 0.3135007321834564\n",
      "Train: Epoch [21], Batch [815/938], Loss: 0.4283241331577301\n",
      "Train: Epoch [21], Batch [816/938], Loss: 0.45674484968185425\n",
      "Train: Epoch [21], Batch [817/938], Loss: 0.4444011151790619\n",
      "Train: Epoch [21], Batch [818/938], Loss: 0.4707954227924347\n",
      "Train: Epoch [21], Batch [819/938], Loss: 0.3912047743797302\n",
      "Train: Epoch [21], Batch [820/938], Loss: 0.4045896828174591\n",
      "Train: Epoch [21], Batch [821/938], Loss: 0.34780651330947876\n",
      "Train: Epoch [21], Batch [822/938], Loss: 0.7223663926124573\n",
      "Train: Epoch [21], Batch [823/938], Loss: 0.377691388130188\n",
      "Train: Epoch [21], Batch [824/938], Loss: 0.4650748670101166\n",
      "Train: Epoch [21], Batch [825/938], Loss: 0.5060710310935974\n",
      "Train: Epoch [21], Batch [826/938], Loss: 0.30411243438720703\n",
      "Train: Epoch [21], Batch [827/938], Loss: 0.3321954309940338\n",
      "Train: Epoch [21], Batch [828/938], Loss: 0.3322899043560028\n",
      "Train: Epoch [21], Batch [829/938], Loss: 0.48004183173179626\n",
      "Train: Epoch [21], Batch [830/938], Loss: 0.4495794475078583\n",
      "Train: Epoch [21], Batch [831/938], Loss: 0.48566749691963196\n",
      "Train: Epoch [21], Batch [832/938], Loss: 0.4201158881187439\n",
      "Train: Epoch [21], Batch [833/938], Loss: 0.5279255509376526\n",
      "Train: Epoch [21], Batch [834/938], Loss: 0.8132086396217346\n",
      "Train: Epoch [21], Batch [835/938], Loss: 0.6008719205856323\n",
      "Train: Epoch [21], Batch [836/938], Loss: 0.5066702365875244\n",
      "Train: Epoch [21], Batch [837/938], Loss: 0.556692361831665\n",
      "Train: Epoch [21], Batch [838/938], Loss: 0.38232219219207764\n",
      "Train: Epoch [21], Batch [839/938], Loss: 0.4718170464038849\n",
      "Train: Epoch [21], Batch [840/938], Loss: 0.5691386461257935\n",
      "Train: Epoch [21], Batch [841/938], Loss: 0.2515280544757843\n",
      "Train: Epoch [21], Batch [842/938], Loss: 0.5709216594696045\n",
      "Train: Epoch [21], Batch [843/938], Loss: 0.47798821330070496\n",
      "Train: Epoch [21], Batch [844/938], Loss: 0.5323148369789124\n",
      "Train: Epoch [21], Batch [845/938], Loss: 0.5264046788215637\n",
      "Train: Epoch [21], Batch [846/938], Loss: 0.39692285656929016\n",
      "Train: Epoch [21], Batch [847/938], Loss: 0.3481822609901428\n",
      "Train: Epoch [21], Batch [848/938], Loss: 0.4091704785823822\n",
      "Train: Epoch [21], Batch [849/938], Loss: 0.4966886639595032\n",
      "Train: Epoch [21], Batch [850/938], Loss: 0.5372148752212524\n",
      "Train: Epoch [21], Batch [851/938], Loss: 0.5274102091789246\n",
      "Train: Epoch [21], Batch [852/938], Loss: 0.49511855840682983\n",
      "Train: Epoch [21], Batch [853/938], Loss: 0.409151554107666\n",
      "Train: Epoch [21], Batch [854/938], Loss: 0.5925868153572083\n",
      "Train: Epoch [21], Batch [855/938], Loss: 0.7351988554000854\n",
      "Train: Epoch [21], Batch [856/938], Loss: 0.5304946899414062\n",
      "Train: Epoch [21], Batch [857/938], Loss: 0.32709813117980957\n",
      "Train: Epoch [21], Batch [858/938], Loss: 0.4182170331478119\n",
      "Train: Epoch [21], Batch [859/938], Loss: 0.33554551005363464\n",
      "Train: Epoch [21], Batch [860/938], Loss: 0.6898021101951599\n",
      "Train: Epoch [21], Batch [861/938], Loss: 0.47892624139785767\n",
      "Train: Epoch [21], Batch [862/938], Loss: 0.3454365134239197\n",
      "Train: Epoch [21], Batch [863/938], Loss: 0.4748832881450653\n",
      "Train: Epoch [21], Batch [864/938], Loss: 0.4328001141548157\n",
      "Train: Epoch [21], Batch [865/938], Loss: 0.7341570854187012\n",
      "Train: Epoch [21], Batch [866/938], Loss: 0.47038373351097107\n",
      "Train: Epoch [21], Batch [867/938], Loss: 0.31828421354293823\n",
      "Train: Epoch [21], Batch [868/938], Loss: 0.5114817023277283\n",
      "Train: Epoch [21], Batch [869/938], Loss: 0.44969144463539124\n",
      "Train: Epoch [21], Batch [870/938], Loss: 0.3480638563632965\n",
      "Train: Epoch [21], Batch [871/938], Loss: 0.42617422342300415\n",
      "Train: Epoch [21], Batch [872/938], Loss: 0.41100022196769714\n",
      "Train: Epoch [21], Batch [873/938], Loss: 0.40477198362350464\n",
      "Train: Epoch [21], Batch [874/938], Loss: 0.37924665212631226\n",
      "Train: Epoch [21], Batch [875/938], Loss: 0.5717971324920654\n",
      "Train: Epoch [21], Batch [876/938], Loss: 0.4284234046936035\n",
      "Train: Epoch [21], Batch [877/938], Loss: 0.49528932571411133\n",
      "Train: Epoch [21], Batch [878/938], Loss: 0.3709251284599304\n",
      "Train: Epoch [21], Batch [879/938], Loss: 0.5403645038604736\n",
      "Train: Epoch [21], Batch [880/938], Loss: 0.32106393575668335\n",
      "Train: Epoch [21], Batch [881/938], Loss: 0.4200246036052704\n",
      "Train: Epoch [21], Batch [882/938], Loss: 0.5709710121154785\n",
      "Train: Epoch [21], Batch [883/938], Loss: 0.7549566030502319\n",
      "Train: Epoch [21], Batch [884/938], Loss: 0.4451775550842285\n",
      "Train: Epoch [21], Batch [885/938], Loss: 0.48074010014533997\n",
      "Train: Epoch [21], Batch [886/938], Loss: 0.3328540325164795\n",
      "Train: Epoch [21], Batch [887/938], Loss: 0.4799945056438446\n",
      "Train: Epoch [21], Batch [888/938], Loss: 0.2904450595378876\n",
      "Train: Epoch [21], Batch [889/938], Loss: 0.5301196575164795\n",
      "Train: Epoch [21], Batch [890/938], Loss: 0.446554034948349\n",
      "Train: Epoch [21], Batch [891/938], Loss: 0.5293368101119995\n",
      "Train: Epoch [21], Batch [892/938], Loss: 0.4581090211868286\n",
      "Train: Epoch [21], Batch [893/938], Loss: 0.493992418050766\n",
      "Train: Epoch [21], Batch [894/938], Loss: 0.5095471143722534\n",
      "Train: Epoch [21], Batch [895/938], Loss: 0.4918891191482544\n",
      "Train: Epoch [21], Batch [896/938], Loss: 0.6188331246376038\n",
      "Train: Epoch [21], Batch [897/938], Loss: 0.27881038188934326\n",
      "Train: Epoch [21], Batch [898/938], Loss: 0.31818005442619324\n",
      "Train: Epoch [21], Batch [899/938], Loss: 0.36444202065467834\n",
      "Train: Epoch [21], Batch [900/938], Loss: 0.5440162420272827\n",
      "Train: Epoch [21], Batch [901/938], Loss: 0.7221361398696899\n",
      "Train: Epoch [21], Batch [902/938], Loss: 0.708393931388855\n",
      "Train: Epoch [21], Batch [903/938], Loss: 0.3243025541305542\n",
      "Train: Epoch [21], Batch [904/938], Loss: 0.23817706108093262\n",
      "Train: Epoch [21], Batch [905/938], Loss: 0.3058319687843323\n",
      "Train: Epoch [21], Batch [906/938], Loss: 0.5892288088798523\n",
      "Train: Epoch [21], Batch [907/938], Loss: 0.5806435942649841\n",
      "Train: Epoch [21], Batch [908/938], Loss: 0.5047706365585327\n",
      "Train: Epoch [21], Batch [909/938], Loss: 0.393726110458374\n",
      "Train: Epoch [21], Batch [910/938], Loss: 0.3003033995628357\n",
      "Train: Epoch [21], Batch [911/938], Loss: 0.6122342348098755\n",
      "Train: Epoch [21], Batch [912/938], Loss: 0.5535247325897217\n",
      "Train: Epoch [21], Batch [913/938], Loss: 0.6699496507644653\n",
      "Train: Epoch [21], Batch [914/938], Loss: 0.3687446117401123\n",
      "Train: Epoch [21], Batch [915/938], Loss: 0.5937610864639282\n",
      "Train: Epoch [21], Batch [916/938], Loss: 0.60916668176651\n",
      "Train: Epoch [21], Batch [917/938], Loss: 0.6474658846855164\n",
      "Train: Epoch [21], Batch [918/938], Loss: 0.6406046748161316\n",
      "Train: Epoch [21], Batch [919/938], Loss: 0.619915246963501\n",
      "Train: Epoch [21], Batch [920/938], Loss: 0.4548194408416748\n",
      "Train: Epoch [21], Batch [921/938], Loss: 0.5528315305709839\n",
      "Train: Epoch [21], Batch [922/938], Loss: 0.45359671115875244\n",
      "Train: Epoch [21], Batch [923/938], Loss: 0.48245131969451904\n",
      "Train: Epoch [21], Batch [924/938], Loss: 0.4760509729385376\n",
      "Train: Epoch [21], Batch [925/938], Loss: 0.6826813220977783\n",
      "Train: Epoch [21], Batch [926/938], Loss: 0.3648812472820282\n",
      "Train: Epoch [21], Batch [927/938], Loss: 0.7187749147415161\n",
      "Train: Epoch [21], Batch [928/938], Loss: 0.46419546008110046\n",
      "Train: Epoch [21], Batch [929/938], Loss: 0.3621950149536133\n",
      "Train: Epoch [21], Batch [930/938], Loss: 0.4055219292640686\n",
      "Train: Epoch [21], Batch [931/938], Loss: 0.4544970989227295\n",
      "Train: Epoch [21], Batch [932/938], Loss: 0.3946837782859802\n",
      "Train: Epoch [21], Batch [933/938], Loss: 0.6396151185035706\n",
      "Train: Epoch [21], Batch [934/938], Loss: 0.3005361557006836\n",
      "Train: Epoch [21], Batch [935/938], Loss: 0.30626124143600464\n",
      "Train: Epoch [21], Batch [936/938], Loss: 0.4367550313472748\n",
      "Train: Epoch [21], Batch [937/938], Loss: 0.4720197021961212\n",
      "Train: Epoch [21], Batch [938/938], Loss: 0.4203357398509979\n",
      "Accuracy of train set: 0.8439166666666666\n",
      "Validation: Epoch [21], Batch [1/938], Loss: 0.4778023660182953\n",
      "Validation: Epoch [21], Batch [2/938], Loss: 0.33911117911338806\n",
      "Validation: Epoch [21], Batch [3/938], Loss: 0.5475714206695557\n",
      "Validation: Epoch [21], Batch [4/938], Loss: 0.23728220164775848\n",
      "Validation: Epoch [21], Batch [5/938], Loss: 0.36285436153411865\n",
      "Validation: Epoch [21], Batch [6/938], Loss: 0.47661635279655457\n",
      "Validation: Epoch [21], Batch [7/938], Loss: 0.4801311790943146\n",
      "Validation: Epoch [21], Batch [8/938], Loss: 0.5275909304618835\n",
      "Validation: Epoch [21], Batch [9/938], Loss: 0.5191034078598022\n",
      "Validation: Epoch [21], Batch [10/938], Loss: 0.5387913584709167\n",
      "Validation: Epoch [21], Batch [11/938], Loss: 0.43553626537323\n",
      "Validation: Epoch [21], Batch [12/938], Loss: 0.5099802017211914\n",
      "Validation: Epoch [21], Batch [13/938], Loss: 0.31099236011505127\n",
      "Validation: Epoch [21], Batch [14/938], Loss: 0.41758912801742554\n",
      "Validation: Epoch [21], Batch [15/938], Loss: 0.28349563479423523\n",
      "Validation: Epoch [21], Batch [16/938], Loss: 0.29460880160331726\n",
      "Validation: Epoch [21], Batch [17/938], Loss: 0.4027210474014282\n",
      "Validation: Epoch [21], Batch [18/938], Loss: 0.37530335783958435\n",
      "Validation: Epoch [21], Batch [19/938], Loss: 0.3392556607723236\n",
      "Validation: Epoch [21], Batch [20/938], Loss: 0.35285258293151855\n",
      "Validation: Epoch [21], Batch [21/938], Loss: 0.3837593197822571\n",
      "Validation: Epoch [21], Batch [22/938], Loss: 0.4319031238555908\n",
      "Validation: Epoch [21], Batch [23/938], Loss: 0.44233986735343933\n",
      "Validation: Epoch [21], Batch [24/938], Loss: 0.5548397898674011\n",
      "Validation: Epoch [21], Batch [25/938], Loss: 0.5566295385360718\n",
      "Validation: Epoch [21], Batch [26/938], Loss: 0.34213605523109436\n",
      "Validation: Epoch [21], Batch [27/938], Loss: 0.4318734407424927\n",
      "Validation: Epoch [21], Batch [28/938], Loss: 0.46647387742996216\n",
      "Validation: Epoch [21], Batch [29/938], Loss: 0.6771769523620605\n",
      "Validation: Epoch [21], Batch [30/938], Loss: 0.3592531681060791\n",
      "Validation: Epoch [21], Batch [31/938], Loss: 0.5253103971481323\n",
      "Validation: Epoch [21], Batch [32/938], Loss: 0.5806215405464172\n",
      "Validation: Epoch [21], Batch [33/938], Loss: 0.2884395718574524\n",
      "Validation: Epoch [21], Batch [34/938], Loss: 0.4793868064880371\n",
      "Validation: Epoch [21], Batch [35/938], Loss: 0.4919821619987488\n",
      "Validation: Epoch [21], Batch [36/938], Loss: 0.4042614996433258\n",
      "Validation: Epoch [21], Batch [37/938], Loss: 0.5883855223655701\n",
      "Validation: Epoch [21], Batch [38/938], Loss: 0.5180996656417847\n",
      "Validation: Epoch [21], Batch [39/938], Loss: 0.2627282738685608\n",
      "Validation: Epoch [21], Batch [40/938], Loss: 0.37193846702575684\n",
      "Validation: Epoch [21], Batch [41/938], Loss: 0.3515205383300781\n",
      "Validation: Epoch [21], Batch [42/938], Loss: 0.5665171146392822\n",
      "Validation: Epoch [21], Batch [43/938], Loss: 0.3955531120300293\n",
      "Validation: Epoch [21], Batch [44/938], Loss: 0.35866111516952515\n",
      "Validation: Epoch [21], Batch [45/938], Loss: 0.4012860655784607\n",
      "Validation: Epoch [21], Batch [46/938], Loss: 0.3834700882434845\n",
      "Validation: Epoch [21], Batch [47/938], Loss: 0.5124296545982361\n",
      "Validation: Epoch [21], Batch [48/938], Loss: 0.326834499835968\n",
      "Validation: Epoch [21], Batch [49/938], Loss: 0.44904330372810364\n",
      "Validation: Epoch [21], Batch [50/938], Loss: 0.29870402812957764\n",
      "Validation: Epoch [21], Batch [51/938], Loss: 0.7331105470657349\n",
      "Validation: Epoch [21], Batch [52/938], Loss: 0.3204022943973541\n",
      "Validation: Epoch [21], Batch [53/938], Loss: 0.2821349501609802\n",
      "Validation: Epoch [21], Batch [54/938], Loss: 0.24160397052764893\n",
      "Validation: Epoch [21], Batch [55/938], Loss: 0.5592762231826782\n",
      "Validation: Epoch [21], Batch [56/938], Loss: 0.47245800495147705\n",
      "Validation: Epoch [21], Batch [57/938], Loss: 0.2608374059200287\n",
      "Validation: Epoch [21], Batch [58/938], Loss: 0.48000943660736084\n",
      "Validation: Epoch [21], Batch [59/938], Loss: 0.5282129049301147\n",
      "Validation: Epoch [21], Batch [60/938], Loss: 0.6817232370376587\n",
      "Validation: Epoch [21], Batch [61/938], Loss: 0.4556988775730133\n",
      "Validation: Epoch [21], Batch [62/938], Loss: 0.47382861375808716\n",
      "Validation: Epoch [21], Batch [63/938], Loss: 0.33605268597602844\n",
      "Validation: Epoch [21], Batch [64/938], Loss: 0.46289634704589844\n",
      "Validation: Epoch [21], Batch [65/938], Loss: 0.6802234649658203\n",
      "Validation: Epoch [21], Batch [66/938], Loss: 0.3908560574054718\n",
      "Validation: Epoch [21], Batch [67/938], Loss: 0.4899648129940033\n",
      "Validation: Epoch [21], Batch [68/938], Loss: 0.4959516227245331\n",
      "Validation: Epoch [21], Batch [69/938], Loss: 0.43313145637512207\n",
      "Validation: Epoch [21], Batch [70/938], Loss: 0.4900098443031311\n",
      "Validation: Epoch [21], Batch [71/938], Loss: 0.3532484471797943\n",
      "Validation: Epoch [21], Batch [72/938], Loss: 0.5419753193855286\n",
      "Validation: Epoch [21], Batch [73/938], Loss: 0.33314892649650574\n",
      "Validation: Epoch [21], Batch [74/938], Loss: 0.56414794921875\n",
      "Validation: Epoch [21], Batch [75/938], Loss: 0.48581990599632263\n",
      "Validation: Epoch [21], Batch [76/938], Loss: 0.6769723892211914\n",
      "Validation: Epoch [21], Batch [77/938], Loss: 0.49210643768310547\n",
      "Validation: Epoch [21], Batch [78/938], Loss: 0.7842962145805359\n",
      "Validation: Epoch [21], Batch [79/938], Loss: 0.6211491823196411\n",
      "Validation: Epoch [21], Batch [80/938], Loss: 0.339029997587204\n",
      "Validation: Epoch [21], Batch [81/938], Loss: 0.4055703580379486\n",
      "Validation: Epoch [21], Batch [82/938], Loss: 0.6801833510398865\n",
      "Validation: Epoch [21], Batch [83/938], Loss: 0.4327910840511322\n",
      "Validation: Epoch [21], Batch [84/938], Loss: 0.442679226398468\n",
      "Validation: Epoch [21], Batch [85/938], Loss: 0.48975107073783875\n",
      "Validation: Epoch [21], Batch [86/938], Loss: 0.4439884424209595\n",
      "Validation: Epoch [21], Batch [87/938], Loss: 0.5476123094558716\n",
      "Validation: Epoch [21], Batch [88/938], Loss: 0.38017526268959045\n",
      "Validation: Epoch [21], Batch [89/938], Loss: 0.5897006988525391\n",
      "Validation: Epoch [21], Batch [90/938], Loss: 0.48347386717796326\n",
      "Validation: Epoch [21], Batch [91/938], Loss: 0.542321503162384\n",
      "Validation: Epoch [21], Batch [92/938], Loss: 0.4365488290786743\n",
      "Validation: Epoch [21], Batch [93/938], Loss: 0.48154038190841675\n",
      "Validation: Epoch [21], Batch [94/938], Loss: 0.46474045515060425\n",
      "Validation: Epoch [21], Batch [95/938], Loss: 0.5636441707611084\n",
      "Validation: Epoch [21], Batch [96/938], Loss: 0.4003281593322754\n",
      "Validation: Epoch [21], Batch [97/938], Loss: 0.5435752272605896\n",
      "Validation: Epoch [21], Batch [98/938], Loss: 0.5709006786346436\n",
      "Validation: Epoch [21], Batch [99/938], Loss: 0.36781007051467896\n",
      "Validation: Epoch [21], Batch [100/938], Loss: 0.5929293632507324\n",
      "Validation: Epoch [21], Batch [101/938], Loss: 0.34157902002334595\n",
      "Validation: Epoch [21], Batch [102/938], Loss: 0.41418325901031494\n",
      "Validation: Epoch [21], Batch [103/938], Loss: 0.4905894696712494\n",
      "Validation: Epoch [21], Batch [104/938], Loss: 0.32366618514060974\n",
      "Validation: Epoch [21], Batch [105/938], Loss: 0.49218887090682983\n",
      "Validation: Epoch [21], Batch [106/938], Loss: 0.6207559704780579\n",
      "Validation: Epoch [21], Batch [107/938], Loss: 0.4072988033294678\n",
      "Validation: Epoch [21], Batch [108/938], Loss: 0.3339962363243103\n",
      "Validation: Epoch [21], Batch [109/938], Loss: 0.6866921186447144\n",
      "Validation: Epoch [21], Batch [110/938], Loss: 0.2553673982620239\n",
      "Validation: Epoch [21], Batch [111/938], Loss: 0.44632038474082947\n",
      "Validation: Epoch [21], Batch [112/938], Loss: 0.33167004585266113\n",
      "Validation: Epoch [21], Batch [113/938], Loss: 0.42057013511657715\n",
      "Validation: Epoch [21], Batch [114/938], Loss: 0.4629969298839569\n",
      "Validation: Epoch [21], Batch [115/938], Loss: 0.5353860259056091\n",
      "Validation: Epoch [21], Batch [116/938], Loss: 0.5170191526412964\n",
      "Validation: Epoch [21], Batch [117/938], Loss: 0.519072413444519\n",
      "Validation: Epoch [21], Batch [118/938], Loss: 0.5670256614685059\n",
      "Validation: Epoch [21], Batch [119/938], Loss: 0.506033718585968\n",
      "Validation: Epoch [21], Batch [120/938], Loss: 0.5672802329063416\n",
      "Validation: Epoch [21], Batch [121/938], Loss: 0.4454922676086426\n",
      "Validation: Epoch [21], Batch [122/938], Loss: 0.46962806582450867\n",
      "Validation: Epoch [21], Batch [123/938], Loss: 0.35047316551208496\n",
      "Validation: Epoch [21], Batch [124/938], Loss: 0.49039751291275024\n",
      "Validation: Epoch [21], Batch [125/938], Loss: 0.383179247379303\n",
      "Validation: Epoch [21], Batch [126/938], Loss: 0.40295809507369995\n",
      "Validation: Epoch [21], Batch [127/938], Loss: 0.48752355575561523\n",
      "Validation: Epoch [21], Batch [128/938], Loss: 0.32993006706237793\n",
      "Validation: Epoch [21], Batch [129/938], Loss: 0.37047839164733887\n",
      "Validation: Epoch [21], Batch [130/938], Loss: 0.4642273187637329\n",
      "Validation: Epoch [21], Batch [131/938], Loss: 0.7776219248771667\n",
      "Validation: Epoch [21], Batch [132/938], Loss: 0.4474652409553528\n",
      "Validation: Epoch [21], Batch [133/938], Loss: 0.5627731680870056\n",
      "Validation: Epoch [21], Batch [134/938], Loss: 0.5228279829025269\n",
      "Validation: Epoch [21], Batch [135/938], Loss: 0.24420946836471558\n",
      "Validation: Epoch [21], Batch [136/938], Loss: 0.35552147030830383\n",
      "Validation: Epoch [21], Batch [137/938], Loss: 0.505770206451416\n",
      "Validation: Epoch [21], Batch [138/938], Loss: 0.627716600894928\n",
      "Validation: Epoch [21], Batch [139/938], Loss: 0.4424878656864166\n",
      "Validation: Epoch [21], Batch [140/938], Loss: 0.38142988085746765\n",
      "Validation: Epoch [21], Batch [141/938], Loss: 0.39669227600097656\n",
      "Validation: Epoch [21], Batch [142/938], Loss: 0.4150446355342865\n",
      "Validation: Epoch [21], Batch [143/938], Loss: 0.49052694439888\n",
      "Validation: Epoch [21], Batch [144/938], Loss: 0.41713184118270874\n",
      "Validation: Epoch [21], Batch [145/938], Loss: 0.5114936232566833\n",
      "Validation: Epoch [21], Batch [146/938], Loss: 0.4560803174972534\n",
      "Validation: Epoch [21], Batch [147/938], Loss: 0.3597397804260254\n",
      "Validation: Epoch [21], Batch [148/938], Loss: 0.385015606880188\n",
      "Validation: Epoch [21], Batch [149/938], Loss: 0.3594573140144348\n",
      "Validation: Epoch [21], Batch [150/938], Loss: 0.4588366746902466\n",
      "Validation: Epoch [21], Batch [151/938], Loss: 0.39247724413871765\n",
      "Validation: Epoch [21], Batch [152/938], Loss: 0.3632998466491699\n",
      "Validation: Epoch [21], Batch [153/938], Loss: 0.33871152997016907\n",
      "Validation: Epoch [21], Batch [154/938], Loss: 0.49356377124786377\n",
      "Validation: Epoch [21], Batch [155/938], Loss: 0.4366225004196167\n",
      "Validation: Epoch [21], Batch [156/938], Loss: 0.6744913458824158\n",
      "Validation: Epoch [21], Batch [157/938], Loss: 0.412811815738678\n",
      "Validation: Epoch [21], Batch [158/938], Loss: 0.3489198684692383\n",
      "Validation: Epoch [21], Batch [159/938], Loss: 0.4695179760456085\n",
      "Validation: Epoch [21], Batch [160/938], Loss: 0.41662243008613586\n",
      "Validation: Epoch [21], Batch [161/938], Loss: 0.5956471562385559\n",
      "Validation: Epoch [21], Batch [162/938], Loss: 0.46059170365333557\n",
      "Validation: Epoch [21], Batch [163/938], Loss: 0.45392611622810364\n",
      "Validation: Epoch [21], Batch [164/938], Loss: 0.30172276496887207\n",
      "Validation: Epoch [21], Batch [165/938], Loss: 0.42971718311309814\n",
      "Validation: Epoch [21], Batch [166/938], Loss: 0.45267900824546814\n",
      "Validation: Epoch [21], Batch [167/938], Loss: 0.5975733995437622\n",
      "Validation: Epoch [21], Batch [168/938], Loss: 0.5778652429580688\n",
      "Validation: Epoch [21], Batch [169/938], Loss: 0.45948556065559387\n",
      "Validation: Epoch [21], Batch [170/938], Loss: 0.615644633769989\n",
      "Validation: Epoch [21], Batch [171/938], Loss: 0.23863162100315094\n",
      "Validation: Epoch [21], Batch [172/938], Loss: 0.3024210035800934\n",
      "Validation: Epoch [21], Batch [173/938], Loss: 0.47585442662239075\n",
      "Validation: Epoch [21], Batch [174/938], Loss: 0.5451563000679016\n",
      "Validation: Epoch [21], Batch [175/938], Loss: 0.39652541279792786\n",
      "Validation: Epoch [21], Batch [176/938], Loss: 0.4009389877319336\n",
      "Validation: Epoch [21], Batch [177/938], Loss: 0.34282270073890686\n",
      "Validation: Epoch [21], Batch [178/938], Loss: 0.47304144501686096\n",
      "Validation: Epoch [21], Batch [179/938], Loss: 0.40666356682777405\n",
      "Validation: Epoch [21], Batch [180/938], Loss: 0.42711934447288513\n",
      "Validation: Epoch [21], Batch [181/938], Loss: 0.7354980707168579\n",
      "Validation: Epoch [21], Batch [182/938], Loss: 0.21972747147083282\n",
      "Validation: Epoch [21], Batch [183/938], Loss: 0.5664533972740173\n",
      "Validation: Epoch [21], Batch [184/938], Loss: 0.34399157762527466\n",
      "Validation: Epoch [21], Batch [185/938], Loss: 0.36527472734451294\n",
      "Validation: Epoch [21], Batch [186/938], Loss: 0.41499119997024536\n",
      "Validation: Epoch [21], Batch [187/938], Loss: 0.5179761052131653\n",
      "Validation: Epoch [21], Batch [188/938], Loss: 0.46581605076789856\n",
      "Validation: Epoch [21], Batch [189/938], Loss: 0.5073608160018921\n",
      "Validation: Epoch [21], Batch [190/938], Loss: 0.3078370988368988\n",
      "Validation: Epoch [21], Batch [191/938], Loss: 0.41845786571502686\n",
      "Validation: Epoch [21], Batch [192/938], Loss: 0.6155505180358887\n",
      "Validation: Epoch [21], Batch [193/938], Loss: 0.6287217736244202\n",
      "Validation: Epoch [21], Batch [194/938], Loss: 0.4055231809616089\n",
      "Validation: Epoch [21], Batch [195/938], Loss: 0.5159711837768555\n",
      "Validation: Epoch [21], Batch [196/938], Loss: 0.4858325123786926\n",
      "Validation: Epoch [21], Batch [197/938], Loss: 0.36875319480895996\n",
      "Validation: Epoch [21], Batch [198/938], Loss: 0.5603710412979126\n",
      "Validation: Epoch [21], Batch [199/938], Loss: 0.4676657021045685\n",
      "Validation: Epoch [21], Batch [200/938], Loss: 0.3245459794998169\n",
      "Validation: Epoch [21], Batch [201/938], Loss: 0.4915725588798523\n",
      "Validation: Epoch [21], Batch [202/938], Loss: 0.3598422706127167\n",
      "Validation: Epoch [21], Batch [203/938], Loss: 0.3330047130584717\n",
      "Validation: Epoch [21], Batch [204/938], Loss: 0.6622816324234009\n",
      "Validation: Epoch [21], Batch [205/938], Loss: 0.5552191138267517\n",
      "Validation: Epoch [21], Batch [206/938], Loss: 0.7481176853179932\n",
      "Validation: Epoch [21], Batch [207/938], Loss: 0.47618353366851807\n",
      "Validation: Epoch [21], Batch [208/938], Loss: 0.4095477759838104\n",
      "Validation: Epoch [21], Batch [209/938], Loss: 0.58168625831604\n",
      "Validation: Epoch [21], Batch [210/938], Loss: 0.5059179067611694\n",
      "Validation: Epoch [21], Batch [211/938], Loss: 0.3983627259731293\n",
      "Validation: Epoch [21], Batch [212/938], Loss: 0.4740414619445801\n",
      "Validation: Epoch [21], Batch [213/938], Loss: 0.3506379723548889\n",
      "Validation: Epoch [21], Batch [214/938], Loss: 0.27094149589538574\n",
      "Validation: Epoch [21], Batch [215/938], Loss: 0.5575629472732544\n",
      "Validation: Epoch [21], Batch [216/938], Loss: 0.6229475140571594\n",
      "Validation: Epoch [21], Batch [217/938], Loss: 0.2914432883262634\n",
      "Validation: Epoch [21], Batch [218/938], Loss: 0.44038480520248413\n",
      "Validation: Epoch [21], Batch [219/938], Loss: 0.6490182876586914\n",
      "Validation: Epoch [21], Batch [220/938], Loss: 0.25550687313079834\n",
      "Validation: Epoch [21], Batch [221/938], Loss: 0.3457881808280945\n",
      "Validation: Epoch [21], Batch [222/938], Loss: 0.277914822101593\n",
      "Validation: Epoch [21], Batch [223/938], Loss: 0.5522432327270508\n",
      "Validation: Epoch [21], Batch [224/938], Loss: 0.6314266920089722\n",
      "Validation: Epoch [21], Batch [225/938], Loss: 0.7329533100128174\n",
      "Validation: Epoch [21], Batch [226/938], Loss: 0.4070427119731903\n",
      "Validation: Epoch [21], Batch [227/938], Loss: 0.3690211772918701\n",
      "Validation: Epoch [21], Batch [228/938], Loss: 0.4498025178909302\n",
      "Validation: Epoch [21], Batch [229/938], Loss: 0.45322901010513306\n",
      "Validation: Epoch [21], Batch [230/938], Loss: 0.3662879467010498\n",
      "Validation: Epoch [21], Batch [231/938], Loss: 0.662349283695221\n",
      "Validation: Epoch [21], Batch [232/938], Loss: 0.5657333731651306\n",
      "Validation: Epoch [21], Batch [233/938], Loss: 0.24538594484329224\n",
      "Validation: Epoch [21], Batch [234/938], Loss: 0.3978089988231659\n",
      "Validation: Epoch [21], Batch [235/938], Loss: 0.7606128454208374\n",
      "Validation: Epoch [21], Batch [236/938], Loss: 0.3062259554862976\n",
      "Validation: Epoch [21], Batch [237/938], Loss: 0.3659968972206116\n",
      "Validation: Epoch [21], Batch [238/938], Loss: 0.46425002813339233\n",
      "Validation: Epoch [21], Batch [239/938], Loss: 0.38504353165626526\n",
      "Validation: Epoch [21], Batch [240/938], Loss: 0.3123907446861267\n",
      "Validation: Epoch [21], Batch [241/938], Loss: 0.839844286441803\n",
      "Validation: Epoch [21], Batch [242/938], Loss: 0.4305811822414398\n",
      "Validation: Epoch [21], Batch [243/938], Loss: 0.5982890129089355\n",
      "Validation: Epoch [21], Batch [244/938], Loss: 0.4016130864620209\n",
      "Validation: Epoch [21], Batch [245/938], Loss: 0.4053712487220764\n",
      "Validation: Epoch [21], Batch [246/938], Loss: 0.4416486620903015\n",
      "Validation: Epoch [21], Batch [247/938], Loss: 0.601111888885498\n",
      "Validation: Epoch [21], Batch [248/938], Loss: 0.40678131580352783\n",
      "Validation: Epoch [21], Batch [249/938], Loss: 0.4105585813522339\n",
      "Validation: Epoch [21], Batch [250/938], Loss: 0.4233672618865967\n",
      "Validation: Epoch [21], Batch [251/938], Loss: 0.4655114412307739\n",
      "Validation: Epoch [21], Batch [252/938], Loss: 0.3609519600868225\n",
      "Validation: Epoch [21], Batch [253/938], Loss: 0.3913460969924927\n",
      "Validation: Epoch [21], Batch [254/938], Loss: 0.478495329618454\n",
      "Validation: Epoch [21], Batch [255/938], Loss: 0.5802520513534546\n",
      "Validation: Epoch [21], Batch [256/938], Loss: 0.4141058325767517\n",
      "Validation: Epoch [21], Batch [257/938], Loss: 0.7850546836853027\n",
      "Validation: Epoch [21], Batch [258/938], Loss: 0.4877917170524597\n",
      "Validation: Epoch [21], Batch [259/938], Loss: 0.46038463711738586\n",
      "Validation: Epoch [21], Batch [260/938], Loss: 0.27421626448631287\n",
      "Validation: Epoch [21], Batch [261/938], Loss: 0.3365441560745239\n",
      "Validation: Epoch [21], Batch [262/938], Loss: 0.22387449443340302\n",
      "Validation: Epoch [21], Batch [263/938], Loss: 0.607135534286499\n",
      "Validation: Epoch [21], Batch [264/938], Loss: 0.8147993683815002\n",
      "Validation: Epoch [21], Batch [265/938], Loss: 0.4806618392467499\n",
      "Validation: Epoch [21], Batch [266/938], Loss: 0.493732213973999\n",
      "Validation: Epoch [21], Batch [267/938], Loss: 0.24921825528144836\n",
      "Validation: Epoch [21], Batch [268/938], Loss: 0.4400068521499634\n",
      "Validation: Epoch [21], Batch [269/938], Loss: 0.27991029620170593\n",
      "Validation: Epoch [21], Batch [270/938], Loss: 0.28737154603004456\n",
      "Validation: Epoch [21], Batch [271/938], Loss: 0.38513848185539246\n",
      "Validation: Epoch [21], Batch [272/938], Loss: 0.34317493438720703\n",
      "Validation: Epoch [21], Batch [273/938], Loss: 0.22278963029384613\n",
      "Validation: Epoch [21], Batch [274/938], Loss: 0.375078022480011\n",
      "Validation: Epoch [21], Batch [275/938], Loss: 0.6046837568283081\n",
      "Validation: Epoch [21], Batch [276/938], Loss: 0.4353731572628021\n",
      "Validation: Epoch [21], Batch [277/938], Loss: 0.3532657027244568\n",
      "Validation: Epoch [21], Batch [278/938], Loss: 0.4745774567127228\n",
      "Validation: Epoch [21], Batch [279/938], Loss: 0.5679078698158264\n",
      "Validation: Epoch [21], Batch [280/938], Loss: 0.6119241118431091\n",
      "Validation: Epoch [21], Batch [281/938], Loss: 0.623695433139801\n",
      "Validation: Epoch [21], Batch [282/938], Loss: 0.45320743322372437\n",
      "Validation: Epoch [21], Batch [283/938], Loss: 0.4090665578842163\n",
      "Validation: Epoch [21], Batch [284/938], Loss: 0.33729517459869385\n",
      "Validation: Epoch [21], Batch [285/938], Loss: 0.5402828454971313\n",
      "Validation: Epoch [21], Batch [286/938], Loss: 0.5066954493522644\n",
      "Validation: Epoch [21], Batch [287/938], Loss: 0.4323999881744385\n",
      "Validation: Epoch [21], Batch [288/938], Loss: 0.48981186747550964\n",
      "Validation: Epoch [21], Batch [289/938], Loss: 0.3897246718406677\n",
      "Validation: Epoch [21], Batch [290/938], Loss: 0.5634516477584839\n",
      "Validation: Epoch [21], Batch [291/938], Loss: 0.4703513979911804\n",
      "Validation: Epoch [21], Batch [292/938], Loss: 0.42511942982673645\n",
      "Validation: Epoch [21], Batch [293/938], Loss: 0.34108367562294006\n",
      "Validation: Epoch [21], Batch [294/938], Loss: 0.5242612361907959\n",
      "Validation: Epoch [21], Batch [295/938], Loss: 0.4248221218585968\n",
      "Validation: Epoch [21], Batch [296/938], Loss: 0.39383211731910706\n",
      "Validation: Epoch [21], Batch [297/938], Loss: 0.46583694219589233\n",
      "Validation: Epoch [21], Batch [298/938], Loss: 0.3622502088546753\n",
      "Validation: Epoch [21], Batch [299/938], Loss: 0.27685680985450745\n",
      "Validation: Epoch [21], Batch [300/938], Loss: 0.6889299154281616\n",
      "Validation: Epoch [21], Batch [301/938], Loss: 0.3376635015010834\n",
      "Validation: Epoch [21], Batch [302/938], Loss: 0.37984931468963623\n",
      "Validation: Epoch [21], Batch [303/938], Loss: 0.36539503931999207\n",
      "Validation: Epoch [21], Batch [304/938], Loss: 0.31909117102622986\n",
      "Validation: Epoch [21], Batch [305/938], Loss: 0.6652089357376099\n",
      "Validation: Epoch [21], Batch [306/938], Loss: 0.7484394311904907\n",
      "Validation: Epoch [21], Batch [307/938], Loss: 0.4523939788341522\n",
      "Validation: Epoch [21], Batch [308/938], Loss: 0.4046621024608612\n",
      "Validation: Epoch [21], Batch [309/938], Loss: 0.2617546021938324\n",
      "Validation: Epoch [21], Batch [310/938], Loss: 0.6626797914505005\n",
      "Validation: Epoch [21], Batch [311/938], Loss: 0.405962735414505\n",
      "Validation: Epoch [21], Batch [312/938], Loss: 0.2782861590385437\n",
      "Validation: Epoch [21], Batch [313/938], Loss: 0.5714366436004639\n",
      "Validation: Epoch [21], Batch [314/938], Loss: 0.6656572818756104\n",
      "Validation: Epoch [21], Batch [315/938], Loss: 0.4896324574947357\n",
      "Validation: Epoch [21], Batch [316/938], Loss: 0.26185739040374756\n",
      "Validation: Epoch [21], Batch [317/938], Loss: 0.5172032713890076\n",
      "Validation: Epoch [21], Batch [318/938], Loss: 0.6112900376319885\n",
      "Validation: Epoch [21], Batch [319/938], Loss: 0.32003021240234375\n",
      "Validation: Epoch [21], Batch [320/938], Loss: 0.3849906921386719\n",
      "Validation: Epoch [21], Batch [321/938], Loss: 0.43741729855537415\n",
      "Validation: Epoch [21], Batch [322/938], Loss: 0.4446631968021393\n",
      "Validation: Epoch [21], Batch [323/938], Loss: 0.5386784076690674\n",
      "Validation: Epoch [21], Batch [324/938], Loss: 0.30415382981300354\n",
      "Validation: Epoch [21], Batch [325/938], Loss: 0.5453606247901917\n",
      "Validation: Epoch [21], Batch [326/938], Loss: 0.36001622676849365\n",
      "Validation: Epoch [21], Batch [327/938], Loss: 0.4948185086250305\n",
      "Validation: Epoch [21], Batch [328/938], Loss: 0.4238441288471222\n",
      "Validation: Epoch [21], Batch [329/938], Loss: 0.3515720069408417\n",
      "Validation: Epoch [21], Batch [330/938], Loss: 0.544995129108429\n",
      "Validation: Epoch [21], Batch [331/938], Loss: 0.4735462963581085\n",
      "Validation: Epoch [21], Batch [332/938], Loss: 0.3008066415786743\n",
      "Validation: Epoch [21], Batch [333/938], Loss: 0.5914775729179382\n",
      "Validation: Epoch [21], Batch [334/938], Loss: 0.4182743728160858\n",
      "Validation: Epoch [21], Batch [335/938], Loss: 0.5125236511230469\n",
      "Validation: Epoch [21], Batch [336/938], Loss: 0.5178984999656677\n",
      "Validation: Epoch [21], Batch [337/938], Loss: 0.6970365047454834\n",
      "Validation: Epoch [21], Batch [338/938], Loss: 0.5211441516876221\n",
      "Validation: Epoch [21], Batch [339/938], Loss: 0.47051239013671875\n",
      "Validation: Epoch [21], Batch [340/938], Loss: 0.35113999247550964\n",
      "Validation: Epoch [21], Batch [341/938], Loss: 0.3064599335193634\n",
      "Validation: Epoch [21], Batch [342/938], Loss: 0.33037400245666504\n",
      "Validation: Epoch [21], Batch [343/938], Loss: 0.30095839500427246\n",
      "Validation: Epoch [21], Batch [344/938], Loss: 0.3334348499774933\n",
      "Validation: Epoch [21], Batch [345/938], Loss: 0.5699222087860107\n",
      "Validation: Epoch [21], Batch [346/938], Loss: 0.36178654432296753\n",
      "Validation: Epoch [21], Batch [347/938], Loss: 0.4965992569923401\n",
      "Validation: Epoch [21], Batch [348/938], Loss: 0.40359780192375183\n",
      "Validation: Epoch [21], Batch [349/938], Loss: 0.42644476890563965\n",
      "Validation: Epoch [21], Batch [350/938], Loss: 0.321419894695282\n",
      "Validation: Epoch [21], Batch [351/938], Loss: 0.4551709294319153\n",
      "Validation: Epoch [21], Batch [352/938], Loss: 0.34215158224105835\n",
      "Validation: Epoch [21], Batch [353/938], Loss: 0.39145228266716003\n",
      "Validation: Epoch [21], Batch [354/938], Loss: 0.4609718918800354\n",
      "Validation: Epoch [21], Batch [355/938], Loss: 0.4982221722602844\n",
      "Validation: Epoch [21], Batch [356/938], Loss: 0.5940324664115906\n",
      "Validation: Epoch [21], Batch [357/938], Loss: 0.26091527938842773\n",
      "Validation: Epoch [21], Batch [358/938], Loss: 0.394828736782074\n",
      "Validation: Epoch [21], Batch [359/938], Loss: 0.6715232729911804\n",
      "Validation: Epoch [21], Batch [360/938], Loss: 0.3743577003479004\n",
      "Validation: Epoch [21], Batch [361/938], Loss: 0.4165090322494507\n",
      "Validation: Epoch [21], Batch [362/938], Loss: 0.5372554659843445\n",
      "Validation: Epoch [21], Batch [363/938], Loss: 0.4526432454586029\n",
      "Validation: Epoch [21], Batch [364/938], Loss: 0.7019696831703186\n",
      "Validation: Epoch [21], Batch [365/938], Loss: 0.5010130405426025\n",
      "Validation: Epoch [21], Batch [366/938], Loss: 0.5270578861236572\n",
      "Validation: Epoch [21], Batch [367/938], Loss: 0.4632970988750458\n",
      "Validation: Epoch [21], Batch [368/938], Loss: 0.36777710914611816\n",
      "Validation: Epoch [21], Batch [369/938], Loss: 0.42239445447921753\n",
      "Validation: Epoch [21], Batch [370/938], Loss: 0.30353665351867676\n",
      "Validation: Epoch [21], Batch [371/938], Loss: 0.2755264341831207\n",
      "Validation: Epoch [21], Batch [372/938], Loss: 0.3685624897480011\n",
      "Validation: Epoch [21], Batch [373/938], Loss: 0.41862109303474426\n",
      "Validation: Epoch [21], Batch [374/938], Loss: 0.40502607822418213\n",
      "Validation: Epoch [21], Batch [375/938], Loss: 0.4651481509208679\n",
      "Validation: Epoch [21], Batch [376/938], Loss: 0.4069061577320099\n",
      "Validation: Epoch [21], Batch [377/938], Loss: 0.5980498194694519\n",
      "Validation: Epoch [21], Batch [378/938], Loss: 0.43462562561035156\n",
      "Validation: Epoch [21], Batch [379/938], Loss: 0.30702275037765503\n",
      "Validation: Epoch [21], Batch [380/938], Loss: 0.6095200181007385\n",
      "Validation: Epoch [21], Batch [381/938], Loss: 0.7119187116622925\n",
      "Validation: Epoch [21], Batch [382/938], Loss: 0.3803938329219818\n",
      "Validation: Epoch [21], Batch [383/938], Loss: 0.2875960171222687\n",
      "Validation: Epoch [21], Batch [384/938], Loss: 0.401775062084198\n",
      "Validation: Epoch [21], Batch [385/938], Loss: 0.3579588532447815\n",
      "Validation: Epoch [21], Batch [386/938], Loss: 0.4693794250488281\n",
      "Validation: Epoch [21], Batch [387/938], Loss: 0.46725133061408997\n",
      "Validation: Epoch [21], Batch [388/938], Loss: 0.5169534683227539\n",
      "Validation: Epoch [21], Batch [389/938], Loss: 0.4409654438495636\n",
      "Validation: Epoch [21], Batch [390/938], Loss: 0.5436328649520874\n",
      "Validation: Epoch [21], Batch [391/938], Loss: 0.24617767333984375\n",
      "Validation: Epoch [21], Batch [392/938], Loss: 0.39494872093200684\n",
      "Validation: Epoch [21], Batch [393/938], Loss: 0.6506606340408325\n",
      "Validation: Epoch [21], Batch [394/938], Loss: 0.36435046792030334\n",
      "Validation: Epoch [21], Batch [395/938], Loss: 0.744014322757721\n",
      "Validation: Epoch [21], Batch [396/938], Loss: 0.5697872638702393\n",
      "Validation: Epoch [21], Batch [397/938], Loss: 0.5013423562049866\n",
      "Validation: Epoch [21], Batch [398/938], Loss: 0.4279102087020874\n",
      "Validation: Epoch [21], Batch [399/938], Loss: 0.5130533576011658\n",
      "Validation: Epoch [21], Batch [400/938], Loss: 0.2588058114051819\n",
      "Validation: Epoch [21], Batch [401/938], Loss: 0.7285826206207275\n",
      "Validation: Epoch [21], Batch [402/938], Loss: 0.39145025610923767\n",
      "Validation: Epoch [21], Batch [403/938], Loss: 0.4315621554851532\n",
      "Validation: Epoch [21], Batch [404/938], Loss: 0.37652114033699036\n",
      "Validation: Epoch [21], Batch [405/938], Loss: 0.5186562538146973\n",
      "Validation: Epoch [21], Batch [406/938], Loss: 0.5085587501525879\n",
      "Validation: Epoch [21], Batch [407/938], Loss: 0.3633938431739807\n",
      "Validation: Epoch [21], Batch [408/938], Loss: 0.4850105345249176\n",
      "Validation: Epoch [21], Batch [409/938], Loss: 0.418562114238739\n",
      "Validation: Epoch [21], Batch [410/938], Loss: 0.5278307795524597\n",
      "Validation: Epoch [21], Batch [411/938], Loss: 0.5905640125274658\n",
      "Validation: Epoch [21], Batch [412/938], Loss: 0.4400455951690674\n",
      "Validation: Epoch [21], Batch [413/938], Loss: 0.47381290793418884\n",
      "Validation: Epoch [21], Batch [414/938], Loss: 0.3965317904949188\n",
      "Validation: Epoch [21], Batch [415/938], Loss: 0.6042329668998718\n",
      "Validation: Epoch [21], Batch [416/938], Loss: 0.4665490686893463\n",
      "Validation: Epoch [21], Batch [417/938], Loss: 0.397184818983078\n",
      "Validation: Epoch [21], Batch [418/938], Loss: 0.5512888431549072\n",
      "Validation: Epoch [21], Batch [419/938], Loss: 0.3064866364002228\n",
      "Validation: Epoch [21], Batch [420/938], Loss: 0.4858092665672302\n",
      "Validation: Epoch [21], Batch [421/938], Loss: 0.5432359576225281\n",
      "Validation: Epoch [21], Batch [422/938], Loss: 0.3298080861568451\n",
      "Validation: Epoch [21], Batch [423/938], Loss: 0.41225647926330566\n",
      "Validation: Epoch [21], Batch [424/938], Loss: 0.45696723461151123\n",
      "Validation: Epoch [21], Batch [425/938], Loss: 0.43531492352485657\n",
      "Validation: Epoch [21], Batch [426/938], Loss: 0.35408011078834534\n",
      "Validation: Epoch [21], Batch [427/938], Loss: 0.38633525371551514\n",
      "Validation: Epoch [21], Batch [428/938], Loss: 0.4622560739517212\n",
      "Validation: Epoch [21], Batch [429/938], Loss: 0.6099281311035156\n",
      "Validation: Epoch [21], Batch [430/938], Loss: 0.5568926334381104\n",
      "Validation: Epoch [21], Batch [431/938], Loss: 0.4746384024620056\n",
      "Validation: Epoch [21], Batch [432/938], Loss: 0.5294275879859924\n",
      "Validation: Epoch [21], Batch [433/938], Loss: 0.4425523281097412\n",
      "Validation: Epoch [21], Batch [434/938], Loss: 0.522071897983551\n",
      "Validation: Epoch [21], Batch [435/938], Loss: 0.5422936677932739\n",
      "Validation: Epoch [21], Batch [436/938], Loss: 0.5144491791725159\n",
      "Validation: Epoch [21], Batch [437/938], Loss: 0.6442987322807312\n",
      "Validation: Epoch [21], Batch [438/938], Loss: 0.4544065594673157\n",
      "Validation: Epoch [21], Batch [439/938], Loss: 0.43921738862991333\n",
      "Validation: Epoch [21], Batch [440/938], Loss: 0.43144261837005615\n",
      "Validation: Epoch [21], Batch [441/938], Loss: 0.5541223883628845\n",
      "Validation: Epoch [21], Batch [442/938], Loss: 0.553712785243988\n",
      "Validation: Epoch [21], Batch [443/938], Loss: 0.32715556025505066\n",
      "Validation: Epoch [21], Batch [444/938], Loss: 0.32099756598472595\n",
      "Validation: Epoch [21], Batch [445/938], Loss: 0.264654278755188\n",
      "Validation: Epoch [21], Batch [446/938], Loss: 0.314222514629364\n",
      "Validation: Epoch [21], Batch [447/938], Loss: 0.3999907374382019\n",
      "Validation: Epoch [21], Batch [448/938], Loss: 0.4459064304828644\n",
      "Validation: Epoch [21], Batch [449/938], Loss: 0.47195252776145935\n",
      "Validation: Epoch [21], Batch [450/938], Loss: 0.33513569831848145\n",
      "Validation: Epoch [21], Batch [451/938], Loss: 0.4704577624797821\n",
      "Validation: Epoch [21], Batch [452/938], Loss: 0.35638806223869324\n",
      "Validation: Epoch [21], Batch [453/938], Loss: 0.5783427357673645\n",
      "Validation: Epoch [21], Batch [454/938], Loss: 0.43833136558532715\n",
      "Validation: Epoch [21], Batch [455/938], Loss: 0.4290603995323181\n",
      "Validation: Epoch [21], Batch [456/938], Loss: 0.5187762975692749\n",
      "Validation: Epoch [21], Batch [457/938], Loss: 0.7209736108779907\n",
      "Validation: Epoch [21], Batch [458/938], Loss: 0.36291033029556274\n",
      "Validation: Epoch [21], Batch [459/938], Loss: 0.5209818482398987\n",
      "Validation: Epoch [21], Batch [460/938], Loss: 0.49794912338256836\n",
      "Validation: Epoch [21], Batch [461/938], Loss: 0.4553356170654297\n",
      "Validation: Epoch [21], Batch [462/938], Loss: 0.32291239500045776\n",
      "Validation: Epoch [21], Batch [463/938], Loss: 0.3343522548675537\n",
      "Validation: Epoch [21], Batch [464/938], Loss: 0.5576832294464111\n",
      "Validation: Epoch [21], Batch [465/938], Loss: 0.3181515038013458\n",
      "Validation: Epoch [21], Batch [466/938], Loss: 0.4296495318412781\n",
      "Validation: Epoch [21], Batch [467/938], Loss: 0.4198826551437378\n",
      "Validation: Epoch [21], Batch [468/938], Loss: 0.5252535939216614\n",
      "Validation: Epoch [21], Batch [469/938], Loss: 0.4086155891418457\n",
      "Validation: Epoch [21], Batch [470/938], Loss: 0.4344682991504669\n",
      "Validation: Epoch [21], Batch [471/938], Loss: 0.48004472255706787\n",
      "Validation: Epoch [21], Batch [472/938], Loss: 0.7361924648284912\n",
      "Validation: Epoch [21], Batch [473/938], Loss: 0.346957802772522\n",
      "Validation: Epoch [21], Batch [474/938], Loss: 0.4885869324207306\n",
      "Validation: Epoch [21], Batch [475/938], Loss: 0.47750386595726013\n",
      "Validation: Epoch [21], Batch [476/938], Loss: 0.5108704566955566\n",
      "Validation: Epoch [21], Batch [477/938], Loss: 0.504787802696228\n",
      "Validation: Epoch [21], Batch [478/938], Loss: 0.4082641005516052\n",
      "Validation: Epoch [21], Batch [479/938], Loss: 0.5873290300369263\n",
      "Validation: Epoch [21], Batch [480/938], Loss: 0.37463831901550293\n",
      "Validation: Epoch [21], Batch [481/938], Loss: 0.420235812664032\n",
      "Validation: Epoch [21], Batch [482/938], Loss: 0.17599008977413177\n",
      "Validation: Epoch [21], Batch [483/938], Loss: 0.38269269466400146\n",
      "Validation: Epoch [21], Batch [484/938], Loss: 0.5635313987731934\n",
      "Validation: Epoch [21], Batch [485/938], Loss: 0.358562707901001\n",
      "Validation: Epoch [21], Batch [486/938], Loss: 0.5752865672111511\n",
      "Validation: Epoch [21], Batch [487/938], Loss: 0.23144279420375824\n",
      "Validation: Epoch [21], Batch [488/938], Loss: 0.5718839764595032\n",
      "Validation: Epoch [21], Batch [489/938], Loss: 0.411906361579895\n",
      "Validation: Epoch [21], Batch [490/938], Loss: 0.544187068939209\n",
      "Validation: Epoch [21], Batch [491/938], Loss: 0.41717880964279175\n",
      "Validation: Epoch [21], Batch [492/938], Loss: 0.4203736186027527\n",
      "Validation: Epoch [21], Batch [493/938], Loss: 0.36344534158706665\n",
      "Validation: Epoch [21], Batch [494/938], Loss: 0.48030176758766174\n",
      "Validation: Epoch [21], Batch [495/938], Loss: 0.38142913579940796\n",
      "Validation: Epoch [21], Batch [496/938], Loss: 0.24837712943553925\n",
      "Validation: Epoch [21], Batch [497/938], Loss: 0.3290528357028961\n",
      "Validation: Epoch [21], Batch [498/938], Loss: 0.4199260175228119\n",
      "Validation: Epoch [21], Batch [499/938], Loss: 0.3124796152114868\n",
      "Validation: Epoch [21], Batch [500/938], Loss: 0.4798460602760315\n",
      "Validation: Epoch [21], Batch [501/938], Loss: 0.5038558840751648\n",
      "Validation: Epoch [21], Batch [502/938], Loss: 0.35163480043411255\n",
      "Validation: Epoch [21], Batch [503/938], Loss: 0.3856465220451355\n",
      "Validation: Epoch [21], Batch [504/938], Loss: 0.6705102920532227\n",
      "Validation: Epoch [21], Batch [505/938], Loss: 0.35194963216781616\n",
      "Validation: Epoch [21], Batch [506/938], Loss: 0.5959106683731079\n",
      "Validation: Epoch [21], Batch [507/938], Loss: 0.3530501425266266\n",
      "Validation: Epoch [21], Batch [508/938], Loss: 0.6183865666389465\n",
      "Validation: Epoch [21], Batch [509/938], Loss: 0.40570899844169617\n",
      "Validation: Epoch [21], Batch [510/938], Loss: 0.26663655042648315\n",
      "Validation: Epoch [21], Batch [511/938], Loss: 0.31241461634635925\n",
      "Validation: Epoch [21], Batch [512/938], Loss: 0.33417367935180664\n",
      "Validation: Epoch [21], Batch [513/938], Loss: 0.55815190076828\n",
      "Validation: Epoch [21], Batch [514/938], Loss: 0.7244226932525635\n",
      "Validation: Epoch [21], Batch [515/938], Loss: 0.4588643014431\n",
      "Validation: Epoch [21], Batch [516/938], Loss: 0.5313357710838318\n",
      "Validation: Epoch [21], Batch [517/938], Loss: 0.3309505581855774\n",
      "Validation: Epoch [21], Batch [518/938], Loss: 0.6596863269805908\n",
      "Validation: Epoch [21], Batch [519/938], Loss: 0.390600323677063\n",
      "Validation: Epoch [21], Batch [520/938], Loss: 0.4093984067440033\n",
      "Validation: Epoch [21], Batch [521/938], Loss: 0.5533602237701416\n",
      "Validation: Epoch [21], Batch [522/938], Loss: 0.7069942355155945\n",
      "Validation: Epoch [21], Batch [523/938], Loss: 0.2527410387992859\n",
      "Validation: Epoch [21], Batch [524/938], Loss: 0.37132635712623596\n",
      "Validation: Epoch [21], Batch [525/938], Loss: 0.4451427757740021\n",
      "Validation: Epoch [21], Batch [526/938], Loss: 0.6680851578712463\n",
      "Validation: Epoch [21], Batch [527/938], Loss: 0.42942047119140625\n",
      "Validation: Epoch [21], Batch [528/938], Loss: 0.3837840259075165\n",
      "Validation: Epoch [21], Batch [529/938], Loss: 0.48508185148239136\n",
      "Validation: Epoch [21], Batch [530/938], Loss: 0.38167059421539307\n",
      "Validation: Epoch [21], Batch [531/938], Loss: 0.40630701184272766\n",
      "Validation: Epoch [21], Batch [532/938], Loss: 0.45489439368247986\n",
      "Validation: Epoch [21], Batch [533/938], Loss: 0.4221547842025757\n",
      "Validation: Epoch [21], Batch [534/938], Loss: 0.5082352757453918\n",
      "Validation: Epoch [21], Batch [535/938], Loss: 0.4836268126964569\n",
      "Validation: Epoch [21], Batch [536/938], Loss: 0.35260242223739624\n",
      "Validation: Epoch [21], Batch [537/938], Loss: 0.37958794832229614\n",
      "Validation: Epoch [21], Batch [538/938], Loss: 0.5526878833770752\n",
      "Validation: Epoch [21], Batch [539/938], Loss: 0.47264283895492554\n",
      "Validation: Epoch [21], Batch [540/938], Loss: 0.4141049385070801\n",
      "Validation: Epoch [21], Batch [541/938], Loss: 0.32564863562583923\n",
      "Validation: Epoch [21], Batch [542/938], Loss: 0.5792955756187439\n",
      "Validation: Epoch [21], Batch [543/938], Loss: 0.6011465787887573\n",
      "Validation: Epoch [21], Batch [544/938], Loss: 0.3194166123867035\n",
      "Validation: Epoch [21], Batch [545/938], Loss: 0.4793192446231842\n",
      "Validation: Epoch [21], Batch [546/938], Loss: 0.5428454875946045\n",
      "Validation: Epoch [21], Batch [547/938], Loss: 0.39425820112228394\n",
      "Validation: Epoch [21], Batch [548/938], Loss: 0.2772728502750397\n",
      "Validation: Epoch [21], Batch [549/938], Loss: 0.5025440454483032\n",
      "Validation: Epoch [21], Batch [550/938], Loss: 0.370484858751297\n",
      "Validation: Epoch [21], Batch [551/938], Loss: 0.35526683926582336\n",
      "Validation: Epoch [21], Batch [552/938], Loss: 0.6207080483436584\n",
      "Validation: Epoch [21], Batch [553/938], Loss: 0.4084833860397339\n",
      "Validation: Epoch [21], Batch [554/938], Loss: 0.3394806683063507\n",
      "Validation: Epoch [21], Batch [555/938], Loss: 0.6050165891647339\n",
      "Validation: Epoch [21], Batch [556/938], Loss: 0.2555069625377655\n",
      "Validation: Epoch [21], Batch [557/938], Loss: 0.48384445905685425\n",
      "Validation: Epoch [21], Batch [558/938], Loss: 0.4410790801048279\n",
      "Validation: Epoch [21], Batch [559/938], Loss: 0.3287731409072876\n",
      "Validation: Epoch [21], Batch [560/938], Loss: 0.28290796279907227\n",
      "Validation: Epoch [21], Batch [561/938], Loss: 0.532595694065094\n",
      "Validation: Epoch [21], Batch [562/938], Loss: 0.49378180503845215\n",
      "Validation: Epoch [21], Batch [563/938], Loss: 0.35051488876342773\n",
      "Validation: Epoch [21], Batch [564/938], Loss: 0.6227020621299744\n",
      "Validation: Epoch [21], Batch [565/938], Loss: 0.31699877977371216\n",
      "Validation: Epoch [21], Batch [566/938], Loss: 0.4367501437664032\n",
      "Validation: Epoch [21], Batch [567/938], Loss: 0.6045113205909729\n",
      "Validation: Epoch [21], Batch [568/938], Loss: 0.4840557873249054\n",
      "Validation: Epoch [21], Batch [569/938], Loss: 0.3995588421821594\n",
      "Validation: Epoch [21], Batch [570/938], Loss: 0.5496774911880493\n",
      "Validation: Epoch [21], Batch [571/938], Loss: 0.497810959815979\n",
      "Validation: Epoch [21], Batch [572/938], Loss: 0.31160977482795715\n",
      "Validation: Epoch [21], Batch [573/938], Loss: 0.6557922959327698\n",
      "Validation: Epoch [21], Batch [574/938], Loss: 0.49626287817955017\n",
      "Validation: Epoch [21], Batch [575/938], Loss: 0.48050811886787415\n",
      "Validation: Epoch [21], Batch [576/938], Loss: 0.463946133852005\n",
      "Validation: Epoch [21], Batch [577/938], Loss: 0.5135568380355835\n",
      "Validation: Epoch [21], Batch [578/938], Loss: 0.3938615918159485\n",
      "Validation: Epoch [21], Batch [579/938], Loss: 0.352709025144577\n",
      "Validation: Epoch [21], Batch [580/938], Loss: 0.4892410933971405\n",
      "Validation: Epoch [21], Batch [581/938], Loss: 0.42148900032043457\n",
      "Validation: Epoch [21], Batch [582/938], Loss: 0.4462282955646515\n",
      "Validation: Epoch [21], Batch [583/938], Loss: 0.6375908255577087\n",
      "Validation: Epoch [21], Batch [584/938], Loss: 0.33924055099487305\n",
      "Validation: Epoch [21], Batch [585/938], Loss: 0.6227023005485535\n",
      "Validation: Epoch [21], Batch [586/938], Loss: 0.5847809314727783\n",
      "Validation: Epoch [21], Batch [587/938], Loss: 0.36111220717430115\n",
      "Validation: Epoch [21], Batch [588/938], Loss: 0.38118332624435425\n",
      "Validation: Epoch [21], Batch [589/938], Loss: 0.47529181838035583\n",
      "Validation: Epoch [21], Batch [590/938], Loss: 0.4287645220756531\n",
      "Validation: Epoch [21], Batch [591/938], Loss: 0.5469988584518433\n",
      "Validation: Epoch [21], Batch [592/938], Loss: 0.3525882959365845\n",
      "Validation: Epoch [21], Batch [593/938], Loss: 0.44391798973083496\n",
      "Validation: Epoch [21], Batch [594/938], Loss: 0.6098508834838867\n",
      "Validation: Epoch [21], Batch [595/938], Loss: 0.47148397564888\n",
      "Validation: Epoch [21], Batch [596/938], Loss: 0.48153847455978394\n",
      "Validation: Epoch [21], Batch [597/938], Loss: 0.44332846999168396\n",
      "Validation: Epoch [21], Batch [598/938], Loss: 0.4683381915092468\n",
      "Validation: Epoch [21], Batch [599/938], Loss: 0.4675455093383789\n",
      "Validation: Epoch [21], Batch [600/938], Loss: 0.4743369519710541\n",
      "Validation: Epoch [21], Batch [601/938], Loss: 0.5985671281814575\n",
      "Validation: Epoch [21], Batch [602/938], Loss: 0.3374505341053009\n",
      "Validation: Epoch [21], Batch [603/938], Loss: 0.6581457257270813\n",
      "Validation: Epoch [21], Batch [604/938], Loss: 0.37811538577079773\n",
      "Validation: Epoch [21], Batch [605/938], Loss: 0.6401800513267517\n",
      "Validation: Epoch [21], Batch [606/938], Loss: 0.35143208503723145\n",
      "Validation: Epoch [21], Batch [607/938], Loss: 0.6263872385025024\n",
      "Validation: Epoch [21], Batch [608/938], Loss: 0.36860474944114685\n",
      "Validation: Epoch [21], Batch [609/938], Loss: 0.3330177664756775\n",
      "Validation: Epoch [21], Batch [610/938], Loss: 0.36839401721954346\n",
      "Validation: Epoch [21], Batch [611/938], Loss: 0.4176400601863861\n",
      "Validation: Epoch [21], Batch [612/938], Loss: 0.4109072685241699\n",
      "Validation: Epoch [21], Batch [613/938], Loss: 0.3155614137649536\n",
      "Validation: Epoch [21], Batch [614/938], Loss: 0.40004783868789673\n",
      "Validation: Epoch [21], Batch [615/938], Loss: 0.4119485914707184\n",
      "Validation: Epoch [21], Batch [616/938], Loss: 0.5625278949737549\n",
      "Validation: Epoch [21], Batch [617/938], Loss: 0.4651995301246643\n",
      "Validation: Epoch [21], Batch [618/938], Loss: 0.45936036109924316\n",
      "Validation: Epoch [21], Batch [619/938], Loss: 0.3968330919742584\n",
      "Validation: Epoch [21], Batch [620/938], Loss: 0.4289327561855316\n",
      "Validation: Epoch [21], Batch [621/938], Loss: 0.5216267108917236\n",
      "Validation: Epoch [21], Batch [622/938], Loss: 0.4287063181400299\n",
      "Validation: Epoch [21], Batch [623/938], Loss: 0.3941778838634491\n",
      "Validation: Epoch [21], Batch [624/938], Loss: 0.45946672558784485\n",
      "Validation: Epoch [21], Batch [625/938], Loss: 0.3576110005378723\n",
      "Validation: Epoch [21], Batch [626/938], Loss: 0.500430166721344\n",
      "Validation: Epoch [21], Batch [627/938], Loss: 0.3769683539867401\n",
      "Validation: Epoch [21], Batch [628/938], Loss: 0.4589342176914215\n",
      "Validation: Epoch [21], Batch [629/938], Loss: 0.4335225224494934\n",
      "Validation: Epoch [21], Batch [630/938], Loss: 0.43271058797836304\n",
      "Validation: Epoch [21], Batch [631/938], Loss: 0.4596293270587921\n",
      "Validation: Epoch [21], Batch [632/938], Loss: 0.4043489396572113\n",
      "Validation: Epoch [21], Batch [633/938], Loss: 0.41428902745246887\n",
      "Validation: Epoch [21], Batch [634/938], Loss: 0.48571616411209106\n",
      "Validation: Epoch [21], Batch [635/938], Loss: 0.40078476071357727\n",
      "Validation: Epoch [21], Batch [636/938], Loss: 0.6452788710594177\n",
      "Validation: Epoch [21], Batch [637/938], Loss: 0.2952967584133148\n",
      "Validation: Epoch [21], Batch [638/938], Loss: 0.5061742663383484\n",
      "Validation: Epoch [21], Batch [639/938], Loss: 0.4034827649593353\n",
      "Validation: Epoch [21], Batch [640/938], Loss: 0.6358428597450256\n",
      "Validation: Epoch [21], Batch [641/938], Loss: 0.2945873439311981\n",
      "Validation: Epoch [21], Batch [642/938], Loss: 0.3238790035247803\n",
      "Validation: Epoch [21], Batch [643/938], Loss: 0.5711108446121216\n",
      "Validation: Epoch [21], Batch [644/938], Loss: 0.2869050204753876\n",
      "Validation: Epoch [21], Batch [645/938], Loss: 0.5928493738174438\n",
      "Validation: Epoch [21], Batch [646/938], Loss: 0.44131213426589966\n",
      "Validation: Epoch [21], Batch [647/938], Loss: 0.3889508545398712\n",
      "Validation: Epoch [21], Batch [648/938], Loss: 0.5206061005592346\n",
      "Validation: Epoch [21], Batch [649/938], Loss: 0.37378039956092834\n",
      "Validation: Epoch [21], Batch [650/938], Loss: 0.39936384558677673\n",
      "Validation: Epoch [21], Batch [651/938], Loss: 0.4387540817260742\n",
      "Validation: Epoch [21], Batch [652/938], Loss: 0.46127068996429443\n",
      "Validation: Epoch [21], Batch [653/938], Loss: 0.34300827980041504\n",
      "Validation: Epoch [21], Batch [654/938], Loss: 0.36681750416755676\n",
      "Validation: Epoch [21], Batch [655/938], Loss: 0.5149266123771667\n",
      "Validation: Epoch [21], Batch [656/938], Loss: 0.4022250473499298\n",
      "Validation: Epoch [21], Batch [657/938], Loss: 0.46700289845466614\n",
      "Validation: Epoch [21], Batch [658/938], Loss: 0.4757680892944336\n",
      "Validation: Epoch [21], Batch [659/938], Loss: 0.3098122179508209\n",
      "Validation: Epoch [21], Batch [660/938], Loss: 0.707732081413269\n",
      "Validation: Epoch [21], Batch [661/938], Loss: 0.52437424659729\n",
      "Validation: Epoch [21], Batch [662/938], Loss: 0.3855198621749878\n",
      "Validation: Epoch [21], Batch [663/938], Loss: 0.41943636536598206\n",
      "Validation: Epoch [21], Batch [664/938], Loss: 0.3970370888710022\n",
      "Validation: Epoch [21], Batch [665/938], Loss: 0.5152091383934021\n",
      "Validation: Epoch [21], Batch [666/938], Loss: 0.3641268014907837\n",
      "Validation: Epoch [21], Batch [667/938], Loss: 0.3755577802658081\n",
      "Validation: Epoch [21], Batch [668/938], Loss: 0.5286974310874939\n",
      "Validation: Epoch [21], Batch [669/938], Loss: 0.7606823444366455\n",
      "Validation: Epoch [21], Batch [670/938], Loss: 0.3193599581718445\n",
      "Validation: Epoch [21], Batch [671/938], Loss: 0.5067003965377808\n",
      "Validation: Epoch [21], Batch [672/938], Loss: 0.5335459113121033\n",
      "Validation: Epoch [21], Batch [673/938], Loss: 0.924550473690033\n",
      "Validation: Epoch [21], Batch [674/938], Loss: 0.3883313536643982\n",
      "Validation: Epoch [21], Batch [675/938], Loss: 0.449505478143692\n",
      "Validation: Epoch [21], Batch [676/938], Loss: 0.5768488049507141\n",
      "Validation: Epoch [21], Batch [677/938], Loss: 0.3663066029548645\n",
      "Validation: Epoch [21], Batch [678/938], Loss: 0.6937816739082336\n",
      "Validation: Epoch [21], Batch [679/938], Loss: 0.5151454210281372\n",
      "Validation: Epoch [21], Batch [680/938], Loss: 0.3879457414150238\n",
      "Validation: Epoch [21], Batch [681/938], Loss: 0.5067954063415527\n",
      "Validation: Epoch [21], Batch [682/938], Loss: 0.4291337728500366\n",
      "Validation: Epoch [21], Batch [683/938], Loss: 0.5360479354858398\n",
      "Validation: Epoch [21], Batch [684/938], Loss: 0.43566668033599854\n",
      "Validation: Epoch [21], Batch [685/938], Loss: 0.42523112893104553\n",
      "Validation: Epoch [21], Batch [686/938], Loss: 0.3263307809829712\n",
      "Validation: Epoch [21], Batch [687/938], Loss: 0.46477991342544556\n",
      "Validation: Epoch [21], Batch [688/938], Loss: 0.3266572952270508\n",
      "Validation: Epoch [21], Batch [689/938], Loss: 0.5831877589225769\n",
      "Validation: Epoch [21], Batch [690/938], Loss: 0.5432656407356262\n",
      "Validation: Epoch [21], Batch [691/938], Loss: 0.4619275629520416\n",
      "Validation: Epoch [21], Batch [692/938], Loss: 0.3893984854221344\n",
      "Validation: Epoch [21], Batch [693/938], Loss: 0.6148202419281006\n",
      "Validation: Epoch [21], Batch [694/938], Loss: 0.7421633005142212\n",
      "Validation: Epoch [21], Batch [695/938], Loss: 0.378519207239151\n",
      "Validation: Epoch [21], Batch [696/938], Loss: 0.6326549053192139\n",
      "Validation: Epoch [21], Batch [697/938], Loss: 0.3618535101413727\n",
      "Validation: Epoch [21], Batch [698/938], Loss: 0.3205295205116272\n",
      "Validation: Epoch [21], Batch [699/938], Loss: 0.44655075669288635\n",
      "Validation: Epoch [21], Batch [700/938], Loss: 0.2525442838668823\n",
      "Validation: Epoch [21], Batch [701/938], Loss: 0.7033486366271973\n",
      "Validation: Epoch [21], Batch [702/938], Loss: 0.6261337995529175\n",
      "Validation: Epoch [21], Batch [703/938], Loss: 0.5132380723953247\n",
      "Validation: Epoch [21], Batch [704/938], Loss: 0.3417503237724304\n",
      "Validation: Epoch [21], Batch [705/938], Loss: 0.37848877906799316\n",
      "Validation: Epoch [21], Batch [706/938], Loss: 0.35000890493392944\n",
      "Validation: Epoch [21], Batch [707/938], Loss: 0.3782446086406708\n",
      "Validation: Epoch [21], Batch [708/938], Loss: 0.34855377674102783\n",
      "Validation: Epoch [21], Batch [709/938], Loss: 0.517390251159668\n",
      "Validation: Epoch [21], Batch [710/938], Loss: 0.32689225673675537\n",
      "Validation: Epoch [21], Batch [711/938], Loss: 0.42238888144493103\n",
      "Validation: Epoch [21], Batch [712/938], Loss: 0.2867184579372406\n",
      "Validation: Epoch [21], Batch [713/938], Loss: 0.6746532917022705\n",
      "Validation: Epoch [21], Batch [714/938], Loss: 0.5359666347503662\n",
      "Validation: Epoch [21], Batch [715/938], Loss: 0.34198251366615295\n",
      "Validation: Epoch [21], Batch [716/938], Loss: 0.38926875591278076\n",
      "Validation: Epoch [21], Batch [717/938], Loss: 0.4202999770641327\n",
      "Validation: Epoch [21], Batch [718/938], Loss: 0.5460177063941956\n",
      "Validation: Epoch [21], Batch [719/938], Loss: 0.4211987257003784\n",
      "Validation: Epoch [21], Batch [720/938], Loss: 0.6123974919319153\n",
      "Validation: Epoch [21], Batch [721/938], Loss: 0.5397295355796814\n",
      "Validation: Epoch [21], Batch [722/938], Loss: 0.3559315502643585\n",
      "Validation: Epoch [21], Batch [723/938], Loss: 0.3782990872859955\n",
      "Validation: Epoch [21], Batch [724/938], Loss: 0.33157041668891907\n",
      "Validation: Epoch [21], Batch [725/938], Loss: 0.42013171315193176\n",
      "Validation: Epoch [21], Batch [726/938], Loss: 0.586932897567749\n",
      "Validation: Epoch [21], Batch [727/938], Loss: 0.23561222851276398\n",
      "Validation: Epoch [21], Batch [728/938], Loss: 0.4737496078014374\n",
      "Validation: Epoch [21], Batch [729/938], Loss: 0.45574626326560974\n",
      "Validation: Epoch [21], Batch [730/938], Loss: 0.40856707096099854\n",
      "Validation: Epoch [21], Batch [731/938], Loss: 0.34496378898620605\n",
      "Validation: Epoch [21], Batch [732/938], Loss: 0.3768687844276428\n",
      "Validation: Epoch [21], Batch [733/938], Loss: 0.42668136954307556\n",
      "Validation: Epoch [21], Batch [734/938], Loss: 0.42008358240127563\n",
      "Validation: Epoch [21], Batch [735/938], Loss: 0.3885476589202881\n",
      "Validation: Epoch [21], Batch [736/938], Loss: 0.4797942042350769\n",
      "Validation: Epoch [21], Batch [737/938], Loss: 0.336025208234787\n",
      "Validation: Epoch [21], Batch [738/938], Loss: 0.4546525180339813\n",
      "Validation: Epoch [21], Batch [739/938], Loss: 0.4201401472091675\n",
      "Validation: Epoch [21], Batch [740/938], Loss: 0.4431926906108856\n",
      "Validation: Epoch [21], Batch [741/938], Loss: 0.4751894474029541\n",
      "Validation: Epoch [21], Batch [742/938], Loss: 0.45788314938545227\n",
      "Validation: Epoch [21], Batch [743/938], Loss: 0.7868003249168396\n",
      "Validation: Epoch [21], Batch [744/938], Loss: 0.5960142016410828\n",
      "Validation: Epoch [21], Batch [745/938], Loss: 0.3865613639354706\n",
      "Validation: Epoch [21], Batch [746/938], Loss: 0.5324289798736572\n",
      "Validation: Epoch [21], Batch [747/938], Loss: 0.4254687428474426\n",
      "Validation: Epoch [21], Batch [748/938], Loss: 0.29098737239837646\n",
      "Validation: Epoch [21], Batch [749/938], Loss: 0.3401648700237274\n",
      "Validation: Epoch [21], Batch [750/938], Loss: 0.5463591814041138\n",
      "Validation: Epoch [21], Batch [751/938], Loss: 0.34737879037857056\n",
      "Validation: Epoch [21], Batch [752/938], Loss: 0.2847455143928528\n",
      "Validation: Epoch [21], Batch [753/938], Loss: 0.36967554688453674\n",
      "Validation: Epoch [21], Batch [754/938], Loss: 0.48959940671920776\n",
      "Validation: Epoch [21], Batch [755/938], Loss: 0.45353811979293823\n",
      "Validation: Epoch [21], Batch [756/938], Loss: 0.5918422937393188\n",
      "Validation: Epoch [21], Batch [757/938], Loss: 0.3950622081756592\n",
      "Validation: Epoch [21], Batch [758/938], Loss: 0.5509598851203918\n",
      "Validation: Epoch [21], Batch [759/938], Loss: 0.34234780073165894\n",
      "Validation: Epoch [21], Batch [760/938], Loss: 0.4228462278842926\n",
      "Validation: Epoch [21], Batch [761/938], Loss: 0.4774753749370575\n",
      "Validation: Epoch [21], Batch [762/938], Loss: 0.6219504475593567\n",
      "Validation: Epoch [21], Batch [763/938], Loss: 0.34233182668685913\n",
      "Validation: Epoch [21], Batch [764/938], Loss: 0.5095803141593933\n",
      "Validation: Epoch [21], Batch [765/938], Loss: 0.3700571060180664\n",
      "Validation: Epoch [21], Batch [766/938], Loss: 0.6554633378982544\n",
      "Validation: Epoch [21], Batch [767/938], Loss: 0.5468326807022095\n",
      "Validation: Epoch [21], Batch [768/938], Loss: 0.3653314411640167\n",
      "Validation: Epoch [21], Batch [769/938], Loss: 0.4424740970134735\n",
      "Validation: Epoch [21], Batch [770/938], Loss: 0.37662553787231445\n",
      "Validation: Epoch [21], Batch [771/938], Loss: 0.44012880325317383\n",
      "Validation: Epoch [21], Batch [772/938], Loss: 0.626205563545227\n",
      "Validation: Epoch [21], Batch [773/938], Loss: 0.7076993584632874\n",
      "Validation: Epoch [21], Batch [774/938], Loss: 0.5453647375106812\n",
      "Validation: Epoch [21], Batch [775/938], Loss: 0.4242883622646332\n",
      "Validation: Epoch [21], Batch [776/938], Loss: 0.6812006831169128\n",
      "Validation: Epoch [21], Batch [777/938], Loss: 0.44003212451934814\n",
      "Validation: Epoch [21], Batch [778/938], Loss: 0.622687816619873\n",
      "Validation: Epoch [21], Batch [779/938], Loss: 0.5032728910446167\n",
      "Validation: Epoch [21], Batch [780/938], Loss: 0.5482614040374756\n",
      "Validation: Epoch [21], Batch [781/938], Loss: 0.5361884832382202\n",
      "Validation: Epoch [21], Batch [782/938], Loss: 0.615930438041687\n",
      "Validation: Epoch [21], Batch [783/938], Loss: 0.43328797817230225\n",
      "Validation: Epoch [21], Batch [784/938], Loss: 0.6379051208496094\n",
      "Validation: Epoch [21], Batch [785/938], Loss: 0.46628791093826294\n",
      "Validation: Epoch [21], Batch [786/938], Loss: 0.4639574885368347\n",
      "Validation: Epoch [21], Batch [787/938], Loss: 0.3893772065639496\n",
      "Validation: Epoch [21], Batch [788/938], Loss: 0.2499110996723175\n",
      "Validation: Epoch [21], Batch [789/938], Loss: 0.35232073068618774\n",
      "Validation: Epoch [21], Batch [790/938], Loss: 0.3502717912197113\n",
      "Validation: Epoch [21], Batch [791/938], Loss: 0.5204355716705322\n",
      "Validation: Epoch [21], Batch [792/938], Loss: 0.7130030989646912\n",
      "Validation: Epoch [21], Batch [793/938], Loss: 0.508705735206604\n",
      "Validation: Epoch [21], Batch [794/938], Loss: 0.525031566619873\n",
      "Validation: Epoch [21], Batch [795/938], Loss: 0.46243909001350403\n",
      "Validation: Epoch [21], Batch [796/938], Loss: 0.6688105463981628\n",
      "Validation: Epoch [21], Batch [797/938], Loss: 0.39512282609939575\n",
      "Validation: Epoch [21], Batch [798/938], Loss: 0.41292497515678406\n",
      "Validation: Epoch [21], Batch [799/938], Loss: 0.5319666862487793\n",
      "Validation: Epoch [21], Batch [800/938], Loss: 0.3792721927165985\n",
      "Validation: Epoch [21], Batch [801/938], Loss: 0.45375463366508484\n",
      "Validation: Epoch [21], Batch [802/938], Loss: 0.46644437313079834\n",
      "Validation: Epoch [21], Batch [803/938], Loss: 0.46468737721443176\n",
      "Validation: Epoch [21], Batch [804/938], Loss: 0.538868248462677\n",
      "Validation: Epoch [21], Batch [805/938], Loss: 0.36628878116607666\n",
      "Validation: Epoch [21], Batch [806/938], Loss: 0.49705520272254944\n",
      "Validation: Epoch [21], Batch [807/938], Loss: 0.5755534768104553\n",
      "Validation: Epoch [21], Batch [808/938], Loss: 0.3482654094696045\n",
      "Validation: Epoch [21], Batch [809/938], Loss: 0.40436476469039917\n",
      "Validation: Epoch [21], Batch [810/938], Loss: 0.47443675994873047\n",
      "Validation: Epoch [21], Batch [811/938], Loss: 0.39544978737831116\n",
      "Validation: Epoch [21], Batch [812/938], Loss: 0.30375543236732483\n",
      "Validation: Epoch [21], Batch [813/938], Loss: 0.3355189263820648\n",
      "Validation: Epoch [21], Batch [814/938], Loss: 0.4981745481491089\n",
      "Validation: Epoch [21], Batch [815/938], Loss: 0.45594096183776855\n",
      "Validation: Epoch [21], Batch [816/938], Loss: 0.5008743405342102\n",
      "Validation: Epoch [21], Batch [817/938], Loss: 0.24324998259544373\n",
      "Validation: Epoch [21], Batch [818/938], Loss: 0.3721780478954315\n",
      "Validation: Epoch [21], Batch [819/938], Loss: 0.46881040930747986\n",
      "Validation: Epoch [21], Batch [820/938], Loss: 0.2522093653678894\n",
      "Validation: Epoch [21], Batch [821/938], Loss: 0.48203203082084656\n",
      "Validation: Epoch [21], Batch [822/938], Loss: 0.5190480947494507\n",
      "Validation: Epoch [21], Batch [823/938], Loss: 0.34035563468933105\n",
      "Validation: Epoch [21], Batch [824/938], Loss: 0.31950849294662476\n",
      "Validation: Epoch [21], Batch [825/938], Loss: 0.4165100157260895\n",
      "Validation: Epoch [21], Batch [826/938], Loss: 0.30448830127716064\n",
      "Validation: Epoch [21], Batch [827/938], Loss: 0.35456040501594543\n",
      "Validation: Epoch [21], Batch [828/938], Loss: 0.34994521737098694\n",
      "Validation: Epoch [21], Batch [829/938], Loss: 0.3946535885334015\n",
      "Validation: Epoch [21], Batch [830/938], Loss: 0.34073182940483093\n",
      "Validation: Epoch [21], Batch [831/938], Loss: 0.2684645354747772\n",
      "Validation: Epoch [21], Batch [832/938], Loss: 0.4645887613296509\n",
      "Validation: Epoch [21], Batch [833/938], Loss: 0.5323903560638428\n",
      "Validation: Epoch [21], Batch [834/938], Loss: 0.42561832070350647\n",
      "Validation: Epoch [21], Batch [835/938], Loss: 0.35596731305122375\n",
      "Validation: Epoch [21], Batch [836/938], Loss: 0.46110600233078003\n",
      "Validation: Epoch [21], Batch [837/938], Loss: 0.452505499124527\n",
      "Validation: Epoch [21], Batch [838/938], Loss: 0.542867124080658\n",
      "Validation: Epoch [21], Batch [839/938], Loss: 0.6143205165863037\n",
      "Validation: Epoch [21], Batch [840/938], Loss: 0.2569900155067444\n",
      "Validation: Epoch [21], Batch [841/938], Loss: 0.538811206817627\n",
      "Validation: Epoch [21], Batch [842/938], Loss: 0.44968393445014954\n",
      "Validation: Epoch [21], Batch [843/938], Loss: 0.427601158618927\n",
      "Validation: Epoch [21], Batch [844/938], Loss: 0.5241485834121704\n",
      "Validation: Epoch [21], Batch [845/938], Loss: 0.40591585636138916\n",
      "Validation: Epoch [21], Batch [846/938], Loss: 0.2641317844390869\n",
      "Validation: Epoch [21], Batch [847/938], Loss: 0.4391203820705414\n",
      "Validation: Epoch [21], Batch [848/938], Loss: 0.4944981336593628\n",
      "Validation: Epoch [21], Batch [849/938], Loss: 0.5145250558853149\n",
      "Validation: Epoch [21], Batch [850/938], Loss: 0.28917035460472107\n",
      "Validation: Epoch [21], Batch [851/938], Loss: 0.4800556004047394\n",
      "Validation: Epoch [21], Batch [852/938], Loss: 0.31566891074180603\n",
      "Validation: Epoch [21], Batch [853/938], Loss: 0.5126847624778748\n",
      "Validation: Epoch [21], Batch [854/938], Loss: 0.5079343318939209\n",
      "Validation: Epoch [21], Batch [855/938], Loss: 0.4209830164909363\n",
      "Validation: Epoch [21], Batch [856/938], Loss: 0.3152608573436737\n",
      "Validation: Epoch [21], Batch [857/938], Loss: 0.611636757850647\n",
      "Validation: Epoch [21], Batch [858/938], Loss: 0.6539021134376526\n",
      "Validation: Epoch [21], Batch [859/938], Loss: 0.36971214413642883\n",
      "Validation: Epoch [21], Batch [860/938], Loss: 0.3807792067527771\n",
      "Validation: Epoch [21], Batch [861/938], Loss: 0.31626781821250916\n",
      "Validation: Epoch [21], Batch [862/938], Loss: 0.42731890082359314\n",
      "Validation: Epoch [21], Batch [863/938], Loss: 0.4326632022857666\n",
      "Validation: Epoch [21], Batch [864/938], Loss: 0.6812428832054138\n",
      "Validation: Epoch [21], Batch [865/938], Loss: 0.7155592441558838\n",
      "Validation: Epoch [21], Batch [866/938], Loss: 0.3956667184829712\n",
      "Validation: Epoch [21], Batch [867/938], Loss: 0.5285203456878662\n",
      "Validation: Epoch [21], Batch [868/938], Loss: 0.49397021532058716\n",
      "Validation: Epoch [21], Batch [869/938], Loss: 0.31140127778053284\n",
      "Validation: Epoch [21], Batch [870/938], Loss: 0.6794454455375671\n",
      "Validation: Epoch [21], Batch [871/938], Loss: 0.47644826769828796\n",
      "Validation: Epoch [21], Batch [872/938], Loss: 0.4971812963485718\n",
      "Validation: Epoch [21], Batch [873/938], Loss: 0.4434671401977539\n",
      "Validation: Epoch [21], Batch [874/938], Loss: 0.4268658459186554\n",
      "Validation: Epoch [21], Batch [875/938], Loss: 0.6710222959518433\n",
      "Validation: Epoch [21], Batch [876/938], Loss: 0.37474924325942993\n",
      "Validation: Epoch [21], Batch [877/938], Loss: 0.5674968361854553\n",
      "Validation: Epoch [21], Batch [878/938], Loss: 0.3208523988723755\n",
      "Validation: Epoch [21], Batch [879/938], Loss: 0.4022042751312256\n",
      "Validation: Epoch [21], Batch [880/938], Loss: 0.419435054063797\n",
      "Validation: Epoch [21], Batch [881/938], Loss: 0.4484512507915497\n",
      "Validation: Epoch [21], Batch [882/938], Loss: 0.4176080524921417\n",
      "Validation: Epoch [21], Batch [883/938], Loss: 0.32556942105293274\n",
      "Validation: Epoch [21], Batch [884/938], Loss: 0.35115697979927063\n",
      "Validation: Epoch [21], Batch [885/938], Loss: 0.40286245942115784\n",
      "Validation: Epoch [21], Batch [886/938], Loss: 0.42125993967056274\n",
      "Validation: Epoch [21], Batch [887/938], Loss: 0.5733968019485474\n",
      "Validation: Epoch [21], Batch [888/938], Loss: 0.6833223104476929\n",
      "Validation: Epoch [21], Batch [889/938], Loss: 0.35780951380729675\n",
      "Validation: Epoch [21], Batch [890/938], Loss: 0.4050876498222351\n",
      "Validation: Epoch [21], Batch [891/938], Loss: 0.6211900115013123\n",
      "Validation: Epoch [21], Batch [892/938], Loss: 0.3364339768886566\n",
      "Validation: Epoch [21], Batch [893/938], Loss: 0.5914989113807678\n",
      "Validation: Epoch [21], Batch [894/938], Loss: 0.673660397529602\n",
      "Validation: Epoch [21], Batch [895/938], Loss: 0.4489539563655853\n",
      "Validation: Epoch [21], Batch [896/938], Loss: 0.39021310210227966\n",
      "Validation: Epoch [21], Batch [897/938], Loss: 0.4848122000694275\n",
      "Validation: Epoch [21], Batch [898/938], Loss: 0.5381914973258972\n",
      "Validation: Epoch [21], Batch [899/938], Loss: 0.24684838950634003\n",
      "Validation: Epoch [21], Batch [900/938], Loss: 0.4701547920703888\n",
      "Validation: Epoch [21], Batch [901/938], Loss: 0.5292975902557373\n",
      "Validation: Epoch [21], Batch [902/938], Loss: 0.46535611152648926\n",
      "Validation: Epoch [21], Batch [903/938], Loss: 0.31155216693878174\n",
      "Validation: Epoch [21], Batch [904/938], Loss: 0.5488772392272949\n",
      "Validation: Epoch [21], Batch [905/938], Loss: 0.46329230070114136\n",
      "Validation: Epoch [21], Batch [906/938], Loss: 0.38113507628440857\n",
      "Validation: Epoch [21], Batch [907/938], Loss: 0.5186447501182556\n",
      "Validation: Epoch [21], Batch [908/938], Loss: 0.3740830719470978\n",
      "Validation: Epoch [21], Batch [909/938], Loss: 0.2835353910923004\n",
      "Validation: Epoch [21], Batch [910/938], Loss: 0.4476410448551178\n",
      "Validation: Epoch [21], Batch [911/938], Loss: 0.35572418570518494\n",
      "Validation: Epoch [21], Batch [912/938], Loss: 0.4079192876815796\n",
      "Validation: Epoch [21], Batch [913/938], Loss: 0.31204622983932495\n",
      "Validation: Epoch [21], Batch [914/938], Loss: 0.23216962814331055\n",
      "Validation: Epoch [21], Batch [915/938], Loss: 0.39623141288757324\n",
      "Validation: Epoch [21], Batch [916/938], Loss: 0.5687679648399353\n",
      "Validation: Epoch [21], Batch [917/938], Loss: 0.454251766204834\n",
      "Validation: Epoch [21], Batch [918/938], Loss: 0.3327816426753998\n",
      "Validation: Epoch [21], Batch [919/938], Loss: 0.5430569648742676\n",
      "Validation: Epoch [21], Batch [920/938], Loss: 0.5511740446090698\n",
      "Validation: Epoch [21], Batch [921/938], Loss: 0.39930400252342224\n",
      "Validation: Epoch [21], Batch [922/938], Loss: 0.542401909828186\n",
      "Validation: Epoch [21], Batch [923/938], Loss: 0.5700092911720276\n",
      "Validation: Epoch [21], Batch [924/938], Loss: 0.3261028528213501\n",
      "Validation: Epoch [21], Batch [925/938], Loss: 0.4231637120246887\n",
      "Validation: Epoch [21], Batch [926/938], Loss: 0.3770188093185425\n",
      "Validation: Epoch [21], Batch [927/938], Loss: 0.27185770869255066\n",
      "Validation: Epoch [21], Batch [928/938], Loss: 0.4426473081111908\n",
      "Validation: Epoch [21], Batch [929/938], Loss: 0.397762656211853\n",
      "Validation: Epoch [21], Batch [930/938], Loss: 0.4394680857658386\n",
      "Validation: Epoch [21], Batch [931/938], Loss: 0.5208914875984192\n",
      "Validation: Epoch [21], Batch [932/938], Loss: 0.3645572364330292\n",
      "Validation: Epoch [21], Batch [933/938], Loss: 0.48532363772392273\n",
      "Validation: Epoch [21], Batch [934/938], Loss: 0.46564120054244995\n",
      "Validation: Epoch [21], Batch [935/938], Loss: 0.3650820255279541\n",
      "Validation: Epoch [21], Batch [936/938], Loss: 0.5108988285064697\n",
      "Validation: Epoch [21], Batch [937/938], Loss: 0.44493889808654785\n",
      "Validation: Epoch [21], Batch [938/938], Loss: 0.21972279250621796\n",
      "Accuracy of test set: 0.8453\n",
      "Train: Epoch [22], Batch [1/938], Loss: 0.5045800805091858\n",
      "Train: Epoch [22], Batch [2/938], Loss: 0.6303828954696655\n",
      "Train: Epoch [22], Batch [3/938], Loss: 0.2892428934574127\n",
      "Train: Epoch [22], Batch [4/938], Loss: 0.5016818046569824\n",
      "Train: Epoch [22], Batch [5/938], Loss: 0.5469264388084412\n",
      "Train: Epoch [22], Batch [6/938], Loss: 0.4257374405860901\n",
      "Train: Epoch [22], Batch [7/938], Loss: 0.3647284507751465\n",
      "Train: Epoch [22], Batch [8/938], Loss: 0.48238110542297363\n",
      "Train: Epoch [22], Batch [9/938], Loss: 0.5103076100349426\n",
      "Train: Epoch [22], Batch [10/938], Loss: 0.4919736087322235\n",
      "Train: Epoch [22], Batch [11/938], Loss: 0.43419480323791504\n",
      "Train: Epoch [22], Batch [12/938], Loss: 0.5626300573348999\n",
      "Train: Epoch [22], Batch [13/938], Loss: 0.44206708669662476\n",
      "Train: Epoch [22], Batch [14/938], Loss: 0.5840749740600586\n",
      "Train: Epoch [22], Batch [15/938], Loss: 0.31851619482040405\n",
      "Train: Epoch [22], Batch [16/938], Loss: 0.285041481256485\n",
      "Train: Epoch [22], Batch [17/938], Loss: 0.5047354698181152\n",
      "Train: Epoch [22], Batch [18/938], Loss: 0.41569802165031433\n",
      "Train: Epoch [22], Batch [19/938], Loss: 0.3776094317436218\n",
      "Train: Epoch [22], Batch [20/938], Loss: 0.5214384198188782\n",
      "Train: Epoch [22], Batch [21/938], Loss: 0.3580497205257416\n",
      "Train: Epoch [22], Batch [22/938], Loss: 0.3270428478717804\n",
      "Train: Epoch [22], Batch [23/938], Loss: 0.711060643196106\n",
      "Train: Epoch [22], Batch [24/938], Loss: 0.3329719007015228\n",
      "Train: Epoch [22], Batch [25/938], Loss: 0.4469292163848877\n",
      "Train: Epoch [22], Batch [26/938], Loss: 0.46837353706359863\n",
      "Train: Epoch [22], Batch [27/938], Loss: 0.45325231552124023\n",
      "Train: Epoch [22], Batch [28/938], Loss: 0.5108826756477356\n",
      "Train: Epoch [22], Batch [29/938], Loss: 0.6507803797721863\n",
      "Train: Epoch [22], Batch [30/938], Loss: 0.23501117527484894\n",
      "Train: Epoch [22], Batch [31/938], Loss: 0.3678913116455078\n",
      "Train: Epoch [22], Batch [32/938], Loss: 0.26394203305244446\n",
      "Train: Epoch [22], Batch [33/938], Loss: 0.257739394903183\n",
      "Train: Epoch [22], Batch [34/938], Loss: 0.48231565952301025\n",
      "Train: Epoch [22], Batch [35/938], Loss: 0.36196470260620117\n",
      "Train: Epoch [22], Batch [36/938], Loss: 0.3548131585121155\n",
      "Train: Epoch [22], Batch [37/938], Loss: 0.4313148558139801\n",
      "Train: Epoch [22], Batch [38/938], Loss: 0.37597960233688354\n",
      "Train: Epoch [22], Batch [39/938], Loss: 0.5353069305419922\n",
      "Train: Epoch [22], Batch [40/938], Loss: 0.3653625249862671\n",
      "Train: Epoch [22], Batch [41/938], Loss: 0.5300224423408508\n",
      "Train: Epoch [22], Batch [42/938], Loss: 0.3625214397907257\n",
      "Train: Epoch [22], Batch [43/938], Loss: 0.8813501596450806\n",
      "Train: Epoch [22], Batch [44/938], Loss: 0.3254840672016144\n",
      "Train: Epoch [22], Batch [45/938], Loss: 0.46650224924087524\n",
      "Train: Epoch [22], Batch [46/938], Loss: 0.2242300808429718\n",
      "Train: Epoch [22], Batch [47/938], Loss: 0.5420559644699097\n",
      "Train: Epoch [22], Batch [48/938], Loss: 0.3918839991092682\n",
      "Train: Epoch [22], Batch [49/938], Loss: 0.2898055911064148\n",
      "Train: Epoch [22], Batch [50/938], Loss: 0.6234045028686523\n",
      "Train: Epoch [22], Batch [51/938], Loss: 0.4216132164001465\n",
      "Train: Epoch [22], Batch [52/938], Loss: 0.43183720111846924\n",
      "Train: Epoch [22], Batch [53/938], Loss: 0.3623720407485962\n",
      "Train: Epoch [22], Batch [54/938], Loss: 0.5333879590034485\n",
      "Train: Epoch [22], Batch [55/938], Loss: 0.4791985750198364\n",
      "Train: Epoch [22], Batch [56/938], Loss: 0.36568841338157654\n",
      "Train: Epoch [22], Batch [57/938], Loss: 0.507156252861023\n",
      "Train: Epoch [22], Batch [58/938], Loss: 0.39205944538116455\n",
      "Train: Epoch [22], Batch [59/938], Loss: 0.4283343553543091\n",
      "Train: Epoch [22], Batch [60/938], Loss: 0.5046651363372803\n",
      "Train: Epoch [22], Batch [61/938], Loss: 0.2855245769023895\n",
      "Train: Epoch [22], Batch [62/938], Loss: 0.28100866079330444\n",
      "Train: Epoch [22], Batch [63/938], Loss: 0.5205304026603699\n",
      "Train: Epoch [22], Batch [64/938], Loss: 0.37675994634628296\n",
      "Train: Epoch [22], Batch [65/938], Loss: 0.3815189301967621\n",
      "Train: Epoch [22], Batch [66/938], Loss: 0.36291563510894775\n",
      "Train: Epoch [22], Batch [67/938], Loss: 0.39507418870925903\n",
      "Train: Epoch [22], Batch [68/938], Loss: 0.48867255449295044\n",
      "Train: Epoch [22], Batch [69/938], Loss: 0.7148737907409668\n",
      "Train: Epoch [22], Batch [70/938], Loss: 0.5519096255302429\n",
      "Train: Epoch [22], Batch [71/938], Loss: 0.4818854331970215\n",
      "Train: Epoch [22], Batch [72/938], Loss: 0.44141533970832825\n",
      "Train: Epoch [22], Batch [73/938], Loss: 0.3983505964279175\n",
      "Train: Epoch [22], Batch [74/938], Loss: 0.35967618227005005\n",
      "Train: Epoch [22], Batch [75/938], Loss: 0.5309041142463684\n",
      "Train: Epoch [22], Batch [76/938], Loss: 0.46700724959373474\n",
      "Train: Epoch [22], Batch [77/938], Loss: 0.4919760525226593\n",
      "Train: Epoch [22], Batch [78/938], Loss: 0.4575759172439575\n",
      "Train: Epoch [22], Batch [79/938], Loss: 0.23312005400657654\n",
      "Train: Epoch [22], Batch [80/938], Loss: 0.5667393207550049\n",
      "Train: Epoch [22], Batch [81/938], Loss: 0.348681777715683\n",
      "Train: Epoch [22], Batch [82/938], Loss: 0.40914541482925415\n",
      "Train: Epoch [22], Batch [83/938], Loss: 0.6115925908088684\n",
      "Train: Epoch [22], Batch [84/938], Loss: 0.6607061624526978\n",
      "Train: Epoch [22], Batch [85/938], Loss: 0.47211071848869324\n",
      "Train: Epoch [22], Batch [86/938], Loss: 0.43855082988739014\n",
      "Train: Epoch [22], Batch [87/938], Loss: 0.44908568263053894\n",
      "Train: Epoch [22], Batch [88/938], Loss: 0.6831170916557312\n",
      "Train: Epoch [22], Batch [89/938], Loss: 0.4387478530406952\n",
      "Train: Epoch [22], Batch [90/938], Loss: 0.3457963168621063\n",
      "Train: Epoch [22], Batch [91/938], Loss: 0.5967239141464233\n",
      "Train: Epoch [22], Batch [92/938], Loss: 0.48070335388183594\n",
      "Train: Epoch [22], Batch [93/938], Loss: 0.43529656529426575\n",
      "Train: Epoch [22], Batch [94/938], Loss: 0.28696373105049133\n",
      "Train: Epoch [22], Batch [95/938], Loss: 0.39672356843948364\n",
      "Train: Epoch [22], Batch [96/938], Loss: 0.3926931619644165\n",
      "Train: Epoch [22], Batch [97/938], Loss: 0.44125866889953613\n",
      "Train: Epoch [22], Batch [98/938], Loss: 0.19531238079071045\n",
      "Train: Epoch [22], Batch [99/938], Loss: 0.3507137596607208\n",
      "Train: Epoch [22], Batch [100/938], Loss: 0.5348256826400757\n",
      "Train: Epoch [22], Batch [101/938], Loss: 0.3575034737586975\n",
      "Train: Epoch [22], Batch [102/938], Loss: 0.3156929314136505\n",
      "Train: Epoch [22], Batch [103/938], Loss: 0.4397123157978058\n",
      "Train: Epoch [22], Batch [104/938], Loss: 0.39898306131362915\n",
      "Train: Epoch [22], Batch [105/938], Loss: 0.3707335293292999\n",
      "Train: Epoch [22], Batch [106/938], Loss: 0.3107258379459381\n",
      "Train: Epoch [22], Batch [107/938], Loss: 0.5181232690811157\n",
      "Train: Epoch [22], Batch [108/938], Loss: 0.6531316041946411\n",
      "Train: Epoch [22], Batch [109/938], Loss: 0.46599113941192627\n",
      "Train: Epoch [22], Batch [110/938], Loss: 0.36922192573547363\n",
      "Train: Epoch [22], Batch [111/938], Loss: 0.3319742679595947\n",
      "Train: Epoch [22], Batch [112/938], Loss: 0.4767891764640808\n",
      "Train: Epoch [22], Batch [113/938], Loss: 0.4963584840297699\n",
      "Train: Epoch [22], Batch [114/938], Loss: 0.46297237277030945\n",
      "Train: Epoch [22], Batch [115/938], Loss: 0.3648826777935028\n",
      "Train: Epoch [22], Batch [116/938], Loss: 0.43924158811569214\n",
      "Train: Epoch [22], Batch [117/938], Loss: 0.6191223859786987\n",
      "Train: Epoch [22], Batch [118/938], Loss: 0.5792410373687744\n",
      "Train: Epoch [22], Batch [119/938], Loss: 0.3613600730895996\n",
      "Train: Epoch [22], Batch [120/938], Loss: 0.4515543580055237\n",
      "Train: Epoch [22], Batch [121/938], Loss: 0.3659740686416626\n",
      "Train: Epoch [22], Batch [122/938], Loss: 0.4957197904586792\n",
      "Train: Epoch [22], Batch [123/938], Loss: 0.5500051379203796\n",
      "Train: Epoch [22], Batch [124/938], Loss: 0.47391054034233093\n",
      "Train: Epoch [22], Batch [125/938], Loss: 0.40647995471954346\n",
      "Train: Epoch [22], Batch [126/938], Loss: 0.5273504853248596\n",
      "Train: Epoch [22], Batch [127/938], Loss: 0.34951871633529663\n",
      "Train: Epoch [22], Batch [128/938], Loss: 0.4127463102340698\n",
      "Train: Epoch [22], Batch [129/938], Loss: 0.42248350381851196\n",
      "Train: Epoch [22], Batch [130/938], Loss: 0.3186756670475006\n",
      "Train: Epoch [22], Batch [131/938], Loss: 0.4024186432361603\n",
      "Train: Epoch [22], Batch [132/938], Loss: 0.47321298718452454\n",
      "Train: Epoch [22], Batch [133/938], Loss: 0.3306350111961365\n",
      "Train: Epoch [22], Batch [134/938], Loss: 0.41565167903900146\n",
      "Train: Epoch [22], Batch [135/938], Loss: 0.4788118898868561\n",
      "Train: Epoch [22], Batch [136/938], Loss: 0.6576986312866211\n",
      "Train: Epoch [22], Batch [137/938], Loss: 0.36305832862854004\n",
      "Train: Epoch [22], Batch [138/938], Loss: 0.3682381212711334\n",
      "Train: Epoch [22], Batch [139/938], Loss: 0.733535885810852\n",
      "Train: Epoch [22], Batch [140/938], Loss: 0.6623156666755676\n",
      "Train: Epoch [22], Batch [141/938], Loss: 0.4741065204143524\n",
      "Train: Epoch [22], Batch [142/938], Loss: 0.35747963190078735\n",
      "Train: Epoch [22], Batch [143/938], Loss: 0.2658703029155731\n",
      "Train: Epoch [22], Batch [144/938], Loss: 0.3948148190975189\n",
      "Train: Epoch [22], Batch [145/938], Loss: 0.40780317783355713\n",
      "Train: Epoch [22], Batch [146/938], Loss: 0.3974840044975281\n",
      "Train: Epoch [22], Batch [147/938], Loss: 0.532634437084198\n",
      "Train: Epoch [22], Batch [148/938], Loss: 0.42499640583992004\n",
      "Train: Epoch [22], Batch [149/938], Loss: 0.46822455525398254\n",
      "Train: Epoch [22], Batch [150/938], Loss: 0.5147912502288818\n",
      "Train: Epoch [22], Batch [151/938], Loss: 0.5074419975280762\n",
      "Train: Epoch [22], Batch [152/938], Loss: 0.3350539803504944\n",
      "Train: Epoch [22], Batch [153/938], Loss: 0.25163155794143677\n",
      "Train: Epoch [22], Batch [154/938], Loss: 0.4523737132549286\n",
      "Train: Epoch [22], Batch [155/938], Loss: 0.5079143643379211\n",
      "Train: Epoch [22], Batch [156/938], Loss: 0.4727034866809845\n",
      "Train: Epoch [22], Batch [157/938], Loss: 0.5007805824279785\n",
      "Train: Epoch [22], Batch [158/938], Loss: 0.27880334854125977\n",
      "Train: Epoch [22], Batch [159/938], Loss: 0.4349178969860077\n",
      "Train: Epoch [22], Batch [160/938], Loss: 0.5083239078521729\n",
      "Train: Epoch [22], Batch [161/938], Loss: 0.2903927266597748\n",
      "Train: Epoch [22], Batch [162/938], Loss: 0.5869179368019104\n",
      "Train: Epoch [22], Batch [163/938], Loss: 0.37009188532829285\n",
      "Train: Epoch [22], Batch [164/938], Loss: 0.43953582644462585\n",
      "Train: Epoch [22], Batch [165/938], Loss: 0.557253360748291\n",
      "Train: Epoch [22], Batch [166/938], Loss: 0.4898076057434082\n",
      "Train: Epoch [22], Batch [167/938], Loss: 0.4240081310272217\n",
      "Train: Epoch [22], Batch [168/938], Loss: 0.4023561477661133\n",
      "Train: Epoch [22], Batch [169/938], Loss: 0.4372812807559967\n",
      "Train: Epoch [22], Batch [170/938], Loss: 0.47339195013046265\n",
      "Train: Epoch [22], Batch [171/938], Loss: 0.39881059527397156\n",
      "Train: Epoch [22], Batch [172/938], Loss: 0.636610209941864\n",
      "Train: Epoch [22], Batch [173/938], Loss: 0.7232979536056519\n",
      "Train: Epoch [22], Batch [174/938], Loss: 0.6456511616706848\n",
      "Train: Epoch [22], Batch [175/938], Loss: 0.5043749213218689\n",
      "Train: Epoch [22], Batch [176/938], Loss: 0.4766615033149719\n",
      "Train: Epoch [22], Batch [177/938], Loss: 0.5157504081726074\n",
      "Train: Epoch [22], Batch [178/938], Loss: 0.29957300424575806\n",
      "Train: Epoch [22], Batch [179/938], Loss: 0.2588571012020111\n",
      "Train: Epoch [22], Batch [180/938], Loss: 0.6222787499427795\n",
      "Train: Epoch [22], Batch [181/938], Loss: 0.5327730178833008\n",
      "Train: Epoch [22], Batch [182/938], Loss: 0.6279201507568359\n",
      "Train: Epoch [22], Batch [183/938], Loss: 0.5554643273353577\n",
      "Train: Epoch [22], Batch [184/938], Loss: 0.3084685802459717\n",
      "Train: Epoch [22], Batch [185/938], Loss: 0.5167927145957947\n",
      "Train: Epoch [22], Batch [186/938], Loss: 0.2699680030345917\n",
      "Train: Epoch [22], Batch [187/938], Loss: 0.3928695023059845\n",
      "Train: Epoch [22], Batch [188/938], Loss: 0.4645537734031677\n",
      "Train: Epoch [22], Batch [189/938], Loss: 0.3549634516239166\n",
      "Train: Epoch [22], Batch [190/938], Loss: 0.5550541877746582\n",
      "Train: Epoch [22], Batch [191/938], Loss: 0.5748583078384399\n",
      "Train: Epoch [22], Batch [192/938], Loss: 0.47269511222839355\n",
      "Train: Epoch [22], Batch [193/938], Loss: 0.4664493203163147\n",
      "Train: Epoch [22], Batch [194/938], Loss: 0.6593185663223267\n",
      "Train: Epoch [22], Batch [195/938], Loss: 0.3700910210609436\n",
      "Train: Epoch [22], Batch [196/938], Loss: 0.5703065991401672\n",
      "Train: Epoch [22], Batch [197/938], Loss: 0.7053516507148743\n",
      "Train: Epoch [22], Batch [198/938], Loss: 0.3928753137588501\n",
      "Train: Epoch [22], Batch [199/938], Loss: 0.34550055861473083\n",
      "Train: Epoch [22], Batch [200/938], Loss: 0.5304405093193054\n",
      "Train: Epoch [22], Batch [201/938], Loss: 0.31339192390441895\n",
      "Train: Epoch [22], Batch [202/938], Loss: 0.356499046087265\n",
      "Train: Epoch [22], Batch [203/938], Loss: 0.5169061422348022\n",
      "Train: Epoch [22], Batch [204/938], Loss: 0.4576936960220337\n",
      "Train: Epoch [22], Batch [205/938], Loss: 0.47623875737190247\n",
      "Train: Epoch [22], Batch [206/938], Loss: 0.46999701857566833\n",
      "Train: Epoch [22], Batch [207/938], Loss: 0.5584434270858765\n",
      "Train: Epoch [22], Batch [208/938], Loss: 0.44374722242355347\n",
      "Train: Epoch [22], Batch [209/938], Loss: 0.3414311110973358\n",
      "Train: Epoch [22], Batch [210/938], Loss: 0.5360583662986755\n",
      "Train: Epoch [22], Batch [211/938], Loss: 0.29895660281181335\n",
      "Train: Epoch [22], Batch [212/938], Loss: 0.4203428328037262\n",
      "Train: Epoch [22], Batch [213/938], Loss: 0.6439923048019409\n",
      "Train: Epoch [22], Batch [214/938], Loss: 0.45019543170928955\n",
      "Train: Epoch [22], Batch [215/938], Loss: 0.4821860194206238\n",
      "Train: Epoch [22], Batch [216/938], Loss: 0.3650115728378296\n",
      "Train: Epoch [22], Batch [217/938], Loss: 0.41993799805641174\n",
      "Train: Epoch [22], Batch [218/938], Loss: 0.4608059525489807\n",
      "Train: Epoch [22], Batch [219/938], Loss: 0.7240131497383118\n",
      "Train: Epoch [22], Batch [220/938], Loss: 0.5002644658088684\n",
      "Train: Epoch [22], Batch [221/938], Loss: 0.4553316831588745\n",
      "Train: Epoch [22], Batch [222/938], Loss: 0.45961788296699524\n",
      "Train: Epoch [22], Batch [223/938], Loss: 0.32043689489364624\n",
      "Train: Epoch [22], Batch [224/938], Loss: 0.383743017911911\n",
      "Train: Epoch [22], Batch [225/938], Loss: 0.46079230308532715\n",
      "Train: Epoch [22], Batch [226/938], Loss: 0.3174038231372833\n",
      "Train: Epoch [22], Batch [227/938], Loss: 0.2651219666004181\n",
      "Train: Epoch [22], Batch [228/938], Loss: 0.5165019035339355\n",
      "Train: Epoch [22], Batch [229/938], Loss: 0.46499699354171753\n",
      "Train: Epoch [22], Batch [230/938], Loss: 0.3648456931114197\n",
      "Train: Epoch [22], Batch [231/938], Loss: 0.41002988815307617\n",
      "Train: Epoch [22], Batch [232/938], Loss: 0.47849613428115845\n",
      "Train: Epoch [22], Batch [233/938], Loss: 0.4285522699356079\n",
      "Train: Epoch [22], Batch [234/938], Loss: 0.3127628564834595\n",
      "Train: Epoch [22], Batch [235/938], Loss: 0.3245415687561035\n",
      "Train: Epoch [22], Batch [236/938], Loss: 0.4647250771522522\n",
      "Train: Epoch [22], Batch [237/938], Loss: 0.48121511936187744\n",
      "Train: Epoch [22], Batch [238/938], Loss: 0.47445350885391235\n",
      "Train: Epoch [22], Batch [239/938], Loss: 0.5682059526443481\n",
      "Train: Epoch [22], Batch [240/938], Loss: 0.456821084022522\n",
      "Train: Epoch [22], Batch [241/938], Loss: 0.6327324509620667\n",
      "Train: Epoch [22], Batch [242/938], Loss: 0.39763420820236206\n",
      "Train: Epoch [22], Batch [243/938], Loss: 0.4086517095565796\n",
      "Train: Epoch [22], Batch [244/938], Loss: 0.47320669889450073\n",
      "Train: Epoch [22], Batch [245/938], Loss: 0.47645145654678345\n",
      "Train: Epoch [22], Batch [246/938], Loss: 0.4040408432483673\n",
      "Train: Epoch [22], Batch [247/938], Loss: 0.5810621380805969\n",
      "Train: Epoch [22], Batch [248/938], Loss: 0.5418390035629272\n",
      "Train: Epoch [22], Batch [249/938], Loss: 0.47608640789985657\n",
      "Train: Epoch [22], Batch [250/938], Loss: 0.5703721642494202\n",
      "Train: Epoch [22], Batch [251/938], Loss: 0.6961482763290405\n",
      "Train: Epoch [22], Batch [252/938], Loss: 0.6410908699035645\n",
      "Train: Epoch [22], Batch [253/938], Loss: 0.34276893734931946\n",
      "Train: Epoch [22], Batch [254/938], Loss: 0.38528546690940857\n",
      "Train: Epoch [22], Batch [255/938], Loss: 0.3117409646511078\n",
      "Train: Epoch [22], Batch [256/938], Loss: 0.5402502417564392\n",
      "Train: Epoch [22], Batch [257/938], Loss: 0.5263296365737915\n",
      "Train: Epoch [22], Batch [258/938], Loss: 0.774880588054657\n",
      "Train: Epoch [22], Batch [259/938], Loss: 0.5128469467163086\n",
      "Train: Epoch [22], Batch [260/938], Loss: 0.5658156871795654\n",
      "Train: Epoch [22], Batch [261/938], Loss: 0.2807587683200836\n",
      "Train: Epoch [22], Batch [262/938], Loss: 0.4634968638420105\n",
      "Train: Epoch [22], Batch [263/938], Loss: 0.41186606884002686\n",
      "Train: Epoch [22], Batch [264/938], Loss: 0.4633883237838745\n",
      "Train: Epoch [22], Batch [265/938], Loss: 0.38031697273254395\n",
      "Train: Epoch [22], Batch [266/938], Loss: 0.4050629138946533\n",
      "Train: Epoch [22], Batch [267/938], Loss: 0.4211495518684387\n",
      "Train: Epoch [22], Batch [268/938], Loss: 0.510901153087616\n",
      "Train: Epoch [22], Batch [269/938], Loss: 0.47599267959594727\n",
      "Train: Epoch [22], Batch [270/938], Loss: 0.5713608264923096\n",
      "Train: Epoch [22], Batch [271/938], Loss: 0.38723254203796387\n",
      "Train: Epoch [22], Batch [272/938], Loss: 0.43351471424102783\n",
      "Train: Epoch [22], Batch [273/938], Loss: 0.4180733859539032\n",
      "Train: Epoch [22], Batch [274/938], Loss: 0.38560792803764343\n",
      "Train: Epoch [22], Batch [275/938], Loss: 0.5660645365715027\n",
      "Train: Epoch [22], Batch [276/938], Loss: 0.38769033551216125\n",
      "Train: Epoch [22], Batch [277/938], Loss: 0.38316184282302856\n",
      "Train: Epoch [22], Batch [278/938], Loss: 0.3863203525543213\n",
      "Train: Epoch [22], Batch [279/938], Loss: 0.30642953515052795\n",
      "Train: Epoch [22], Batch [280/938], Loss: 0.35401925444602966\n",
      "Train: Epoch [22], Batch [281/938], Loss: 0.3549323081970215\n",
      "Train: Epoch [22], Batch [282/938], Loss: 0.3126106858253479\n",
      "Train: Epoch [22], Batch [283/938], Loss: 0.4002183675765991\n",
      "Train: Epoch [22], Batch [284/938], Loss: 0.5402891635894775\n",
      "Train: Epoch [22], Batch [285/938], Loss: 0.49286553263664246\n",
      "Train: Epoch [22], Batch [286/938], Loss: 0.5911092162132263\n",
      "Train: Epoch [22], Batch [287/938], Loss: 0.42188259959220886\n",
      "Train: Epoch [22], Batch [288/938], Loss: 0.4781040549278259\n",
      "Train: Epoch [22], Batch [289/938], Loss: 0.37165844440460205\n",
      "Train: Epoch [22], Batch [290/938], Loss: 0.4231128990650177\n",
      "Train: Epoch [22], Batch [291/938], Loss: 0.4928131103515625\n",
      "Train: Epoch [22], Batch [292/938], Loss: 0.4445239007472992\n",
      "Train: Epoch [22], Batch [293/938], Loss: 0.48557525873184204\n",
      "Train: Epoch [22], Batch [294/938], Loss: 0.4101591408252716\n",
      "Train: Epoch [22], Batch [295/938], Loss: 0.5473126769065857\n",
      "Train: Epoch [22], Batch [296/938], Loss: 0.46134018898010254\n",
      "Train: Epoch [22], Batch [297/938], Loss: 0.37470871210098267\n",
      "Train: Epoch [22], Batch [298/938], Loss: 0.45481717586517334\n",
      "Train: Epoch [22], Batch [299/938], Loss: 0.4270115792751312\n",
      "Train: Epoch [22], Batch [300/938], Loss: 0.269594669342041\n",
      "Train: Epoch [22], Batch [301/938], Loss: 0.504402756690979\n",
      "Train: Epoch [22], Batch [302/938], Loss: 0.3241327702999115\n",
      "Train: Epoch [22], Batch [303/938], Loss: 0.3260135054588318\n",
      "Train: Epoch [22], Batch [304/938], Loss: 0.47864776849746704\n",
      "Train: Epoch [22], Batch [305/938], Loss: 0.46468067169189453\n",
      "Train: Epoch [22], Batch [306/938], Loss: 0.4153282642364502\n",
      "Train: Epoch [22], Batch [307/938], Loss: 0.40592390298843384\n",
      "Train: Epoch [22], Batch [308/938], Loss: 0.3784274756908417\n",
      "Train: Epoch [22], Batch [309/938], Loss: 0.4311717450618744\n",
      "Train: Epoch [22], Batch [310/938], Loss: 0.3561742305755615\n",
      "Train: Epoch [22], Batch [311/938], Loss: 0.27061793208122253\n",
      "Train: Epoch [22], Batch [312/938], Loss: 0.41432011127471924\n",
      "Train: Epoch [22], Batch [313/938], Loss: 0.5117688179016113\n",
      "Train: Epoch [22], Batch [314/938], Loss: 0.38433483242988586\n",
      "Train: Epoch [22], Batch [315/938], Loss: 0.2829343378543854\n",
      "Train: Epoch [22], Batch [316/938], Loss: 0.5330915451049805\n",
      "Train: Epoch [22], Batch [317/938], Loss: 0.6395862102508545\n",
      "Train: Epoch [22], Batch [318/938], Loss: 0.6110532283782959\n",
      "Train: Epoch [22], Batch [319/938], Loss: 0.43041980266571045\n",
      "Train: Epoch [22], Batch [320/938], Loss: 0.3207728862762451\n",
      "Train: Epoch [22], Batch [321/938], Loss: 0.36967167258262634\n",
      "Train: Epoch [22], Batch [322/938], Loss: 0.542458176612854\n",
      "Train: Epoch [22], Batch [323/938], Loss: 0.42126426100730896\n",
      "Train: Epoch [22], Batch [324/938], Loss: 0.31271329522132874\n",
      "Train: Epoch [22], Batch [325/938], Loss: 0.5320129990577698\n",
      "Train: Epoch [22], Batch [326/938], Loss: 0.41948601603507996\n",
      "Train: Epoch [22], Batch [327/938], Loss: 0.342604398727417\n",
      "Train: Epoch [22], Batch [328/938], Loss: 0.34226906299591064\n",
      "Train: Epoch [22], Batch [329/938], Loss: 0.41448915004730225\n",
      "Train: Epoch [22], Batch [330/938], Loss: 0.3050774037837982\n",
      "Train: Epoch [22], Batch [331/938], Loss: 0.2948240339756012\n",
      "Train: Epoch [22], Batch [332/938], Loss: 0.4562455415725708\n",
      "Train: Epoch [22], Batch [333/938], Loss: 0.28535962104797363\n",
      "Train: Epoch [22], Batch [334/938], Loss: 0.5392084121704102\n",
      "Train: Epoch [22], Batch [335/938], Loss: 0.5409358143806458\n",
      "Train: Epoch [22], Batch [336/938], Loss: 0.42229709029197693\n",
      "Train: Epoch [22], Batch [337/938], Loss: 0.6111430525779724\n",
      "Train: Epoch [22], Batch [338/938], Loss: 0.3805256187915802\n",
      "Train: Epoch [22], Batch [339/938], Loss: 0.32282841205596924\n",
      "Train: Epoch [22], Batch [340/938], Loss: 0.4405849575996399\n",
      "Train: Epoch [22], Batch [341/938], Loss: 0.46180450916290283\n",
      "Train: Epoch [22], Batch [342/938], Loss: 0.3647877871990204\n",
      "Train: Epoch [22], Batch [343/938], Loss: 0.43757933378219604\n",
      "Train: Epoch [22], Batch [344/938], Loss: 0.34545737504959106\n",
      "Train: Epoch [22], Batch [345/938], Loss: 0.46400633454322815\n",
      "Train: Epoch [22], Batch [346/938], Loss: 0.8066327571868896\n",
      "Train: Epoch [22], Batch [347/938], Loss: 0.2594752907752991\n",
      "Train: Epoch [22], Batch [348/938], Loss: 0.5582588315010071\n",
      "Train: Epoch [22], Batch [349/938], Loss: 0.5189323425292969\n",
      "Train: Epoch [22], Batch [350/938], Loss: 0.5484619736671448\n",
      "Train: Epoch [22], Batch [351/938], Loss: 0.7349227666854858\n",
      "Train: Epoch [22], Batch [352/938], Loss: 0.6696311235427856\n",
      "Train: Epoch [22], Batch [353/938], Loss: 0.5081861615180969\n",
      "Train: Epoch [22], Batch [354/938], Loss: 0.4020184874534607\n",
      "Train: Epoch [22], Batch [355/938], Loss: 0.6554314494132996\n",
      "Train: Epoch [22], Batch [356/938], Loss: 0.5328985452651978\n",
      "Train: Epoch [22], Batch [357/938], Loss: 0.5148485898971558\n",
      "Train: Epoch [22], Batch [358/938], Loss: 0.3482128977775574\n",
      "Train: Epoch [22], Batch [359/938], Loss: 0.44587793946266174\n",
      "Train: Epoch [22], Batch [360/938], Loss: 0.5884580016136169\n",
      "Train: Epoch [22], Batch [361/938], Loss: 0.3079654574394226\n",
      "Train: Epoch [22], Batch [362/938], Loss: 0.4486875534057617\n",
      "Train: Epoch [22], Batch [363/938], Loss: 0.518385648727417\n",
      "Train: Epoch [22], Batch [364/938], Loss: 0.3417377471923828\n",
      "Train: Epoch [22], Batch [365/938], Loss: 0.42970597743988037\n",
      "Train: Epoch [22], Batch [366/938], Loss: 0.393454372882843\n",
      "Train: Epoch [22], Batch [367/938], Loss: 0.518194854259491\n",
      "Train: Epoch [22], Batch [368/938], Loss: 0.505425751209259\n",
      "Train: Epoch [22], Batch [369/938], Loss: 0.4513322412967682\n",
      "Train: Epoch [22], Batch [370/938], Loss: 0.5663210153579712\n",
      "Train: Epoch [22], Batch [371/938], Loss: 0.4947793185710907\n",
      "Train: Epoch [22], Batch [372/938], Loss: 0.3140745759010315\n",
      "Train: Epoch [22], Batch [373/938], Loss: 0.3740759491920471\n",
      "Train: Epoch [22], Batch [374/938], Loss: 0.5903288722038269\n",
      "Train: Epoch [22], Batch [375/938], Loss: 0.461379736661911\n",
      "Train: Epoch [22], Batch [376/938], Loss: 0.35180598497390747\n",
      "Train: Epoch [22], Batch [377/938], Loss: 0.27253979444503784\n",
      "Train: Epoch [22], Batch [378/938], Loss: 0.5785555243492126\n",
      "Train: Epoch [22], Batch [379/938], Loss: 0.31330499053001404\n",
      "Train: Epoch [22], Batch [380/938], Loss: 0.3289027214050293\n",
      "Train: Epoch [22], Batch [381/938], Loss: 0.3417994976043701\n",
      "Train: Epoch [22], Batch [382/938], Loss: 0.39451196789741516\n",
      "Train: Epoch [22], Batch [383/938], Loss: 0.31847208738327026\n",
      "Train: Epoch [22], Batch [384/938], Loss: 0.26989686489105225\n",
      "Train: Epoch [22], Batch [385/938], Loss: 0.3659348487854004\n",
      "Train: Epoch [22], Batch [386/938], Loss: 0.5150312185287476\n",
      "Train: Epoch [22], Batch [387/938], Loss: 0.3633882701396942\n",
      "Train: Epoch [22], Batch [388/938], Loss: 0.40187665820121765\n",
      "Train: Epoch [22], Batch [389/938], Loss: 0.4926052987575531\n",
      "Train: Epoch [22], Batch [390/938], Loss: 0.3755924105644226\n",
      "Train: Epoch [22], Batch [391/938], Loss: 0.41319739818573\n",
      "Train: Epoch [22], Batch [392/938], Loss: 0.40084323287010193\n",
      "Train: Epoch [22], Batch [393/938], Loss: 0.3731575310230255\n",
      "Train: Epoch [22], Batch [394/938], Loss: 0.3790785074234009\n",
      "Train: Epoch [22], Batch [395/938], Loss: 0.6177117824554443\n",
      "Train: Epoch [22], Batch [396/938], Loss: 0.4789138436317444\n",
      "Train: Epoch [22], Batch [397/938], Loss: 0.5525339841842651\n",
      "Train: Epoch [22], Batch [398/938], Loss: 0.43520721793174744\n",
      "Train: Epoch [22], Batch [399/938], Loss: 0.6699037551879883\n",
      "Train: Epoch [22], Batch [400/938], Loss: 0.47816839814186096\n",
      "Train: Epoch [22], Batch [401/938], Loss: 0.6065025925636292\n",
      "Train: Epoch [22], Batch [402/938], Loss: 0.36786240339279175\n",
      "Train: Epoch [22], Batch [403/938], Loss: 0.5693451762199402\n",
      "Train: Epoch [22], Batch [404/938], Loss: 0.3184356987476349\n",
      "Train: Epoch [22], Batch [405/938], Loss: 0.4810847043991089\n",
      "Train: Epoch [22], Batch [406/938], Loss: 0.7257052659988403\n",
      "Train: Epoch [22], Batch [407/938], Loss: 0.3037372827529907\n",
      "Train: Epoch [22], Batch [408/938], Loss: 0.3948500156402588\n",
      "Train: Epoch [22], Batch [409/938], Loss: 0.3826428949832916\n",
      "Train: Epoch [22], Batch [410/938], Loss: 0.42033863067626953\n",
      "Train: Epoch [22], Batch [411/938], Loss: 0.46514666080474854\n",
      "Train: Epoch [22], Batch [412/938], Loss: 0.34389185905456543\n",
      "Train: Epoch [22], Batch [413/938], Loss: 0.3267587721347809\n",
      "Train: Epoch [22], Batch [414/938], Loss: 0.4502398371696472\n",
      "Train: Epoch [22], Batch [415/938], Loss: 0.5342994928359985\n",
      "Train: Epoch [22], Batch [416/938], Loss: 0.5173037648200989\n",
      "Train: Epoch [22], Batch [417/938], Loss: 0.3212610185146332\n",
      "Train: Epoch [22], Batch [418/938], Loss: 0.4309321939945221\n",
      "Train: Epoch [22], Batch [419/938], Loss: 0.4942575991153717\n",
      "Train: Epoch [22], Batch [420/938], Loss: 0.33001449704170227\n",
      "Train: Epoch [22], Batch [421/938], Loss: 0.29766231775283813\n",
      "Train: Epoch [22], Batch [422/938], Loss: 0.5983639359474182\n",
      "Train: Epoch [22], Batch [423/938], Loss: 0.5081530809402466\n",
      "Train: Epoch [22], Batch [424/938], Loss: 0.494312584400177\n",
      "Train: Epoch [22], Batch [425/938], Loss: 0.4085186719894409\n",
      "Train: Epoch [22], Batch [426/938], Loss: 0.5510407090187073\n",
      "Train: Epoch [22], Batch [427/938], Loss: 0.36013421416282654\n",
      "Train: Epoch [22], Batch [428/938], Loss: 0.4422970116138458\n",
      "Train: Epoch [22], Batch [429/938], Loss: 0.4203633964061737\n",
      "Train: Epoch [22], Batch [430/938], Loss: 0.4527813196182251\n",
      "Train: Epoch [22], Batch [431/938], Loss: 0.38476651906967163\n",
      "Train: Epoch [22], Batch [432/938], Loss: 0.4729578197002411\n",
      "Train: Epoch [22], Batch [433/938], Loss: 0.3139038681983948\n",
      "Train: Epoch [22], Batch [434/938], Loss: 0.28796905279159546\n",
      "Train: Epoch [22], Batch [435/938], Loss: 0.31370553374290466\n",
      "Train: Epoch [22], Batch [436/938], Loss: 0.3756023645401001\n",
      "Train: Epoch [22], Batch [437/938], Loss: 0.5150531530380249\n",
      "Train: Epoch [22], Batch [438/938], Loss: 0.7374984622001648\n",
      "Train: Epoch [22], Batch [439/938], Loss: 0.4298979938030243\n",
      "Train: Epoch [22], Batch [440/938], Loss: 0.5354625582695007\n",
      "Train: Epoch [22], Batch [441/938], Loss: 0.5221918821334839\n",
      "Train: Epoch [22], Batch [442/938], Loss: 0.5830227732658386\n",
      "Train: Epoch [22], Batch [443/938], Loss: 0.4256307780742645\n",
      "Train: Epoch [22], Batch [444/938], Loss: 0.5635834336280823\n",
      "Train: Epoch [22], Batch [445/938], Loss: 0.7366738319396973\n",
      "Train: Epoch [22], Batch [446/938], Loss: 0.40062662959098816\n",
      "Train: Epoch [22], Batch [447/938], Loss: 0.504962682723999\n",
      "Train: Epoch [22], Batch [448/938], Loss: 0.3966462314128876\n",
      "Train: Epoch [22], Batch [449/938], Loss: 0.6913328766822815\n",
      "Train: Epoch [22], Batch [450/938], Loss: 0.37856611609458923\n",
      "Train: Epoch [22], Batch [451/938], Loss: 0.4764174818992615\n",
      "Train: Epoch [22], Batch [452/938], Loss: 0.3332187235355377\n",
      "Train: Epoch [22], Batch [453/938], Loss: 0.5514166355133057\n",
      "Train: Epoch [22], Batch [454/938], Loss: 0.44810986518859863\n",
      "Train: Epoch [22], Batch [455/938], Loss: 0.437385618686676\n",
      "Train: Epoch [22], Batch [456/938], Loss: 0.48422107100486755\n",
      "Train: Epoch [22], Batch [457/938], Loss: 0.40023162961006165\n",
      "Train: Epoch [22], Batch [458/938], Loss: 0.3392918109893799\n",
      "Train: Epoch [22], Batch [459/938], Loss: 0.3548477292060852\n",
      "Train: Epoch [22], Batch [460/938], Loss: 0.5844029784202576\n",
      "Train: Epoch [22], Batch [461/938], Loss: 0.32498830556869507\n",
      "Train: Epoch [22], Batch [462/938], Loss: 0.5973634123802185\n",
      "Train: Epoch [22], Batch [463/938], Loss: 0.4538656771183014\n",
      "Train: Epoch [22], Batch [464/938], Loss: 0.5346147418022156\n",
      "Train: Epoch [22], Batch [465/938], Loss: 0.4243769645690918\n",
      "Train: Epoch [22], Batch [466/938], Loss: 0.3708292245864868\n",
      "Train: Epoch [22], Batch [467/938], Loss: 0.5692520141601562\n",
      "Train: Epoch [22], Batch [468/938], Loss: 0.5578229427337646\n",
      "Train: Epoch [22], Batch [469/938], Loss: 0.5922769904136658\n",
      "Train: Epoch [22], Batch [470/938], Loss: 0.2857622504234314\n",
      "Train: Epoch [22], Batch [471/938], Loss: 0.5000969767570496\n",
      "Train: Epoch [22], Batch [472/938], Loss: 0.24633735418319702\n",
      "Train: Epoch [22], Batch [473/938], Loss: 0.5849448442459106\n",
      "Train: Epoch [22], Batch [474/938], Loss: 0.5971533060073853\n",
      "Train: Epoch [22], Batch [475/938], Loss: 0.4936598241329193\n",
      "Train: Epoch [22], Batch [476/938], Loss: 0.541858434677124\n",
      "Train: Epoch [22], Batch [477/938], Loss: 0.5145886540412903\n",
      "Train: Epoch [22], Batch [478/938], Loss: 0.33335548639297485\n",
      "Train: Epoch [22], Batch [479/938], Loss: 0.35464444756507874\n",
      "Train: Epoch [22], Batch [480/938], Loss: 0.5180534720420837\n",
      "Train: Epoch [22], Batch [481/938], Loss: 0.24546265602111816\n",
      "Train: Epoch [22], Batch [482/938], Loss: 0.30362585186958313\n",
      "Train: Epoch [22], Batch [483/938], Loss: 0.4320964813232422\n",
      "Train: Epoch [22], Batch [484/938], Loss: 0.5541627407073975\n",
      "Train: Epoch [22], Batch [485/938], Loss: 0.4552614688873291\n",
      "Train: Epoch [22], Batch [486/938], Loss: 0.3984163701534271\n",
      "Train: Epoch [22], Batch [487/938], Loss: 0.3705560564994812\n",
      "Train: Epoch [22], Batch [488/938], Loss: 0.6584113240242004\n",
      "Train: Epoch [22], Batch [489/938], Loss: 0.30210012197494507\n",
      "Train: Epoch [22], Batch [490/938], Loss: 0.6153342723846436\n",
      "Train: Epoch [22], Batch [491/938], Loss: 0.5794402956962585\n",
      "Train: Epoch [22], Batch [492/938], Loss: 0.5700119733810425\n",
      "Train: Epoch [22], Batch [493/938], Loss: 0.5082875490188599\n",
      "Train: Epoch [22], Batch [494/938], Loss: 0.24201884865760803\n",
      "Train: Epoch [22], Batch [495/938], Loss: 0.5033189654350281\n",
      "Train: Epoch [22], Batch [496/938], Loss: 0.42858564853668213\n",
      "Train: Epoch [22], Batch [497/938], Loss: 0.5651454925537109\n",
      "Train: Epoch [22], Batch [498/938], Loss: 0.296734094619751\n",
      "Train: Epoch [22], Batch [499/938], Loss: 0.3840150535106659\n",
      "Train: Epoch [22], Batch [500/938], Loss: 0.5296013951301575\n",
      "Train: Epoch [22], Batch [501/938], Loss: 0.349645733833313\n",
      "Train: Epoch [22], Batch [502/938], Loss: 0.46485066413879395\n",
      "Train: Epoch [22], Batch [503/938], Loss: 0.3892444372177124\n",
      "Train: Epoch [22], Batch [504/938], Loss: 0.3127434551715851\n",
      "Train: Epoch [22], Batch [505/938], Loss: 0.33037692308425903\n",
      "Train: Epoch [22], Batch [506/938], Loss: 0.40202009677886963\n",
      "Train: Epoch [22], Batch [507/938], Loss: 0.4594724178314209\n",
      "Train: Epoch [22], Batch [508/938], Loss: 0.42334914207458496\n",
      "Train: Epoch [22], Batch [509/938], Loss: 0.47875964641571045\n",
      "Train: Epoch [22], Batch [510/938], Loss: 0.4827597141265869\n",
      "Train: Epoch [22], Batch [511/938], Loss: 0.3741040825843811\n",
      "Train: Epoch [22], Batch [512/938], Loss: 0.37996843457221985\n",
      "Train: Epoch [22], Batch [513/938], Loss: 0.45453208684921265\n",
      "Train: Epoch [22], Batch [514/938], Loss: 0.5025254487991333\n",
      "Train: Epoch [22], Batch [515/938], Loss: 0.26496630907058716\n",
      "Train: Epoch [22], Batch [516/938], Loss: 0.4853852689266205\n",
      "Train: Epoch [22], Batch [517/938], Loss: 0.42434632778167725\n",
      "Train: Epoch [22], Batch [518/938], Loss: 0.5424351692199707\n",
      "Train: Epoch [22], Batch [519/938], Loss: 0.3633892238140106\n",
      "Train: Epoch [22], Batch [520/938], Loss: 0.24767698347568512\n",
      "Train: Epoch [22], Batch [521/938], Loss: 0.3043452501296997\n",
      "Train: Epoch [22], Batch [522/938], Loss: 0.2746048867702484\n",
      "Train: Epoch [22], Batch [523/938], Loss: 0.44455844163894653\n",
      "Train: Epoch [22], Batch [524/938], Loss: 0.3947434723377228\n",
      "Train: Epoch [22], Batch [525/938], Loss: 0.31892845034599304\n",
      "Train: Epoch [22], Batch [526/938], Loss: 0.33549079298973083\n",
      "Train: Epoch [22], Batch [527/938], Loss: 0.5345583558082581\n",
      "Train: Epoch [22], Batch [528/938], Loss: 0.5214353203773499\n",
      "Train: Epoch [22], Batch [529/938], Loss: 0.45006003975868225\n",
      "Train: Epoch [22], Batch [530/938], Loss: 0.4307926595211029\n",
      "Train: Epoch [22], Batch [531/938], Loss: 0.4143936336040497\n",
      "Train: Epoch [22], Batch [532/938], Loss: 0.34782272577285767\n",
      "Train: Epoch [22], Batch [533/938], Loss: 0.33197370171546936\n",
      "Train: Epoch [22], Batch [534/938], Loss: 0.5512421727180481\n",
      "Train: Epoch [22], Batch [535/938], Loss: 0.3593606650829315\n",
      "Train: Epoch [22], Batch [536/938], Loss: 0.3749042749404907\n",
      "Train: Epoch [22], Batch [537/938], Loss: 0.5038207769393921\n",
      "Train: Epoch [22], Batch [538/938], Loss: 0.3983716070652008\n",
      "Train: Epoch [22], Batch [539/938], Loss: 0.5032538771629333\n",
      "Train: Epoch [22], Batch [540/938], Loss: 0.36807239055633545\n",
      "Train: Epoch [22], Batch [541/938], Loss: 0.23212237656116486\n",
      "Train: Epoch [22], Batch [542/938], Loss: 0.37110936641693115\n",
      "Train: Epoch [22], Batch [543/938], Loss: 0.5788799524307251\n",
      "Train: Epoch [22], Batch [544/938], Loss: 0.4815008044242859\n",
      "Train: Epoch [22], Batch [545/938], Loss: 0.3417099416255951\n",
      "Train: Epoch [22], Batch [546/938], Loss: 0.3072797954082489\n",
      "Train: Epoch [22], Batch [547/938], Loss: 0.665543794631958\n",
      "Train: Epoch [22], Batch [548/938], Loss: 0.28980565071105957\n",
      "Train: Epoch [22], Batch [549/938], Loss: 0.4788430631160736\n",
      "Train: Epoch [22], Batch [550/938], Loss: 0.3609537184238434\n",
      "Train: Epoch [22], Batch [551/938], Loss: 0.3963117003440857\n",
      "Train: Epoch [22], Batch [552/938], Loss: 0.4622551202774048\n",
      "Train: Epoch [22], Batch [553/938], Loss: 0.49741971492767334\n",
      "Train: Epoch [22], Batch [554/938], Loss: 0.44826740026474\n",
      "Train: Epoch [22], Batch [555/938], Loss: 0.35330650210380554\n",
      "Train: Epoch [22], Batch [556/938], Loss: 0.44371846318244934\n",
      "Train: Epoch [22], Batch [557/938], Loss: 0.38494396209716797\n",
      "Train: Epoch [22], Batch [558/938], Loss: 0.4027531147003174\n",
      "Train: Epoch [22], Batch [559/938], Loss: 0.4972240626811981\n",
      "Train: Epoch [22], Batch [560/938], Loss: 0.4570959210395813\n",
      "Train: Epoch [22], Batch [561/938], Loss: 0.5305618643760681\n",
      "Train: Epoch [22], Batch [562/938], Loss: 0.3206411600112915\n",
      "Train: Epoch [22], Batch [563/938], Loss: 0.4098106920719147\n",
      "Train: Epoch [22], Batch [564/938], Loss: 0.32473528385162354\n",
      "Train: Epoch [22], Batch [565/938], Loss: 0.28357240557670593\n",
      "Train: Epoch [22], Batch [566/938], Loss: 0.3945419192314148\n",
      "Train: Epoch [22], Batch [567/938], Loss: 0.6358462572097778\n",
      "Train: Epoch [22], Batch [568/938], Loss: 0.3045337498188019\n",
      "Train: Epoch [22], Batch [569/938], Loss: 0.36042454838752747\n",
      "Train: Epoch [22], Batch [570/938], Loss: 0.38879358768463135\n",
      "Train: Epoch [22], Batch [571/938], Loss: 0.5698314905166626\n",
      "Train: Epoch [22], Batch [572/938], Loss: 0.5725909471511841\n",
      "Train: Epoch [22], Batch [573/938], Loss: 0.3223791718482971\n",
      "Train: Epoch [22], Batch [574/938], Loss: 0.45512697100639343\n",
      "Train: Epoch [22], Batch [575/938], Loss: 0.35616177320480347\n",
      "Train: Epoch [22], Batch [576/938], Loss: 0.36526572704315186\n",
      "Train: Epoch [22], Batch [577/938], Loss: 0.4740256369113922\n",
      "Train: Epoch [22], Batch [578/938], Loss: 0.4034412205219269\n",
      "Train: Epoch [22], Batch [579/938], Loss: 0.3852977156639099\n",
      "Train: Epoch [22], Batch [580/938], Loss: 0.551144003868103\n",
      "Train: Epoch [22], Batch [581/938], Loss: 0.34431612491607666\n",
      "Train: Epoch [22], Batch [582/938], Loss: 0.4531395137310028\n",
      "Train: Epoch [22], Batch [583/938], Loss: 0.39149805903434753\n",
      "Train: Epoch [22], Batch [584/938], Loss: 0.32801929116249084\n",
      "Train: Epoch [22], Batch [585/938], Loss: 0.6768189668655396\n",
      "Train: Epoch [22], Batch [586/938], Loss: 0.32721981406211853\n",
      "Train: Epoch [22], Batch [587/938], Loss: 0.48971283435821533\n",
      "Train: Epoch [22], Batch [588/938], Loss: 0.3748376965522766\n",
      "Train: Epoch [22], Batch [589/938], Loss: 0.4819146394729614\n",
      "Train: Epoch [22], Batch [590/938], Loss: 0.6765474081039429\n",
      "Train: Epoch [22], Batch [591/938], Loss: 0.3422776162624359\n",
      "Train: Epoch [22], Batch [592/938], Loss: 0.41354817152023315\n",
      "Train: Epoch [22], Batch [593/938], Loss: 0.3137550950050354\n",
      "Train: Epoch [22], Batch [594/938], Loss: 0.4560450613498688\n",
      "Train: Epoch [22], Batch [595/938], Loss: 0.4131217300891876\n",
      "Train: Epoch [22], Batch [596/938], Loss: 0.2689675986766815\n",
      "Train: Epoch [22], Batch [597/938], Loss: 0.5854614973068237\n",
      "Train: Epoch [22], Batch [598/938], Loss: 0.6309729814529419\n",
      "Train: Epoch [22], Batch [599/938], Loss: 0.6245074272155762\n",
      "Train: Epoch [22], Batch [600/938], Loss: 0.34309884905815125\n",
      "Train: Epoch [22], Batch [601/938], Loss: 0.4475085437297821\n",
      "Train: Epoch [22], Batch [602/938], Loss: 0.341387003660202\n",
      "Train: Epoch [22], Batch [603/938], Loss: 0.4571053683757782\n",
      "Train: Epoch [22], Batch [604/938], Loss: 0.4672742187976837\n",
      "Train: Epoch [22], Batch [605/938], Loss: 0.5219557285308838\n",
      "Train: Epoch [22], Batch [606/938], Loss: 0.6216899156570435\n",
      "Train: Epoch [22], Batch [607/938], Loss: 0.4416143298149109\n",
      "Train: Epoch [22], Batch [608/938], Loss: 0.34678250551223755\n",
      "Train: Epoch [22], Batch [609/938], Loss: 0.3265453577041626\n",
      "Train: Epoch [22], Batch [610/938], Loss: 0.5100095868110657\n",
      "Train: Epoch [22], Batch [611/938], Loss: 0.4010329842567444\n",
      "Train: Epoch [22], Batch [612/938], Loss: 0.7289031744003296\n",
      "Train: Epoch [22], Batch [613/938], Loss: 0.4720485210418701\n",
      "Train: Epoch [22], Batch [614/938], Loss: 0.6084637641906738\n",
      "Train: Epoch [22], Batch [615/938], Loss: 0.4218195974826813\n",
      "Train: Epoch [22], Batch [616/938], Loss: 0.6248300671577454\n",
      "Train: Epoch [22], Batch [617/938], Loss: 0.3914299011230469\n",
      "Train: Epoch [22], Batch [618/938], Loss: 0.38572314381599426\n",
      "Train: Epoch [22], Batch [619/938], Loss: 0.5312329530715942\n",
      "Train: Epoch [22], Batch [620/938], Loss: 0.2906232178211212\n",
      "Train: Epoch [22], Batch [621/938], Loss: 0.5789373517036438\n",
      "Train: Epoch [22], Batch [622/938], Loss: 0.4176400899887085\n",
      "Train: Epoch [22], Batch [623/938], Loss: 0.37155652046203613\n",
      "Train: Epoch [22], Batch [624/938], Loss: 0.4848851263523102\n",
      "Train: Epoch [22], Batch [625/938], Loss: 0.5007328391075134\n",
      "Train: Epoch [22], Batch [626/938], Loss: 0.808840274810791\n",
      "Train: Epoch [22], Batch [627/938], Loss: 0.523645281791687\n",
      "Train: Epoch [22], Batch [628/938], Loss: 0.5025516152381897\n",
      "Train: Epoch [22], Batch [629/938], Loss: 0.40641143918037415\n",
      "Train: Epoch [22], Batch [630/938], Loss: 0.3901172876358032\n",
      "Train: Epoch [22], Batch [631/938], Loss: 0.3705431818962097\n",
      "Train: Epoch [22], Batch [632/938], Loss: 0.5026366114616394\n",
      "Train: Epoch [22], Batch [633/938], Loss: 0.4537515640258789\n",
      "Train: Epoch [22], Batch [634/938], Loss: 0.5080367922782898\n",
      "Train: Epoch [22], Batch [635/938], Loss: 0.26834768056869507\n",
      "Train: Epoch [22], Batch [636/938], Loss: 0.458110511302948\n",
      "Train: Epoch [22], Batch [637/938], Loss: 0.362122118473053\n",
      "Train: Epoch [22], Batch [638/938], Loss: 0.3835737705230713\n",
      "Train: Epoch [22], Batch [639/938], Loss: 0.5764855146408081\n",
      "Train: Epoch [22], Batch [640/938], Loss: 0.5750352740287781\n",
      "Train: Epoch [22], Batch [641/938], Loss: 0.5839344263076782\n",
      "Train: Epoch [22], Batch [642/938], Loss: 0.3510623872280121\n",
      "Train: Epoch [22], Batch [643/938], Loss: 0.5663455724716187\n",
      "Train: Epoch [22], Batch [644/938], Loss: 0.2670069634914398\n",
      "Train: Epoch [22], Batch [645/938], Loss: 0.463847815990448\n",
      "Train: Epoch [22], Batch [646/938], Loss: 0.4665314257144928\n",
      "Train: Epoch [22], Batch [647/938], Loss: 0.40173593163490295\n",
      "Train: Epoch [22], Batch [648/938], Loss: 0.386274516582489\n",
      "Train: Epoch [22], Batch [649/938], Loss: 0.5137441158294678\n",
      "Train: Epoch [22], Batch [650/938], Loss: 0.3529191017150879\n",
      "Train: Epoch [22], Batch [651/938], Loss: 0.7420914173126221\n",
      "Train: Epoch [22], Batch [652/938], Loss: 0.5761390924453735\n",
      "Train: Epoch [22], Batch [653/938], Loss: 0.3314828872680664\n",
      "Train: Epoch [22], Batch [654/938], Loss: 0.6504849195480347\n",
      "Train: Epoch [22], Batch [655/938], Loss: 0.31494900584220886\n",
      "Train: Epoch [22], Batch [656/938], Loss: 0.44786858558654785\n",
      "Train: Epoch [22], Batch [657/938], Loss: 0.3295707702636719\n",
      "Train: Epoch [22], Batch [658/938], Loss: 0.4018906056880951\n",
      "Train: Epoch [22], Batch [659/938], Loss: 0.44263792037963867\n",
      "Train: Epoch [22], Batch [660/938], Loss: 0.35013535618782043\n",
      "Train: Epoch [22], Batch [661/938], Loss: 0.5016425251960754\n",
      "Train: Epoch [22], Batch [662/938], Loss: 0.3268004059791565\n",
      "Train: Epoch [22], Batch [663/938], Loss: 0.3156183362007141\n",
      "Train: Epoch [22], Batch [664/938], Loss: 0.3376062214374542\n",
      "Train: Epoch [22], Batch [665/938], Loss: 0.41637465357780457\n",
      "Train: Epoch [22], Batch [666/938], Loss: 0.6036801338195801\n",
      "Train: Epoch [22], Batch [667/938], Loss: 0.3944224417209625\n",
      "Train: Epoch [22], Batch [668/938], Loss: 0.47468823194503784\n",
      "Train: Epoch [22], Batch [669/938], Loss: 0.32613933086395264\n",
      "Train: Epoch [22], Batch [670/938], Loss: 0.5708402395248413\n",
      "Train: Epoch [22], Batch [671/938], Loss: 0.36320674419403076\n",
      "Train: Epoch [22], Batch [672/938], Loss: 0.218545600771904\n",
      "Train: Epoch [22], Batch [673/938], Loss: 0.5233631134033203\n",
      "Train: Epoch [22], Batch [674/938], Loss: 0.3844410479068756\n",
      "Train: Epoch [22], Batch [675/938], Loss: 0.4896756112575531\n",
      "Train: Epoch [22], Batch [676/938], Loss: 0.3454117476940155\n",
      "Train: Epoch [22], Batch [677/938], Loss: 0.4738670587539673\n",
      "Train: Epoch [22], Batch [678/938], Loss: 0.39463502168655396\n",
      "Train: Epoch [22], Batch [679/938], Loss: 0.26751720905303955\n",
      "Train: Epoch [22], Batch [680/938], Loss: 0.6372083425521851\n",
      "Train: Epoch [22], Batch [681/938], Loss: 0.37367528676986694\n",
      "Train: Epoch [22], Batch [682/938], Loss: 0.5539251565933228\n",
      "Train: Epoch [22], Batch [683/938], Loss: 0.4515310227870941\n",
      "Train: Epoch [22], Batch [684/938], Loss: 0.38156673312187195\n",
      "Train: Epoch [22], Batch [685/938], Loss: 0.5864290595054626\n",
      "Train: Epoch [22], Batch [686/938], Loss: 0.4566444158554077\n",
      "Train: Epoch [22], Batch [687/938], Loss: 0.3731626272201538\n",
      "Train: Epoch [22], Batch [688/938], Loss: 0.2445722371339798\n",
      "Train: Epoch [22], Batch [689/938], Loss: 0.38742172718048096\n",
      "Train: Epoch [22], Batch [690/938], Loss: 0.46269893646240234\n",
      "Train: Epoch [22], Batch [691/938], Loss: 0.4336424469947815\n",
      "Train: Epoch [22], Batch [692/938], Loss: 0.4162496328353882\n",
      "Train: Epoch [22], Batch [693/938], Loss: 0.42696982622146606\n",
      "Train: Epoch [22], Batch [694/938], Loss: 0.33748897910118103\n",
      "Train: Epoch [22], Batch [695/938], Loss: 0.4272734522819519\n",
      "Train: Epoch [22], Batch [696/938], Loss: 0.3733963370323181\n",
      "Train: Epoch [22], Batch [697/938], Loss: 0.3990030884742737\n",
      "Train: Epoch [22], Batch [698/938], Loss: 0.5528555512428284\n",
      "Train: Epoch [22], Batch [699/938], Loss: 0.4136030673980713\n",
      "Train: Epoch [22], Batch [700/938], Loss: 0.49578604102134705\n",
      "Train: Epoch [22], Batch [701/938], Loss: 0.575785756111145\n",
      "Train: Epoch [22], Batch [702/938], Loss: 0.4437733292579651\n",
      "Train: Epoch [22], Batch [703/938], Loss: 0.28713956475257874\n",
      "Train: Epoch [22], Batch [704/938], Loss: 0.38806864619255066\n",
      "Train: Epoch [22], Batch [705/938], Loss: 0.31593865156173706\n",
      "Train: Epoch [22], Batch [706/938], Loss: 0.3487398326396942\n",
      "Train: Epoch [22], Batch [707/938], Loss: 0.34273800253868103\n",
      "Train: Epoch [22], Batch [708/938], Loss: 0.47596150636672974\n",
      "Train: Epoch [22], Batch [709/938], Loss: 0.276277095079422\n",
      "Train: Epoch [22], Batch [710/938], Loss: 0.7023948431015015\n",
      "Train: Epoch [22], Batch [711/938], Loss: 0.5240525007247925\n",
      "Train: Epoch [22], Batch [712/938], Loss: 0.5416455268859863\n",
      "Train: Epoch [22], Batch [713/938], Loss: 0.5141715407371521\n",
      "Train: Epoch [22], Batch [714/938], Loss: 0.44813254475593567\n",
      "Train: Epoch [22], Batch [715/938], Loss: 0.2805512845516205\n",
      "Train: Epoch [22], Batch [716/938], Loss: 0.352235347032547\n",
      "Train: Epoch [22], Batch [717/938], Loss: 0.3860100209712982\n",
      "Train: Epoch [22], Batch [718/938], Loss: 0.5765764117240906\n",
      "Train: Epoch [22], Batch [719/938], Loss: 0.41055360436439514\n",
      "Train: Epoch [22], Batch [720/938], Loss: 0.5044252872467041\n",
      "Train: Epoch [22], Batch [721/938], Loss: 0.3264053463935852\n",
      "Train: Epoch [22], Batch [722/938], Loss: 0.4627169370651245\n",
      "Train: Epoch [22], Batch [723/938], Loss: 0.39072105288505554\n",
      "Train: Epoch [22], Batch [724/938], Loss: 0.3561869263648987\n",
      "Train: Epoch [22], Batch [725/938], Loss: 0.3740750253200531\n",
      "Train: Epoch [22], Batch [726/938], Loss: 0.6329162120819092\n",
      "Train: Epoch [22], Batch [727/938], Loss: 0.3497963845729828\n",
      "Train: Epoch [22], Batch [728/938], Loss: 0.557431697845459\n",
      "Train: Epoch [22], Batch [729/938], Loss: 0.5499109029769897\n",
      "Train: Epoch [22], Batch [730/938], Loss: 0.41401728987693787\n",
      "Train: Epoch [22], Batch [731/938], Loss: 0.46676892042160034\n",
      "Train: Epoch [22], Batch [732/938], Loss: 0.30004507303237915\n",
      "Train: Epoch [22], Batch [733/938], Loss: 0.33569180965423584\n",
      "Train: Epoch [22], Batch [734/938], Loss: 0.5696333050727844\n",
      "Train: Epoch [22], Batch [735/938], Loss: 0.4521472752094269\n",
      "Train: Epoch [22], Batch [736/938], Loss: 0.4222087860107422\n",
      "Train: Epoch [22], Batch [737/938], Loss: 0.5378291010856628\n",
      "Train: Epoch [22], Batch [738/938], Loss: 0.48678430914878845\n",
      "Train: Epoch [22], Batch [739/938], Loss: 0.39509421586990356\n",
      "Train: Epoch [22], Batch [740/938], Loss: 0.42606833577156067\n",
      "Train: Epoch [22], Batch [741/938], Loss: 0.4250902831554413\n",
      "Train: Epoch [22], Batch [742/938], Loss: 0.35127222537994385\n",
      "Train: Epoch [22], Batch [743/938], Loss: 0.2530374825000763\n",
      "Train: Epoch [22], Batch [744/938], Loss: 0.5878525376319885\n",
      "Train: Epoch [22], Batch [745/938], Loss: 0.5119550228118896\n",
      "Train: Epoch [22], Batch [746/938], Loss: 0.3875255584716797\n",
      "Train: Epoch [22], Batch [747/938], Loss: 0.526993453502655\n",
      "Train: Epoch [22], Batch [748/938], Loss: 0.3824743330478668\n",
      "Train: Epoch [22], Batch [749/938], Loss: 0.5628131628036499\n",
      "Train: Epoch [22], Batch [750/938], Loss: 0.5161824822425842\n",
      "Train: Epoch [22], Batch [751/938], Loss: 0.5374744534492493\n",
      "Train: Epoch [22], Batch [752/938], Loss: 0.41725921630859375\n",
      "Train: Epoch [22], Batch [753/938], Loss: 0.4310559928417206\n",
      "Train: Epoch [22], Batch [754/938], Loss: 0.5845845937728882\n",
      "Train: Epoch [22], Batch [755/938], Loss: 0.508989155292511\n",
      "Train: Epoch [22], Batch [756/938], Loss: 0.7385293245315552\n",
      "Train: Epoch [22], Batch [757/938], Loss: 0.35615605115890503\n",
      "Train: Epoch [22], Batch [758/938], Loss: 0.4699797034263611\n",
      "Train: Epoch [22], Batch [759/938], Loss: 0.6238650679588318\n",
      "Train: Epoch [22], Batch [760/938], Loss: 0.5280085802078247\n",
      "Train: Epoch [22], Batch [761/938], Loss: 0.5488606095314026\n",
      "Train: Epoch [22], Batch [762/938], Loss: 0.3486362397670746\n",
      "Train: Epoch [22], Batch [763/938], Loss: 0.47981882095336914\n",
      "Train: Epoch [22], Batch [764/938], Loss: 0.3647094964981079\n",
      "Train: Epoch [22], Batch [765/938], Loss: 0.361575722694397\n",
      "Train: Epoch [22], Batch [766/938], Loss: 0.4555068016052246\n",
      "Train: Epoch [22], Batch [767/938], Loss: 0.49309366941452026\n",
      "Train: Epoch [22], Batch [768/938], Loss: 0.3764473497867584\n",
      "Train: Epoch [22], Batch [769/938], Loss: 0.3539709150791168\n",
      "Train: Epoch [22], Batch [770/938], Loss: 0.5085610151290894\n",
      "Train: Epoch [22], Batch [771/938], Loss: 0.5121917128562927\n",
      "Train: Epoch [22], Batch [772/938], Loss: 0.3328688144683838\n",
      "Train: Epoch [22], Batch [773/938], Loss: 0.7053448557853699\n",
      "Train: Epoch [22], Batch [774/938], Loss: 0.2870049476623535\n",
      "Train: Epoch [22], Batch [775/938], Loss: 0.321154922246933\n",
      "Train: Epoch [22], Batch [776/938], Loss: 0.2923072278499603\n",
      "Train: Epoch [22], Batch [777/938], Loss: 0.5439746379852295\n",
      "Train: Epoch [22], Batch [778/938], Loss: 0.4496588408946991\n",
      "Train: Epoch [22], Batch [779/938], Loss: 0.377469003200531\n",
      "Train: Epoch [22], Batch [780/938], Loss: 0.4975723624229431\n",
      "Train: Epoch [22], Batch [781/938], Loss: 0.4778447449207306\n",
      "Train: Epoch [22], Batch [782/938], Loss: 0.29580873250961304\n",
      "Train: Epoch [22], Batch [783/938], Loss: 0.3303529620170593\n",
      "Train: Epoch [22], Batch [784/938], Loss: 0.5182134509086609\n",
      "Train: Epoch [22], Batch [785/938], Loss: 0.433646023273468\n",
      "Train: Epoch [22], Batch [786/938], Loss: 0.5940088033676147\n",
      "Train: Epoch [22], Batch [787/938], Loss: 0.5049516558647156\n",
      "Train: Epoch [22], Batch [788/938], Loss: 0.349473237991333\n",
      "Train: Epoch [22], Batch [789/938], Loss: 0.5239020586013794\n",
      "Train: Epoch [22], Batch [790/938], Loss: 0.3152768909931183\n",
      "Train: Epoch [22], Batch [791/938], Loss: 0.5256244540214539\n",
      "Train: Epoch [22], Batch [792/938], Loss: 0.634267270565033\n",
      "Train: Epoch [22], Batch [793/938], Loss: 0.4287920594215393\n",
      "Train: Epoch [22], Batch [794/938], Loss: 0.36064067482948303\n",
      "Train: Epoch [22], Batch [795/938], Loss: 0.5697106719017029\n",
      "Train: Epoch [22], Batch [796/938], Loss: 0.44673410058021545\n",
      "Train: Epoch [22], Batch [797/938], Loss: 0.6744021773338318\n",
      "Train: Epoch [22], Batch [798/938], Loss: 0.8209282159805298\n",
      "Train: Epoch [22], Batch [799/938], Loss: 0.5342938303947449\n",
      "Train: Epoch [22], Batch [800/938], Loss: 0.5930725336074829\n",
      "Train: Epoch [22], Batch [801/938], Loss: 0.45689916610717773\n",
      "Train: Epoch [22], Batch [802/938], Loss: 0.5037626028060913\n",
      "Train: Epoch [22], Batch [803/938], Loss: 0.4040941596031189\n",
      "Train: Epoch [22], Batch [804/938], Loss: 0.4038023352622986\n",
      "Train: Epoch [22], Batch [805/938], Loss: 0.4698902666568756\n",
      "Train: Epoch [22], Batch [806/938], Loss: 0.535943329334259\n",
      "Train: Epoch [22], Batch [807/938], Loss: 0.29034629464149475\n",
      "Train: Epoch [22], Batch [808/938], Loss: 0.6271670460700989\n",
      "Train: Epoch [22], Batch [809/938], Loss: 0.365522176027298\n",
      "Train: Epoch [22], Batch [810/938], Loss: 0.43583688139915466\n",
      "Train: Epoch [22], Batch [811/938], Loss: 0.6375718116760254\n",
      "Train: Epoch [22], Batch [812/938], Loss: 0.3786367177963257\n",
      "Train: Epoch [22], Batch [813/938], Loss: 0.37956568598747253\n",
      "Train: Epoch [22], Batch [814/938], Loss: 0.46302008628845215\n",
      "Train: Epoch [22], Batch [815/938], Loss: 0.4645107388496399\n",
      "Train: Epoch [22], Batch [816/938], Loss: 0.3555627763271332\n",
      "Train: Epoch [22], Batch [817/938], Loss: 0.43714791536331177\n",
      "Train: Epoch [22], Batch [818/938], Loss: 0.5562217235565186\n",
      "Train: Epoch [22], Batch [819/938], Loss: 0.166810542345047\n",
      "Train: Epoch [22], Batch [820/938], Loss: 0.44088292121887207\n",
      "Train: Epoch [22], Batch [821/938], Loss: 0.42198318243026733\n",
      "Train: Epoch [22], Batch [822/938], Loss: 0.45761603116989136\n",
      "Train: Epoch [22], Batch [823/938], Loss: 0.4285656213760376\n",
      "Train: Epoch [22], Batch [824/938], Loss: 0.44112157821655273\n",
      "Train: Epoch [22], Batch [825/938], Loss: 0.4732142984867096\n",
      "Train: Epoch [22], Batch [826/938], Loss: 0.5967830419540405\n",
      "Train: Epoch [22], Batch [827/938], Loss: 0.41058582067489624\n",
      "Train: Epoch [22], Batch [828/938], Loss: 0.4966849982738495\n",
      "Train: Epoch [22], Batch [829/938], Loss: 0.4061340093612671\n",
      "Train: Epoch [22], Batch [830/938], Loss: 0.3306180536746979\n",
      "Train: Epoch [22], Batch [831/938], Loss: 0.7413426041603088\n",
      "Train: Epoch [22], Batch [832/938], Loss: 0.4224396049976349\n",
      "Train: Epoch [22], Batch [833/938], Loss: 0.47052812576293945\n",
      "Train: Epoch [22], Batch [834/938], Loss: 0.5273844599723816\n",
      "Train: Epoch [22], Batch [835/938], Loss: 0.5234333276748657\n",
      "Train: Epoch [22], Batch [836/938], Loss: 0.5197170376777649\n",
      "Train: Epoch [22], Batch [837/938], Loss: 0.49284827709198\n",
      "Train: Epoch [22], Batch [838/938], Loss: 0.515120267868042\n",
      "Train: Epoch [22], Batch [839/938], Loss: 0.4524020552635193\n",
      "Train: Epoch [22], Batch [840/938], Loss: 0.5274113416671753\n",
      "Train: Epoch [22], Batch [841/938], Loss: 0.5270490050315857\n",
      "Train: Epoch [22], Batch [842/938], Loss: 0.44857093691825867\n",
      "Train: Epoch [22], Batch [843/938], Loss: 0.5667003989219666\n",
      "Train: Epoch [22], Batch [844/938], Loss: 0.4637525677680969\n",
      "Train: Epoch [22], Batch [845/938], Loss: 0.4028584361076355\n",
      "Train: Epoch [22], Batch [846/938], Loss: 0.29855117201805115\n",
      "Train: Epoch [22], Batch [847/938], Loss: 0.2850319445133209\n",
      "Train: Epoch [22], Batch [848/938], Loss: 0.5608764290809631\n",
      "Train: Epoch [22], Batch [849/938], Loss: 0.36485809087753296\n",
      "Train: Epoch [22], Batch [850/938], Loss: 0.3601953983306885\n",
      "Train: Epoch [22], Batch [851/938], Loss: 0.4031783640384674\n",
      "Train: Epoch [22], Batch [852/938], Loss: 0.29129964113235474\n",
      "Train: Epoch [22], Batch [853/938], Loss: 0.5009523034095764\n",
      "Train: Epoch [22], Batch [854/938], Loss: 0.4788944721221924\n",
      "Train: Epoch [22], Batch [855/938], Loss: 0.26083603501319885\n",
      "Train: Epoch [22], Batch [856/938], Loss: 0.5213204026222229\n",
      "Train: Epoch [22], Batch [857/938], Loss: 0.6334021091461182\n",
      "Train: Epoch [22], Batch [858/938], Loss: 0.4694826602935791\n",
      "Train: Epoch [22], Batch [859/938], Loss: 0.37224626541137695\n",
      "Train: Epoch [22], Batch [860/938], Loss: 0.5112170577049255\n",
      "Train: Epoch [22], Batch [861/938], Loss: 0.3875044286251068\n",
      "Train: Epoch [22], Batch [862/938], Loss: 0.41957682371139526\n",
      "Train: Epoch [22], Batch [863/938], Loss: 0.5458902716636658\n",
      "Train: Epoch [22], Batch [864/938], Loss: 0.45416712760925293\n",
      "Train: Epoch [22], Batch [865/938], Loss: 0.4275608956813812\n",
      "Train: Epoch [22], Batch [866/938], Loss: 0.3886494040489197\n",
      "Train: Epoch [22], Batch [867/938], Loss: 0.41034093499183655\n",
      "Train: Epoch [22], Batch [868/938], Loss: 0.5369165539741516\n",
      "Train: Epoch [22], Batch [869/938], Loss: 0.35807496309280396\n",
      "Train: Epoch [22], Batch [870/938], Loss: 0.37950950860977173\n",
      "Train: Epoch [22], Batch [871/938], Loss: 0.41323941946029663\n",
      "Train: Epoch [22], Batch [872/938], Loss: 0.26785051822662354\n",
      "Train: Epoch [22], Batch [873/938], Loss: 0.39081719517707825\n",
      "Train: Epoch [22], Batch [874/938], Loss: 0.5288048386573792\n",
      "Train: Epoch [22], Batch [875/938], Loss: 0.44449612498283386\n",
      "Train: Epoch [22], Batch [876/938], Loss: 0.640419602394104\n",
      "Train: Epoch [22], Batch [877/938], Loss: 0.33688363432884216\n",
      "Train: Epoch [22], Batch [878/938], Loss: 0.2368013709783554\n",
      "Train: Epoch [22], Batch [879/938], Loss: 0.590386152267456\n",
      "Train: Epoch [22], Batch [880/938], Loss: 0.3909769356250763\n",
      "Train: Epoch [22], Batch [881/938], Loss: 0.48813825845718384\n",
      "Train: Epoch [22], Batch [882/938], Loss: 0.3932984471321106\n",
      "Train: Epoch [22], Batch [883/938], Loss: 0.7726974487304688\n",
      "Train: Epoch [22], Batch [884/938], Loss: 0.536530077457428\n",
      "Train: Epoch [22], Batch [885/938], Loss: 0.3947211503982544\n",
      "Train: Epoch [22], Batch [886/938], Loss: 0.45169323682785034\n",
      "Train: Epoch [22], Batch [887/938], Loss: 0.5280622839927673\n",
      "Train: Epoch [22], Batch [888/938], Loss: 0.5776097774505615\n",
      "Train: Epoch [22], Batch [889/938], Loss: 0.5954661965370178\n",
      "Train: Epoch [22], Batch [890/938], Loss: 0.3502882122993469\n",
      "Train: Epoch [22], Batch [891/938], Loss: 0.4270082116127014\n",
      "Train: Epoch [22], Batch [892/938], Loss: 0.40316200256347656\n",
      "Train: Epoch [22], Batch [893/938], Loss: 0.47572046518325806\n",
      "Train: Epoch [22], Batch [894/938], Loss: 0.345171183347702\n",
      "Train: Epoch [22], Batch [895/938], Loss: 0.41482672095298767\n",
      "Train: Epoch [22], Batch [896/938], Loss: 0.3730083107948303\n",
      "Train: Epoch [22], Batch [897/938], Loss: 0.38907524943351746\n",
      "Train: Epoch [22], Batch [898/938], Loss: 0.5274982452392578\n",
      "Train: Epoch [22], Batch [899/938], Loss: 0.5468474626541138\n",
      "Train: Epoch [22], Batch [900/938], Loss: 0.26352575421333313\n",
      "Train: Epoch [22], Batch [901/938], Loss: 0.40149587392807007\n",
      "Train: Epoch [22], Batch [902/938], Loss: 0.3781028091907501\n",
      "Train: Epoch [22], Batch [903/938], Loss: 0.5900642275810242\n",
      "Train: Epoch [22], Batch [904/938], Loss: 0.4773200452327728\n",
      "Train: Epoch [22], Batch [905/938], Loss: 0.38854336738586426\n",
      "Train: Epoch [22], Batch [906/938], Loss: 0.29960936307907104\n",
      "Train: Epoch [22], Batch [907/938], Loss: 0.325814813375473\n",
      "Train: Epoch [22], Batch [908/938], Loss: 0.3391001522541046\n",
      "Train: Epoch [22], Batch [909/938], Loss: 0.42547088861465454\n",
      "Train: Epoch [22], Batch [910/938], Loss: 0.21420380473136902\n",
      "Train: Epoch [22], Batch [911/938], Loss: 0.5182274580001831\n",
      "Train: Epoch [22], Batch [912/938], Loss: 0.39937064051628113\n",
      "Train: Epoch [22], Batch [913/938], Loss: 0.6007332801818848\n",
      "Train: Epoch [22], Batch [914/938], Loss: 0.5502033829689026\n",
      "Train: Epoch [22], Batch [915/938], Loss: 0.23308221995830536\n",
      "Train: Epoch [22], Batch [916/938], Loss: 0.4225049316883087\n",
      "Train: Epoch [22], Batch [917/938], Loss: 0.5408080816268921\n",
      "Train: Epoch [22], Batch [918/938], Loss: 0.18783769011497498\n",
      "Train: Epoch [22], Batch [919/938], Loss: 0.538697361946106\n",
      "Train: Epoch [22], Batch [920/938], Loss: 0.489700049161911\n",
      "Train: Epoch [22], Batch [921/938], Loss: 0.4200068712234497\n",
      "Train: Epoch [22], Batch [922/938], Loss: 0.3991408050060272\n",
      "Train: Epoch [22], Batch [923/938], Loss: 0.434944748878479\n",
      "Train: Epoch [22], Batch [924/938], Loss: 0.4189828634262085\n",
      "Train: Epoch [22], Batch [925/938], Loss: 0.4481806457042694\n",
      "Train: Epoch [22], Batch [926/938], Loss: 0.6791762113571167\n",
      "Train: Epoch [22], Batch [927/938], Loss: 0.3380657136440277\n",
      "Train: Epoch [22], Batch [928/938], Loss: 0.412509948015213\n",
      "Train: Epoch [22], Batch [929/938], Loss: 0.47481659054756165\n",
      "Train: Epoch [22], Batch [930/938], Loss: 0.41414499282836914\n",
      "Train: Epoch [22], Batch [931/938], Loss: 0.31721529364585876\n",
      "Train: Epoch [22], Batch [932/938], Loss: 0.4354158937931061\n",
      "Train: Epoch [22], Batch [933/938], Loss: 0.288699746131897\n",
      "Train: Epoch [22], Batch [934/938], Loss: 0.36354219913482666\n",
      "Train: Epoch [22], Batch [935/938], Loss: 0.24923881888389587\n",
      "Train: Epoch [22], Batch [936/938], Loss: 0.2961994707584381\n",
      "Train: Epoch [22], Batch [937/938], Loss: 0.3546544015407562\n",
      "Train: Epoch [22], Batch [938/938], Loss: 0.6119564771652222\n",
      "Accuracy of train set: 0.8461333333333333\n",
      "Validation: Epoch [22], Batch [1/938], Loss: 0.3587987720966339\n",
      "Validation: Epoch [22], Batch [2/938], Loss: 0.5141386985778809\n",
      "Validation: Epoch [22], Batch [3/938], Loss: 0.5391750931739807\n",
      "Validation: Epoch [22], Batch [4/938], Loss: 0.43977606296539307\n",
      "Validation: Epoch [22], Batch [5/938], Loss: 0.5154232978820801\n",
      "Validation: Epoch [22], Batch [6/938], Loss: 0.26364564895629883\n",
      "Validation: Epoch [22], Batch [7/938], Loss: 0.506496012210846\n",
      "Validation: Epoch [22], Batch [8/938], Loss: 0.4459373950958252\n",
      "Validation: Epoch [22], Batch [9/938], Loss: 0.2636966109275818\n",
      "Validation: Epoch [22], Batch [10/938], Loss: 0.5761340856552124\n",
      "Validation: Epoch [22], Batch [11/938], Loss: 0.5427783727645874\n",
      "Validation: Epoch [22], Batch [12/938], Loss: 0.48687905073165894\n",
      "Validation: Epoch [22], Batch [13/938], Loss: 0.4495319724082947\n",
      "Validation: Epoch [22], Batch [14/938], Loss: 0.5071871876716614\n",
      "Validation: Epoch [22], Batch [15/938], Loss: 0.6053195595741272\n",
      "Validation: Epoch [22], Batch [16/938], Loss: 0.3881310820579529\n",
      "Validation: Epoch [22], Batch [17/938], Loss: 0.30913737416267395\n",
      "Validation: Epoch [22], Batch [18/938], Loss: 0.29176101088523865\n",
      "Validation: Epoch [22], Batch [19/938], Loss: 0.5832621455192566\n",
      "Validation: Epoch [22], Batch [20/938], Loss: 0.4468406140804291\n",
      "Validation: Epoch [22], Batch [21/938], Loss: 0.5975680351257324\n",
      "Validation: Epoch [22], Batch [22/938], Loss: 0.5335158109664917\n",
      "Validation: Epoch [22], Batch [23/938], Loss: 0.37691280245780945\n",
      "Validation: Epoch [22], Batch [24/938], Loss: 0.3633536994457245\n",
      "Validation: Epoch [22], Batch [25/938], Loss: 0.3426910638809204\n",
      "Validation: Epoch [22], Batch [26/938], Loss: 0.4974760413169861\n",
      "Validation: Epoch [22], Batch [27/938], Loss: 0.5122097730636597\n",
      "Validation: Epoch [22], Batch [28/938], Loss: 0.4243725538253784\n",
      "Validation: Epoch [22], Batch [29/938], Loss: 0.4013170897960663\n",
      "Validation: Epoch [22], Batch [30/938], Loss: 0.38017410039901733\n",
      "Validation: Epoch [22], Batch [31/938], Loss: 0.29634273052215576\n",
      "Validation: Epoch [22], Batch [32/938], Loss: 0.6125856637954712\n",
      "Validation: Epoch [22], Batch [33/938], Loss: 0.5108888745307922\n",
      "Validation: Epoch [22], Batch [34/938], Loss: 0.41068899631500244\n",
      "Validation: Epoch [22], Batch [35/938], Loss: 0.5218392610549927\n",
      "Validation: Epoch [22], Batch [36/938], Loss: 0.3642854690551758\n",
      "Validation: Epoch [22], Batch [37/938], Loss: 0.4826706647872925\n",
      "Validation: Epoch [22], Batch [38/938], Loss: 0.45749419927597046\n",
      "Validation: Epoch [22], Batch [39/938], Loss: 0.29858121275901794\n",
      "Validation: Epoch [22], Batch [40/938], Loss: 0.40804558992385864\n",
      "Validation: Epoch [22], Batch [41/938], Loss: 0.3756886422634125\n",
      "Validation: Epoch [22], Batch [42/938], Loss: 0.2779533267021179\n",
      "Validation: Epoch [22], Batch [43/938], Loss: 0.5040370225906372\n",
      "Validation: Epoch [22], Batch [44/938], Loss: 0.4011310338973999\n",
      "Validation: Epoch [22], Batch [45/938], Loss: 0.36430662870407104\n",
      "Validation: Epoch [22], Batch [46/938], Loss: 0.49407002329826355\n",
      "Validation: Epoch [22], Batch [47/938], Loss: 0.3730884790420532\n",
      "Validation: Epoch [22], Batch [48/938], Loss: 0.496442586183548\n",
      "Validation: Epoch [22], Batch [49/938], Loss: 0.24492742121219635\n",
      "Validation: Epoch [22], Batch [50/938], Loss: 0.46040546894073486\n",
      "Validation: Epoch [22], Batch [51/938], Loss: 0.30497848987579346\n",
      "Validation: Epoch [22], Batch [52/938], Loss: 0.6284449100494385\n",
      "Validation: Epoch [22], Batch [53/938], Loss: 0.49515917897224426\n",
      "Validation: Epoch [22], Batch [54/938], Loss: 0.38283395767211914\n",
      "Validation: Epoch [22], Batch [55/938], Loss: 0.5333797931671143\n",
      "Validation: Epoch [22], Batch [56/938], Loss: 0.49593880772590637\n",
      "Validation: Epoch [22], Batch [57/938], Loss: 0.31979691982269287\n",
      "Validation: Epoch [22], Batch [58/938], Loss: 0.4940689504146576\n",
      "Validation: Epoch [22], Batch [59/938], Loss: 0.4428252577781677\n",
      "Validation: Epoch [22], Batch [60/938], Loss: 0.36629176139831543\n",
      "Validation: Epoch [22], Batch [61/938], Loss: 0.45445162057876587\n",
      "Validation: Epoch [22], Batch [62/938], Loss: 0.34448033571243286\n",
      "Validation: Epoch [22], Batch [63/938], Loss: 0.429574579000473\n",
      "Validation: Epoch [22], Batch [64/938], Loss: 0.5231195092201233\n",
      "Validation: Epoch [22], Batch [65/938], Loss: 0.4935281276702881\n",
      "Validation: Epoch [22], Batch [66/938], Loss: 0.5805904865264893\n",
      "Validation: Epoch [22], Batch [67/938], Loss: 0.34903568029403687\n",
      "Validation: Epoch [22], Batch [68/938], Loss: 0.6479945778846741\n",
      "Validation: Epoch [22], Batch [69/938], Loss: 0.5032469034194946\n",
      "Validation: Epoch [22], Batch [70/938], Loss: 0.28410691022872925\n",
      "Validation: Epoch [22], Batch [71/938], Loss: 0.33623993396759033\n",
      "Validation: Epoch [22], Batch [72/938], Loss: 0.5876138806343079\n",
      "Validation: Epoch [22], Batch [73/938], Loss: 0.4617357850074768\n",
      "Validation: Epoch [22], Batch [74/938], Loss: 0.41445136070251465\n",
      "Validation: Epoch [22], Batch [75/938], Loss: 0.4321826994419098\n",
      "Validation: Epoch [22], Batch [76/938], Loss: 0.3649691641330719\n",
      "Validation: Epoch [22], Batch [77/938], Loss: 0.2058769166469574\n",
      "Validation: Epoch [22], Batch [78/938], Loss: 0.37877586483955383\n",
      "Validation: Epoch [22], Batch [79/938], Loss: 0.38794979453086853\n",
      "Validation: Epoch [22], Batch [80/938], Loss: 0.34780269861221313\n",
      "Validation: Epoch [22], Batch [81/938], Loss: 0.4798045754432678\n",
      "Validation: Epoch [22], Batch [82/938], Loss: 0.43354859948158264\n",
      "Validation: Epoch [22], Batch [83/938], Loss: 0.5214980840682983\n",
      "Validation: Epoch [22], Batch [84/938], Loss: 0.3043081760406494\n",
      "Validation: Epoch [22], Batch [85/938], Loss: 0.2633642852306366\n",
      "Validation: Epoch [22], Batch [86/938], Loss: 0.3626101016998291\n",
      "Validation: Epoch [22], Batch [87/938], Loss: 0.5093845129013062\n",
      "Validation: Epoch [22], Batch [88/938], Loss: 0.5187731385231018\n",
      "Validation: Epoch [22], Batch [89/938], Loss: 0.3317730724811554\n",
      "Validation: Epoch [22], Batch [90/938], Loss: 0.3394877016544342\n",
      "Validation: Epoch [22], Batch [91/938], Loss: 0.5732312202453613\n",
      "Validation: Epoch [22], Batch [92/938], Loss: 0.42200201749801636\n",
      "Validation: Epoch [22], Batch [93/938], Loss: 0.5083082914352417\n",
      "Validation: Epoch [22], Batch [94/938], Loss: 0.2577926516532898\n",
      "Validation: Epoch [22], Batch [95/938], Loss: 0.3486767113208771\n",
      "Validation: Epoch [22], Batch [96/938], Loss: 0.27236151695251465\n",
      "Validation: Epoch [22], Batch [97/938], Loss: 0.4335651099681854\n",
      "Validation: Epoch [22], Batch [98/938], Loss: 0.40985342860221863\n",
      "Validation: Epoch [22], Batch [99/938], Loss: 0.28633424639701843\n",
      "Validation: Epoch [22], Batch [100/938], Loss: 0.44980281591415405\n",
      "Validation: Epoch [22], Batch [101/938], Loss: 0.4587037265300751\n",
      "Validation: Epoch [22], Batch [102/938], Loss: 0.4284575879573822\n",
      "Validation: Epoch [22], Batch [103/938], Loss: 0.3874035179615021\n",
      "Validation: Epoch [22], Batch [104/938], Loss: 0.7474830150604248\n",
      "Validation: Epoch [22], Batch [105/938], Loss: 0.4056359827518463\n",
      "Validation: Epoch [22], Batch [106/938], Loss: 0.4565744400024414\n",
      "Validation: Epoch [22], Batch [107/938], Loss: 0.464724600315094\n",
      "Validation: Epoch [22], Batch [108/938], Loss: 0.3901142179965973\n",
      "Validation: Epoch [22], Batch [109/938], Loss: 0.4831271767616272\n",
      "Validation: Epoch [22], Batch [110/938], Loss: 0.4386928677558899\n",
      "Validation: Epoch [22], Batch [111/938], Loss: 0.44132643938064575\n",
      "Validation: Epoch [22], Batch [112/938], Loss: 0.5314565300941467\n",
      "Validation: Epoch [22], Batch [113/938], Loss: 0.6230851411819458\n",
      "Validation: Epoch [22], Batch [114/938], Loss: 0.2908557951450348\n",
      "Validation: Epoch [22], Batch [115/938], Loss: 0.49302732944488525\n",
      "Validation: Epoch [22], Batch [116/938], Loss: 0.34435468912124634\n",
      "Validation: Epoch [22], Batch [117/938], Loss: 0.37675443291664124\n",
      "Validation: Epoch [22], Batch [118/938], Loss: 0.34455159306526184\n",
      "Validation: Epoch [22], Batch [119/938], Loss: 0.42174801230430603\n",
      "Validation: Epoch [22], Batch [120/938], Loss: 0.4297325015068054\n",
      "Validation: Epoch [22], Batch [121/938], Loss: 0.35385891795158386\n",
      "Validation: Epoch [22], Batch [122/938], Loss: 0.3106558620929718\n",
      "Validation: Epoch [22], Batch [123/938], Loss: 0.3729095757007599\n",
      "Validation: Epoch [22], Batch [124/938], Loss: 0.4021351933479309\n",
      "Validation: Epoch [22], Batch [125/938], Loss: 0.36603111028671265\n",
      "Validation: Epoch [22], Batch [126/938], Loss: 0.50071120262146\n",
      "Validation: Epoch [22], Batch [127/938], Loss: 0.341827929019928\n",
      "Validation: Epoch [22], Batch [128/938], Loss: 0.3438301980495453\n",
      "Validation: Epoch [22], Batch [129/938], Loss: 0.547613799571991\n",
      "Validation: Epoch [22], Batch [130/938], Loss: 0.6273453235626221\n",
      "Validation: Epoch [22], Batch [131/938], Loss: 0.42032337188720703\n",
      "Validation: Epoch [22], Batch [132/938], Loss: 0.43548402190208435\n",
      "Validation: Epoch [22], Batch [133/938], Loss: 0.4125886559486389\n",
      "Validation: Epoch [22], Batch [134/938], Loss: 0.4721248149871826\n",
      "Validation: Epoch [22], Batch [135/938], Loss: 0.4729326367378235\n",
      "Validation: Epoch [22], Batch [136/938], Loss: 0.5180683135986328\n",
      "Validation: Epoch [22], Batch [137/938], Loss: 0.4653870165348053\n",
      "Validation: Epoch [22], Batch [138/938], Loss: 0.30106818675994873\n",
      "Validation: Epoch [22], Batch [139/938], Loss: 0.3660237193107605\n",
      "Validation: Epoch [22], Batch [140/938], Loss: 0.6473249197006226\n",
      "Validation: Epoch [22], Batch [141/938], Loss: 0.4626505374908447\n",
      "Validation: Epoch [22], Batch [142/938], Loss: 0.41859132051467896\n",
      "Validation: Epoch [22], Batch [143/938], Loss: 0.3878013789653778\n",
      "Validation: Epoch [22], Batch [144/938], Loss: 0.34409308433532715\n",
      "Validation: Epoch [22], Batch [145/938], Loss: 0.7230044603347778\n",
      "Validation: Epoch [22], Batch [146/938], Loss: 0.4152486324310303\n",
      "Validation: Epoch [22], Batch [147/938], Loss: 0.3943620026111603\n",
      "Validation: Epoch [22], Batch [148/938], Loss: 0.39098602533340454\n",
      "Validation: Epoch [22], Batch [149/938], Loss: 0.3463858962059021\n",
      "Validation: Epoch [22], Batch [150/938], Loss: 0.44489189982414246\n",
      "Validation: Epoch [22], Batch [151/938], Loss: 0.40567898750305176\n",
      "Validation: Epoch [22], Batch [152/938], Loss: 0.3188629448413849\n",
      "Validation: Epoch [22], Batch [153/938], Loss: 0.5438046455383301\n",
      "Validation: Epoch [22], Batch [154/938], Loss: 0.32840317487716675\n",
      "Validation: Epoch [22], Batch [155/938], Loss: 0.46192750334739685\n",
      "Validation: Epoch [22], Batch [156/938], Loss: 0.506051778793335\n",
      "Validation: Epoch [22], Batch [157/938], Loss: 0.3614128828048706\n",
      "Validation: Epoch [22], Batch [158/938], Loss: 0.3315131366252899\n",
      "Validation: Epoch [22], Batch [159/938], Loss: 0.4497091770172119\n",
      "Validation: Epoch [22], Batch [160/938], Loss: 0.34710797667503357\n",
      "Validation: Epoch [22], Batch [161/938], Loss: 0.3250891864299774\n",
      "Validation: Epoch [22], Batch [162/938], Loss: 0.40905946493148804\n",
      "Validation: Epoch [22], Batch [163/938], Loss: 0.46305370330810547\n",
      "Validation: Epoch [22], Batch [164/938], Loss: 0.6193304061889648\n",
      "Validation: Epoch [22], Batch [165/938], Loss: 0.5486173033714294\n",
      "Validation: Epoch [22], Batch [166/938], Loss: 0.47036418318748474\n",
      "Validation: Epoch [22], Batch [167/938], Loss: 0.4898669719696045\n",
      "Validation: Epoch [22], Batch [168/938], Loss: 0.35407862067222595\n",
      "Validation: Epoch [22], Batch [169/938], Loss: 0.623528242111206\n",
      "Validation: Epoch [22], Batch [170/938], Loss: 0.24773694574832916\n",
      "Validation: Epoch [22], Batch [171/938], Loss: 0.30705681443214417\n",
      "Validation: Epoch [22], Batch [172/938], Loss: 0.27646684646606445\n",
      "Validation: Epoch [22], Batch [173/938], Loss: 0.6676892042160034\n",
      "Validation: Epoch [22], Batch [174/938], Loss: 0.6031957268714905\n",
      "Validation: Epoch [22], Batch [175/938], Loss: 0.5144445896148682\n",
      "Validation: Epoch [22], Batch [176/938], Loss: 0.46457353234291077\n",
      "Validation: Epoch [22], Batch [177/938], Loss: 0.4138587415218353\n",
      "Validation: Epoch [22], Batch [178/938], Loss: 0.5250977873802185\n",
      "Validation: Epoch [22], Batch [179/938], Loss: 0.7357125878334045\n",
      "Validation: Epoch [22], Batch [180/938], Loss: 0.28394484519958496\n",
      "Validation: Epoch [22], Batch [181/938], Loss: 0.7086774706840515\n",
      "Validation: Epoch [22], Batch [182/938], Loss: 0.5096709728240967\n",
      "Validation: Epoch [22], Batch [183/938], Loss: 0.5616855025291443\n",
      "Validation: Epoch [22], Batch [184/938], Loss: 0.5817100405693054\n",
      "Validation: Epoch [22], Batch [185/938], Loss: 0.5483220815658569\n",
      "Validation: Epoch [22], Batch [186/938], Loss: 0.42817553877830505\n",
      "Validation: Epoch [22], Batch [187/938], Loss: 0.30594372749328613\n",
      "Validation: Epoch [22], Batch [188/938], Loss: 0.32026323676109314\n",
      "Validation: Epoch [22], Batch [189/938], Loss: 0.4780951142311096\n",
      "Validation: Epoch [22], Batch [190/938], Loss: 0.4520355463027954\n",
      "Validation: Epoch [22], Batch [191/938], Loss: 0.3293555676937103\n",
      "Validation: Epoch [22], Batch [192/938], Loss: 0.40734395384788513\n",
      "Validation: Epoch [22], Batch [193/938], Loss: 0.3984890580177307\n",
      "Validation: Epoch [22], Batch [194/938], Loss: 0.28763261437416077\n",
      "Validation: Epoch [22], Batch [195/938], Loss: 0.414441853761673\n",
      "Validation: Epoch [22], Batch [196/938], Loss: 0.2281474769115448\n",
      "Validation: Epoch [22], Batch [197/938], Loss: 0.4153052270412445\n",
      "Validation: Epoch [22], Batch [198/938], Loss: 0.3410365581512451\n",
      "Validation: Epoch [22], Batch [199/938], Loss: 0.39578232169151306\n",
      "Validation: Epoch [22], Batch [200/938], Loss: 0.4093981385231018\n",
      "Validation: Epoch [22], Batch [201/938], Loss: 0.4369073808193207\n",
      "Validation: Epoch [22], Batch [202/938], Loss: 0.33814918994903564\n",
      "Validation: Epoch [22], Batch [203/938], Loss: 0.3218472898006439\n",
      "Validation: Epoch [22], Batch [204/938], Loss: 0.49304062128067017\n",
      "Validation: Epoch [22], Batch [205/938], Loss: 0.516089677810669\n",
      "Validation: Epoch [22], Batch [206/938], Loss: 0.5247354507446289\n",
      "Validation: Epoch [22], Batch [207/938], Loss: 0.5042557120323181\n",
      "Validation: Epoch [22], Batch [208/938], Loss: 0.5722463130950928\n",
      "Validation: Epoch [22], Batch [209/938], Loss: 0.45392507314682007\n",
      "Validation: Epoch [22], Batch [210/938], Loss: 0.47316354513168335\n",
      "Validation: Epoch [22], Batch [211/938], Loss: 0.5756685733795166\n",
      "Validation: Epoch [22], Batch [212/938], Loss: 0.2903890907764435\n",
      "Validation: Epoch [22], Batch [213/938], Loss: 0.3140942454338074\n",
      "Validation: Epoch [22], Batch [214/938], Loss: 0.6291400790214539\n",
      "Validation: Epoch [22], Batch [215/938], Loss: 0.5590866804122925\n",
      "Validation: Epoch [22], Batch [216/938], Loss: 0.2765616774559021\n",
      "Validation: Epoch [22], Batch [217/938], Loss: 0.5001893639564514\n",
      "Validation: Epoch [22], Batch [218/938], Loss: 0.6092175245285034\n",
      "Validation: Epoch [22], Batch [219/938], Loss: 0.34302905201911926\n",
      "Validation: Epoch [22], Batch [220/938], Loss: 0.4832763969898224\n",
      "Validation: Epoch [22], Batch [221/938], Loss: 0.5886510610580444\n",
      "Validation: Epoch [22], Batch [222/938], Loss: 0.5012755393981934\n",
      "Validation: Epoch [22], Batch [223/938], Loss: 0.3430323004722595\n",
      "Validation: Epoch [22], Batch [224/938], Loss: 0.5996538996696472\n",
      "Validation: Epoch [22], Batch [225/938], Loss: 0.37737947702407837\n",
      "Validation: Epoch [22], Batch [226/938], Loss: 0.35226014256477356\n",
      "Validation: Epoch [22], Batch [227/938], Loss: 0.3568013906478882\n",
      "Validation: Epoch [22], Batch [228/938], Loss: 0.5135190486907959\n",
      "Validation: Epoch [22], Batch [229/938], Loss: 0.6186264753341675\n",
      "Validation: Epoch [22], Batch [230/938], Loss: 0.3170419931411743\n",
      "Validation: Epoch [22], Batch [231/938], Loss: 0.4911079704761505\n",
      "Validation: Epoch [22], Batch [232/938], Loss: 0.38909217715263367\n",
      "Validation: Epoch [22], Batch [233/938], Loss: 0.5220154523849487\n",
      "Validation: Epoch [22], Batch [234/938], Loss: 0.5643166899681091\n",
      "Validation: Epoch [22], Batch [235/938], Loss: 0.3348146378993988\n",
      "Validation: Epoch [22], Batch [236/938], Loss: 0.5185956954956055\n",
      "Validation: Epoch [22], Batch [237/938], Loss: 0.5122673511505127\n",
      "Validation: Epoch [22], Batch [238/938], Loss: 0.6537133455276489\n",
      "Validation: Epoch [22], Batch [239/938], Loss: 0.5828489661216736\n",
      "Validation: Epoch [22], Batch [240/938], Loss: 0.2910517454147339\n",
      "Validation: Epoch [22], Batch [241/938], Loss: 0.5239318609237671\n",
      "Validation: Epoch [22], Batch [242/938], Loss: 0.618247926235199\n",
      "Validation: Epoch [22], Batch [243/938], Loss: 0.42145323753356934\n",
      "Validation: Epoch [22], Batch [244/938], Loss: 0.5421838760375977\n",
      "Validation: Epoch [22], Batch [245/938], Loss: 0.41591712832450867\n",
      "Validation: Epoch [22], Batch [246/938], Loss: 0.4381454586982727\n",
      "Validation: Epoch [22], Batch [247/938], Loss: 0.5111371874809265\n",
      "Validation: Epoch [22], Batch [248/938], Loss: 0.43253782391548157\n",
      "Validation: Epoch [22], Batch [249/938], Loss: 0.5664346218109131\n",
      "Validation: Epoch [22], Batch [250/938], Loss: 0.4517810344696045\n",
      "Validation: Epoch [22], Batch [251/938], Loss: 0.45269477367401123\n",
      "Validation: Epoch [22], Batch [252/938], Loss: 0.33498090505599976\n",
      "Validation: Epoch [22], Batch [253/938], Loss: 0.5172871351242065\n",
      "Validation: Epoch [22], Batch [254/938], Loss: 0.5919116735458374\n",
      "Validation: Epoch [22], Batch [255/938], Loss: 0.4701380133628845\n",
      "Validation: Epoch [22], Batch [256/938], Loss: 0.42461031675338745\n",
      "Validation: Epoch [22], Batch [257/938], Loss: 0.5581037998199463\n",
      "Validation: Epoch [22], Batch [258/938], Loss: 0.3950386643409729\n",
      "Validation: Epoch [22], Batch [259/938], Loss: 0.44867846369743347\n",
      "Validation: Epoch [22], Batch [260/938], Loss: 0.34219521284103394\n",
      "Validation: Epoch [22], Batch [261/938], Loss: 0.26682648062705994\n",
      "Validation: Epoch [22], Batch [262/938], Loss: 0.5120922327041626\n",
      "Validation: Epoch [22], Batch [263/938], Loss: 0.4770621061325073\n",
      "Validation: Epoch [22], Batch [264/938], Loss: 0.47252392768859863\n",
      "Validation: Epoch [22], Batch [265/938], Loss: 0.3126521706581116\n",
      "Validation: Epoch [22], Batch [266/938], Loss: 0.3007287085056305\n",
      "Validation: Epoch [22], Batch [267/938], Loss: 0.4908779561519623\n",
      "Validation: Epoch [22], Batch [268/938], Loss: 0.5375173687934875\n",
      "Validation: Epoch [22], Batch [269/938], Loss: 0.5027951002120972\n",
      "Validation: Epoch [22], Batch [270/938], Loss: 0.4606047570705414\n",
      "Validation: Epoch [22], Batch [271/938], Loss: 0.552144467830658\n",
      "Validation: Epoch [22], Batch [272/938], Loss: 0.44857174158096313\n",
      "Validation: Epoch [22], Batch [273/938], Loss: 0.4670727849006653\n",
      "Validation: Epoch [22], Batch [274/938], Loss: 0.46655210852622986\n",
      "Validation: Epoch [22], Batch [275/938], Loss: 0.3620518743991852\n",
      "Validation: Epoch [22], Batch [276/938], Loss: 0.563223123550415\n",
      "Validation: Epoch [22], Batch [277/938], Loss: 0.2953707277774811\n",
      "Validation: Epoch [22], Batch [278/938], Loss: 0.4559604525566101\n",
      "Validation: Epoch [22], Batch [279/938], Loss: 0.4006223678588867\n",
      "Validation: Epoch [22], Batch [280/938], Loss: 0.4381239116191864\n",
      "Validation: Epoch [22], Batch [281/938], Loss: 0.5987374782562256\n",
      "Validation: Epoch [22], Batch [282/938], Loss: 0.312786728143692\n",
      "Validation: Epoch [22], Batch [283/938], Loss: 0.5302785634994507\n",
      "Validation: Epoch [22], Batch [284/938], Loss: 0.5373411774635315\n",
      "Validation: Epoch [22], Batch [285/938], Loss: 0.48451873660087585\n",
      "Validation: Epoch [22], Batch [286/938], Loss: 0.4632161855697632\n",
      "Validation: Epoch [22], Batch [287/938], Loss: 0.6832965016365051\n",
      "Validation: Epoch [22], Batch [288/938], Loss: 0.34816673398017883\n",
      "Validation: Epoch [22], Batch [289/938], Loss: 0.5904157161712646\n",
      "Validation: Epoch [22], Batch [290/938], Loss: 0.677701473236084\n",
      "Validation: Epoch [22], Batch [291/938], Loss: 0.5059053897857666\n",
      "Validation: Epoch [22], Batch [292/938], Loss: 0.3706369698047638\n",
      "Validation: Epoch [22], Batch [293/938], Loss: 0.5118234753608704\n",
      "Validation: Epoch [22], Batch [294/938], Loss: 0.444357305765152\n",
      "Validation: Epoch [22], Batch [295/938], Loss: 0.45056095719337463\n",
      "Validation: Epoch [22], Batch [296/938], Loss: 0.5856955051422119\n",
      "Validation: Epoch [22], Batch [297/938], Loss: 0.4020812213420868\n",
      "Validation: Epoch [22], Batch [298/938], Loss: 0.40145593881607056\n",
      "Validation: Epoch [22], Batch [299/938], Loss: 0.4905741214752197\n",
      "Validation: Epoch [22], Batch [300/938], Loss: 0.6269228458404541\n",
      "Validation: Epoch [22], Batch [301/938], Loss: 0.42620500922203064\n",
      "Validation: Epoch [22], Batch [302/938], Loss: 0.8847872614860535\n",
      "Validation: Epoch [22], Batch [303/938], Loss: 0.4231027662754059\n",
      "Validation: Epoch [22], Batch [304/938], Loss: 0.5262826681137085\n",
      "Validation: Epoch [22], Batch [305/938], Loss: 0.4239722490310669\n",
      "Validation: Epoch [22], Batch [306/938], Loss: 0.5571458339691162\n",
      "Validation: Epoch [22], Batch [307/938], Loss: 0.43727388978004456\n",
      "Validation: Epoch [22], Batch [308/938], Loss: 0.36180710792541504\n",
      "Validation: Epoch [22], Batch [309/938], Loss: 0.5102249383926392\n",
      "Validation: Epoch [22], Batch [310/938], Loss: 0.42519888281822205\n",
      "Validation: Epoch [22], Batch [311/938], Loss: 0.3968539535999298\n",
      "Validation: Epoch [22], Batch [312/938], Loss: 0.34409284591674805\n",
      "Validation: Epoch [22], Batch [313/938], Loss: 0.2144552320241928\n",
      "Validation: Epoch [22], Batch [314/938], Loss: 0.31946682929992676\n",
      "Validation: Epoch [22], Batch [315/938], Loss: 0.35575729608535767\n",
      "Validation: Epoch [22], Batch [316/938], Loss: 0.4589274525642395\n",
      "Validation: Epoch [22], Batch [317/938], Loss: 0.17154014110565186\n",
      "Validation: Epoch [22], Batch [318/938], Loss: 0.3008052706718445\n",
      "Validation: Epoch [22], Batch [319/938], Loss: 0.37117183208465576\n",
      "Validation: Epoch [22], Batch [320/938], Loss: 0.40180811285972595\n",
      "Validation: Epoch [22], Batch [321/938], Loss: 0.24849741160869598\n",
      "Validation: Epoch [22], Batch [322/938], Loss: 0.3349766433238983\n",
      "Validation: Epoch [22], Batch [323/938], Loss: 0.5379574298858643\n",
      "Validation: Epoch [22], Batch [324/938], Loss: 0.378053218126297\n",
      "Validation: Epoch [22], Batch [325/938], Loss: 0.3560173213481903\n",
      "Validation: Epoch [22], Batch [326/938], Loss: 0.5060084462165833\n",
      "Validation: Epoch [22], Batch [327/938], Loss: 0.6736976504325867\n",
      "Validation: Epoch [22], Batch [328/938], Loss: 0.509891152381897\n",
      "Validation: Epoch [22], Batch [329/938], Loss: 0.7241045236587524\n",
      "Validation: Epoch [22], Batch [330/938], Loss: 0.5282267928123474\n",
      "Validation: Epoch [22], Batch [331/938], Loss: 0.5334306955337524\n",
      "Validation: Epoch [22], Batch [332/938], Loss: 0.541247546672821\n",
      "Validation: Epoch [22], Batch [333/938], Loss: 0.40373340249061584\n",
      "Validation: Epoch [22], Batch [334/938], Loss: 0.44817131757736206\n",
      "Validation: Epoch [22], Batch [335/938], Loss: 0.5340740084648132\n",
      "Validation: Epoch [22], Batch [336/938], Loss: 0.3706793189048767\n",
      "Validation: Epoch [22], Batch [337/938], Loss: 0.4694175124168396\n",
      "Validation: Epoch [22], Batch [338/938], Loss: 0.47969335317611694\n",
      "Validation: Epoch [22], Batch [339/938], Loss: 0.3421333432197571\n",
      "Validation: Epoch [22], Batch [340/938], Loss: 0.4553000330924988\n",
      "Validation: Epoch [22], Batch [341/938], Loss: 0.40879592299461365\n",
      "Validation: Epoch [22], Batch [342/938], Loss: 0.5462164282798767\n",
      "Validation: Epoch [22], Batch [343/938], Loss: 0.46705561876296997\n",
      "Validation: Epoch [22], Batch [344/938], Loss: 0.48845425248146057\n",
      "Validation: Epoch [22], Batch [345/938], Loss: 0.33352917432785034\n",
      "Validation: Epoch [22], Batch [346/938], Loss: 0.5048892498016357\n",
      "Validation: Epoch [22], Batch [347/938], Loss: 0.47724851965904236\n",
      "Validation: Epoch [22], Batch [348/938], Loss: 0.43221932649612427\n",
      "Validation: Epoch [22], Batch [349/938], Loss: 0.4418718218803406\n",
      "Validation: Epoch [22], Batch [350/938], Loss: 0.5236353874206543\n",
      "Validation: Epoch [22], Batch [351/938], Loss: 0.324217826128006\n",
      "Validation: Epoch [22], Batch [352/938], Loss: 0.3653751611709595\n",
      "Validation: Epoch [22], Batch [353/938], Loss: 0.4037057161331177\n",
      "Validation: Epoch [22], Batch [354/938], Loss: 0.654106616973877\n",
      "Validation: Epoch [22], Batch [355/938], Loss: 0.37544918060302734\n",
      "Validation: Epoch [22], Batch [356/938], Loss: 0.5650112628936768\n",
      "Validation: Epoch [22], Batch [357/938], Loss: 0.6026617884635925\n",
      "Validation: Epoch [22], Batch [358/938], Loss: 0.4367116689682007\n",
      "Validation: Epoch [22], Batch [359/938], Loss: 0.6569682359695435\n",
      "Validation: Epoch [22], Batch [360/938], Loss: 0.23118075728416443\n",
      "Validation: Epoch [22], Batch [361/938], Loss: 0.5437488555908203\n",
      "Validation: Epoch [22], Batch [362/938], Loss: 0.5800804495811462\n",
      "Validation: Epoch [22], Batch [363/938], Loss: 0.637098491191864\n",
      "Validation: Epoch [22], Batch [364/938], Loss: 0.3749127984046936\n",
      "Validation: Epoch [22], Batch [365/938], Loss: 0.34429284930229187\n",
      "Validation: Epoch [22], Batch [366/938], Loss: 0.4659111797809601\n",
      "Validation: Epoch [22], Batch [367/938], Loss: 0.6116952300071716\n",
      "Validation: Epoch [22], Batch [368/938], Loss: 0.5823944211006165\n",
      "Validation: Epoch [22], Batch [369/938], Loss: 0.46466541290283203\n",
      "Validation: Epoch [22], Batch [370/938], Loss: 0.4775456190109253\n",
      "Validation: Epoch [22], Batch [371/938], Loss: 0.5198926329612732\n",
      "Validation: Epoch [22], Batch [372/938], Loss: 0.2846949100494385\n",
      "Validation: Epoch [22], Batch [373/938], Loss: 0.6417540907859802\n",
      "Validation: Epoch [22], Batch [374/938], Loss: 0.34706681966781616\n",
      "Validation: Epoch [22], Batch [375/938], Loss: 0.3802863359451294\n",
      "Validation: Epoch [22], Batch [376/938], Loss: 0.5906370878219604\n",
      "Validation: Epoch [22], Batch [377/938], Loss: 0.8320273160934448\n",
      "Validation: Epoch [22], Batch [378/938], Loss: 0.33997175097465515\n",
      "Validation: Epoch [22], Batch [379/938], Loss: 0.4128835201263428\n",
      "Validation: Epoch [22], Batch [380/938], Loss: 0.5805587768554688\n",
      "Validation: Epoch [22], Batch [381/938], Loss: 0.5845616459846497\n",
      "Validation: Epoch [22], Batch [382/938], Loss: 0.4503154158592224\n",
      "Validation: Epoch [22], Batch [383/938], Loss: 0.402373731136322\n",
      "Validation: Epoch [22], Batch [384/938], Loss: 0.5590848922729492\n",
      "Validation: Epoch [22], Batch [385/938], Loss: 0.5877937078475952\n",
      "Validation: Epoch [22], Batch [386/938], Loss: 0.4241634011268616\n",
      "Validation: Epoch [22], Batch [387/938], Loss: 0.6575302481651306\n",
      "Validation: Epoch [22], Batch [388/938], Loss: 0.5034697651863098\n",
      "Validation: Epoch [22], Batch [389/938], Loss: 0.5794863104820251\n",
      "Validation: Epoch [22], Batch [390/938], Loss: 0.37514379620552063\n",
      "Validation: Epoch [22], Batch [391/938], Loss: 0.24818798899650574\n",
      "Validation: Epoch [22], Batch [392/938], Loss: 0.2913033664226532\n",
      "Validation: Epoch [22], Batch [393/938], Loss: 0.5818760991096497\n",
      "Validation: Epoch [22], Batch [394/938], Loss: 0.5302635431289673\n",
      "Validation: Epoch [22], Batch [395/938], Loss: 0.6281997561454773\n",
      "Validation: Epoch [22], Batch [396/938], Loss: 0.5026106834411621\n",
      "Validation: Epoch [22], Batch [397/938], Loss: 0.41761264204978943\n",
      "Validation: Epoch [22], Batch [398/938], Loss: 0.5612640976905823\n",
      "Validation: Epoch [22], Batch [399/938], Loss: 0.45826584100723267\n",
      "Validation: Epoch [22], Batch [400/938], Loss: 0.3493778109550476\n",
      "Validation: Epoch [22], Batch [401/938], Loss: 0.4606429934501648\n",
      "Validation: Epoch [22], Batch [402/938], Loss: 0.4305426776409149\n",
      "Validation: Epoch [22], Batch [403/938], Loss: 0.5407971739768982\n",
      "Validation: Epoch [22], Batch [404/938], Loss: 0.28889909386634827\n",
      "Validation: Epoch [22], Batch [405/938], Loss: 0.5224147439002991\n",
      "Validation: Epoch [22], Batch [406/938], Loss: 0.3002578616142273\n",
      "Validation: Epoch [22], Batch [407/938], Loss: 0.5238849520683289\n",
      "Validation: Epoch [22], Batch [408/938], Loss: 0.39809146523475647\n",
      "Validation: Epoch [22], Batch [409/938], Loss: 0.3695715367794037\n",
      "Validation: Epoch [22], Batch [410/938], Loss: 0.38118991255760193\n",
      "Validation: Epoch [22], Batch [411/938], Loss: 0.6037948131561279\n",
      "Validation: Epoch [22], Batch [412/938], Loss: 0.41318976879119873\n",
      "Validation: Epoch [22], Batch [413/938], Loss: 0.4284170866012573\n",
      "Validation: Epoch [22], Batch [414/938], Loss: 0.3996002972126007\n",
      "Validation: Epoch [22], Batch [415/938], Loss: 0.4716409146785736\n",
      "Validation: Epoch [22], Batch [416/938], Loss: 0.3653123378753662\n",
      "Validation: Epoch [22], Batch [417/938], Loss: 0.4897298514842987\n",
      "Validation: Epoch [22], Batch [418/938], Loss: 0.6293871402740479\n",
      "Validation: Epoch [22], Batch [419/938], Loss: 0.6578598618507385\n",
      "Validation: Epoch [22], Batch [420/938], Loss: 0.2548637390136719\n",
      "Validation: Epoch [22], Batch [421/938], Loss: 0.44038039445877075\n",
      "Validation: Epoch [22], Batch [422/938], Loss: 0.27379822731018066\n",
      "Validation: Epoch [22], Batch [423/938], Loss: 0.38066068291664124\n",
      "Validation: Epoch [22], Batch [424/938], Loss: 0.5270643830299377\n",
      "Validation: Epoch [22], Batch [425/938], Loss: 0.35642528533935547\n",
      "Validation: Epoch [22], Batch [426/938], Loss: 0.37548530101776123\n",
      "Validation: Epoch [22], Batch [427/938], Loss: 0.41729485988616943\n",
      "Validation: Epoch [22], Batch [428/938], Loss: 0.3133397102355957\n",
      "Validation: Epoch [22], Batch [429/938], Loss: 0.39569368958473206\n",
      "Validation: Epoch [22], Batch [430/938], Loss: 0.2849125564098358\n",
      "Validation: Epoch [22], Batch [431/938], Loss: 0.3382857143878937\n",
      "Validation: Epoch [22], Batch [432/938], Loss: 0.505961537361145\n",
      "Validation: Epoch [22], Batch [433/938], Loss: 0.37542468309402466\n",
      "Validation: Epoch [22], Batch [434/938], Loss: 0.5163111686706543\n",
      "Validation: Epoch [22], Batch [435/938], Loss: 0.6100644469261169\n",
      "Validation: Epoch [22], Batch [436/938], Loss: 0.4237515330314636\n",
      "Validation: Epoch [22], Batch [437/938], Loss: 0.5447194576263428\n",
      "Validation: Epoch [22], Batch [438/938], Loss: 0.6170427799224854\n",
      "Validation: Epoch [22], Batch [439/938], Loss: 0.498759001493454\n",
      "Validation: Epoch [22], Batch [440/938], Loss: 0.5613547563552856\n",
      "Validation: Epoch [22], Batch [441/938], Loss: 0.4278443157672882\n",
      "Validation: Epoch [22], Batch [442/938], Loss: 0.25549569725990295\n",
      "Validation: Epoch [22], Batch [443/938], Loss: 0.41034260392189026\n",
      "Validation: Epoch [22], Batch [444/938], Loss: 0.4866199493408203\n",
      "Validation: Epoch [22], Batch [445/938], Loss: 0.39138054847717285\n",
      "Validation: Epoch [22], Batch [446/938], Loss: 0.5760948657989502\n",
      "Validation: Epoch [22], Batch [447/938], Loss: 0.7072944045066833\n",
      "Validation: Epoch [22], Batch [448/938], Loss: 0.6330715417861938\n",
      "Validation: Epoch [22], Batch [449/938], Loss: 0.44881677627563477\n",
      "Validation: Epoch [22], Batch [450/938], Loss: 0.39315086603164673\n",
      "Validation: Epoch [22], Batch [451/938], Loss: 0.5153002738952637\n",
      "Validation: Epoch [22], Batch [452/938], Loss: 0.666387140750885\n",
      "Validation: Epoch [22], Batch [453/938], Loss: 0.42965105175971985\n",
      "Validation: Epoch [22], Batch [454/938], Loss: 0.3668108880519867\n",
      "Validation: Epoch [22], Batch [455/938], Loss: 0.32364487648010254\n",
      "Validation: Epoch [22], Batch [456/938], Loss: 0.36566662788391113\n",
      "Validation: Epoch [22], Batch [457/938], Loss: 0.43591755628585815\n",
      "Validation: Epoch [22], Batch [458/938], Loss: 0.2913050949573517\n",
      "Validation: Epoch [22], Batch [459/938], Loss: 0.2718636393547058\n",
      "Validation: Epoch [22], Batch [460/938], Loss: 0.3989531695842743\n",
      "Validation: Epoch [22], Batch [461/938], Loss: 0.2973223924636841\n",
      "Validation: Epoch [22], Batch [462/938], Loss: 0.39850762486457825\n",
      "Validation: Epoch [22], Batch [463/938], Loss: 0.3307695984840393\n",
      "Validation: Epoch [22], Batch [464/938], Loss: 0.3255603015422821\n",
      "Validation: Epoch [22], Batch [465/938], Loss: 0.5766639709472656\n",
      "Validation: Epoch [22], Batch [466/938], Loss: 0.6593976616859436\n",
      "Validation: Epoch [22], Batch [467/938], Loss: 0.37294521927833557\n",
      "Validation: Epoch [22], Batch [468/938], Loss: 0.4661157727241516\n",
      "Validation: Epoch [22], Batch [469/938], Loss: 0.4680294692516327\n",
      "Validation: Epoch [22], Batch [470/938], Loss: 0.4275212287902832\n",
      "Validation: Epoch [22], Batch [471/938], Loss: 0.5643081665039062\n",
      "Validation: Epoch [22], Batch [472/938], Loss: 0.3307456970214844\n",
      "Validation: Epoch [22], Batch [473/938], Loss: 0.6183653473854065\n",
      "Validation: Epoch [22], Batch [474/938], Loss: 0.26719897985458374\n",
      "Validation: Epoch [22], Batch [475/938], Loss: 0.40833142399787903\n",
      "Validation: Epoch [22], Batch [476/938], Loss: 0.6362404227256775\n",
      "Validation: Epoch [22], Batch [477/938], Loss: 0.5905666351318359\n",
      "Validation: Epoch [22], Batch [478/938], Loss: 0.45952945947647095\n",
      "Validation: Epoch [22], Batch [479/938], Loss: 0.5799678564071655\n",
      "Validation: Epoch [22], Batch [480/938], Loss: 0.4392167925834656\n",
      "Validation: Epoch [22], Batch [481/938], Loss: 0.5358501076698303\n",
      "Validation: Epoch [22], Batch [482/938], Loss: 0.44801315665245056\n",
      "Validation: Epoch [22], Batch [483/938], Loss: 0.400959312915802\n",
      "Validation: Epoch [22], Batch [484/938], Loss: 0.46267661452293396\n",
      "Validation: Epoch [22], Batch [485/938], Loss: 0.33801794052124023\n",
      "Validation: Epoch [22], Batch [486/938], Loss: 0.37369391322135925\n",
      "Validation: Epoch [22], Batch [487/938], Loss: 0.45642346143722534\n",
      "Validation: Epoch [22], Batch [488/938], Loss: 0.44376155734062195\n",
      "Validation: Epoch [22], Batch [489/938], Loss: 0.6052136421203613\n",
      "Validation: Epoch [22], Batch [490/938], Loss: 0.515235960483551\n",
      "Validation: Epoch [22], Batch [491/938], Loss: 0.49217990040779114\n",
      "Validation: Epoch [22], Batch [492/938], Loss: 0.43967604637145996\n",
      "Validation: Epoch [22], Batch [493/938], Loss: 0.603746235370636\n",
      "Validation: Epoch [22], Batch [494/938], Loss: 0.2726513147354126\n",
      "Validation: Epoch [22], Batch [495/938], Loss: 0.4536037743091583\n",
      "Validation: Epoch [22], Batch [496/938], Loss: 0.45611464977264404\n",
      "Validation: Epoch [22], Batch [497/938], Loss: 0.3677300214767456\n",
      "Validation: Epoch [22], Batch [498/938], Loss: 0.45077604055404663\n",
      "Validation: Epoch [22], Batch [499/938], Loss: 0.387142151594162\n",
      "Validation: Epoch [22], Batch [500/938], Loss: 0.3113280236721039\n",
      "Validation: Epoch [22], Batch [501/938], Loss: 0.3867376446723938\n",
      "Validation: Epoch [22], Batch [502/938], Loss: 0.32509687542915344\n",
      "Validation: Epoch [22], Batch [503/938], Loss: 0.3005964159965515\n",
      "Validation: Epoch [22], Batch [504/938], Loss: 0.3733063340187073\n",
      "Validation: Epoch [22], Batch [505/938], Loss: 0.3052941858768463\n",
      "Validation: Epoch [22], Batch [506/938], Loss: 0.35730472207069397\n",
      "Validation: Epoch [22], Batch [507/938], Loss: 0.5181764960289001\n",
      "Validation: Epoch [22], Batch [508/938], Loss: 0.3768816888332367\n",
      "Validation: Epoch [22], Batch [509/938], Loss: 0.42329105734825134\n",
      "Validation: Epoch [22], Batch [510/938], Loss: 0.36739447712898254\n",
      "Validation: Epoch [22], Batch [511/938], Loss: 0.5003122687339783\n",
      "Validation: Epoch [22], Batch [512/938], Loss: 0.4525262117385864\n",
      "Validation: Epoch [22], Batch [513/938], Loss: 0.4915008544921875\n",
      "Validation: Epoch [22], Batch [514/938], Loss: 0.5811502933502197\n",
      "Validation: Epoch [22], Batch [515/938], Loss: 0.44293898344039917\n",
      "Validation: Epoch [22], Batch [516/938], Loss: 0.4344041347503662\n",
      "Validation: Epoch [22], Batch [517/938], Loss: 0.4499383270740509\n",
      "Validation: Epoch [22], Batch [518/938], Loss: 0.45736679434776306\n",
      "Validation: Epoch [22], Batch [519/938], Loss: 0.3176850378513336\n",
      "Validation: Epoch [22], Batch [520/938], Loss: 0.4244014620780945\n",
      "Validation: Epoch [22], Batch [521/938], Loss: 0.37671151757240295\n",
      "Validation: Epoch [22], Batch [522/938], Loss: 0.41898250579833984\n",
      "Validation: Epoch [22], Batch [523/938], Loss: 0.5852399468421936\n",
      "Validation: Epoch [22], Batch [524/938], Loss: 0.49593329429626465\n",
      "Validation: Epoch [22], Batch [525/938], Loss: 0.5604540109634399\n",
      "Validation: Epoch [22], Batch [526/938], Loss: 0.6112841963768005\n",
      "Validation: Epoch [22], Batch [527/938], Loss: 0.49323219060897827\n",
      "Validation: Epoch [22], Batch [528/938], Loss: 0.4810033440589905\n",
      "Validation: Epoch [22], Batch [529/938], Loss: 0.37254080176353455\n",
      "Validation: Epoch [22], Batch [530/938], Loss: 0.44293516874313354\n",
      "Validation: Epoch [22], Batch [531/938], Loss: 0.5083661675453186\n",
      "Validation: Epoch [22], Batch [532/938], Loss: 0.2814713418483734\n",
      "Validation: Epoch [22], Batch [533/938], Loss: 0.39362043142318726\n",
      "Validation: Epoch [22], Batch [534/938], Loss: 0.40486326813697815\n",
      "Validation: Epoch [22], Batch [535/938], Loss: 0.35248538851737976\n",
      "Validation: Epoch [22], Batch [536/938], Loss: 0.5242750644683838\n",
      "Validation: Epoch [22], Batch [537/938], Loss: 0.33040186762809753\n",
      "Validation: Epoch [22], Batch [538/938], Loss: 0.734801173210144\n",
      "Validation: Epoch [22], Batch [539/938], Loss: 0.524370551109314\n",
      "Validation: Epoch [22], Batch [540/938], Loss: 0.33965951204299927\n",
      "Validation: Epoch [22], Batch [541/938], Loss: 0.272577166557312\n",
      "Validation: Epoch [22], Batch [542/938], Loss: 0.2759677469730377\n",
      "Validation: Epoch [22], Batch [543/938], Loss: 0.4553069770336151\n",
      "Validation: Epoch [22], Batch [544/938], Loss: 0.5747469663619995\n",
      "Validation: Epoch [22], Batch [545/938], Loss: 0.44475996494293213\n",
      "Validation: Epoch [22], Batch [546/938], Loss: 0.5878117084503174\n",
      "Validation: Epoch [22], Batch [547/938], Loss: 0.5639256238937378\n",
      "Validation: Epoch [22], Batch [548/938], Loss: 0.5918495059013367\n",
      "Validation: Epoch [22], Batch [549/938], Loss: 0.3560482859611511\n",
      "Validation: Epoch [22], Batch [550/938], Loss: 0.550930380821228\n",
      "Validation: Epoch [22], Batch [551/938], Loss: 0.4753108620643616\n",
      "Validation: Epoch [22], Batch [552/938], Loss: 0.3670108914375305\n",
      "Validation: Epoch [22], Batch [553/938], Loss: 0.4917154908180237\n",
      "Validation: Epoch [22], Batch [554/938], Loss: 0.544830858707428\n",
      "Validation: Epoch [22], Batch [555/938], Loss: 0.3336653411388397\n",
      "Validation: Epoch [22], Batch [556/938], Loss: 0.349560022354126\n",
      "Validation: Epoch [22], Batch [557/938], Loss: 0.2509281635284424\n",
      "Validation: Epoch [22], Batch [558/938], Loss: 0.40166759490966797\n",
      "Validation: Epoch [22], Batch [559/938], Loss: 0.5735535025596619\n",
      "Validation: Epoch [22], Batch [560/938], Loss: 0.3284167945384979\n",
      "Validation: Epoch [22], Batch [561/938], Loss: 0.5517230033874512\n",
      "Validation: Epoch [22], Batch [562/938], Loss: 0.34546586871147156\n",
      "Validation: Epoch [22], Batch [563/938], Loss: 0.6539111137390137\n",
      "Validation: Epoch [22], Batch [564/938], Loss: 0.5000397562980652\n",
      "Validation: Epoch [22], Batch [565/938], Loss: 0.5069819688796997\n",
      "Validation: Epoch [22], Batch [566/938], Loss: 0.46516403555870056\n",
      "Validation: Epoch [22], Batch [567/938], Loss: 0.4357624650001526\n",
      "Validation: Epoch [22], Batch [568/938], Loss: 0.4291638135910034\n",
      "Validation: Epoch [22], Batch [569/938], Loss: 0.4691314399242401\n",
      "Validation: Epoch [22], Batch [570/938], Loss: 0.4644509255886078\n",
      "Validation: Epoch [22], Batch [571/938], Loss: 0.5639179348945618\n",
      "Validation: Epoch [22], Batch [572/938], Loss: 0.3971478343009949\n",
      "Validation: Epoch [22], Batch [573/938], Loss: 0.4067707657814026\n",
      "Validation: Epoch [22], Batch [574/938], Loss: 0.36669325828552246\n",
      "Validation: Epoch [22], Batch [575/938], Loss: 0.5584345459938049\n",
      "Validation: Epoch [22], Batch [576/938], Loss: 0.6039093732833862\n",
      "Validation: Epoch [22], Batch [577/938], Loss: 0.29527997970581055\n",
      "Validation: Epoch [22], Batch [578/938], Loss: 0.5121554136276245\n",
      "Validation: Epoch [22], Batch [579/938], Loss: 0.48754602670669556\n",
      "Validation: Epoch [22], Batch [580/938], Loss: 0.3111352026462555\n",
      "Validation: Epoch [22], Batch [581/938], Loss: 0.32379406690597534\n",
      "Validation: Epoch [22], Batch [582/938], Loss: 0.33240312337875366\n",
      "Validation: Epoch [22], Batch [583/938], Loss: 0.4541037976741791\n",
      "Validation: Epoch [22], Batch [584/938], Loss: 0.5565305948257446\n",
      "Validation: Epoch [22], Batch [585/938], Loss: 0.2862217128276825\n",
      "Validation: Epoch [22], Batch [586/938], Loss: 0.5147662162780762\n",
      "Validation: Epoch [22], Batch [587/938], Loss: 0.44928866624832153\n",
      "Validation: Epoch [22], Batch [588/938], Loss: 0.3090639114379883\n",
      "Validation: Epoch [22], Batch [589/938], Loss: 0.7133914828300476\n",
      "Validation: Epoch [22], Batch [590/938], Loss: 0.4415578544139862\n",
      "Validation: Epoch [22], Batch [591/938], Loss: 0.42086589336395264\n",
      "Validation: Epoch [22], Batch [592/938], Loss: 0.4350394606590271\n",
      "Validation: Epoch [22], Batch [593/938], Loss: 0.3241465389728546\n",
      "Validation: Epoch [22], Batch [594/938], Loss: 0.3904300928115845\n",
      "Validation: Epoch [22], Batch [595/938], Loss: 0.5625159740447998\n",
      "Validation: Epoch [22], Batch [596/938], Loss: 0.3047221302986145\n",
      "Validation: Epoch [22], Batch [597/938], Loss: 0.4471045434474945\n",
      "Validation: Epoch [22], Batch [598/938], Loss: 0.4542446732521057\n",
      "Validation: Epoch [22], Batch [599/938], Loss: 0.3978070616722107\n",
      "Validation: Epoch [22], Batch [600/938], Loss: 0.42838215827941895\n",
      "Validation: Epoch [22], Batch [601/938], Loss: 0.3736216127872467\n",
      "Validation: Epoch [22], Batch [602/938], Loss: 0.4042290151119232\n",
      "Validation: Epoch [22], Batch [603/938], Loss: 0.5737605690956116\n",
      "Validation: Epoch [22], Batch [604/938], Loss: 0.4165867567062378\n",
      "Validation: Epoch [22], Batch [605/938], Loss: 0.28141096234321594\n",
      "Validation: Epoch [22], Batch [606/938], Loss: 0.5291893482208252\n",
      "Validation: Epoch [22], Batch [607/938], Loss: 0.3998398780822754\n",
      "Validation: Epoch [22], Batch [608/938], Loss: 0.4114212691783905\n",
      "Validation: Epoch [22], Batch [609/938], Loss: 0.6499258279800415\n",
      "Validation: Epoch [22], Batch [610/938], Loss: 0.29283756017684937\n",
      "Validation: Epoch [22], Batch [611/938], Loss: 0.3439740836620331\n",
      "Validation: Epoch [22], Batch [612/938], Loss: 0.41668516397476196\n",
      "Validation: Epoch [22], Batch [613/938], Loss: 0.4469459652900696\n",
      "Validation: Epoch [22], Batch [614/938], Loss: 0.5443487763404846\n",
      "Validation: Epoch [22], Batch [615/938], Loss: 0.4869168698787689\n",
      "Validation: Epoch [22], Batch [616/938], Loss: 0.36803293228149414\n",
      "Validation: Epoch [22], Batch [617/938], Loss: 0.4697534441947937\n",
      "Validation: Epoch [22], Batch [618/938], Loss: 0.26667580008506775\n",
      "Validation: Epoch [22], Batch [619/938], Loss: 0.5965905785560608\n",
      "Validation: Epoch [22], Batch [620/938], Loss: 0.4771508574485779\n",
      "Validation: Epoch [22], Batch [621/938], Loss: 0.3788646459579468\n",
      "Validation: Epoch [22], Batch [622/938], Loss: 0.6718530058860779\n",
      "Validation: Epoch [22], Batch [623/938], Loss: 0.42017215490341187\n",
      "Validation: Epoch [22], Batch [624/938], Loss: 0.42821618914604187\n",
      "Validation: Epoch [22], Batch [625/938], Loss: 0.48384132981300354\n",
      "Validation: Epoch [22], Batch [626/938], Loss: 0.5242518186569214\n",
      "Validation: Epoch [22], Batch [627/938], Loss: 0.3647400736808777\n",
      "Validation: Epoch [22], Batch [628/938], Loss: 0.44192248582839966\n",
      "Validation: Epoch [22], Batch [629/938], Loss: 0.5556676387786865\n",
      "Validation: Epoch [22], Batch [630/938], Loss: 0.5840950608253479\n",
      "Validation: Epoch [22], Batch [631/938], Loss: 0.5076101422309875\n",
      "Validation: Epoch [22], Batch [632/938], Loss: 0.4052084684371948\n",
      "Validation: Epoch [22], Batch [633/938], Loss: 0.5926194190979004\n",
      "Validation: Epoch [22], Batch [634/938], Loss: 0.5874263048171997\n",
      "Validation: Epoch [22], Batch [635/938], Loss: 0.3566201627254486\n",
      "Validation: Epoch [22], Batch [636/938], Loss: 0.5276908874511719\n",
      "Validation: Epoch [22], Batch [637/938], Loss: 0.39640316367149353\n",
      "Validation: Epoch [22], Batch [638/938], Loss: 0.42792826890945435\n",
      "Validation: Epoch [22], Batch [639/938], Loss: 0.5093303322792053\n",
      "Validation: Epoch [22], Batch [640/938], Loss: 0.3040206730365753\n",
      "Validation: Epoch [22], Batch [641/938], Loss: 0.4445289373397827\n",
      "Validation: Epoch [22], Batch [642/938], Loss: 0.5331800580024719\n",
      "Validation: Epoch [22], Batch [643/938], Loss: 0.3444855511188507\n",
      "Validation: Epoch [22], Batch [644/938], Loss: 0.49405595660209656\n",
      "Validation: Epoch [22], Batch [645/938], Loss: 0.4008021652698517\n",
      "Validation: Epoch [22], Batch [646/938], Loss: 0.5395247936248779\n",
      "Validation: Epoch [22], Batch [647/938], Loss: 0.33064404129981995\n",
      "Validation: Epoch [22], Batch [648/938], Loss: 0.5112588405609131\n",
      "Validation: Epoch [22], Batch [649/938], Loss: 0.5198261141777039\n",
      "Validation: Epoch [22], Batch [650/938], Loss: 0.3743937015533447\n",
      "Validation: Epoch [22], Batch [651/938], Loss: 0.34977564215660095\n",
      "Validation: Epoch [22], Batch [652/938], Loss: 0.5011447072029114\n",
      "Validation: Epoch [22], Batch [653/938], Loss: 0.3724347651004791\n",
      "Validation: Epoch [22], Batch [654/938], Loss: 0.5009759664535522\n",
      "Validation: Epoch [22], Batch [655/938], Loss: 0.4691022038459778\n",
      "Validation: Epoch [22], Batch [656/938], Loss: 0.5978344678878784\n",
      "Validation: Epoch [22], Batch [657/938], Loss: 0.5041932463645935\n",
      "Validation: Epoch [22], Batch [658/938], Loss: 0.3457513153553009\n",
      "Validation: Epoch [22], Batch [659/938], Loss: 0.546326756477356\n",
      "Validation: Epoch [22], Batch [660/938], Loss: 0.3262344300746918\n",
      "Validation: Epoch [22], Batch [661/938], Loss: 0.43743738532066345\n",
      "Validation: Epoch [22], Batch [662/938], Loss: 0.3050573468208313\n",
      "Validation: Epoch [22], Batch [663/938], Loss: 0.45136284828186035\n",
      "Validation: Epoch [22], Batch [664/938], Loss: 0.6115922331809998\n",
      "Validation: Epoch [22], Batch [665/938], Loss: 0.6107820868492126\n",
      "Validation: Epoch [22], Batch [666/938], Loss: 0.7231369614601135\n",
      "Validation: Epoch [22], Batch [667/938], Loss: 0.4443853497505188\n",
      "Validation: Epoch [22], Batch [668/938], Loss: 0.39625152945518494\n",
      "Validation: Epoch [22], Batch [669/938], Loss: 0.6087745428085327\n",
      "Validation: Epoch [22], Batch [670/938], Loss: 0.5192957520484924\n",
      "Validation: Epoch [22], Batch [671/938], Loss: 0.6196719408035278\n",
      "Validation: Epoch [22], Batch [672/938], Loss: 0.3504730761051178\n",
      "Validation: Epoch [22], Batch [673/938], Loss: 0.5706749558448792\n",
      "Validation: Epoch [22], Batch [674/938], Loss: 0.48641765117645264\n",
      "Validation: Epoch [22], Batch [675/938], Loss: 0.5213390588760376\n",
      "Validation: Epoch [22], Batch [676/938], Loss: 0.3859139084815979\n",
      "Validation: Epoch [22], Batch [677/938], Loss: 0.549740731716156\n",
      "Validation: Epoch [22], Batch [678/938], Loss: 0.4974685311317444\n",
      "Validation: Epoch [22], Batch [679/938], Loss: 0.4945613741874695\n",
      "Validation: Epoch [22], Batch [680/938], Loss: 0.6322517991065979\n",
      "Validation: Epoch [22], Batch [681/938], Loss: 0.5070766806602478\n",
      "Validation: Epoch [22], Batch [682/938], Loss: 0.4542022943496704\n",
      "Validation: Epoch [22], Batch [683/938], Loss: 0.6392581462860107\n",
      "Validation: Epoch [22], Batch [684/938], Loss: 0.5312445163726807\n",
      "Validation: Epoch [22], Batch [685/938], Loss: 0.6874703764915466\n",
      "Validation: Epoch [22], Batch [686/938], Loss: 0.4270685017108917\n",
      "Validation: Epoch [22], Batch [687/938], Loss: 0.5555158853530884\n",
      "Validation: Epoch [22], Batch [688/938], Loss: 0.4427783489227295\n",
      "Validation: Epoch [22], Batch [689/938], Loss: 0.49205318093299866\n",
      "Validation: Epoch [22], Batch [690/938], Loss: 0.5499740242958069\n",
      "Validation: Epoch [22], Batch [691/938], Loss: 0.489452600479126\n",
      "Validation: Epoch [22], Batch [692/938], Loss: 0.2675080895423889\n",
      "Validation: Epoch [22], Batch [693/938], Loss: 0.36853182315826416\n",
      "Validation: Epoch [22], Batch [694/938], Loss: 0.37705230712890625\n",
      "Validation: Epoch [22], Batch [695/938], Loss: 0.44042542576789856\n",
      "Validation: Epoch [22], Batch [696/938], Loss: 0.7803063988685608\n",
      "Validation: Epoch [22], Batch [697/938], Loss: 0.4878957271575928\n",
      "Validation: Epoch [22], Batch [698/938], Loss: 0.6110589504241943\n",
      "Validation: Epoch [22], Batch [699/938], Loss: 0.34674638509750366\n",
      "Validation: Epoch [22], Batch [700/938], Loss: 0.5746116638183594\n",
      "Validation: Epoch [22], Batch [701/938], Loss: 0.47425445914268494\n",
      "Validation: Epoch [22], Batch [702/938], Loss: 0.5756955146789551\n",
      "Validation: Epoch [22], Batch [703/938], Loss: 0.3395131230354309\n",
      "Validation: Epoch [22], Batch [704/938], Loss: 0.6191964745521545\n",
      "Validation: Epoch [22], Batch [705/938], Loss: 0.3890616297721863\n",
      "Validation: Epoch [22], Batch [706/938], Loss: 0.5691013336181641\n",
      "Validation: Epoch [22], Batch [707/938], Loss: 0.503007709980011\n",
      "Validation: Epoch [22], Batch [708/938], Loss: 0.4261447489261627\n",
      "Validation: Epoch [22], Batch [709/938], Loss: 0.4946352243423462\n",
      "Validation: Epoch [22], Batch [710/938], Loss: 0.3217320442199707\n",
      "Validation: Epoch [22], Batch [711/938], Loss: 0.4743840992450714\n",
      "Validation: Epoch [22], Batch [712/938], Loss: 0.6495484113693237\n",
      "Validation: Epoch [22], Batch [713/938], Loss: 0.47400742769241333\n",
      "Validation: Epoch [22], Batch [714/938], Loss: 0.3712136745452881\n",
      "Validation: Epoch [22], Batch [715/938], Loss: 0.3504607677459717\n",
      "Validation: Epoch [22], Batch [716/938], Loss: 0.48880699276924133\n",
      "Validation: Epoch [22], Batch [717/938], Loss: 0.292599081993103\n",
      "Validation: Epoch [22], Batch [718/938], Loss: 0.479984849691391\n",
      "Validation: Epoch [22], Batch [719/938], Loss: 0.5268647074699402\n",
      "Validation: Epoch [22], Batch [720/938], Loss: 0.5349322557449341\n",
      "Validation: Epoch [22], Batch [721/938], Loss: 0.6957594752311707\n",
      "Validation: Epoch [22], Batch [722/938], Loss: 0.30007392168045044\n",
      "Validation: Epoch [22], Batch [723/938], Loss: 0.5052151083946228\n",
      "Validation: Epoch [22], Batch [724/938], Loss: 0.5092437863349915\n",
      "Validation: Epoch [22], Batch [725/938], Loss: 0.4586195945739746\n",
      "Validation: Epoch [22], Batch [726/938], Loss: 0.46472975611686707\n",
      "Validation: Epoch [22], Batch [727/938], Loss: 0.31325623393058777\n",
      "Validation: Epoch [22], Batch [728/938], Loss: 0.4455898404121399\n",
      "Validation: Epoch [22], Batch [729/938], Loss: 0.3918711245059967\n",
      "Validation: Epoch [22], Batch [730/938], Loss: 0.3345486521720886\n",
      "Validation: Epoch [22], Batch [731/938], Loss: 0.6638867855072021\n",
      "Validation: Epoch [22], Batch [732/938], Loss: 0.36154526472091675\n",
      "Validation: Epoch [22], Batch [733/938], Loss: 0.3582476079463959\n",
      "Validation: Epoch [22], Batch [734/938], Loss: 0.5917149782180786\n",
      "Validation: Epoch [22], Batch [735/938], Loss: 0.5384548306465149\n",
      "Validation: Epoch [22], Batch [736/938], Loss: 0.41874295473098755\n",
      "Validation: Epoch [22], Batch [737/938], Loss: 0.4428609311580658\n",
      "Validation: Epoch [22], Batch [738/938], Loss: 0.3111392855644226\n",
      "Validation: Epoch [22], Batch [739/938], Loss: 0.36263659596443176\n",
      "Validation: Epoch [22], Batch [740/938], Loss: 0.6023375391960144\n",
      "Validation: Epoch [22], Batch [741/938], Loss: 0.47099801898002625\n",
      "Validation: Epoch [22], Batch [742/938], Loss: 0.45117688179016113\n",
      "Validation: Epoch [22], Batch [743/938], Loss: 0.573803722858429\n",
      "Validation: Epoch [22], Batch [744/938], Loss: 0.3993913233280182\n",
      "Validation: Epoch [22], Batch [745/938], Loss: 0.40183407068252563\n",
      "Validation: Epoch [22], Batch [746/938], Loss: 0.47441744804382324\n",
      "Validation: Epoch [22], Batch [747/938], Loss: 0.6766481995582581\n",
      "Validation: Epoch [22], Batch [748/938], Loss: 0.4911164939403534\n",
      "Validation: Epoch [22], Batch [749/938], Loss: 0.2726130187511444\n",
      "Validation: Epoch [22], Batch [750/938], Loss: 0.4708285927772522\n",
      "Validation: Epoch [22], Batch [751/938], Loss: 0.39931806921958923\n",
      "Validation: Epoch [22], Batch [752/938], Loss: 0.5263404846191406\n",
      "Validation: Epoch [22], Batch [753/938], Loss: 0.41646477580070496\n",
      "Validation: Epoch [22], Batch [754/938], Loss: 0.6059749722480774\n",
      "Validation: Epoch [22], Batch [755/938], Loss: 0.19974501430988312\n",
      "Validation: Epoch [22], Batch [756/938], Loss: 0.38879165053367615\n",
      "Validation: Epoch [22], Batch [757/938], Loss: 0.5121753811836243\n",
      "Validation: Epoch [22], Batch [758/938], Loss: 0.3298843502998352\n",
      "Validation: Epoch [22], Batch [759/938], Loss: 0.3440360426902771\n",
      "Validation: Epoch [22], Batch [760/938], Loss: 0.29077789187431335\n",
      "Validation: Epoch [22], Batch [761/938], Loss: 0.6394952535629272\n",
      "Validation: Epoch [22], Batch [762/938], Loss: 0.42865419387817383\n",
      "Validation: Epoch [22], Batch [763/938], Loss: 0.5797792077064514\n",
      "Validation: Epoch [22], Batch [764/938], Loss: 0.6449763774871826\n",
      "Validation: Epoch [22], Batch [765/938], Loss: 0.5947946310043335\n",
      "Validation: Epoch [22], Batch [766/938], Loss: 0.24757082760334015\n",
      "Validation: Epoch [22], Batch [767/938], Loss: 0.605008602142334\n",
      "Validation: Epoch [22], Batch [768/938], Loss: 0.38666969537734985\n",
      "Validation: Epoch [22], Batch [769/938], Loss: 0.42084574699401855\n",
      "Validation: Epoch [22], Batch [770/938], Loss: 0.3188491463661194\n",
      "Validation: Epoch [22], Batch [771/938], Loss: 0.5490556359291077\n",
      "Validation: Epoch [22], Batch [772/938], Loss: 0.38632073998451233\n",
      "Validation: Epoch [22], Batch [773/938], Loss: 0.48334747552871704\n",
      "Validation: Epoch [22], Batch [774/938], Loss: 0.5105780363082886\n",
      "Validation: Epoch [22], Batch [775/938], Loss: 0.28309503197669983\n",
      "Validation: Epoch [22], Batch [776/938], Loss: 0.4402669370174408\n",
      "Validation: Epoch [22], Batch [777/938], Loss: 0.6153545379638672\n",
      "Validation: Epoch [22], Batch [778/938], Loss: 0.5554379820823669\n",
      "Validation: Epoch [22], Batch [779/938], Loss: 0.447530597448349\n",
      "Validation: Epoch [22], Batch [780/938], Loss: 0.4634256362915039\n",
      "Validation: Epoch [22], Batch [781/938], Loss: 0.5936427116394043\n",
      "Validation: Epoch [22], Batch [782/938], Loss: 0.51161789894104\n",
      "Validation: Epoch [22], Batch [783/938], Loss: 0.19863112270832062\n",
      "Validation: Epoch [22], Batch [784/938], Loss: 0.42554157972335815\n",
      "Validation: Epoch [22], Batch [785/938], Loss: 0.5261632204055786\n",
      "Validation: Epoch [22], Batch [786/938], Loss: 0.5387374758720398\n",
      "Validation: Epoch [22], Batch [787/938], Loss: 0.37710586190223694\n",
      "Validation: Epoch [22], Batch [788/938], Loss: 0.5139850378036499\n",
      "Validation: Epoch [22], Batch [789/938], Loss: 0.4390996992588043\n",
      "Validation: Epoch [22], Batch [790/938], Loss: 0.6125516891479492\n",
      "Validation: Epoch [22], Batch [791/938], Loss: 0.44155535101890564\n",
      "Validation: Epoch [22], Batch [792/938], Loss: 0.2924310863018036\n",
      "Validation: Epoch [22], Batch [793/938], Loss: 0.45798367261886597\n",
      "Validation: Epoch [22], Batch [794/938], Loss: 0.4269394874572754\n",
      "Validation: Epoch [22], Batch [795/938], Loss: 0.2755793631076813\n",
      "Validation: Epoch [22], Batch [796/938], Loss: 0.3593442738056183\n",
      "Validation: Epoch [22], Batch [797/938], Loss: 0.3551514744758606\n",
      "Validation: Epoch [22], Batch [798/938], Loss: 0.19967323541641235\n",
      "Validation: Epoch [22], Batch [799/938], Loss: 0.3309814929962158\n",
      "Validation: Epoch [22], Batch [800/938], Loss: 0.3460867702960968\n",
      "Validation: Epoch [22], Batch [801/938], Loss: 0.4588220715522766\n",
      "Validation: Epoch [22], Batch [802/938], Loss: 0.6071900129318237\n",
      "Validation: Epoch [22], Batch [803/938], Loss: 0.4585389196872711\n",
      "Validation: Epoch [22], Batch [804/938], Loss: 0.4115244746208191\n",
      "Validation: Epoch [22], Batch [805/938], Loss: 0.3043345808982849\n",
      "Validation: Epoch [22], Batch [806/938], Loss: 0.36002734303474426\n",
      "Validation: Epoch [22], Batch [807/938], Loss: 0.5752426385879517\n",
      "Validation: Epoch [22], Batch [808/938], Loss: 0.37019845843315125\n",
      "Validation: Epoch [22], Batch [809/938], Loss: 0.4019976258277893\n",
      "Validation: Epoch [22], Batch [810/938], Loss: 0.43797650933265686\n",
      "Validation: Epoch [22], Batch [811/938], Loss: 0.48113417625427246\n",
      "Validation: Epoch [22], Batch [812/938], Loss: 0.5405573844909668\n",
      "Validation: Epoch [22], Batch [813/938], Loss: 0.384490966796875\n",
      "Validation: Epoch [22], Batch [814/938], Loss: 0.4577455222606659\n",
      "Validation: Epoch [22], Batch [815/938], Loss: 0.26773524284362793\n",
      "Validation: Epoch [22], Batch [816/938], Loss: 0.4099987745285034\n",
      "Validation: Epoch [22], Batch [817/938], Loss: 0.19864775240421295\n",
      "Validation: Epoch [22], Batch [818/938], Loss: 0.3819328844547272\n",
      "Validation: Epoch [22], Batch [819/938], Loss: 0.8841170072555542\n",
      "Validation: Epoch [22], Batch [820/938], Loss: 0.2852742075920105\n",
      "Validation: Epoch [22], Batch [821/938], Loss: 0.34711208939552307\n",
      "Validation: Epoch [22], Batch [822/938], Loss: 0.3401719033718109\n",
      "Validation: Epoch [22], Batch [823/938], Loss: 0.5371184349060059\n",
      "Validation: Epoch [22], Batch [824/938], Loss: 0.5061343312263489\n",
      "Validation: Epoch [22], Batch [825/938], Loss: 0.6006876826286316\n",
      "Validation: Epoch [22], Batch [826/938], Loss: 0.5566697120666504\n",
      "Validation: Epoch [22], Batch [827/938], Loss: 0.3183276653289795\n",
      "Validation: Epoch [22], Batch [828/938], Loss: 0.4174887239933014\n",
      "Validation: Epoch [22], Batch [829/938], Loss: 0.48575130105018616\n",
      "Validation: Epoch [22], Batch [830/938], Loss: 0.20173005759716034\n",
      "Validation: Epoch [22], Batch [831/938], Loss: 0.42008182406425476\n",
      "Validation: Epoch [22], Batch [832/938], Loss: 0.40723979473114014\n",
      "Validation: Epoch [22], Batch [833/938], Loss: 0.4627036154270172\n",
      "Validation: Epoch [22], Batch [834/938], Loss: 0.40524205565452576\n",
      "Validation: Epoch [22], Batch [835/938], Loss: 0.5271502733230591\n",
      "Validation: Epoch [22], Batch [836/938], Loss: 0.5482282042503357\n",
      "Validation: Epoch [22], Batch [837/938], Loss: 0.5352022647857666\n",
      "Validation: Epoch [22], Batch [838/938], Loss: 0.3985329866409302\n",
      "Validation: Epoch [22], Batch [839/938], Loss: 0.17183098196983337\n",
      "Validation: Epoch [22], Batch [840/938], Loss: 0.5364651083946228\n",
      "Validation: Epoch [22], Batch [841/938], Loss: 0.36837038397789\n",
      "Validation: Epoch [22], Batch [842/938], Loss: 0.5871272087097168\n",
      "Validation: Epoch [22], Batch [843/938], Loss: 0.4573093354701996\n",
      "Validation: Epoch [22], Batch [844/938], Loss: 0.623706042766571\n",
      "Validation: Epoch [22], Batch [845/938], Loss: 0.5211286544799805\n",
      "Validation: Epoch [22], Batch [846/938], Loss: 0.26568499207496643\n",
      "Validation: Epoch [22], Batch [847/938], Loss: 0.4411225914955139\n",
      "Validation: Epoch [22], Batch [848/938], Loss: 0.4983646869659424\n",
      "Validation: Epoch [22], Batch [849/938], Loss: 0.490433007478714\n",
      "Validation: Epoch [22], Batch [850/938], Loss: 0.7148069143295288\n",
      "Validation: Epoch [22], Batch [851/938], Loss: 0.34330296516418457\n",
      "Validation: Epoch [22], Batch [852/938], Loss: 0.4091382324695587\n",
      "Validation: Epoch [22], Batch [853/938], Loss: 0.4312630295753479\n",
      "Validation: Epoch [22], Batch [854/938], Loss: 0.30041274428367615\n",
      "Validation: Epoch [22], Batch [855/938], Loss: 0.3886438012123108\n",
      "Validation: Epoch [22], Batch [856/938], Loss: 0.372650146484375\n",
      "Validation: Epoch [22], Batch [857/938], Loss: 0.6331842541694641\n",
      "Validation: Epoch [22], Batch [858/938], Loss: 0.5474218130111694\n",
      "Validation: Epoch [22], Batch [859/938], Loss: 0.6061875820159912\n",
      "Validation: Epoch [22], Batch [860/938], Loss: 0.4366299510002136\n",
      "Validation: Epoch [22], Batch [861/938], Loss: 0.43032193183898926\n",
      "Validation: Epoch [22], Batch [862/938], Loss: 0.40905267000198364\n",
      "Validation: Epoch [22], Batch [863/938], Loss: 0.502682089805603\n",
      "Validation: Epoch [22], Batch [864/938], Loss: 0.43222886323928833\n",
      "Validation: Epoch [22], Batch [865/938], Loss: 0.4212360978126526\n",
      "Validation: Epoch [22], Batch [866/938], Loss: 0.39898017048835754\n",
      "Validation: Epoch [22], Batch [867/938], Loss: 0.457221657037735\n",
      "Validation: Epoch [22], Batch [868/938], Loss: 0.3696235120296478\n",
      "Validation: Epoch [22], Batch [869/938], Loss: 0.8088055849075317\n",
      "Validation: Epoch [22], Batch [870/938], Loss: 0.4870675504207611\n",
      "Validation: Epoch [22], Batch [871/938], Loss: 0.3257940411567688\n",
      "Validation: Epoch [22], Batch [872/938], Loss: 0.48914697766304016\n",
      "Validation: Epoch [22], Batch [873/938], Loss: 0.28235235810279846\n",
      "Validation: Epoch [22], Batch [874/938], Loss: 0.5211889147758484\n",
      "Validation: Epoch [22], Batch [875/938], Loss: 0.5217601656913757\n",
      "Validation: Epoch [22], Batch [876/938], Loss: 0.38468697667121887\n",
      "Validation: Epoch [22], Batch [877/938], Loss: 0.40606382489204407\n",
      "Validation: Epoch [22], Batch [878/938], Loss: 0.3048005998134613\n",
      "Validation: Epoch [22], Batch [879/938], Loss: 0.6883814334869385\n",
      "Validation: Epoch [22], Batch [880/938], Loss: 0.3694208562374115\n",
      "Validation: Epoch [22], Batch [881/938], Loss: 0.41316840052604675\n",
      "Validation: Epoch [22], Batch [882/938], Loss: 0.46130117774009705\n",
      "Validation: Epoch [22], Batch [883/938], Loss: 0.4000144898891449\n",
      "Validation: Epoch [22], Batch [884/938], Loss: 0.37622979283332825\n",
      "Validation: Epoch [22], Batch [885/938], Loss: 0.5459883809089661\n",
      "Validation: Epoch [22], Batch [886/938], Loss: 0.42650413513183594\n",
      "Validation: Epoch [22], Batch [887/938], Loss: 0.6273192763328552\n",
      "Validation: Epoch [22], Batch [888/938], Loss: 0.25547197461128235\n",
      "Validation: Epoch [22], Batch [889/938], Loss: 0.5227453708648682\n",
      "Validation: Epoch [22], Batch [890/938], Loss: 0.41339606046676636\n",
      "Validation: Epoch [22], Batch [891/938], Loss: 0.5167849659919739\n",
      "Validation: Epoch [22], Batch [892/938], Loss: 0.39394640922546387\n",
      "Validation: Epoch [22], Batch [893/938], Loss: 0.42561089992523193\n",
      "Validation: Epoch [22], Batch [894/938], Loss: 0.5473532676696777\n",
      "Validation: Epoch [22], Batch [895/938], Loss: 0.3695134222507477\n",
      "Validation: Epoch [22], Batch [896/938], Loss: 0.4929945468902588\n",
      "Validation: Epoch [22], Batch [897/938], Loss: 0.3601454198360443\n",
      "Validation: Epoch [22], Batch [898/938], Loss: 0.5986849665641785\n",
      "Validation: Epoch [22], Batch [899/938], Loss: 0.5465978980064392\n",
      "Validation: Epoch [22], Batch [900/938], Loss: 0.42633581161499023\n",
      "Validation: Epoch [22], Batch [901/938], Loss: 0.7274072766304016\n",
      "Validation: Epoch [22], Batch [902/938], Loss: 0.3821446895599365\n",
      "Validation: Epoch [22], Batch [903/938], Loss: 0.28128015995025635\n",
      "Validation: Epoch [22], Batch [904/938], Loss: 0.4825292229652405\n",
      "Validation: Epoch [22], Batch [905/938], Loss: 0.36696961522102356\n",
      "Validation: Epoch [22], Batch [906/938], Loss: 0.733178436756134\n",
      "Validation: Epoch [22], Batch [907/938], Loss: 0.5219190716743469\n",
      "Validation: Epoch [22], Batch [908/938], Loss: 0.3521018922328949\n",
      "Validation: Epoch [22], Batch [909/938], Loss: 0.4290538430213928\n",
      "Validation: Epoch [22], Batch [910/938], Loss: 0.6655925512313843\n",
      "Validation: Epoch [22], Batch [911/938], Loss: 0.42059946060180664\n",
      "Validation: Epoch [22], Batch [912/938], Loss: 0.541210949420929\n",
      "Validation: Epoch [22], Batch [913/938], Loss: 0.4496399164199829\n",
      "Validation: Epoch [22], Batch [914/938], Loss: 0.3586611747741699\n",
      "Validation: Epoch [22], Batch [915/938], Loss: 0.4653467535972595\n",
      "Validation: Epoch [22], Batch [916/938], Loss: 0.616374671459198\n",
      "Validation: Epoch [22], Batch [917/938], Loss: 0.4791489243507385\n",
      "Validation: Epoch [22], Batch [918/938], Loss: 0.44009459018707275\n",
      "Validation: Epoch [22], Batch [919/938], Loss: 0.5849488973617554\n",
      "Validation: Epoch [22], Batch [920/938], Loss: 0.6769835948944092\n",
      "Validation: Epoch [22], Batch [921/938], Loss: 0.30460894107818604\n",
      "Validation: Epoch [22], Batch [922/938], Loss: 0.44831299781799316\n",
      "Validation: Epoch [22], Batch [923/938], Loss: 0.5904836654663086\n",
      "Validation: Epoch [22], Batch [924/938], Loss: 0.391184538602829\n",
      "Validation: Epoch [22], Batch [925/938], Loss: 0.3405750095844269\n",
      "Validation: Epoch [22], Batch [926/938], Loss: 0.5120120644569397\n",
      "Validation: Epoch [22], Batch [927/938], Loss: 0.48758748173713684\n",
      "Validation: Epoch [22], Batch [928/938], Loss: 0.32606494426727295\n",
      "Validation: Epoch [22], Batch [929/938], Loss: 0.37206581234931946\n",
      "Validation: Epoch [22], Batch [930/938], Loss: 0.5063521862030029\n",
      "Validation: Epoch [22], Batch [931/938], Loss: 0.44834738969802856\n",
      "Validation: Epoch [22], Batch [932/938], Loss: 0.37183547019958496\n",
      "Validation: Epoch [22], Batch [933/938], Loss: 0.39532536268234253\n",
      "Validation: Epoch [22], Batch [934/938], Loss: 0.5824904441833496\n",
      "Validation: Epoch [22], Batch [935/938], Loss: 0.5465760231018066\n",
      "Validation: Epoch [22], Batch [936/938], Loss: 0.4817856252193451\n",
      "Validation: Epoch [22], Batch [937/938], Loss: 0.3319077789783478\n",
      "Validation: Epoch [22], Batch [938/938], Loss: 0.48737090826034546\n",
      "Accuracy of test set: 0.8402166666666666\n",
      "Train: Epoch [23], Batch [1/938], Loss: 0.6834694743156433\n",
      "Train: Epoch [23], Batch [2/938], Loss: 0.4449600279331207\n",
      "Train: Epoch [23], Batch [3/938], Loss: 0.5480078458786011\n",
      "Train: Epoch [23], Batch [4/938], Loss: 0.460348904132843\n",
      "Train: Epoch [23], Batch [5/938], Loss: 0.5460121631622314\n",
      "Train: Epoch [23], Batch [6/938], Loss: 0.36777281761169434\n",
      "Train: Epoch [23], Batch [7/938], Loss: 0.632991373538971\n",
      "Train: Epoch [23], Batch [8/938], Loss: 0.38996052742004395\n",
      "Train: Epoch [23], Batch [9/938], Loss: 0.3061729073524475\n",
      "Train: Epoch [23], Batch [10/938], Loss: 0.4816606342792511\n",
      "Train: Epoch [23], Batch [11/938], Loss: 0.4712437391281128\n",
      "Train: Epoch [23], Batch [12/938], Loss: 0.2839035391807556\n",
      "Train: Epoch [23], Batch [13/938], Loss: 0.6164659857749939\n",
      "Train: Epoch [23], Batch [14/938], Loss: 0.412476509809494\n",
      "Train: Epoch [23], Batch [15/938], Loss: 0.42111653089523315\n",
      "Train: Epoch [23], Batch [16/938], Loss: 0.25178930163383484\n",
      "Train: Epoch [23], Batch [17/938], Loss: 0.23470290005207062\n",
      "Train: Epoch [23], Batch [18/938], Loss: 0.35941851139068604\n",
      "Train: Epoch [23], Batch [19/938], Loss: 0.4303290545940399\n",
      "Train: Epoch [23], Batch [20/938], Loss: 0.3925459682941437\n",
      "Train: Epoch [23], Batch [21/938], Loss: 0.5249910354614258\n",
      "Train: Epoch [23], Batch [22/938], Loss: 0.6191362738609314\n",
      "Train: Epoch [23], Batch [23/938], Loss: 0.4854896366596222\n",
      "Train: Epoch [23], Batch [24/938], Loss: 0.610030472278595\n",
      "Train: Epoch [23], Batch [25/938], Loss: 0.3680056035518646\n",
      "Train: Epoch [23], Batch [26/938], Loss: 0.4781372547149658\n",
      "Train: Epoch [23], Batch [27/938], Loss: 0.42333510518074036\n",
      "Train: Epoch [23], Batch [28/938], Loss: 0.35126835107803345\n",
      "Train: Epoch [23], Batch [29/938], Loss: 0.5380373001098633\n",
      "Train: Epoch [23], Batch [30/938], Loss: 0.3098839521408081\n",
      "Train: Epoch [23], Batch [31/938], Loss: 0.5343582630157471\n",
      "Train: Epoch [23], Batch [32/938], Loss: 0.518762469291687\n",
      "Train: Epoch [23], Batch [33/938], Loss: 0.43630480766296387\n",
      "Train: Epoch [23], Batch [34/938], Loss: 0.3947378993034363\n",
      "Train: Epoch [23], Batch [35/938], Loss: 0.5372380614280701\n",
      "Train: Epoch [23], Batch [36/938], Loss: 0.3395158052444458\n",
      "Train: Epoch [23], Batch [37/938], Loss: 0.37300920486450195\n",
      "Train: Epoch [23], Batch [38/938], Loss: 0.6393343806266785\n",
      "Train: Epoch [23], Batch [39/938], Loss: 0.314938485622406\n",
      "Train: Epoch [23], Batch [40/938], Loss: 0.5232484340667725\n",
      "Train: Epoch [23], Batch [41/938], Loss: 0.2665763199329376\n",
      "Train: Epoch [23], Batch [42/938], Loss: 0.13403719663619995\n",
      "Train: Epoch [23], Batch [43/938], Loss: 0.33056607842445374\n",
      "Train: Epoch [23], Batch [44/938], Loss: 0.46669936180114746\n",
      "Train: Epoch [23], Batch [45/938], Loss: 0.5016686320304871\n",
      "Train: Epoch [23], Batch [46/938], Loss: 0.5931859016418457\n",
      "Train: Epoch [23], Batch [47/938], Loss: 0.30031344294548035\n",
      "Train: Epoch [23], Batch [48/938], Loss: 0.386393666267395\n",
      "Train: Epoch [23], Batch [49/938], Loss: 0.5053282976150513\n",
      "Train: Epoch [23], Batch [50/938], Loss: 0.5381811261177063\n",
      "Train: Epoch [23], Batch [51/938], Loss: 0.5008375644683838\n",
      "Train: Epoch [23], Batch [52/938], Loss: 0.5758672952651978\n",
      "Train: Epoch [23], Batch [53/938], Loss: 0.550260603427887\n",
      "Train: Epoch [23], Batch [54/938], Loss: 0.294685423374176\n",
      "Train: Epoch [23], Batch [55/938], Loss: 0.4271658957004547\n",
      "Train: Epoch [23], Batch [56/938], Loss: 0.5340434908866882\n",
      "Train: Epoch [23], Batch [57/938], Loss: 0.4391864240169525\n",
      "Train: Epoch [23], Batch [58/938], Loss: 0.3946905732154846\n",
      "Train: Epoch [23], Batch [59/938], Loss: 0.36286699771881104\n",
      "Train: Epoch [23], Batch [60/938], Loss: 0.53522789478302\n",
      "Train: Epoch [23], Batch [61/938], Loss: 0.4123280942440033\n",
      "Train: Epoch [23], Batch [62/938], Loss: 0.4428117573261261\n",
      "Train: Epoch [23], Batch [63/938], Loss: 0.44814586639404297\n",
      "Train: Epoch [23], Batch [64/938], Loss: 0.3498048782348633\n",
      "Train: Epoch [23], Batch [65/938], Loss: 0.5453004837036133\n",
      "Train: Epoch [23], Batch [66/938], Loss: 0.5546075105667114\n",
      "Train: Epoch [23], Batch [67/938], Loss: 0.39012062549591064\n",
      "Train: Epoch [23], Batch [68/938], Loss: 0.3362788259983063\n",
      "Train: Epoch [23], Batch [69/938], Loss: 0.33714962005615234\n",
      "Train: Epoch [23], Batch [70/938], Loss: 0.3405003547668457\n",
      "Train: Epoch [23], Batch [71/938], Loss: 0.45916125178337097\n",
      "Train: Epoch [23], Batch [72/938], Loss: 0.4135582447052002\n",
      "Train: Epoch [23], Batch [73/938], Loss: 0.6062975525856018\n",
      "Train: Epoch [23], Batch [74/938], Loss: 0.3913124203681946\n",
      "Train: Epoch [23], Batch [75/938], Loss: 0.47937649488449097\n",
      "Train: Epoch [23], Batch [76/938], Loss: 0.4675920307636261\n",
      "Train: Epoch [23], Batch [77/938], Loss: 0.6685028076171875\n",
      "Train: Epoch [23], Batch [78/938], Loss: 0.5094341039657593\n",
      "Train: Epoch [23], Batch [79/938], Loss: 0.5654581785202026\n",
      "Train: Epoch [23], Batch [80/938], Loss: 0.6561561822891235\n",
      "Train: Epoch [23], Batch [81/938], Loss: 0.35458076000213623\n",
      "Train: Epoch [23], Batch [82/938], Loss: 0.3471730947494507\n",
      "Train: Epoch [23], Batch [83/938], Loss: 0.5122709274291992\n",
      "Train: Epoch [23], Batch [84/938], Loss: 0.42034006118774414\n",
      "Train: Epoch [23], Batch [85/938], Loss: 0.42097848653793335\n",
      "Train: Epoch [23], Batch [86/938], Loss: 0.5468076467514038\n",
      "Train: Epoch [23], Batch [87/938], Loss: 0.28030794858932495\n",
      "Train: Epoch [23], Batch [88/938], Loss: 0.5062512159347534\n",
      "Train: Epoch [23], Batch [89/938], Loss: 0.6317847967147827\n",
      "Train: Epoch [23], Batch [90/938], Loss: 0.5243692994117737\n",
      "Train: Epoch [23], Batch [91/938], Loss: 0.6267799139022827\n",
      "Train: Epoch [23], Batch [92/938], Loss: 0.316999226808548\n",
      "Train: Epoch [23], Batch [93/938], Loss: 0.34413331747055054\n",
      "Train: Epoch [23], Batch [94/938], Loss: 0.31595277786254883\n",
      "Train: Epoch [23], Batch [95/938], Loss: 0.5580456852912903\n",
      "Train: Epoch [23], Batch [96/938], Loss: 0.48261553049087524\n",
      "Train: Epoch [23], Batch [97/938], Loss: 0.37837833166122437\n",
      "Train: Epoch [23], Batch [98/938], Loss: 0.26197969913482666\n",
      "Train: Epoch [23], Batch [99/938], Loss: 0.26727187633514404\n",
      "Train: Epoch [23], Batch [100/938], Loss: 0.45988917350769043\n",
      "Train: Epoch [23], Batch [101/938], Loss: 0.49045804142951965\n",
      "Train: Epoch [23], Batch [102/938], Loss: 0.36876699328422546\n",
      "Train: Epoch [23], Batch [103/938], Loss: 0.3905145227909088\n",
      "Train: Epoch [23], Batch [104/938], Loss: 0.7172378897666931\n",
      "Train: Epoch [23], Batch [105/938], Loss: 0.5789815187454224\n",
      "Train: Epoch [23], Batch [106/938], Loss: 0.4682563841342926\n",
      "Train: Epoch [23], Batch [107/938], Loss: 0.4460664987564087\n",
      "Train: Epoch [23], Batch [108/938], Loss: 0.6270945072174072\n",
      "Train: Epoch [23], Batch [109/938], Loss: 0.43713247776031494\n",
      "Train: Epoch [23], Batch [110/938], Loss: 0.37548625469207764\n",
      "Train: Epoch [23], Batch [111/938], Loss: 0.4186696410179138\n",
      "Train: Epoch [23], Batch [112/938], Loss: 0.31293177604675293\n",
      "Train: Epoch [23], Batch [113/938], Loss: 0.5473195910453796\n",
      "Train: Epoch [23], Batch [114/938], Loss: 0.5642711520195007\n",
      "Train: Epoch [23], Batch [115/938], Loss: 0.42790913581848145\n",
      "Train: Epoch [23], Batch [116/938], Loss: 0.3060457706451416\n",
      "Train: Epoch [23], Batch [117/938], Loss: 0.5039032697677612\n",
      "Train: Epoch [23], Batch [118/938], Loss: 0.6176486015319824\n",
      "Train: Epoch [23], Batch [119/938], Loss: 0.5230648517608643\n",
      "Train: Epoch [23], Batch [120/938], Loss: 0.4579118490219116\n",
      "Train: Epoch [23], Batch [121/938], Loss: 0.4981636106967926\n",
      "Train: Epoch [23], Batch [122/938], Loss: 0.5256460905075073\n",
      "Train: Epoch [23], Batch [123/938], Loss: 0.4116853177547455\n",
      "Train: Epoch [23], Batch [124/938], Loss: 0.2750754654407501\n",
      "Train: Epoch [23], Batch [125/938], Loss: 0.40528762340545654\n",
      "Train: Epoch [23], Batch [126/938], Loss: 0.3645234704017639\n",
      "Train: Epoch [23], Batch [127/938], Loss: 0.2751469910144806\n",
      "Train: Epoch [23], Batch [128/938], Loss: 0.360537052154541\n",
      "Train: Epoch [23], Batch [129/938], Loss: 0.4259178340435028\n",
      "Train: Epoch [23], Batch [130/938], Loss: 0.22664335370063782\n",
      "Train: Epoch [23], Batch [131/938], Loss: 0.46904200315475464\n",
      "Train: Epoch [23], Batch [132/938], Loss: 0.46733200550079346\n",
      "Train: Epoch [23], Batch [133/938], Loss: 0.3933350443840027\n",
      "Train: Epoch [23], Batch [134/938], Loss: 0.47462886571884155\n",
      "Train: Epoch [23], Batch [135/938], Loss: 0.3691369593143463\n",
      "Train: Epoch [23], Batch [136/938], Loss: 0.4523889720439911\n",
      "Train: Epoch [23], Batch [137/938], Loss: 0.511118471622467\n",
      "Train: Epoch [23], Batch [138/938], Loss: 0.2670288681983948\n",
      "Train: Epoch [23], Batch [139/938], Loss: 0.45372551679611206\n",
      "Train: Epoch [23], Batch [140/938], Loss: 0.38507160544395447\n",
      "Train: Epoch [23], Batch [141/938], Loss: 0.28811413049697876\n",
      "Train: Epoch [23], Batch [142/938], Loss: 0.41635483503341675\n",
      "Train: Epoch [23], Batch [143/938], Loss: 0.748533308506012\n",
      "Train: Epoch [23], Batch [144/938], Loss: 0.5029536485671997\n",
      "Train: Epoch [23], Batch [145/938], Loss: 0.37775343656539917\n",
      "Train: Epoch [23], Batch [146/938], Loss: 0.72330641746521\n",
      "Train: Epoch [23], Batch [147/938], Loss: 0.4894494116306305\n",
      "Train: Epoch [23], Batch [148/938], Loss: 0.5355349779129028\n",
      "Train: Epoch [23], Batch [149/938], Loss: 0.6670084595680237\n",
      "Train: Epoch [23], Batch [150/938], Loss: 0.30020231008529663\n",
      "Train: Epoch [23], Batch [151/938], Loss: 0.5139471888542175\n",
      "Train: Epoch [23], Batch [152/938], Loss: 0.3214101195335388\n",
      "Train: Epoch [23], Batch [153/938], Loss: 0.3161023259162903\n",
      "Train: Epoch [23], Batch [154/938], Loss: 0.6474425196647644\n",
      "Train: Epoch [23], Batch [155/938], Loss: 0.29885780811309814\n",
      "Train: Epoch [23], Batch [156/938], Loss: 0.3484959006309509\n",
      "Train: Epoch [23], Batch [157/938], Loss: 0.5170016884803772\n",
      "Train: Epoch [23], Batch [158/938], Loss: 0.35017549991607666\n",
      "Train: Epoch [23], Batch [159/938], Loss: 0.6124299764633179\n",
      "Train: Epoch [23], Batch [160/938], Loss: 0.5305250883102417\n",
      "Train: Epoch [23], Batch [161/938], Loss: 0.2743159532546997\n",
      "Train: Epoch [23], Batch [162/938], Loss: 0.5384947061538696\n",
      "Train: Epoch [23], Batch [163/938], Loss: 0.4069523513317108\n",
      "Train: Epoch [23], Batch [164/938], Loss: 0.2739449441432953\n",
      "Train: Epoch [23], Batch [165/938], Loss: 0.45544877648353577\n",
      "Train: Epoch [23], Batch [166/938], Loss: 0.5143555998802185\n",
      "Train: Epoch [23], Batch [167/938], Loss: 0.5615436434745789\n",
      "Train: Epoch [23], Batch [168/938], Loss: 0.610809326171875\n",
      "Train: Epoch [23], Batch [169/938], Loss: 0.3744271397590637\n",
      "Train: Epoch [23], Batch [170/938], Loss: 0.5535441637039185\n",
      "Train: Epoch [23], Batch [171/938], Loss: 0.3269555866718292\n",
      "Train: Epoch [23], Batch [172/938], Loss: 0.5965452790260315\n",
      "Train: Epoch [23], Batch [173/938], Loss: 0.3212077021598816\n",
      "Train: Epoch [23], Batch [174/938], Loss: 0.4926864802837372\n",
      "Train: Epoch [23], Batch [175/938], Loss: 0.3697940409183502\n",
      "Train: Epoch [23], Batch [176/938], Loss: 0.374680757522583\n",
      "Train: Epoch [23], Batch [177/938], Loss: 0.27242547273635864\n",
      "Train: Epoch [23], Batch [178/938], Loss: 0.4794461727142334\n",
      "Train: Epoch [23], Batch [179/938], Loss: 0.4908289313316345\n",
      "Train: Epoch [23], Batch [180/938], Loss: 0.32216981053352356\n",
      "Train: Epoch [23], Batch [181/938], Loss: 0.45036718249320984\n",
      "Train: Epoch [23], Batch [182/938], Loss: 0.3388485312461853\n",
      "Train: Epoch [23], Batch [183/938], Loss: 0.41404300928115845\n",
      "Train: Epoch [23], Batch [184/938], Loss: 0.37554267048835754\n",
      "Train: Epoch [23], Batch [185/938], Loss: 0.31702837347984314\n",
      "Train: Epoch [23], Batch [186/938], Loss: 0.2923582196235657\n",
      "Train: Epoch [23], Batch [187/938], Loss: 0.3807215690612793\n",
      "Train: Epoch [23], Batch [188/938], Loss: 0.47613760828971863\n",
      "Train: Epoch [23], Batch [189/938], Loss: 0.45127660036087036\n",
      "Train: Epoch [23], Batch [190/938], Loss: 0.2837977707386017\n",
      "Train: Epoch [23], Batch [191/938], Loss: 0.4631292521953583\n",
      "Train: Epoch [23], Batch [192/938], Loss: 0.36944225430488586\n",
      "Train: Epoch [23], Batch [193/938], Loss: 0.5560697317123413\n",
      "Train: Epoch [23], Batch [194/938], Loss: 0.4713168740272522\n",
      "Train: Epoch [23], Batch [195/938], Loss: 0.3359692096710205\n",
      "Train: Epoch [23], Batch [196/938], Loss: 0.3433770537376404\n",
      "Train: Epoch [23], Batch [197/938], Loss: 0.341672420501709\n",
      "Train: Epoch [23], Batch [198/938], Loss: 0.2823904752731323\n",
      "Train: Epoch [23], Batch [199/938], Loss: 0.5524238348007202\n",
      "Train: Epoch [23], Batch [200/938], Loss: 0.39050450921058655\n",
      "Train: Epoch [23], Batch [201/938], Loss: 0.2845621705055237\n",
      "Train: Epoch [23], Batch [202/938], Loss: 0.5231163501739502\n",
      "Train: Epoch [23], Batch [203/938], Loss: 0.1662302017211914\n",
      "Train: Epoch [23], Batch [204/938], Loss: 0.5461761355400085\n",
      "Train: Epoch [23], Batch [205/938], Loss: 0.42090535163879395\n",
      "Train: Epoch [23], Batch [206/938], Loss: 0.3199586570262909\n",
      "Train: Epoch [23], Batch [207/938], Loss: 0.3040088415145874\n",
      "Train: Epoch [23], Batch [208/938], Loss: 0.44810256361961365\n",
      "Train: Epoch [23], Batch [209/938], Loss: 0.5089924931526184\n",
      "Train: Epoch [23], Batch [210/938], Loss: 0.5798143148422241\n",
      "Train: Epoch [23], Batch [211/938], Loss: 0.33940252661705017\n",
      "Train: Epoch [23], Batch [212/938], Loss: 0.5118027329444885\n",
      "Train: Epoch [23], Batch [213/938], Loss: 0.4023655354976654\n",
      "Train: Epoch [23], Batch [214/938], Loss: 0.26172879338264465\n",
      "Train: Epoch [23], Batch [215/938], Loss: 0.3541102409362793\n",
      "Train: Epoch [23], Batch [216/938], Loss: 0.33678704500198364\n",
      "Train: Epoch [23], Batch [217/938], Loss: 0.5197147130966187\n",
      "Train: Epoch [23], Batch [218/938], Loss: 0.36578890681266785\n",
      "Train: Epoch [23], Batch [219/938], Loss: 0.4711305499076843\n",
      "Train: Epoch [23], Batch [220/938], Loss: 0.2852811813354492\n",
      "Train: Epoch [23], Batch [221/938], Loss: 0.29290053248405457\n",
      "Train: Epoch [23], Batch [222/938], Loss: 0.2764236330986023\n",
      "Train: Epoch [23], Batch [223/938], Loss: 0.23296834528446198\n",
      "Train: Epoch [23], Batch [224/938], Loss: 0.5046737194061279\n",
      "Train: Epoch [23], Batch [225/938], Loss: 0.3680965304374695\n",
      "Train: Epoch [23], Batch [226/938], Loss: 0.36970892548561096\n",
      "Train: Epoch [23], Batch [227/938], Loss: 0.2868044972419739\n",
      "Train: Epoch [23], Batch [228/938], Loss: 0.36481282114982605\n",
      "Train: Epoch [23], Batch [229/938], Loss: 0.6021379828453064\n",
      "Train: Epoch [23], Batch [230/938], Loss: 0.3498750329017639\n",
      "Train: Epoch [23], Batch [231/938], Loss: 0.5424624681472778\n",
      "Train: Epoch [23], Batch [232/938], Loss: 0.4179968237876892\n",
      "Train: Epoch [23], Batch [233/938], Loss: 0.26679158210754395\n",
      "Train: Epoch [23], Batch [234/938], Loss: 0.46109068393707275\n",
      "Train: Epoch [23], Batch [235/938], Loss: 0.5625746846199036\n",
      "Train: Epoch [23], Batch [236/938], Loss: 0.4362296462059021\n",
      "Train: Epoch [23], Batch [237/938], Loss: 0.4846763014793396\n",
      "Train: Epoch [23], Batch [238/938], Loss: 0.42853692173957825\n",
      "Train: Epoch [23], Batch [239/938], Loss: 0.30248594284057617\n",
      "Train: Epoch [23], Batch [240/938], Loss: 0.5586989521980286\n",
      "Train: Epoch [23], Batch [241/938], Loss: 0.5733103156089783\n",
      "Train: Epoch [23], Batch [242/938], Loss: 0.44558417797088623\n",
      "Train: Epoch [23], Batch [243/938], Loss: 0.42237553000450134\n",
      "Train: Epoch [23], Batch [244/938], Loss: 0.3529083728790283\n",
      "Train: Epoch [23], Batch [245/938], Loss: 0.3597886860370636\n",
      "Train: Epoch [23], Batch [246/938], Loss: 0.37395521998405457\n",
      "Train: Epoch [23], Batch [247/938], Loss: 0.3920396864414215\n",
      "Train: Epoch [23], Batch [248/938], Loss: 0.47589054703712463\n",
      "Train: Epoch [23], Batch [249/938], Loss: 0.34386929869651794\n",
      "Train: Epoch [23], Batch [250/938], Loss: 0.27820056676864624\n",
      "Train: Epoch [23], Batch [251/938], Loss: 0.4397301971912384\n",
      "Train: Epoch [23], Batch [252/938], Loss: 0.3444862961769104\n",
      "Train: Epoch [23], Batch [253/938], Loss: 0.4182188808917999\n",
      "Train: Epoch [23], Batch [254/938], Loss: 0.8602343201637268\n",
      "Train: Epoch [23], Batch [255/938], Loss: 0.5212988257408142\n",
      "Train: Epoch [23], Batch [256/938], Loss: 0.6186606287956238\n",
      "Train: Epoch [23], Batch [257/938], Loss: 0.4144173860549927\n",
      "Train: Epoch [23], Batch [258/938], Loss: 0.5589029788970947\n",
      "Train: Epoch [23], Batch [259/938], Loss: 0.47840946912765503\n",
      "Train: Epoch [23], Batch [260/938], Loss: 0.30786189436912537\n",
      "Train: Epoch [23], Batch [261/938], Loss: 0.7209618091583252\n",
      "Train: Epoch [23], Batch [262/938], Loss: 0.23740197718143463\n",
      "Train: Epoch [23], Batch [263/938], Loss: 0.45319604873657227\n",
      "Train: Epoch [23], Batch [264/938], Loss: 0.2874983549118042\n",
      "Train: Epoch [23], Batch [265/938], Loss: 0.3474014103412628\n",
      "Train: Epoch [23], Batch [266/938], Loss: 0.38580092787742615\n",
      "Train: Epoch [23], Batch [267/938], Loss: 0.4626389145851135\n",
      "Train: Epoch [23], Batch [268/938], Loss: 0.6292497515678406\n",
      "Train: Epoch [23], Batch [269/938], Loss: 0.4196935296058655\n",
      "Train: Epoch [23], Batch [270/938], Loss: 0.4358517825603485\n",
      "Train: Epoch [23], Batch [271/938], Loss: 0.43364381790161133\n",
      "Train: Epoch [23], Batch [272/938], Loss: 0.37042760848999023\n",
      "Train: Epoch [23], Batch [273/938], Loss: 0.3560107946395874\n",
      "Train: Epoch [23], Batch [274/938], Loss: 0.529900074005127\n",
      "Train: Epoch [23], Batch [275/938], Loss: 0.4117037057876587\n",
      "Train: Epoch [23], Batch [276/938], Loss: 0.34539034962654114\n",
      "Train: Epoch [23], Batch [277/938], Loss: 0.5330049395561218\n",
      "Train: Epoch [23], Batch [278/938], Loss: 0.18346211314201355\n",
      "Train: Epoch [23], Batch [279/938], Loss: 0.5013079643249512\n",
      "Train: Epoch [23], Batch [280/938], Loss: 0.3369726538658142\n",
      "Train: Epoch [23], Batch [281/938], Loss: 0.44770073890686035\n",
      "Train: Epoch [23], Batch [282/938], Loss: 0.4829624891281128\n",
      "Train: Epoch [23], Batch [283/938], Loss: 0.5462148189544678\n",
      "Train: Epoch [23], Batch [284/938], Loss: 0.4126448333263397\n",
      "Train: Epoch [23], Batch [285/938], Loss: 0.3440844416618347\n",
      "Train: Epoch [23], Batch [286/938], Loss: 0.4059975743293762\n",
      "Train: Epoch [23], Batch [287/938], Loss: 0.5619226098060608\n",
      "Train: Epoch [23], Batch [288/938], Loss: 0.27506497502326965\n",
      "Train: Epoch [23], Batch [289/938], Loss: 0.4382467269897461\n",
      "Train: Epoch [23], Batch [290/938], Loss: 0.3698360323905945\n",
      "Train: Epoch [23], Batch [291/938], Loss: 0.66808021068573\n",
      "Train: Epoch [23], Batch [292/938], Loss: 0.503546416759491\n",
      "Train: Epoch [23], Batch [293/938], Loss: 0.48597171902656555\n",
      "Train: Epoch [23], Batch [294/938], Loss: 0.4302372932434082\n",
      "Train: Epoch [23], Batch [295/938], Loss: 0.3873187005519867\n",
      "Train: Epoch [23], Batch [296/938], Loss: 0.1831025630235672\n",
      "Train: Epoch [23], Batch [297/938], Loss: 0.4498680830001831\n",
      "Train: Epoch [23], Batch [298/938], Loss: 0.3134053647518158\n",
      "Train: Epoch [23], Batch [299/938], Loss: 0.3141598105430603\n",
      "Train: Epoch [23], Batch [300/938], Loss: 0.2901493012905121\n",
      "Train: Epoch [23], Batch [301/938], Loss: 0.27899858355522156\n",
      "Train: Epoch [23], Batch [302/938], Loss: 0.557352602481842\n",
      "Train: Epoch [23], Batch [303/938], Loss: 0.5300189256668091\n",
      "Train: Epoch [23], Batch [304/938], Loss: 0.5793126225471497\n",
      "Train: Epoch [23], Batch [305/938], Loss: 0.5260968804359436\n",
      "Train: Epoch [23], Batch [306/938], Loss: 0.47426629066467285\n",
      "Train: Epoch [23], Batch [307/938], Loss: 0.47536104917526245\n",
      "Train: Epoch [23], Batch [308/938], Loss: 0.3773897886276245\n",
      "Train: Epoch [23], Batch [309/938], Loss: 0.3402835726737976\n",
      "Train: Epoch [23], Batch [310/938], Loss: 0.47786998748779297\n",
      "Train: Epoch [23], Batch [311/938], Loss: 0.5020085573196411\n",
      "Train: Epoch [23], Batch [312/938], Loss: 0.3994198143482208\n",
      "Train: Epoch [23], Batch [313/938], Loss: 0.6090244054794312\n",
      "Train: Epoch [23], Batch [314/938], Loss: 0.43052858114242554\n",
      "Train: Epoch [23], Batch [315/938], Loss: 0.45579859614372253\n",
      "Train: Epoch [23], Batch [316/938], Loss: 0.35143616795539856\n",
      "Train: Epoch [23], Batch [317/938], Loss: 0.34432274103164673\n",
      "Train: Epoch [23], Batch [318/938], Loss: 0.3855007290840149\n",
      "Train: Epoch [23], Batch [319/938], Loss: 0.416670560836792\n",
      "Train: Epoch [23], Batch [320/938], Loss: 0.6125220656394958\n",
      "Train: Epoch [23], Batch [321/938], Loss: 0.4765344560146332\n",
      "Train: Epoch [23], Batch [322/938], Loss: 0.37851467728614807\n",
      "Train: Epoch [23], Batch [323/938], Loss: 0.3755930960178375\n",
      "Train: Epoch [23], Batch [324/938], Loss: 0.46932366490364075\n",
      "Train: Epoch [23], Batch [325/938], Loss: 0.5474372506141663\n",
      "Train: Epoch [23], Batch [326/938], Loss: 0.4079227149486542\n",
      "Train: Epoch [23], Batch [327/938], Loss: 0.39748644828796387\n",
      "Train: Epoch [23], Batch [328/938], Loss: 0.5294739603996277\n",
      "Train: Epoch [23], Batch [329/938], Loss: 0.5768523812294006\n",
      "Train: Epoch [23], Batch [330/938], Loss: 0.33227086067199707\n",
      "Train: Epoch [23], Batch [331/938], Loss: 0.43580156564712524\n",
      "Train: Epoch [23], Batch [332/938], Loss: 0.40866586565971375\n",
      "Train: Epoch [23], Batch [333/938], Loss: 0.30811136960983276\n",
      "Train: Epoch [23], Batch [334/938], Loss: 0.598902702331543\n",
      "Train: Epoch [23], Batch [335/938], Loss: 0.5233007669448853\n",
      "Train: Epoch [23], Batch [336/938], Loss: 0.31131574511528015\n",
      "Train: Epoch [23], Batch [337/938], Loss: 0.46745604276657104\n",
      "Train: Epoch [23], Batch [338/938], Loss: 0.5621739029884338\n",
      "Train: Epoch [23], Batch [339/938], Loss: 0.39013367891311646\n",
      "Train: Epoch [23], Batch [340/938], Loss: 0.41305720806121826\n",
      "Train: Epoch [23], Batch [341/938], Loss: 0.40643322467803955\n",
      "Train: Epoch [23], Batch [342/938], Loss: 0.4487610161304474\n",
      "Train: Epoch [23], Batch [343/938], Loss: 0.39309072494506836\n",
      "Train: Epoch [23], Batch [344/938], Loss: 0.3279549479484558\n",
      "Train: Epoch [23], Batch [345/938], Loss: 0.32231566309928894\n",
      "Train: Epoch [23], Batch [346/938], Loss: 0.31502217054367065\n",
      "Train: Epoch [23], Batch [347/938], Loss: 0.6502086520195007\n",
      "Train: Epoch [23], Batch [348/938], Loss: 0.3896644711494446\n",
      "Train: Epoch [23], Batch [349/938], Loss: 0.3545927107334137\n",
      "Train: Epoch [23], Batch [350/938], Loss: 0.4854828119277954\n",
      "Train: Epoch [23], Batch [351/938], Loss: 0.422093003988266\n",
      "Train: Epoch [23], Batch [352/938], Loss: 0.4600503444671631\n",
      "Train: Epoch [23], Batch [353/938], Loss: 0.6929982900619507\n",
      "Train: Epoch [23], Batch [354/938], Loss: 0.39250826835632324\n",
      "Train: Epoch [23], Batch [355/938], Loss: 0.5812005400657654\n",
      "Train: Epoch [23], Batch [356/938], Loss: 0.3779258728027344\n",
      "Train: Epoch [23], Batch [357/938], Loss: 0.3224349021911621\n",
      "Train: Epoch [23], Batch [358/938], Loss: 0.5617261528968811\n",
      "Train: Epoch [23], Batch [359/938], Loss: 0.39552438259124756\n",
      "Train: Epoch [23], Batch [360/938], Loss: 0.2981826066970825\n",
      "Train: Epoch [23], Batch [361/938], Loss: 0.48768508434295654\n",
      "Train: Epoch [23], Batch [362/938], Loss: 0.49629366397857666\n",
      "Train: Epoch [23], Batch [363/938], Loss: 0.49198395013809204\n",
      "Train: Epoch [23], Batch [364/938], Loss: 0.5007315278053284\n",
      "Train: Epoch [23], Batch [365/938], Loss: 0.33627310395240784\n",
      "Train: Epoch [23], Batch [366/938], Loss: 0.45257559418678284\n",
      "Train: Epoch [23], Batch [367/938], Loss: 0.28369832038879395\n",
      "Train: Epoch [23], Batch [368/938], Loss: 0.4313241243362427\n",
      "Train: Epoch [23], Batch [369/938], Loss: 0.5061665177345276\n",
      "Train: Epoch [23], Batch [370/938], Loss: 0.4406362473964691\n",
      "Train: Epoch [23], Batch [371/938], Loss: 0.4347282350063324\n",
      "Train: Epoch [23], Batch [372/938], Loss: 0.3511051535606384\n",
      "Train: Epoch [23], Batch [373/938], Loss: 0.5278263092041016\n",
      "Train: Epoch [23], Batch [374/938], Loss: 0.5609458088874817\n",
      "Train: Epoch [23], Batch [375/938], Loss: 0.37181392312049866\n",
      "Train: Epoch [23], Batch [376/938], Loss: 0.5305227637290955\n",
      "Train: Epoch [23], Batch [377/938], Loss: 0.3339678943157196\n",
      "Train: Epoch [23], Batch [378/938], Loss: 0.37737277150154114\n",
      "Train: Epoch [23], Batch [379/938], Loss: 0.3254488706588745\n",
      "Train: Epoch [23], Batch [380/938], Loss: 0.404253214597702\n",
      "Train: Epoch [23], Batch [381/938], Loss: 0.5129077434539795\n",
      "Train: Epoch [23], Batch [382/938], Loss: 0.4700767397880554\n",
      "Train: Epoch [23], Batch [383/938], Loss: 0.4884975850582123\n",
      "Train: Epoch [23], Batch [384/938], Loss: 0.6497023105621338\n",
      "Train: Epoch [23], Batch [385/938], Loss: 0.5165427923202515\n",
      "Train: Epoch [23], Batch [386/938], Loss: 0.5870432257652283\n",
      "Train: Epoch [23], Batch [387/938], Loss: 0.5598840713500977\n",
      "Train: Epoch [23], Batch [388/938], Loss: 0.4090938866138458\n",
      "Train: Epoch [23], Batch [389/938], Loss: 0.6485971212387085\n",
      "Train: Epoch [23], Batch [390/938], Loss: 0.3625733554363251\n",
      "Train: Epoch [23], Batch [391/938], Loss: 0.3663405776023865\n",
      "Train: Epoch [23], Batch [392/938], Loss: 0.42363089323043823\n",
      "Train: Epoch [23], Batch [393/938], Loss: 0.48942500352859497\n",
      "Train: Epoch [23], Batch [394/938], Loss: 0.35519129037857056\n",
      "Train: Epoch [23], Batch [395/938], Loss: 0.24965877830982208\n",
      "Train: Epoch [23], Batch [396/938], Loss: 0.3898787200450897\n",
      "Train: Epoch [23], Batch [397/938], Loss: 0.4561999440193176\n",
      "Train: Epoch [23], Batch [398/938], Loss: 0.32948148250579834\n",
      "Train: Epoch [23], Batch [399/938], Loss: 0.3207717537879944\n",
      "Train: Epoch [23], Batch [400/938], Loss: 0.5082979798316956\n",
      "Train: Epoch [23], Batch [401/938], Loss: 0.5736662149429321\n",
      "Train: Epoch [23], Batch [402/938], Loss: 0.4307873845100403\n",
      "Train: Epoch [23], Batch [403/938], Loss: 0.4330437481403351\n",
      "Train: Epoch [23], Batch [404/938], Loss: 0.46676522493362427\n",
      "Train: Epoch [23], Batch [405/938], Loss: 0.23584088683128357\n",
      "Train: Epoch [23], Batch [406/938], Loss: 0.27570703625679016\n",
      "Train: Epoch [23], Batch [407/938], Loss: 0.1613008677959442\n",
      "Train: Epoch [23], Batch [408/938], Loss: 0.4665100574493408\n",
      "Train: Epoch [23], Batch [409/938], Loss: 0.3996920883655548\n",
      "Train: Epoch [23], Batch [410/938], Loss: 0.3649553656578064\n",
      "Train: Epoch [23], Batch [411/938], Loss: 0.5051661133766174\n",
      "Train: Epoch [23], Batch [412/938], Loss: 0.44938501715660095\n",
      "Train: Epoch [23], Batch [413/938], Loss: 0.6696519255638123\n",
      "Train: Epoch [23], Batch [414/938], Loss: 0.30462804436683655\n",
      "Train: Epoch [23], Batch [415/938], Loss: 0.5567061901092529\n",
      "Train: Epoch [23], Batch [416/938], Loss: 0.4012662470340729\n",
      "Train: Epoch [23], Batch [417/938], Loss: 0.4446316361427307\n",
      "Train: Epoch [23], Batch [418/938], Loss: 0.5612561702728271\n",
      "Train: Epoch [23], Batch [419/938], Loss: 0.26562628149986267\n",
      "Train: Epoch [23], Batch [420/938], Loss: 0.6322800517082214\n",
      "Train: Epoch [23], Batch [421/938], Loss: 0.426804780960083\n",
      "Train: Epoch [23], Batch [422/938], Loss: 0.3741896152496338\n",
      "Train: Epoch [23], Batch [423/938], Loss: 0.5245147943496704\n",
      "Train: Epoch [23], Batch [424/938], Loss: 0.37865692377090454\n",
      "Train: Epoch [23], Batch [425/938], Loss: 0.344841867685318\n",
      "Train: Epoch [23], Batch [426/938], Loss: 0.2898760139942169\n",
      "Train: Epoch [23], Batch [427/938], Loss: 0.5862300992012024\n",
      "Train: Epoch [23], Batch [428/938], Loss: 0.47024261951446533\n",
      "Train: Epoch [23], Batch [429/938], Loss: 0.35270363092422485\n",
      "Train: Epoch [23], Batch [430/938], Loss: 0.4146818518638611\n",
      "Train: Epoch [23], Batch [431/938], Loss: 0.38774389028549194\n",
      "Train: Epoch [23], Batch [432/938], Loss: 0.3211178779602051\n",
      "Train: Epoch [23], Batch [433/938], Loss: 0.2798314690589905\n",
      "Train: Epoch [23], Batch [434/938], Loss: 0.5137975811958313\n",
      "Train: Epoch [23], Batch [435/938], Loss: 0.6600696444511414\n",
      "Train: Epoch [23], Batch [436/938], Loss: 0.4211043119430542\n",
      "Train: Epoch [23], Batch [437/938], Loss: 0.3674297630786896\n",
      "Train: Epoch [23], Batch [438/938], Loss: 0.5007830262184143\n",
      "Train: Epoch [23], Batch [439/938], Loss: 0.42648330330848694\n",
      "Train: Epoch [23], Batch [440/938], Loss: 0.37871041893959045\n",
      "Train: Epoch [23], Batch [441/938], Loss: 0.3999282717704773\n",
      "Train: Epoch [23], Batch [442/938], Loss: 0.47895145416259766\n",
      "Train: Epoch [23], Batch [443/938], Loss: 0.42873528599739075\n",
      "Train: Epoch [23], Batch [444/938], Loss: 0.4162362217903137\n",
      "Train: Epoch [23], Batch [445/938], Loss: 0.49483364820480347\n",
      "Train: Epoch [23], Batch [446/938], Loss: 0.43285247683525085\n",
      "Train: Epoch [23], Batch [447/938], Loss: 0.3782745599746704\n",
      "Train: Epoch [23], Batch [448/938], Loss: 0.44177043437957764\n",
      "Train: Epoch [23], Batch [449/938], Loss: 0.3524356186389923\n",
      "Train: Epoch [23], Batch [450/938], Loss: 0.44360002875328064\n",
      "Train: Epoch [23], Batch [451/938], Loss: 0.24036788940429688\n",
      "Train: Epoch [23], Batch [452/938], Loss: 0.3063596487045288\n",
      "Train: Epoch [23], Batch [453/938], Loss: 0.2953365743160248\n",
      "Train: Epoch [23], Batch [454/938], Loss: 0.48113617300987244\n",
      "Train: Epoch [23], Batch [455/938], Loss: 0.470104843378067\n",
      "Train: Epoch [23], Batch [456/938], Loss: 0.2903873026371002\n",
      "Train: Epoch [23], Batch [457/938], Loss: 0.3800274431705475\n",
      "Train: Epoch [23], Batch [458/938], Loss: 0.40074580907821655\n",
      "Train: Epoch [23], Batch [459/938], Loss: 0.31307631731033325\n",
      "Train: Epoch [23], Batch [460/938], Loss: 0.3815914988517761\n",
      "Train: Epoch [23], Batch [461/938], Loss: 0.5281725525856018\n",
      "Train: Epoch [23], Batch [462/938], Loss: 0.5524559617042542\n",
      "Train: Epoch [23], Batch [463/938], Loss: 0.44458237290382385\n",
      "Train: Epoch [23], Batch [464/938], Loss: 0.36365196108818054\n",
      "Train: Epoch [23], Batch [465/938], Loss: 0.4691234827041626\n",
      "Train: Epoch [23], Batch [466/938], Loss: 0.38498806953430176\n",
      "Train: Epoch [23], Batch [467/938], Loss: 0.48413482308387756\n",
      "Train: Epoch [23], Batch [468/938], Loss: 0.442614883184433\n",
      "Train: Epoch [23], Batch [469/938], Loss: 0.24885430932044983\n",
      "Train: Epoch [23], Batch [470/938], Loss: 0.48914045095443726\n",
      "Train: Epoch [23], Batch [471/938], Loss: 0.32967814803123474\n",
      "Train: Epoch [23], Batch [472/938], Loss: 0.37114226818084717\n",
      "Train: Epoch [23], Batch [473/938], Loss: 0.4215700030326843\n",
      "Train: Epoch [23], Batch [474/938], Loss: 0.4149461090564728\n",
      "Train: Epoch [23], Batch [475/938], Loss: 0.368592232465744\n",
      "Train: Epoch [23], Batch [476/938], Loss: 0.5561627745628357\n",
      "Train: Epoch [23], Batch [477/938], Loss: 0.38271939754486084\n",
      "Train: Epoch [23], Batch [478/938], Loss: 0.5863463878631592\n",
      "Train: Epoch [23], Batch [479/938], Loss: 0.31889358162879944\n",
      "Train: Epoch [23], Batch [480/938], Loss: 0.4464716911315918\n",
      "Train: Epoch [23], Batch [481/938], Loss: 0.3689837157726288\n",
      "Train: Epoch [23], Batch [482/938], Loss: 0.21617361903190613\n",
      "Train: Epoch [23], Batch [483/938], Loss: 0.7846208810806274\n",
      "Train: Epoch [23], Batch [484/938], Loss: 0.27281761169433594\n",
      "Train: Epoch [23], Batch [485/938], Loss: 0.3574598431587219\n",
      "Train: Epoch [23], Batch [486/938], Loss: 0.31922292709350586\n",
      "Train: Epoch [23], Batch [487/938], Loss: 0.42606985569000244\n",
      "Train: Epoch [23], Batch [488/938], Loss: 0.31585732102394104\n",
      "Train: Epoch [23], Batch [489/938], Loss: 0.24015025794506073\n",
      "Train: Epoch [23], Batch [490/938], Loss: 0.3256072402000427\n",
      "Train: Epoch [23], Batch [491/938], Loss: 0.40818315744400024\n",
      "Train: Epoch [23], Batch [492/938], Loss: 0.606976330280304\n",
      "Train: Epoch [23], Batch [493/938], Loss: 0.5661399960517883\n",
      "Train: Epoch [23], Batch [494/938], Loss: 0.4488375782966614\n",
      "Train: Epoch [23], Batch [495/938], Loss: 0.4840528964996338\n",
      "Train: Epoch [23], Batch [496/938], Loss: 0.30075135827064514\n",
      "Train: Epoch [23], Batch [497/938], Loss: 0.3484860360622406\n",
      "Train: Epoch [23], Batch [498/938], Loss: 0.46311017870903015\n",
      "Train: Epoch [23], Batch [499/938], Loss: 0.46403324604034424\n",
      "Train: Epoch [23], Batch [500/938], Loss: 0.40081578493118286\n",
      "Train: Epoch [23], Batch [501/938], Loss: 0.4043315351009369\n",
      "Train: Epoch [23], Batch [502/938], Loss: 0.5394839644432068\n",
      "Train: Epoch [23], Batch [503/938], Loss: 0.4750928282737732\n",
      "Train: Epoch [23], Batch [504/938], Loss: 0.38623490929603577\n",
      "Train: Epoch [23], Batch [505/938], Loss: 0.4186495542526245\n",
      "Train: Epoch [23], Batch [506/938], Loss: 0.5169796943664551\n",
      "Train: Epoch [23], Batch [507/938], Loss: 0.3805117905139923\n",
      "Train: Epoch [23], Batch [508/938], Loss: 0.6160569787025452\n",
      "Train: Epoch [23], Batch [509/938], Loss: 0.5130630731582642\n",
      "Train: Epoch [23], Batch [510/938], Loss: 0.5641539692878723\n",
      "Train: Epoch [23], Batch [511/938], Loss: 0.46698588132858276\n",
      "Train: Epoch [23], Batch [512/938], Loss: 0.32581934332847595\n",
      "Train: Epoch [23], Batch [513/938], Loss: 0.3660118579864502\n",
      "Train: Epoch [23], Batch [514/938], Loss: 0.3809833228588104\n",
      "Train: Epoch [23], Batch [515/938], Loss: 0.2447194904088974\n",
      "Train: Epoch [23], Batch [516/938], Loss: 0.35879236459732056\n",
      "Train: Epoch [23], Batch [517/938], Loss: 0.29123997688293457\n",
      "Train: Epoch [23], Batch [518/938], Loss: 0.47966936230659485\n",
      "Train: Epoch [23], Batch [519/938], Loss: 0.4991832971572876\n",
      "Train: Epoch [23], Batch [520/938], Loss: 0.5030543208122253\n",
      "Train: Epoch [23], Batch [521/938], Loss: 0.3520444333553314\n",
      "Train: Epoch [23], Batch [522/938], Loss: 0.44295716285705566\n",
      "Train: Epoch [23], Batch [523/938], Loss: 0.4359123706817627\n",
      "Train: Epoch [23], Batch [524/938], Loss: 0.43693670630455017\n",
      "Train: Epoch [23], Batch [525/938], Loss: 0.42829155921936035\n",
      "Train: Epoch [23], Batch [526/938], Loss: 0.3160313665866852\n",
      "Train: Epoch [23], Batch [527/938], Loss: 0.37588730454444885\n",
      "Train: Epoch [23], Batch [528/938], Loss: 0.443938285112381\n",
      "Train: Epoch [23], Batch [529/938], Loss: 0.42582154273986816\n",
      "Train: Epoch [23], Batch [530/938], Loss: 0.6690789461135864\n",
      "Train: Epoch [23], Batch [531/938], Loss: 0.39421412348747253\n",
      "Train: Epoch [23], Batch [532/938], Loss: 0.7928618788719177\n",
      "Train: Epoch [23], Batch [533/938], Loss: 0.3825683891773224\n",
      "Train: Epoch [23], Batch [534/938], Loss: 0.5955773591995239\n",
      "Train: Epoch [23], Batch [535/938], Loss: 0.35905715823173523\n",
      "Train: Epoch [23], Batch [536/938], Loss: 0.4606150686740875\n",
      "Train: Epoch [23], Batch [537/938], Loss: 0.5225287079811096\n",
      "Train: Epoch [23], Batch [538/938], Loss: 0.3082443177700043\n",
      "Train: Epoch [23], Batch [539/938], Loss: 0.599108099937439\n",
      "Train: Epoch [23], Batch [540/938], Loss: 0.27786824107170105\n",
      "Train: Epoch [23], Batch [541/938], Loss: 0.33579355478286743\n",
      "Train: Epoch [23], Batch [542/938], Loss: 0.6037631034851074\n",
      "Train: Epoch [23], Batch [543/938], Loss: 0.4016941785812378\n",
      "Train: Epoch [23], Batch [544/938], Loss: 0.2745078206062317\n",
      "Train: Epoch [23], Batch [545/938], Loss: 0.45340317487716675\n",
      "Train: Epoch [23], Batch [546/938], Loss: 0.4135861396789551\n",
      "Train: Epoch [23], Batch [547/938], Loss: 0.5234309434890747\n",
      "Train: Epoch [23], Batch [548/938], Loss: 0.467268168926239\n",
      "Train: Epoch [23], Batch [549/938], Loss: 0.5480685234069824\n",
      "Train: Epoch [23], Batch [550/938], Loss: 0.31293821334838867\n",
      "Train: Epoch [23], Batch [551/938], Loss: 0.38830000162124634\n",
      "Train: Epoch [23], Batch [552/938], Loss: 0.8278340101242065\n",
      "Train: Epoch [23], Batch [553/938], Loss: 0.4373075067996979\n",
      "Train: Epoch [23], Batch [554/938], Loss: 0.5353283882141113\n",
      "Train: Epoch [23], Batch [555/938], Loss: 0.20390959084033966\n",
      "Train: Epoch [23], Batch [556/938], Loss: 0.4015090763568878\n",
      "Train: Epoch [23], Batch [557/938], Loss: 0.5055967569351196\n",
      "Train: Epoch [23], Batch [558/938], Loss: 0.30948376655578613\n",
      "Train: Epoch [23], Batch [559/938], Loss: 0.42421814799308777\n",
      "Train: Epoch [23], Batch [560/938], Loss: 0.40858474373817444\n",
      "Train: Epoch [23], Batch [561/938], Loss: 0.4977755546569824\n",
      "Train: Epoch [23], Batch [562/938], Loss: 0.3621240556240082\n",
      "Train: Epoch [23], Batch [563/938], Loss: 0.45014408230781555\n",
      "Train: Epoch [23], Batch [564/938], Loss: 0.5154653787612915\n",
      "Train: Epoch [23], Batch [565/938], Loss: 0.4133766293525696\n",
      "Train: Epoch [23], Batch [566/938], Loss: 0.44760769605636597\n",
      "Train: Epoch [23], Batch [567/938], Loss: 0.46019142866134644\n",
      "Train: Epoch [23], Batch [568/938], Loss: 0.5079788565635681\n",
      "Train: Epoch [23], Batch [569/938], Loss: 0.5295377969741821\n",
      "Train: Epoch [23], Batch [570/938], Loss: 0.38431525230407715\n",
      "Train: Epoch [23], Batch [571/938], Loss: 0.5821306109428406\n",
      "Train: Epoch [23], Batch [572/938], Loss: 0.4891735017299652\n",
      "Train: Epoch [23], Batch [573/938], Loss: 0.22235189378261566\n",
      "Train: Epoch [23], Batch [574/938], Loss: 0.3974170386791229\n",
      "Train: Epoch [23], Batch [575/938], Loss: 0.36533036828041077\n",
      "Train: Epoch [23], Batch [576/938], Loss: 0.20626558363437653\n",
      "Train: Epoch [23], Batch [577/938], Loss: 0.3353951871395111\n",
      "Train: Epoch [23], Batch [578/938], Loss: 0.39518988132476807\n",
      "Train: Epoch [23], Batch [579/938], Loss: 0.6697380542755127\n",
      "Train: Epoch [23], Batch [580/938], Loss: 0.31720879673957825\n",
      "Train: Epoch [23], Batch [581/938], Loss: 0.2800882160663605\n",
      "Train: Epoch [23], Batch [582/938], Loss: 0.3799825608730316\n",
      "Train: Epoch [23], Batch [583/938], Loss: 0.6175506711006165\n",
      "Train: Epoch [23], Batch [584/938], Loss: 0.4006819725036621\n",
      "Train: Epoch [23], Batch [585/938], Loss: 0.3014642596244812\n",
      "Train: Epoch [23], Batch [586/938], Loss: 0.4557707607746124\n",
      "Train: Epoch [23], Batch [587/938], Loss: 0.30448493361473083\n",
      "Train: Epoch [23], Batch [588/938], Loss: 0.4660275876522064\n",
      "Train: Epoch [23], Batch [589/938], Loss: 0.8135761022567749\n",
      "Train: Epoch [23], Batch [590/938], Loss: 0.6011018753051758\n",
      "Train: Epoch [23], Batch [591/938], Loss: 0.45189031958580017\n",
      "Train: Epoch [23], Batch [592/938], Loss: 0.33936837315559387\n",
      "Train: Epoch [23], Batch [593/938], Loss: 0.5187112092971802\n",
      "Train: Epoch [23], Batch [594/938], Loss: 0.23810938000679016\n",
      "Train: Epoch [23], Batch [595/938], Loss: 0.3460472524166107\n",
      "Train: Epoch [23], Batch [596/938], Loss: 0.5019747018814087\n",
      "Train: Epoch [23], Batch [597/938], Loss: 0.34676429629325867\n",
      "Train: Epoch [23], Batch [598/938], Loss: 0.5022101402282715\n",
      "Train: Epoch [23], Batch [599/938], Loss: 0.45516809821128845\n",
      "Train: Epoch [23], Batch [600/938], Loss: 0.46290087699890137\n",
      "Train: Epoch [23], Batch [601/938], Loss: 0.5759286880493164\n",
      "Train: Epoch [23], Batch [602/938], Loss: 0.37704968452453613\n",
      "Train: Epoch [23], Batch [603/938], Loss: 0.3554415702819824\n",
      "Train: Epoch [23], Batch [604/938], Loss: 0.3331504464149475\n",
      "Train: Epoch [23], Batch [605/938], Loss: 0.3742367923259735\n",
      "Train: Epoch [23], Batch [606/938], Loss: 0.5966688394546509\n",
      "Train: Epoch [23], Batch [607/938], Loss: 0.41059789061546326\n",
      "Train: Epoch [23], Batch [608/938], Loss: 0.5248350501060486\n",
      "Train: Epoch [23], Batch [609/938], Loss: 0.41574299335479736\n",
      "Train: Epoch [23], Batch [610/938], Loss: 0.6451797485351562\n",
      "Train: Epoch [23], Batch [611/938], Loss: 0.32615071535110474\n",
      "Train: Epoch [23], Batch [612/938], Loss: 0.4145635962486267\n",
      "Train: Epoch [23], Batch [613/938], Loss: 0.4718100428581238\n",
      "Train: Epoch [23], Batch [614/938], Loss: 0.42118749022483826\n",
      "Train: Epoch [23], Batch [615/938], Loss: 0.5244276523590088\n",
      "Train: Epoch [23], Batch [616/938], Loss: 0.3799256980419159\n",
      "Train: Epoch [23], Batch [617/938], Loss: 0.5009728670120239\n",
      "Train: Epoch [23], Batch [618/938], Loss: 0.41599082946777344\n",
      "Train: Epoch [23], Batch [619/938], Loss: 0.3167845904827118\n",
      "Train: Epoch [23], Batch [620/938], Loss: 0.6498185992240906\n",
      "Train: Epoch [23], Batch [621/938], Loss: 0.3578079342842102\n",
      "Train: Epoch [23], Batch [622/938], Loss: 0.3654041290283203\n",
      "Train: Epoch [23], Batch [623/938], Loss: 0.413463830947876\n",
      "Train: Epoch [23], Batch [624/938], Loss: 0.31799280643463135\n",
      "Train: Epoch [23], Batch [625/938], Loss: 0.3315414786338806\n",
      "Train: Epoch [23], Batch [626/938], Loss: 0.5084243416786194\n",
      "Train: Epoch [23], Batch [627/938], Loss: 0.3966761827468872\n",
      "Train: Epoch [23], Batch [628/938], Loss: 0.28587719798088074\n",
      "Train: Epoch [23], Batch [629/938], Loss: 0.442072331905365\n",
      "Train: Epoch [23], Batch [630/938], Loss: 0.4984121322631836\n",
      "Train: Epoch [23], Batch [631/938], Loss: 0.2748022973537445\n",
      "Train: Epoch [23], Batch [632/938], Loss: 0.46550121903419495\n",
      "Train: Epoch [23], Batch [633/938], Loss: 0.4581671357154846\n",
      "Train: Epoch [23], Batch [634/938], Loss: 0.663415253162384\n",
      "Train: Epoch [23], Batch [635/938], Loss: 0.4335237443447113\n",
      "Train: Epoch [23], Batch [636/938], Loss: 0.47168460488319397\n",
      "Train: Epoch [23], Batch [637/938], Loss: 0.3720228374004364\n",
      "Train: Epoch [23], Batch [638/938], Loss: 0.4150201678276062\n",
      "Train: Epoch [23], Batch [639/938], Loss: 0.4524270296096802\n",
      "Train: Epoch [23], Batch [640/938], Loss: 0.4283796548843384\n",
      "Train: Epoch [23], Batch [641/938], Loss: 0.40096715092658997\n",
      "Train: Epoch [23], Batch [642/938], Loss: 0.39813247323036194\n",
      "Train: Epoch [23], Batch [643/938], Loss: 0.4398561120033264\n",
      "Train: Epoch [23], Batch [644/938], Loss: 0.42286235094070435\n",
      "Train: Epoch [23], Batch [645/938], Loss: 0.525619626045227\n",
      "Train: Epoch [23], Batch [646/938], Loss: 0.6476916074752808\n",
      "Train: Epoch [23], Batch [647/938], Loss: 0.23834377527236938\n",
      "Train: Epoch [23], Batch [648/938], Loss: 0.42009925842285156\n",
      "Train: Epoch [23], Batch [649/938], Loss: 0.3275011479854584\n",
      "Train: Epoch [23], Batch [650/938], Loss: 0.44600799679756165\n",
      "Train: Epoch [23], Batch [651/938], Loss: 0.5418066382408142\n",
      "Train: Epoch [23], Batch [652/938], Loss: 0.42731255292892456\n",
      "Train: Epoch [23], Batch [653/938], Loss: 0.3533681631088257\n",
      "Train: Epoch [23], Batch [654/938], Loss: 0.27504271268844604\n",
      "Train: Epoch [23], Batch [655/938], Loss: 0.28836992383003235\n",
      "Train: Epoch [23], Batch [656/938], Loss: 0.46006447076797485\n",
      "Train: Epoch [23], Batch [657/938], Loss: 0.4845133423805237\n",
      "Train: Epoch [23], Batch [658/938], Loss: 0.3393690884113312\n",
      "Train: Epoch [23], Batch [659/938], Loss: 0.27669060230255127\n",
      "Train: Epoch [23], Batch [660/938], Loss: 0.2894193232059479\n",
      "Train: Epoch [23], Batch [661/938], Loss: 0.24449914693832397\n",
      "Train: Epoch [23], Batch [662/938], Loss: 0.38776177167892456\n",
      "Train: Epoch [23], Batch [663/938], Loss: 0.48724305629730225\n",
      "Train: Epoch [23], Batch [664/938], Loss: 0.41354691982269287\n",
      "Train: Epoch [23], Batch [665/938], Loss: 0.47924157977104187\n",
      "Train: Epoch [23], Batch [666/938], Loss: 0.37123367190361023\n",
      "Train: Epoch [23], Batch [667/938], Loss: 0.4313044548034668\n",
      "Train: Epoch [23], Batch [668/938], Loss: 0.4557039439678192\n",
      "Train: Epoch [23], Batch [669/938], Loss: 0.5830668210983276\n",
      "Train: Epoch [23], Batch [670/938], Loss: 0.3492017090320587\n",
      "Train: Epoch [23], Batch [671/938], Loss: 0.3091011643409729\n",
      "Train: Epoch [23], Batch [672/938], Loss: 0.34382495284080505\n",
      "Train: Epoch [23], Batch [673/938], Loss: 0.702129602432251\n",
      "Train: Epoch [23], Batch [674/938], Loss: 0.5033732056617737\n",
      "Train: Epoch [23], Batch [675/938], Loss: 0.442367285490036\n",
      "Train: Epoch [23], Batch [676/938], Loss: 0.577002763748169\n",
      "Train: Epoch [23], Batch [677/938], Loss: 0.4579419493675232\n",
      "Train: Epoch [23], Batch [678/938], Loss: 0.3801276385784149\n",
      "Train: Epoch [23], Batch [679/938], Loss: 0.48740237951278687\n",
      "Train: Epoch [23], Batch [680/938], Loss: 0.47019004821777344\n",
      "Train: Epoch [23], Batch [681/938], Loss: 0.5274844169616699\n",
      "Train: Epoch [23], Batch [682/938], Loss: 0.35201579332351685\n",
      "Train: Epoch [23], Batch [683/938], Loss: 0.6771372556686401\n",
      "Train: Epoch [23], Batch [684/938], Loss: 0.37697798013687134\n",
      "Train: Epoch [23], Batch [685/938], Loss: 0.362003892660141\n",
      "Train: Epoch [23], Batch [686/938], Loss: 0.6009607911109924\n",
      "Train: Epoch [23], Batch [687/938], Loss: 0.6733625531196594\n",
      "Train: Epoch [23], Batch [688/938], Loss: 0.3590831756591797\n",
      "Train: Epoch [23], Batch [689/938], Loss: 0.4184315800666809\n",
      "Train: Epoch [23], Batch [690/938], Loss: 0.47607123851776123\n",
      "Train: Epoch [23], Batch [691/938], Loss: 0.6311917304992676\n",
      "Train: Epoch [23], Batch [692/938], Loss: 0.44448938965797424\n",
      "Train: Epoch [23], Batch [693/938], Loss: 0.3476162552833557\n",
      "Train: Epoch [23], Batch [694/938], Loss: 0.3720478415489197\n",
      "Train: Epoch [23], Batch [695/938], Loss: 0.7670285105705261\n",
      "Train: Epoch [23], Batch [696/938], Loss: 0.46593326330184937\n",
      "Train: Epoch [23], Batch [697/938], Loss: 0.28793710470199585\n",
      "Train: Epoch [23], Batch [698/938], Loss: 0.6227573156356812\n",
      "Train: Epoch [23], Batch [699/938], Loss: 0.2576269209384918\n",
      "Train: Epoch [23], Batch [700/938], Loss: 0.27366214990615845\n",
      "Train: Epoch [23], Batch [701/938], Loss: 0.4010551869869232\n",
      "Train: Epoch [23], Batch [702/938], Loss: 0.40549784898757935\n",
      "Train: Epoch [23], Batch [703/938], Loss: 0.4357415735721588\n",
      "Train: Epoch [23], Batch [704/938], Loss: 0.5114309787750244\n",
      "Train: Epoch [23], Batch [705/938], Loss: 0.6199713349342346\n",
      "Train: Epoch [23], Batch [706/938], Loss: 0.4832867383956909\n",
      "Train: Epoch [23], Batch [707/938], Loss: 0.6117783188819885\n",
      "Train: Epoch [23], Batch [708/938], Loss: 0.42307135462760925\n",
      "Train: Epoch [23], Batch [709/938], Loss: 0.4674457013607025\n",
      "Train: Epoch [23], Batch [710/938], Loss: 0.2654911279678345\n",
      "Train: Epoch [23], Batch [711/938], Loss: 0.42939847707748413\n",
      "Train: Epoch [23], Batch [712/938], Loss: 0.3896498680114746\n",
      "Train: Epoch [23], Batch [713/938], Loss: 0.41245734691619873\n",
      "Train: Epoch [23], Batch [714/938], Loss: 0.38524168729782104\n",
      "Train: Epoch [23], Batch [715/938], Loss: 0.2990620732307434\n",
      "Train: Epoch [23], Batch [716/938], Loss: 0.5072161555290222\n",
      "Train: Epoch [23], Batch [717/938], Loss: 0.5362420678138733\n",
      "Train: Epoch [23], Batch [718/938], Loss: 0.5128644108772278\n",
      "Train: Epoch [23], Batch [719/938], Loss: 0.25089311599731445\n",
      "Train: Epoch [23], Batch [720/938], Loss: 0.4256194531917572\n",
      "Train: Epoch [23], Batch [721/938], Loss: 0.3519117832183838\n",
      "Train: Epoch [23], Batch [722/938], Loss: 0.333538681268692\n",
      "Train: Epoch [23], Batch [723/938], Loss: 0.40089425444602966\n",
      "Train: Epoch [23], Batch [724/938], Loss: 0.31086358428001404\n",
      "Train: Epoch [23], Batch [725/938], Loss: 0.5171072483062744\n",
      "Train: Epoch [23], Batch [726/938], Loss: 0.6663473844528198\n",
      "Train: Epoch [23], Batch [727/938], Loss: 0.30773815512657166\n",
      "Train: Epoch [23], Batch [728/938], Loss: 0.4235302209854126\n",
      "Train: Epoch [23], Batch [729/938], Loss: 0.6995128393173218\n",
      "Train: Epoch [23], Batch [730/938], Loss: 0.7005516886711121\n",
      "Train: Epoch [23], Batch [731/938], Loss: 0.41213274002075195\n",
      "Train: Epoch [23], Batch [732/938], Loss: 0.3436528444290161\n",
      "Train: Epoch [23], Batch [733/938], Loss: 0.5293338894844055\n",
      "Train: Epoch [23], Batch [734/938], Loss: 0.5372027158737183\n",
      "Train: Epoch [23], Batch [735/938], Loss: 0.40113940834999084\n",
      "Train: Epoch [23], Batch [736/938], Loss: 0.32619526982307434\n",
      "Train: Epoch [23], Batch [737/938], Loss: 0.2747814357280731\n",
      "Train: Epoch [23], Batch [738/938], Loss: 0.3480161130428314\n",
      "Train: Epoch [23], Batch [739/938], Loss: 0.3473682701587677\n",
      "Train: Epoch [23], Batch [740/938], Loss: 0.6382724642753601\n",
      "Train: Epoch [23], Batch [741/938], Loss: 0.3635408282279968\n",
      "Train: Epoch [23], Batch [742/938], Loss: 0.42182657122612\n",
      "Train: Epoch [23], Batch [743/938], Loss: 0.430978000164032\n",
      "Train: Epoch [23], Batch [744/938], Loss: 0.26782041788101196\n",
      "Train: Epoch [23], Batch [745/938], Loss: 0.39814239740371704\n",
      "Train: Epoch [23], Batch [746/938], Loss: 0.5209779739379883\n",
      "Train: Epoch [23], Batch [747/938], Loss: 0.348448783159256\n",
      "Train: Epoch [23], Batch [748/938], Loss: 0.7256206274032593\n",
      "Train: Epoch [23], Batch [749/938], Loss: 0.3587057590484619\n",
      "Train: Epoch [23], Batch [750/938], Loss: 0.39792707562446594\n",
      "Train: Epoch [23], Batch [751/938], Loss: 0.2691899538040161\n",
      "Train: Epoch [23], Batch [752/938], Loss: 0.44480207562446594\n",
      "Train: Epoch [23], Batch [753/938], Loss: 0.47878870368003845\n",
      "Train: Epoch [23], Batch [754/938], Loss: 0.3501814901828766\n",
      "Train: Epoch [23], Batch [755/938], Loss: 0.4457782804965973\n",
      "Train: Epoch [23], Batch [756/938], Loss: 0.36629369854927063\n",
      "Train: Epoch [23], Batch [757/938], Loss: 0.4212285578250885\n",
      "Train: Epoch [23], Batch [758/938], Loss: 0.5999124050140381\n",
      "Train: Epoch [23], Batch [759/938], Loss: 0.5530664920806885\n",
      "Train: Epoch [23], Batch [760/938], Loss: 0.33787593245506287\n",
      "Train: Epoch [23], Batch [761/938], Loss: 0.5605193376541138\n",
      "Train: Epoch [23], Batch [762/938], Loss: 0.3752167224884033\n",
      "Train: Epoch [23], Batch [763/938], Loss: 0.41903746128082275\n",
      "Train: Epoch [23], Batch [764/938], Loss: 0.3638516068458557\n",
      "Train: Epoch [23], Batch [765/938], Loss: 0.434876024723053\n",
      "Train: Epoch [23], Batch [766/938], Loss: 0.6415252089500427\n",
      "Train: Epoch [23], Batch [767/938], Loss: 0.46514907479286194\n",
      "Train: Epoch [23], Batch [768/938], Loss: 0.2739593982696533\n",
      "Train: Epoch [23], Batch [769/938], Loss: 0.3555188775062561\n",
      "Train: Epoch [23], Batch [770/938], Loss: 0.4007139801979065\n",
      "Train: Epoch [23], Batch [771/938], Loss: 0.2829355001449585\n",
      "Train: Epoch [23], Batch [772/938], Loss: 0.574000358581543\n",
      "Train: Epoch [23], Batch [773/938], Loss: 0.5124199986457825\n",
      "Train: Epoch [23], Batch [774/938], Loss: 0.39609941840171814\n",
      "Train: Epoch [23], Batch [775/938], Loss: 0.47812119126319885\n",
      "Train: Epoch [23], Batch [776/938], Loss: 0.5237095355987549\n",
      "Train: Epoch [23], Batch [777/938], Loss: 0.5318155884742737\n",
      "Train: Epoch [23], Batch [778/938], Loss: 0.4890673756599426\n",
      "Train: Epoch [23], Batch [779/938], Loss: 0.4315048158168793\n",
      "Train: Epoch [23], Batch [780/938], Loss: 0.4452233910560608\n",
      "Train: Epoch [23], Batch [781/938], Loss: 0.5708877444267273\n",
      "Train: Epoch [23], Batch [782/938], Loss: 0.36638781428337097\n",
      "Train: Epoch [23], Batch [783/938], Loss: 0.5055232048034668\n",
      "Train: Epoch [23], Batch [784/938], Loss: 0.2761341631412506\n",
      "Train: Epoch [23], Batch [785/938], Loss: 0.38959652185440063\n",
      "Train: Epoch [23], Batch [786/938], Loss: 0.5285170078277588\n",
      "Train: Epoch [23], Batch [787/938], Loss: 0.3738354444503784\n",
      "Train: Epoch [23], Batch [788/938], Loss: 0.4315738081932068\n",
      "Train: Epoch [23], Batch [789/938], Loss: 0.43399548530578613\n",
      "Train: Epoch [23], Batch [790/938], Loss: 0.44313502311706543\n",
      "Train: Epoch [23], Batch [791/938], Loss: 0.4505307972431183\n",
      "Train: Epoch [23], Batch [792/938], Loss: 0.37730979919433594\n",
      "Train: Epoch [23], Batch [793/938], Loss: 0.3977370858192444\n",
      "Train: Epoch [23], Batch [794/938], Loss: 0.4169645309448242\n",
      "Train: Epoch [23], Batch [795/938], Loss: 0.5169636011123657\n",
      "Train: Epoch [23], Batch [796/938], Loss: 0.37799885869026184\n",
      "Train: Epoch [23], Batch [797/938], Loss: 0.5697677731513977\n",
      "Train: Epoch [23], Batch [798/938], Loss: 0.3831580877304077\n",
      "Train: Epoch [23], Batch [799/938], Loss: 0.305741548538208\n",
      "Train: Epoch [23], Batch [800/938], Loss: 0.4309941530227661\n",
      "Train: Epoch [23], Batch [801/938], Loss: 0.42955178022384644\n",
      "Train: Epoch [23], Batch [802/938], Loss: 0.47684207558631897\n",
      "Train: Epoch [23], Batch [803/938], Loss: 0.2724279463291168\n",
      "Train: Epoch [23], Batch [804/938], Loss: 0.44349488615989685\n",
      "Train: Epoch [23], Batch [805/938], Loss: 0.4843508005142212\n",
      "Train: Epoch [23], Batch [806/938], Loss: 0.4952768087387085\n",
      "Train: Epoch [23], Batch [807/938], Loss: 0.32492753863334656\n",
      "Train: Epoch [23], Batch [808/938], Loss: 0.3498559296131134\n",
      "Train: Epoch [23], Batch [809/938], Loss: 0.47670525312423706\n",
      "Train: Epoch [23], Batch [810/938], Loss: 0.30156221985816956\n",
      "Train: Epoch [23], Batch [811/938], Loss: 0.48753926157951355\n",
      "Train: Epoch [23], Batch [812/938], Loss: 0.5378034710884094\n",
      "Train: Epoch [23], Batch [813/938], Loss: 0.25914648175239563\n",
      "Train: Epoch [23], Batch [814/938], Loss: 0.6361762285232544\n",
      "Train: Epoch [23], Batch [815/938], Loss: 0.5940561890602112\n",
      "Train: Epoch [23], Batch [816/938], Loss: 0.48676109313964844\n",
      "Train: Epoch [23], Batch [817/938], Loss: 0.27769410610198975\n",
      "Train: Epoch [23], Batch [818/938], Loss: 0.28247806429862976\n",
      "Train: Epoch [23], Batch [819/938], Loss: 0.44893378019332886\n",
      "Train: Epoch [23], Batch [820/938], Loss: 0.4101729989051819\n",
      "Train: Epoch [23], Batch [821/938], Loss: 0.397993266582489\n",
      "Train: Epoch [23], Batch [822/938], Loss: 0.6050820350646973\n",
      "Train: Epoch [23], Batch [823/938], Loss: 0.3393508493900299\n",
      "Train: Epoch [23], Batch [824/938], Loss: 0.3326912224292755\n",
      "Train: Epoch [23], Batch [825/938], Loss: 0.3199516534805298\n",
      "Train: Epoch [23], Batch [826/938], Loss: 0.4170116186141968\n",
      "Train: Epoch [23], Batch [827/938], Loss: 0.3367266058921814\n",
      "Train: Epoch [23], Batch [828/938], Loss: 0.4045078456401825\n",
      "Train: Epoch [23], Batch [829/938], Loss: 0.4279211163520813\n",
      "Train: Epoch [23], Batch [830/938], Loss: 0.5633363723754883\n",
      "Train: Epoch [23], Batch [831/938], Loss: 0.3907977342605591\n",
      "Train: Epoch [23], Batch [832/938], Loss: 0.30847299098968506\n",
      "Train: Epoch [23], Batch [833/938], Loss: 0.625612735748291\n",
      "Train: Epoch [23], Batch [834/938], Loss: 0.4713549017906189\n",
      "Train: Epoch [23], Batch [835/938], Loss: 0.5038224458694458\n",
      "Train: Epoch [23], Batch [836/938], Loss: 0.5178033709526062\n",
      "Train: Epoch [23], Batch [837/938], Loss: 0.43549400568008423\n",
      "Train: Epoch [23], Batch [838/938], Loss: 0.43193137645721436\n",
      "Train: Epoch [23], Batch [839/938], Loss: 0.4232482314109802\n",
      "Train: Epoch [23], Batch [840/938], Loss: 0.5950279831886292\n",
      "Train: Epoch [23], Batch [841/938], Loss: 0.5517699122428894\n",
      "Train: Epoch [23], Batch [842/938], Loss: 0.4790538549423218\n",
      "Train: Epoch [23], Batch [843/938], Loss: 0.38163310289382935\n",
      "Train: Epoch [23], Batch [844/938], Loss: 0.4872589707374573\n",
      "Train: Epoch [23], Batch [845/938], Loss: 0.5601266622543335\n",
      "Train: Epoch [23], Batch [846/938], Loss: 0.5352880954742432\n",
      "Train: Epoch [23], Batch [847/938], Loss: 0.43398427963256836\n",
      "Train: Epoch [23], Batch [848/938], Loss: 0.4662370979785919\n",
      "Train: Epoch [23], Batch [849/938], Loss: 0.4712226390838623\n",
      "Train: Epoch [23], Batch [850/938], Loss: 0.4011298716068268\n",
      "Train: Epoch [23], Batch [851/938], Loss: 0.3776244521141052\n",
      "Train: Epoch [23], Batch [852/938], Loss: 0.3541300296783447\n",
      "Train: Epoch [23], Batch [853/938], Loss: 0.2933444380760193\n",
      "Train: Epoch [23], Batch [854/938], Loss: 0.3870164155960083\n",
      "Train: Epoch [23], Batch [855/938], Loss: 0.39576438069343567\n",
      "Train: Epoch [23], Batch [856/938], Loss: 0.4489256739616394\n",
      "Train: Epoch [23], Batch [857/938], Loss: 0.6781172752380371\n",
      "Train: Epoch [23], Batch [858/938], Loss: 0.6701697707176208\n",
      "Train: Epoch [23], Batch [859/938], Loss: 0.4154908359050751\n",
      "Train: Epoch [23], Batch [860/938], Loss: 0.26730629801750183\n",
      "Train: Epoch [23], Batch [861/938], Loss: 0.45803672075271606\n",
      "Train: Epoch [23], Batch [862/938], Loss: 0.39926403760910034\n",
      "Train: Epoch [23], Batch [863/938], Loss: 0.3655630052089691\n",
      "Train: Epoch [23], Batch [864/938], Loss: 0.38914814591407776\n",
      "Train: Epoch [23], Batch [865/938], Loss: 0.3778281807899475\n",
      "Train: Epoch [23], Batch [866/938], Loss: 0.4360145330429077\n",
      "Train: Epoch [23], Batch [867/938], Loss: 0.3047982156276703\n",
      "Train: Epoch [23], Batch [868/938], Loss: 0.5028762221336365\n",
      "Train: Epoch [23], Batch [869/938], Loss: 0.7970377206802368\n",
      "Train: Epoch [23], Batch [870/938], Loss: 0.4025900661945343\n",
      "Train: Epoch [23], Batch [871/938], Loss: 0.3503630459308624\n",
      "Train: Epoch [23], Batch [872/938], Loss: 0.345111221075058\n",
      "Train: Epoch [23], Batch [873/938], Loss: 0.3545854687690735\n",
      "Train: Epoch [23], Batch [874/938], Loss: 0.28861477971076965\n",
      "Train: Epoch [23], Batch [875/938], Loss: 0.39674049615859985\n",
      "Train: Epoch [23], Batch [876/938], Loss: 0.4135635793209076\n",
      "Train: Epoch [23], Batch [877/938], Loss: 0.40340545773506165\n",
      "Train: Epoch [23], Batch [878/938], Loss: 0.4072108268737793\n",
      "Train: Epoch [23], Batch [879/938], Loss: 0.49054211378097534\n",
      "Train: Epoch [23], Batch [880/938], Loss: 0.6050839424133301\n",
      "Train: Epoch [23], Batch [881/938], Loss: 0.36287692189216614\n",
      "Train: Epoch [23], Batch [882/938], Loss: 0.45674771070480347\n",
      "Train: Epoch [23], Batch [883/938], Loss: 0.5086489319801331\n",
      "Train: Epoch [23], Batch [884/938], Loss: 0.47785505652427673\n",
      "Train: Epoch [23], Batch [885/938], Loss: 0.39516955614089966\n",
      "Train: Epoch [23], Batch [886/938], Loss: 0.442352294921875\n",
      "Train: Epoch [23], Batch [887/938], Loss: 0.4491519331932068\n",
      "Train: Epoch [23], Batch [888/938], Loss: 0.554029643535614\n",
      "Train: Epoch [23], Batch [889/938], Loss: 0.37795183062553406\n",
      "Train: Epoch [23], Batch [890/938], Loss: 0.47228413820266724\n",
      "Train: Epoch [23], Batch [891/938], Loss: 0.43852972984313965\n",
      "Train: Epoch [23], Batch [892/938], Loss: 0.42684394121170044\n",
      "Train: Epoch [23], Batch [893/938], Loss: 0.3195108473300934\n",
      "Train: Epoch [23], Batch [894/938], Loss: 0.4273110628128052\n",
      "Train: Epoch [23], Batch [895/938], Loss: 0.28490516543388367\n",
      "Train: Epoch [23], Batch [896/938], Loss: 0.404788613319397\n",
      "Train: Epoch [23], Batch [897/938], Loss: 0.38923680782318115\n",
      "Train: Epoch [23], Batch [898/938], Loss: 0.3681574761867523\n",
      "Train: Epoch [23], Batch [899/938], Loss: 0.3744617700576782\n",
      "Train: Epoch [23], Batch [900/938], Loss: 0.3861224055290222\n",
      "Train: Epoch [23], Batch [901/938], Loss: 0.47312864661216736\n",
      "Train: Epoch [23], Batch [902/938], Loss: 0.46245962381362915\n",
      "Train: Epoch [23], Batch [903/938], Loss: 0.343936562538147\n",
      "Train: Epoch [23], Batch [904/938], Loss: 0.3211590647697449\n",
      "Train: Epoch [23], Batch [905/938], Loss: 0.39781811833381653\n",
      "Train: Epoch [23], Batch [906/938], Loss: 0.32280489802360535\n",
      "Train: Epoch [23], Batch [907/938], Loss: 0.5781711339950562\n",
      "Train: Epoch [23], Batch [908/938], Loss: 0.39153289794921875\n",
      "Train: Epoch [23], Batch [909/938], Loss: 0.2878980040550232\n",
      "Train: Epoch [23], Batch [910/938], Loss: 0.4409351348876953\n",
      "Train: Epoch [23], Batch [911/938], Loss: 0.45475152134895325\n",
      "Train: Epoch [23], Batch [912/938], Loss: 0.6849755048751831\n",
      "Train: Epoch [23], Batch [913/938], Loss: 0.38977986574172974\n",
      "Train: Epoch [23], Batch [914/938], Loss: 0.435070663690567\n",
      "Train: Epoch [23], Batch [915/938], Loss: 0.3817237615585327\n",
      "Train: Epoch [23], Batch [916/938], Loss: 0.368910551071167\n",
      "Train: Epoch [23], Batch [917/938], Loss: 0.3681199550628662\n",
      "Train: Epoch [23], Batch [918/938], Loss: 0.43777674436569214\n",
      "Train: Epoch [23], Batch [919/938], Loss: 0.37010830640792847\n",
      "Train: Epoch [23], Batch [920/938], Loss: 0.34164679050445557\n",
      "Train: Epoch [23], Batch [921/938], Loss: 0.3438040018081665\n",
      "Train: Epoch [23], Batch [922/938], Loss: 0.5374336838722229\n",
      "Train: Epoch [23], Batch [923/938], Loss: 0.40318921208381653\n",
      "Train: Epoch [23], Batch [924/938], Loss: 0.2557987868785858\n",
      "Train: Epoch [23], Batch [925/938], Loss: 0.6783575415611267\n",
      "Train: Epoch [23], Batch [926/938], Loss: 0.3445322811603546\n",
      "Train: Epoch [23], Batch [927/938], Loss: 0.5207921266555786\n",
      "Train: Epoch [23], Batch [928/938], Loss: 0.3780125379562378\n",
      "Train: Epoch [23], Batch [929/938], Loss: 0.3861519694328308\n",
      "Train: Epoch [23], Batch [930/938], Loss: 0.4365600645542145\n",
      "Train: Epoch [23], Batch [931/938], Loss: 0.6437362432479858\n",
      "Train: Epoch [23], Batch [932/938], Loss: 0.6320279836654663\n",
      "Train: Epoch [23], Batch [933/938], Loss: 0.43582025170326233\n",
      "Train: Epoch [23], Batch [934/938], Loss: 0.42790165543556213\n",
      "Train: Epoch [23], Batch [935/938], Loss: 0.37812191247940063\n",
      "Train: Epoch [23], Batch [936/938], Loss: 0.3368559777736664\n",
      "Train: Epoch [23], Batch [937/938], Loss: 0.2676084637641907\n",
      "Train: Epoch [23], Batch [938/938], Loss: 0.48691055178642273\n",
      "Accuracy of train set: 0.8486833333333333\n",
      "Validation: Epoch [23], Batch [1/938], Loss: 0.5174348950386047\n",
      "Validation: Epoch [23], Batch [2/938], Loss: 0.44754037261009216\n",
      "Validation: Epoch [23], Batch [3/938], Loss: 0.6163772344589233\n",
      "Validation: Epoch [23], Batch [4/938], Loss: 0.7999618053436279\n",
      "Validation: Epoch [23], Batch [5/938], Loss: 0.4158767759799957\n",
      "Validation: Epoch [23], Batch [6/938], Loss: 0.3199651837348938\n",
      "Validation: Epoch [23], Batch [7/938], Loss: 0.3173215687274933\n",
      "Validation: Epoch [23], Batch [8/938], Loss: 0.5714182257652283\n",
      "Validation: Epoch [23], Batch [9/938], Loss: 0.5545846223831177\n",
      "Validation: Epoch [23], Batch [10/938], Loss: 0.47716066241264343\n",
      "Validation: Epoch [23], Batch [11/938], Loss: 0.5828114151954651\n",
      "Validation: Epoch [23], Batch [12/938], Loss: 0.44845160841941833\n",
      "Validation: Epoch [23], Batch [13/938], Loss: 0.34519335627555847\n",
      "Validation: Epoch [23], Batch [14/938], Loss: 0.2829091250896454\n",
      "Validation: Epoch [23], Batch [15/938], Loss: 0.4905053973197937\n",
      "Validation: Epoch [23], Batch [16/938], Loss: 0.838392972946167\n",
      "Validation: Epoch [23], Batch [17/938], Loss: 0.4275204539299011\n",
      "Validation: Epoch [23], Batch [18/938], Loss: 0.6583667993545532\n",
      "Validation: Epoch [23], Batch [19/938], Loss: 0.5071992874145508\n",
      "Validation: Epoch [23], Batch [20/938], Loss: 0.5771298408508301\n",
      "Validation: Epoch [23], Batch [21/938], Loss: 0.3889334499835968\n",
      "Validation: Epoch [23], Batch [22/938], Loss: 0.5250246524810791\n",
      "Validation: Epoch [23], Batch [23/938], Loss: 0.4029974639415741\n",
      "Validation: Epoch [23], Batch [24/938], Loss: 0.4581652283668518\n",
      "Validation: Epoch [23], Batch [25/938], Loss: 0.4603138267993927\n",
      "Validation: Epoch [23], Batch [26/938], Loss: 0.7514996528625488\n",
      "Validation: Epoch [23], Batch [27/938], Loss: 0.3531516194343567\n",
      "Validation: Epoch [23], Batch [28/938], Loss: 0.38107195496559143\n",
      "Validation: Epoch [23], Batch [29/938], Loss: 0.5304247736930847\n",
      "Validation: Epoch [23], Batch [30/938], Loss: 0.4167960584163666\n",
      "Validation: Epoch [23], Batch [31/938], Loss: 0.42108455300331116\n",
      "Validation: Epoch [23], Batch [32/938], Loss: 0.3193487226963043\n",
      "Validation: Epoch [23], Batch [33/938], Loss: 0.3682994544506073\n",
      "Validation: Epoch [23], Batch [34/938], Loss: 0.3571091294288635\n",
      "Validation: Epoch [23], Batch [35/938], Loss: 0.23923102021217346\n",
      "Validation: Epoch [23], Batch [36/938], Loss: 0.5063477754592896\n",
      "Validation: Epoch [23], Batch [37/938], Loss: 0.3977378010749817\n",
      "Validation: Epoch [23], Batch [38/938], Loss: 0.4294017553329468\n",
      "Validation: Epoch [23], Batch [39/938], Loss: 0.66646808385849\n",
      "Validation: Epoch [23], Batch [40/938], Loss: 0.4527420997619629\n",
      "Validation: Epoch [23], Batch [41/938], Loss: 0.6653198003768921\n",
      "Validation: Epoch [23], Batch [42/938], Loss: 0.5780121684074402\n",
      "Validation: Epoch [23], Batch [43/938], Loss: 0.4841635227203369\n",
      "Validation: Epoch [23], Batch [44/938], Loss: 0.3178974986076355\n",
      "Validation: Epoch [23], Batch [45/938], Loss: 0.44515344500541687\n",
      "Validation: Epoch [23], Batch [46/938], Loss: 0.43556249141693115\n",
      "Validation: Epoch [23], Batch [47/938], Loss: 0.4138840436935425\n",
      "Validation: Epoch [23], Batch [48/938], Loss: 0.3679891526699066\n",
      "Validation: Epoch [23], Batch [49/938], Loss: 0.6459475755691528\n",
      "Validation: Epoch [23], Batch [50/938], Loss: 0.5514212250709534\n",
      "Validation: Epoch [23], Batch [51/938], Loss: 0.3207958936691284\n",
      "Validation: Epoch [23], Batch [52/938], Loss: 0.7192664742469788\n",
      "Validation: Epoch [23], Batch [53/938], Loss: 0.42034730315208435\n",
      "Validation: Epoch [23], Batch [54/938], Loss: 0.23810335993766785\n",
      "Validation: Epoch [23], Batch [55/938], Loss: 0.6153820753097534\n",
      "Validation: Epoch [23], Batch [56/938], Loss: 0.28319719433784485\n",
      "Validation: Epoch [23], Batch [57/938], Loss: 0.38626882433891296\n",
      "Validation: Epoch [23], Batch [58/938], Loss: 0.5004451870918274\n",
      "Validation: Epoch [23], Batch [59/938], Loss: 0.321114718914032\n",
      "Validation: Epoch [23], Batch [60/938], Loss: 0.29568740725517273\n",
      "Validation: Epoch [23], Batch [61/938], Loss: 0.3557320237159729\n",
      "Validation: Epoch [23], Batch [62/938], Loss: 0.5344433188438416\n",
      "Validation: Epoch [23], Batch [63/938], Loss: 0.342894971370697\n",
      "Validation: Epoch [23], Batch [64/938], Loss: 0.39084094762802124\n",
      "Validation: Epoch [23], Batch [65/938], Loss: 0.46156200766563416\n",
      "Validation: Epoch [23], Batch [66/938], Loss: 0.48564234375953674\n",
      "Validation: Epoch [23], Batch [67/938], Loss: 0.4109487533569336\n",
      "Validation: Epoch [23], Batch [68/938], Loss: 0.8452324867248535\n",
      "Validation: Epoch [23], Batch [69/938], Loss: 0.36704111099243164\n",
      "Validation: Epoch [23], Batch [70/938], Loss: 0.29715925455093384\n",
      "Validation: Epoch [23], Batch [71/938], Loss: 0.4374803304672241\n",
      "Validation: Epoch [23], Batch [72/938], Loss: 0.40183186531066895\n",
      "Validation: Epoch [23], Batch [73/938], Loss: 0.4019771218299866\n",
      "Validation: Epoch [23], Batch [74/938], Loss: 0.4252380132675171\n",
      "Validation: Epoch [23], Batch [75/938], Loss: 0.6956735849380493\n",
      "Validation: Epoch [23], Batch [76/938], Loss: 0.5438013672828674\n",
      "Validation: Epoch [23], Batch [77/938], Loss: 0.6292762756347656\n",
      "Validation: Epoch [23], Batch [78/938], Loss: 0.525223970413208\n",
      "Validation: Epoch [23], Batch [79/938], Loss: 0.2805005609989166\n",
      "Validation: Epoch [23], Batch [80/938], Loss: 0.5831674337387085\n",
      "Validation: Epoch [23], Batch [81/938], Loss: 0.296504944562912\n",
      "Validation: Epoch [23], Batch [82/938], Loss: 0.709339439868927\n",
      "Validation: Epoch [23], Batch [83/938], Loss: 0.43077993392944336\n",
      "Validation: Epoch [23], Batch [84/938], Loss: 0.4854587912559509\n",
      "Validation: Epoch [23], Batch [85/938], Loss: 0.3487683832645416\n",
      "Validation: Epoch [23], Batch [86/938], Loss: 0.45777398347854614\n",
      "Validation: Epoch [23], Batch [87/938], Loss: 0.7555577754974365\n",
      "Validation: Epoch [23], Batch [88/938], Loss: 0.33849287033081055\n",
      "Validation: Epoch [23], Batch [89/938], Loss: 0.3774036169052124\n",
      "Validation: Epoch [23], Batch [90/938], Loss: 0.576690137386322\n",
      "Validation: Epoch [23], Batch [91/938], Loss: 0.3082062602043152\n",
      "Validation: Epoch [23], Batch [92/938], Loss: 0.5997878909111023\n",
      "Validation: Epoch [23], Batch [93/938], Loss: 0.3377952575683594\n",
      "Validation: Epoch [23], Batch [94/938], Loss: 0.6785964965820312\n",
      "Validation: Epoch [23], Batch [95/938], Loss: 0.5604623556137085\n",
      "Validation: Epoch [23], Batch [96/938], Loss: 0.34079694747924805\n",
      "Validation: Epoch [23], Batch [97/938], Loss: 0.4664323925971985\n",
      "Validation: Epoch [23], Batch [98/938], Loss: 0.3143254220485687\n",
      "Validation: Epoch [23], Batch [99/938], Loss: 0.397785484790802\n",
      "Validation: Epoch [23], Batch [100/938], Loss: 0.42612457275390625\n",
      "Validation: Epoch [23], Batch [101/938], Loss: 0.47775521874427795\n",
      "Validation: Epoch [23], Batch [102/938], Loss: 0.5099378228187561\n",
      "Validation: Epoch [23], Batch [103/938], Loss: 0.38625842332839966\n",
      "Validation: Epoch [23], Batch [104/938], Loss: 0.5033594369888306\n",
      "Validation: Epoch [23], Batch [105/938], Loss: 0.6879959106445312\n",
      "Validation: Epoch [23], Batch [106/938], Loss: 0.37833815813064575\n",
      "Validation: Epoch [23], Batch [107/938], Loss: 0.5185486674308777\n",
      "Validation: Epoch [23], Batch [108/938], Loss: 0.36438611149787903\n",
      "Validation: Epoch [23], Batch [109/938], Loss: 0.47149214148521423\n",
      "Validation: Epoch [23], Batch [110/938], Loss: 0.3359251320362091\n",
      "Validation: Epoch [23], Batch [111/938], Loss: 0.40398934483528137\n",
      "Validation: Epoch [23], Batch [112/938], Loss: 0.2242933064699173\n",
      "Validation: Epoch [23], Batch [113/938], Loss: 0.3606654405593872\n",
      "Validation: Epoch [23], Batch [114/938], Loss: 0.464384526014328\n",
      "Validation: Epoch [23], Batch [115/938], Loss: 0.5992884635925293\n",
      "Validation: Epoch [23], Batch [116/938], Loss: 0.6162732243537903\n",
      "Validation: Epoch [23], Batch [117/938], Loss: 0.4774002730846405\n",
      "Validation: Epoch [23], Batch [118/938], Loss: 0.3177187442779541\n",
      "Validation: Epoch [23], Batch [119/938], Loss: 0.6249264478683472\n",
      "Validation: Epoch [23], Batch [120/938], Loss: 0.7394397854804993\n",
      "Validation: Epoch [23], Batch [121/938], Loss: 0.440453439950943\n",
      "Validation: Epoch [23], Batch [122/938], Loss: 0.5111809968948364\n",
      "Validation: Epoch [23], Batch [123/938], Loss: 0.45047566294670105\n",
      "Validation: Epoch [23], Batch [124/938], Loss: 0.3287811279296875\n",
      "Validation: Epoch [23], Batch [125/938], Loss: 0.3389102816581726\n",
      "Validation: Epoch [23], Batch [126/938], Loss: 0.35395893454551697\n",
      "Validation: Epoch [23], Batch [127/938], Loss: 0.5262545943260193\n",
      "Validation: Epoch [23], Batch [128/938], Loss: 0.5080217719078064\n",
      "Validation: Epoch [23], Batch [129/938], Loss: 0.4155785143375397\n",
      "Validation: Epoch [23], Batch [130/938], Loss: 0.2735210955142975\n",
      "Validation: Epoch [23], Batch [131/938], Loss: 0.444309800863266\n",
      "Validation: Epoch [23], Batch [132/938], Loss: 0.40276432037353516\n",
      "Validation: Epoch [23], Batch [133/938], Loss: 0.45883071422576904\n",
      "Validation: Epoch [23], Batch [134/938], Loss: 0.4705534875392914\n",
      "Validation: Epoch [23], Batch [135/938], Loss: 0.8093450665473938\n",
      "Validation: Epoch [23], Batch [136/938], Loss: 0.5500853061676025\n",
      "Validation: Epoch [23], Batch [137/938], Loss: 0.3451882004737854\n",
      "Validation: Epoch [23], Batch [138/938], Loss: 0.4584091305732727\n",
      "Validation: Epoch [23], Batch [139/938], Loss: 0.5424519181251526\n",
      "Validation: Epoch [23], Batch [140/938], Loss: 0.6113997101783752\n",
      "Validation: Epoch [23], Batch [141/938], Loss: 0.25828200578689575\n",
      "Validation: Epoch [23], Batch [142/938], Loss: 0.4611077606678009\n",
      "Validation: Epoch [23], Batch [143/938], Loss: 0.29028502106666565\n",
      "Validation: Epoch [23], Batch [144/938], Loss: 0.35085469484329224\n",
      "Validation: Epoch [23], Batch [145/938], Loss: 0.37464141845703125\n",
      "Validation: Epoch [23], Batch [146/938], Loss: 0.5402858257293701\n",
      "Validation: Epoch [23], Batch [147/938], Loss: 0.38624367117881775\n",
      "Validation: Epoch [23], Batch [148/938], Loss: 0.3406415581703186\n",
      "Validation: Epoch [23], Batch [149/938], Loss: 0.36976316571235657\n",
      "Validation: Epoch [23], Batch [150/938], Loss: 0.5337504744529724\n",
      "Validation: Epoch [23], Batch [151/938], Loss: 0.4529940187931061\n",
      "Validation: Epoch [23], Batch [152/938], Loss: 0.5695541501045227\n",
      "Validation: Epoch [23], Batch [153/938], Loss: 0.4968615174293518\n",
      "Validation: Epoch [23], Batch [154/938], Loss: 0.6029962301254272\n",
      "Validation: Epoch [23], Batch [155/938], Loss: 0.4932495951652527\n",
      "Validation: Epoch [23], Batch [156/938], Loss: 0.40315520763397217\n",
      "Validation: Epoch [23], Batch [157/938], Loss: 0.3879797160625458\n",
      "Validation: Epoch [23], Batch [158/938], Loss: 0.45205456018447876\n",
      "Validation: Epoch [23], Batch [159/938], Loss: 0.39278802275657654\n",
      "Validation: Epoch [23], Batch [160/938], Loss: 0.48749151825904846\n",
      "Validation: Epoch [23], Batch [161/938], Loss: 0.5000070929527283\n",
      "Validation: Epoch [23], Batch [162/938], Loss: 0.8297528624534607\n",
      "Validation: Epoch [23], Batch [163/938], Loss: 0.35658779740333557\n",
      "Validation: Epoch [23], Batch [164/938], Loss: 0.4978106617927551\n",
      "Validation: Epoch [23], Batch [165/938], Loss: 0.47397297620773315\n",
      "Validation: Epoch [23], Batch [166/938], Loss: 0.2197810560464859\n",
      "Validation: Epoch [23], Batch [167/938], Loss: 0.5147826671600342\n",
      "Validation: Epoch [23], Batch [168/938], Loss: 0.3358813226222992\n",
      "Validation: Epoch [23], Batch [169/938], Loss: 0.5039798021316528\n",
      "Validation: Epoch [23], Batch [170/938], Loss: 0.42109233140945435\n",
      "Validation: Epoch [23], Batch [171/938], Loss: 0.3719720244407654\n",
      "Validation: Epoch [23], Batch [172/938], Loss: 0.46820348501205444\n",
      "Validation: Epoch [23], Batch [173/938], Loss: 0.3157458007335663\n",
      "Validation: Epoch [23], Batch [174/938], Loss: 0.5665771961212158\n",
      "Validation: Epoch [23], Batch [175/938], Loss: 0.6096237897872925\n",
      "Validation: Epoch [23], Batch [176/938], Loss: 0.4914292097091675\n",
      "Validation: Epoch [23], Batch [177/938], Loss: 0.30463093519210815\n",
      "Validation: Epoch [23], Batch [178/938], Loss: 0.2843780517578125\n",
      "Validation: Epoch [23], Batch [179/938], Loss: 0.6587697863578796\n",
      "Validation: Epoch [23], Batch [180/938], Loss: 0.5025588274002075\n",
      "Validation: Epoch [23], Batch [181/938], Loss: 0.43606260418891907\n",
      "Validation: Epoch [23], Batch [182/938], Loss: 0.344418466091156\n",
      "Validation: Epoch [23], Batch [183/938], Loss: 0.4994293451309204\n",
      "Validation: Epoch [23], Batch [184/938], Loss: 0.5000589489936829\n",
      "Validation: Epoch [23], Batch [185/938], Loss: 0.5697290301322937\n",
      "Validation: Epoch [23], Batch [186/938], Loss: 0.5188063979148865\n",
      "Validation: Epoch [23], Batch [187/938], Loss: 0.42603951692581177\n",
      "Validation: Epoch [23], Batch [188/938], Loss: 0.43315234780311584\n",
      "Validation: Epoch [23], Batch [189/938], Loss: 0.69244384765625\n",
      "Validation: Epoch [23], Batch [190/938], Loss: 0.24954432249069214\n",
      "Validation: Epoch [23], Batch [191/938], Loss: 0.5757060647010803\n",
      "Validation: Epoch [23], Batch [192/938], Loss: 0.4614783525466919\n",
      "Validation: Epoch [23], Batch [193/938], Loss: 0.3996485769748688\n",
      "Validation: Epoch [23], Batch [194/938], Loss: 0.3697108030319214\n",
      "Validation: Epoch [23], Batch [195/938], Loss: 0.3699120581150055\n",
      "Validation: Epoch [23], Batch [196/938], Loss: 0.3203461766242981\n",
      "Validation: Epoch [23], Batch [197/938], Loss: 0.5247514843940735\n",
      "Validation: Epoch [23], Batch [198/938], Loss: 0.7050292491912842\n",
      "Validation: Epoch [23], Batch [199/938], Loss: 0.5744717121124268\n",
      "Validation: Epoch [23], Batch [200/938], Loss: 0.4174058735370636\n",
      "Validation: Epoch [23], Batch [201/938], Loss: 0.33434802293777466\n",
      "Validation: Epoch [23], Batch [202/938], Loss: 0.4861859977245331\n",
      "Validation: Epoch [23], Batch [203/938], Loss: 0.29257646203041077\n",
      "Validation: Epoch [23], Batch [204/938], Loss: 0.5176035165786743\n",
      "Validation: Epoch [23], Batch [205/938], Loss: 0.39245593547821045\n",
      "Validation: Epoch [23], Batch [206/938], Loss: 0.3945709466934204\n",
      "Validation: Epoch [23], Batch [207/938], Loss: 0.3502780795097351\n",
      "Validation: Epoch [23], Batch [208/938], Loss: 0.46880680322647095\n",
      "Validation: Epoch [23], Batch [209/938], Loss: 0.6201010346412659\n",
      "Validation: Epoch [23], Batch [210/938], Loss: 0.4529576003551483\n",
      "Validation: Epoch [23], Batch [211/938], Loss: 0.5382153987884521\n",
      "Validation: Epoch [23], Batch [212/938], Loss: 0.4329735338687897\n",
      "Validation: Epoch [23], Batch [213/938], Loss: 0.271480917930603\n",
      "Validation: Epoch [23], Batch [214/938], Loss: 0.3153650760650635\n",
      "Validation: Epoch [23], Batch [215/938], Loss: 0.39899587631225586\n",
      "Validation: Epoch [23], Batch [216/938], Loss: 0.5078861117362976\n",
      "Validation: Epoch [23], Batch [217/938], Loss: 0.32886630296707153\n",
      "Validation: Epoch [23], Batch [218/938], Loss: 0.5799970030784607\n",
      "Validation: Epoch [23], Batch [219/938], Loss: 0.6654651761054993\n",
      "Validation: Epoch [23], Batch [220/938], Loss: 0.4796057343482971\n",
      "Validation: Epoch [23], Batch [221/938], Loss: 0.5074716806411743\n",
      "Validation: Epoch [23], Batch [222/938], Loss: 0.4005415439605713\n",
      "Validation: Epoch [23], Batch [223/938], Loss: 0.3624085485935211\n",
      "Validation: Epoch [23], Batch [224/938], Loss: 0.5914644002914429\n",
      "Validation: Epoch [23], Batch [225/938], Loss: 0.4368646740913391\n",
      "Validation: Epoch [23], Batch [226/938], Loss: 0.47632303833961487\n",
      "Validation: Epoch [23], Batch [227/938], Loss: 0.3715304732322693\n",
      "Validation: Epoch [23], Batch [228/938], Loss: 0.39947107434272766\n",
      "Validation: Epoch [23], Batch [229/938], Loss: 0.6378087997436523\n",
      "Validation: Epoch [23], Batch [230/938], Loss: 0.33561402559280396\n",
      "Validation: Epoch [23], Batch [231/938], Loss: 0.6357483267784119\n",
      "Validation: Epoch [23], Batch [232/938], Loss: 0.18306054174900055\n",
      "Validation: Epoch [23], Batch [233/938], Loss: 0.7563984990119934\n",
      "Validation: Epoch [23], Batch [234/938], Loss: 0.46614474058151245\n",
      "Validation: Epoch [23], Batch [235/938], Loss: 0.48832595348358154\n",
      "Validation: Epoch [23], Batch [236/938], Loss: 0.6568021178245544\n",
      "Validation: Epoch [23], Batch [237/938], Loss: 0.3126363754272461\n",
      "Validation: Epoch [23], Batch [238/938], Loss: 0.3019741475582123\n",
      "Validation: Epoch [23], Batch [239/938], Loss: 0.4952823519706726\n",
      "Validation: Epoch [23], Batch [240/938], Loss: 0.5986854434013367\n",
      "Validation: Epoch [23], Batch [241/938], Loss: 0.3831835985183716\n",
      "Validation: Epoch [23], Batch [242/938], Loss: 0.3703671395778656\n",
      "Validation: Epoch [23], Batch [243/938], Loss: 0.3731573820114136\n",
      "Validation: Epoch [23], Batch [244/938], Loss: 0.38458311557769775\n",
      "Validation: Epoch [23], Batch [245/938], Loss: 0.2676459550857544\n",
      "Validation: Epoch [23], Batch [246/938], Loss: 0.558327317237854\n",
      "Validation: Epoch [23], Batch [247/938], Loss: 0.5384431481361389\n",
      "Validation: Epoch [23], Batch [248/938], Loss: 0.33016565442085266\n",
      "Validation: Epoch [23], Batch [249/938], Loss: 0.6492773294448853\n",
      "Validation: Epoch [23], Batch [250/938], Loss: 0.615358293056488\n",
      "Validation: Epoch [23], Batch [251/938], Loss: 0.383789986371994\n",
      "Validation: Epoch [23], Batch [252/938], Loss: 0.3034360408782959\n",
      "Validation: Epoch [23], Batch [253/938], Loss: 0.5046058297157288\n",
      "Validation: Epoch [23], Batch [254/938], Loss: 0.34694913029670715\n",
      "Validation: Epoch [23], Batch [255/938], Loss: 0.4591251611709595\n",
      "Validation: Epoch [23], Batch [256/938], Loss: 0.3340122103691101\n",
      "Validation: Epoch [23], Batch [257/938], Loss: 0.375356525182724\n",
      "Validation: Epoch [23], Batch [258/938], Loss: 0.47504350543022156\n",
      "Validation: Epoch [23], Batch [259/938], Loss: 0.44632065296173096\n",
      "Validation: Epoch [23], Batch [260/938], Loss: 0.3872292935848236\n",
      "Validation: Epoch [23], Batch [261/938], Loss: 0.4538978338241577\n",
      "Validation: Epoch [23], Batch [262/938], Loss: 0.38127756118774414\n",
      "Validation: Epoch [23], Batch [263/938], Loss: 0.43369776010513306\n",
      "Validation: Epoch [23], Batch [264/938], Loss: 0.5143892765045166\n",
      "Validation: Epoch [23], Batch [265/938], Loss: 0.39710840582847595\n",
      "Validation: Epoch [23], Batch [266/938], Loss: 0.3921971023082733\n",
      "Validation: Epoch [23], Batch [267/938], Loss: 0.5161462426185608\n",
      "Validation: Epoch [23], Batch [268/938], Loss: 0.3770388662815094\n",
      "Validation: Epoch [23], Batch [269/938], Loss: 0.2819879949092865\n",
      "Validation: Epoch [23], Batch [270/938], Loss: 0.40274855494499207\n",
      "Validation: Epoch [23], Batch [271/938], Loss: 0.5016164183616638\n",
      "Validation: Epoch [23], Batch [272/938], Loss: 0.8509203195571899\n",
      "Validation: Epoch [23], Batch [273/938], Loss: 0.35693901777267456\n",
      "Validation: Epoch [23], Batch [274/938], Loss: 0.22297883033752441\n",
      "Validation: Epoch [23], Batch [275/938], Loss: 0.4379889667034149\n",
      "Validation: Epoch [23], Batch [276/938], Loss: 0.26253455877304077\n",
      "Validation: Epoch [23], Batch [277/938], Loss: 0.4424271881580353\n",
      "Validation: Epoch [23], Batch [278/938], Loss: 0.31652727723121643\n",
      "Validation: Epoch [23], Batch [279/938], Loss: 0.46240195631980896\n",
      "Validation: Epoch [23], Batch [280/938], Loss: 0.5944744348526001\n",
      "Validation: Epoch [23], Batch [281/938], Loss: 0.4682959020137787\n",
      "Validation: Epoch [23], Batch [282/938], Loss: 0.5073080062866211\n",
      "Validation: Epoch [23], Batch [283/938], Loss: 0.5106227397918701\n",
      "Validation: Epoch [23], Batch [284/938], Loss: 0.5136221051216125\n",
      "Validation: Epoch [23], Batch [285/938], Loss: 0.4994322657585144\n",
      "Validation: Epoch [23], Batch [286/938], Loss: 0.534569263458252\n",
      "Validation: Epoch [23], Batch [287/938], Loss: 0.5441634058952332\n",
      "Validation: Epoch [23], Batch [288/938], Loss: 0.4929277300834656\n",
      "Validation: Epoch [23], Batch [289/938], Loss: 0.44644132256507874\n",
      "Validation: Epoch [23], Batch [290/938], Loss: 0.4236980378627777\n",
      "Validation: Epoch [23], Batch [291/938], Loss: 0.560631513595581\n",
      "Validation: Epoch [23], Batch [292/938], Loss: 0.4462539255619049\n",
      "Validation: Epoch [23], Batch [293/938], Loss: 0.4378919005393982\n",
      "Validation: Epoch [23], Batch [294/938], Loss: 0.744327962398529\n",
      "Validation: Epoch [23], Batch [295/938], Loss: 0.47974175214767456\n",
      "Validation: Epoch [23], Batch [296/938], Loss: 0.28818339109420776\n",
      "Validation: Epoch [23], Batch [297/938], Loss: 0.41745659708976746\n",
      "Validation: Epoch [23], Batch [298/938], Loss: 0.38618332147598267\n",
      "Validation: Epoch [23], Batch [299/938], Loss: 0.37152594327926636\n",
      "Validation: Epoch [23], Batch [300/938], Loss: 0.4769032895565033\n",
      "Validation: Epoch [23], Batch [301/938], Loss: 0.49213606119155884\n",
      "Validation: Epoch [23], Batch [302/938], Loss: 0.47321969270706177\n",
      "Validation: Epoch [23], Batch [303/938], Loss: 0.4471431076526642\n",
      "Validation: Epoch [23], Batch [304/938], Loss: 0.442568302154541\n",
      "Validation: Epoch [23], Batch [305/938], Loss: 0.5622298121452332\n",
      "Validation: Epoch [23], Batch [306/938], Loss: 0.4723234474658966\n",
      "Validation: Epoch [23], Batch [307/938], Loss: 0.5460337400436401\n",
      "Validation: Epoch [23], Batch [308/938], Loss: 0.32999446988105774\n",
      "Validation: Epoch [23], Batch [309/938], Loss: 0.4187074899673462\n",
      "Validation: Epoch [23], Batch [310/938], Loss: 0.573095440864563\n",
      "Validation: Epoch [23], Batch [311/938], Loss: 0.5850911736488342\n",
      "Validation: Epoch [23], Batch [312/938], Loss: 0.34842419624328613\n",
      "Validation: Epoch [23], Batch [313/938], Loss: 0.45458200573921204\n",
      "Validation: Epoch [23], Batch [314/938], Loss: 0.6323839426040649\n",
      "Validation: Epoch [23], Batch [315/938], Loss: 0.3371454179286957\n",
      "Validation: Epoch [23], Batch [316/938], Loss: 0.6649835705757141\n",
      "Validation: Epoch [23], Batch [317/938], Loss: 0.38055989146232605\n",
      "Validation: Epoch [23], Batch [318/938], Loss: 0.3955426514148712\n",
      "Validation: Epoch [23], Batch [319/938], Loss: 0.38441914319992065\n",
      "Validation: Epoch [23], Batch [320/938], Loss: 0.5753067135810852\n",
      "Validation: Epoch [23], Batch [321/938], Loss: 0.4336051344871521\n",
      "Validation: Epoch [23], Batch [322/938], Loss: 0.7762048840522766\n",
      "Validation: Epoch [23], Batch [323/938], Loss: 0.46086573600769043\n",
      "Validation: Epoch [23], Batch [324/938], Loss: 0.4040544033050537\n",
      "Validation: Epoch [23], Batch [325/938], Loss: 0.39210671186447144\n",
      "Validation: Epoch [23], Batch [326/938], Loss: 0.5754890441894531\n",
      "Validation: Epoch [23], Batch [327/938], Loss: 0.46941089630126953\n",
      "Validation: Epoch [23], Batch [328/938], Loss: 0.4551413953304291\n",
      "Validation: Epoch [23], Batch [329/938], Loss: 0.51568204164505\n",
      "Validation: Epoch [23], Batch [330/938], Loss: 0.399476021528244\n",
      "Validation: Epoch [23], Batch [331/938], Loss: 0.4726143479347229\n",
      "Validation: Epoch [23], Batch [332/938], Loss: 0.4255134165287018\n",
      "Validation: Epoch [23], Batch [333/938], Loss: 0.492023766040802\n",
      "Validation: Epoch [23], Batch [334/938], Loss: 0.41895556449890137\n",
      "Validation: Epoch [23], Batch [335/938], Loss: 0.5656291246414185\n",
      "Validation: Epoch [23], Batch [336/938], Loss: 0.4442785978317261\n",
      "Validation: Epoch [23], Batch [337/938], Loss: 0.6055986285209656\n",
      "Validation: Epoch [23], Batch [338/938], Loss: 0.5159361362457275\n",
      "Validation: Epoch [23], Batch [339/938], Loss: 0.35990625619888306\n",
      "Validation: Epoch [23], Batch [340/938], Loss: 0.6805838942527771\n",
      "Validation: Epoch [23], Batch [341/938], Loss: 0.5809418559074402\n",
      "Validation: Epoch [23], Batch [342/938], Loss: 0.3714650869369507\n",
      "Validation: Epoch [23], Batch [343/938], Loss: 0.4375147521495819\n",
      "Validation: Epoch [23], Batch [344/938], Loss: 0.6742239594459534\n",
      "Validation: Epoch [23], Batch [345/938], Loss: 0.2895827889442444\n",
      "Validation: Epoch [23], Batch [346/938], Loss: 0.589770495891571\n",
      "Validation: Epoch [23], Batch [347/938], Loss: 0.49562227725982666\n",
      "Validation: Epoch [23], Batch [348/938], Loss: 0.36892303824424744\n",
      "Validation: Epoch [23], Batch [349/938], Loss: 0.4931051731109619\n",
      "Validation: Epoch [23], Batch [350/938], Loss: 0.34392327070236206\n",
      "Validation: Epoch [23], Batch [351/938], Loss: 0.4383653998374939\n",
      "Validation: Epoch [23], Batch [352/938], Loss: 0.42352166771888733\n",
      "Validation: Epoch [23], Batch [353/938], Loss: 0.5712071657180786\n",
      "Validation: Epoch [23], Batch [354/938], Loss: 0.3953363597393036\n",
      "Validation: Epoch [23], Batch [355/938], Loss: 0.5951708555221558\n",
      "Validation: Epoch [23], Batch [356/938], Loss: 0.2745346128940582\n",
      "Validation: Epoch [23], Batch [357/938], Loss: 0.47068092226982117\n",
      "Validation: Epoch [23], Batch [358/938], Loss: 0.4502485990524292\n",
      "Validation: Epoch [23], Batch [359/938], Loss: 0.4021964371204376\n",
      "Validation: Epoch [23], Batch [360/938], Loss: 0.43982189893722534\n",
      "Validation: Epoch [23], Batch [361/938], Loss: 0.6112152934074402\n",
      "Validation: Epoch [23], Batch [362/938], Loss: 0.47711625695228577\n",
      "Validation: Epoch [23], Batch [363/938], Loss: 0.3918900787830353\n",
      "Validation: Epoch [23], Batch [364/938], Loss: 0.30982059240341187\n",
      "Validation: Epoch [23], Batch [365/938], Loss: 0.28457289934158325\n",
      "Validation: Epoch [23], Batch [366/938], Loss: 0.5548559427261353\n",
      "Validation: Epoch [23], Batch [367/938], Loss: 0.552115797996521\n",
      "Validation: Epoch [23], Batch [368/938], Loss: 0.3488679528236389\n",
      "Validation: Epoch [23], Batch [369/938], Loss: 0.3134543299674988\n",
      "Validation: Epoch [23], Batch [370/938], Loss: 0.4791014790534973\n",
      "Validation: Epoch [23], Batch [371/938], Loss: 0.3617611229419708\n",
      "Validation: Epoch [23], Batch [372/938], Loss: 0.31931620836257935\n",
      "Validation: Epoch [23], Batch [373/938], Loss: 0.40088093280792236\n",
      "Validation: Epoch [23], Batch [374/938], Loss: 0.3805083632469177\n",
      "Validation: Epoch [23], Batch [375/938], Loss: 0.31809163093566895\n",
      "Validation: Epoch [23], Batch [376/938], Loss: 0.41803932189941406\n",
      "Validation: Epoch [23], Batch [377/938], Loss: 0.38066184520721436\n",
      "Validation: Epoch [23], Batch [378/938], Loss: 0.6570477485656738\n",
      "Validation: Epoch [23], Batch [379/938], Loss: 0.6927501559257507\n",
      "Validation: Epoch [23], Batch [380/938], Loss: 0.45573267340660095\n",
      "Validation: Epoch [23], Batch [381/938], Loss: 0.5452306270599365\n",
      "Validation: Epoch [23], Batch [382/938], Loss: 0.6705507636070251\n",
      "Validation: Epoch [23], Batch [383/938], Loss: 0.42443954944610596\n",
      "Validation: Epoch [23], Batch [384/938], Loss: 0.7085926532745361\n",
      "Validation: Epoch [23], Batch [385/938], Loss: 0.47696688771247864\n",
      "Validation: Epoch [23], Batch [386/938], Loss: 0.4804648458957672\n",
      "Validation: Epoch [23], Batch [387/938], Loss: 0.35911068320274353\n",
      "Validation: Epoch [23], Batch [388/938], Loss: 0.474628210067749\n",
      "Validation: Epoch [23], Batch [389/938], Loss: 0.3348127007484436\n",
      "Validation: Epoch [23], Batch [390/938], Loss: 0.5103789567947388\n",
      "Validation: Epoch [23], Batch [391/938], Loss: 0.5546819567680359\n",
      "Validation: Epoch [23], Batch [392/938], Loss: 0.5902879238128662\n",
      "Validation: Epoch [23], Batch [393/938], Loss: 0.5267864465713501\n",
      "Validation: Epoch [23], Batch [394/938], Loss: 0.5152304172515869\n",
      "Validation: Epoch [23], Batch [395/938], Loss: 0.36696672439575195\n",
      "Validation: Epoch [23], Batch [396/938], Loss: 0.3865010738372803\n",
      "Validation: Epoch [23], Batch [397/938], Loss: 0.3958573639392853\n",
      "Validation: Epoch [23], Batch [398/938], Loss: 0.46156415343284607\n",
      "Validation: Epoch [23], Batch [399/938], Loss: 0.4930303990840912\n",
      "Validation: Epoch [23], Batch [400/938], Loss: 0.47034940123558044\n",
      "Validation: Epoch [23], Batch [401/938], Loss: 0.5597100257873535\n",
      "Validation: Epoch [23], Batch [402/938], Loss: 0.5447266101837158\n",
      "Validation: Epoch [23], Batch [403/938], Loss: 0.5094025731086731\n",
      "Validation: Epoch [23], Batch [404/938], Loss: 0.4846682548522949\n",
      "Validation: Epoch [23], Batch [405/938], Loss: 0.39782997965812683\n",
      "Validation: Epoch [23], Batch [406/938], Loss: 0.5213834643363953\n",
      "Validation: Epoch [23], Batch [407/938], Loss: 0.4077465534210205\n",
      "Validation: Epoch [23], Batch [408/938], Loss: 0.4440193772315979\n",
      "Validation: Epoch [23], Batch [409/938], Loss: 0.41984644532203674\n",
      "Validation: Epoch [23], Batch [410/938], Loss: 0.4587278962135315\n",
      "Validation: Epoch [23], Batch [411/938], Loss: 0.45876044034957886\n",
      "Validation: Epoch [23], Batch [412/938], Loss: 0.5026578307151794\n",
      "Validation: Epoch [23], Batch [413/938], Loss: 0.3303253948688507\n",
      "Validation: Epoch [23], Batch [414/938], Loss: 0.47534388303756714\n",
      "Validation: Epoch [23], Batch [415/938], Loss: 0.2805534899234772\n",
      "Validation: Epoch [23], Batch [416/938], Loss: 0.39661574363708496\n",
      "Validation: Epoch [23], Batch [417/938], Loss: 0.3707229197025299\n",
      "Validation: Epoch [23], Batch [418/938], Loss: 0.4957082271575928\n",
      "Validation: Epoch [23], Batch [419/938], Loss: 0.5997505187988281\n",
      "Validation: Epoch [23], Batch [420/938], Loss: 0.6424249410629272\n",
      "Validation: Epoch [23], Batch [421/938], Loss: 0.20098161697387695\n",
      "Validation: Epoch [23], Batch [422/938], Loss: 0.5705993175506592\n",
      "Validation: Epoch [23], Batch [423/938], Loss: 0.7620373964309692\n",
      "Validation: Epoch [23], Batch [424/938], Loss: 0.5009818077087402\n",
      "Validation: Epoch [23], Batch [425/938], Loss: 0.4865990877151489\n",
      "Validation: Epoch [23], Batch [426/938], Loss: 0.33314475417137146\n",
      "Validation: Epoch [23], Batch [427/938], Loss: 0.4897386431694031\n",
      "Validation: Epoch [23], Batch [428/938], Loss: 0.4866739809513092\n",
      "Validation: Epoch [23], Batch [429/938], Loss: 0.43785345554351807\n",
      "Validation: Epoch [23], Batch [430/938], Loss: 0.167062908411026\n",
      "Validation: Epoch [23], Batch [431/938], Loss: 0.5547451376914978\n",
      "Validation: Epoch [23], Batch [432/938], Loss: 0.41302376985549927\n",
      "Validation: Epoch [23], Batch [433/938], Loss: 0.3006622791290283\n",
      "Validation: Epoch [23], Batch [434/938], Loss: 0.4104229211807251\n",
      "Validation: Epoch [23], Batch [435/938], Loss: 0.3997878432273865\n",
      "Validation: Epoch [23], Batch [436/938], Loss: 0.46020302176475525\n",
      "Validation: Epoch [23], Batch [437/938], Loss: 0.3263966739177704\n",
      "Validation: Epoch [23], Batch [438/938], Loss: 0.6483795642852783\n",
      "Validation: Epoch [23], Batch [439/938], Loss: 0.3045896589756012\n",
      "Validation: Epoch [23], Batch [440/938], Loss: 0.32035449147224426\n",
      "Validation: Epoch [23], Batch [441/938], Loss: 0.32129350304603577\n",
      "Validation: Epoch [23], Batch [442/938], Loss: 0.42107072472572327\n",
      "Validation: Epoch [23], Batch [443/938], Loss: 0.46808961033821106\n",
      "Validation: Epoch [23], Batch [444/938], Loss: 0.48631221055984497\n",
      "Validation: Epoch [23], Batch [445/938], Loss: 0.41256996989250183\n",
      "Validation: Epoch [23], Batch [446/938], Loss: 0.3927840292453766\n",
      "Validation: Epoch [23], Batch [447/938], Loss: 0.4408145546913147\n",
      "Validation: Epoch [23], Batch [448/938], Loss: 0.5355703234672546\n",
      "Validation: Epoch [23], Batch [449/938], Loss: 0.4610675275325775\n",
      "Validation: Epoch [23], Batch [450/938], Loss: 0.38755372166633606\n",
      "Validation: Epoch [23], Batch [451/938], Loss: 0.4254220128059387\n",
      "Validation: Epoch [23], Batch [452/938], Loss: 0.3359943926334381\n",
      "Validation: Epoch [23], Batch [453/938], Loss: 0.2866881191730499\n",
      "Validation: Epoch [23], Batch [454/938], Loss: 0.54347163438797\n",
      "Validation: Epoch [23], Batch [455/938], Loss: 0.2930809259414673\n",
      "Validation: Epoch [23], Batch [456/938], Loss: 0.4355543255805969\n",
      "Validation: Epoch [23], Batch [457/938], Loss: 0.6342511773109436\n",
      "Validation: Epoch [23], Batch [458/938], Loss: 0.43456393480300903\n",
      "Validation: Epoch [23], Batch [459/938], Loss: 0.32054147124290466\n",
      "Validation: Epoch [23], Batch [460/938], Loss: 0.5125110745429993\n",
      "Validation: Epoch [23], Batch [461/938], Loss: 0.321540892124176\n",
      "Validation: Epoch [23], Batch [462/938], Loss: 0.4844534695148468\n",
      "Validation: Epoch [23], Batch [463/938], Loss: 0.45942386984825134\n",
      "Validation: Epoch [23], Batch [464/938], Loss: 0.37750953435897827\n",
      "Validation: Epoch [23], Batch [465/938], Loss: 0.32609671354293823\n",
      "Validation: Epoch [23], Batch [466/938], Loss: 0.4015335738658905\n",
      "Validation: Epoch [23], Batch [467/938], Loss: 0.3523765802383423\n",
      "Validation: Epoch [23], Batch [468/938], Loss: 1.0015571117401123\n",
      "Validation: Epoch [23], Batch [469/938], Loss: 0.4506809115409851\n",
      "Validation: Epoch [23], Batch [470/938], Loss: 0.4606054425239563\n",
      "Validation: Epoch [23], Batch [471/938], Loss: 0.49189162254333496\n",
      "Validation: Epoch [23], Batch [472/938], Loss: 0.35200658440589905\n",
      "Validation: Epoch [23], Batch [473/938], Loss: 0.36700597405433655\n",
      "Validation: Epoch [23], Batch [474/938], Loss: 0.5186551213264465\n",
      "Validation: Epoch [23], Batch [475/938], Loss: 0.47363975644111633\n",
      "Validation: Epoch [23], Batch [476/938], Loss: 0.336174339056015\n",
      "Validation: Epoch [23], Batch [477/938], Loss: 0.5547885894775391\n",
      "Validation: Epoch [23], Batch [478/938], Loss: 0.4750518500804901\n",
      "Validation: Epoch [23], Batch [479/938], Loss: 0.46595606207847595\n",
      "Validation: Epoch [23], Batch [480/938], Loss: 0.5140734910964966\n",
      "Validation: Epoch [23], Batch [481/938], Loss: 0.3614615201950073\n",
      "Validation: Epoch [23], Batch [482/938], Loss: 0.5957977175712585\n",
      "Validation: Epoch [23], Batch [483/938], Loss: 0.3573068380355835\n",
      "Validation: Epoch [23], Batch [484/938], Loss: 0.5557396411895752\n",
      "Validation: Epoch [23], Batch [485/938], Loss: 0.3290962278842926\n",
      "Validation: Epoch [23], Batch [486/938], Loss: 0.4139891266822815\n",
      "Validation: Epoch [23], Batch [487/938], Loss: 0.47573211789131165\n",
      "Validation: Epoch [23], Batch [488/938], Loss: 0.35352641344070435\n",
      "Validation: Epoch [23], Batch [489/938], Loss: 0.42334845662117004\n",
      "Validation: Epoch [23], Batch [490/938], Loss: 0.3917540907859802\n",
      "Validation: Epoch [23], Batch [491/938], Loss: 0.38188570737838745\n",
      "Validation: Epoch [23], Batch [492/938], Loss: 0.4799607992172241\n",
      "Validation: Epoch [23], Batch [493/938], Loss: 0.46871936321258545\n",
      "Validation: Epoch [23], Batch [494/938], Loss: 0.6994129419326782\n",
      "Validation: Epoch [23], Batch [495/938], Loss: 0.4996347725391388\n",
      "Validation: Epoch [23], Batch [496/938], Loss: 0.5501198172569275\n",
      "Validation: Epoch [23], Batch [497/938], Loss: 0.43872570991516113\n",
      "Validation: Epoch [23], Batch [498/938], Loss: 0.284099817276001\n",
      "Validation: Epoch [23], Batch [499/938], Loss: 0.4465210735797882\n",
      "Validation: Epoch [23], Batch [500/938], Loss: 0.37046682834625244\n",
      "Validation: Epoch [23], Batch [501/938], Loss: 0.42898693680763245\n",
      "Validation: Epoch [23], Batch [502/938], Loss: 0.5346248149871826\n",
      "Validation: Epoch [23], Batch [503/938], Loss: 0.47305551171302795\n",
      "Validation: Epoch [23], Batch [504/938], Loss: 0.3927042484283447\n",
      "Validation: Epoch [23], Batch [505/938], Loss: 0.42540815472602844\n",
      "Validation: Epoch [23], Batch [506/938], Loss: 0.47021833062171936\n",
      "Validation: Epoch [23], Batch [507/938], Loss: 0.6831247210502625\n",
      "Validation: Epoch [23], Batch [508/938], Loss: 0.3075745701789856\n",
      "Validation: Epoch [23], Batch [509/938], Loss: 0.5885395407676697\n",
      "Validation: Epoch [23], Batch [510/938], Loss: 0.3179045021533966\n",
      "Validation: Epoch [23], Batch [511/938], Loss: 0.6158009171485901\n",
      "Validation: Epoch [23], Batch [512/938], Loss: 0.6777210831642151\n",
      "Validation: Epoch [23], Batch [513/938], Loss: 0.3357904851436615\n",
      "Validation: Epoch [23], Batch [514/938], Loss: 0.3840083181858063\n",
      "Validation: Epoch [23], Batch [515/938], Loss: 0.29449576139450073\n",
      "Validation: Epoch [23], Batch [516/938], Loss: 0.4776953458786011\n",
      "Validation: Epoch [23], Batch [517/938], Loss: 0.45433351397514343\n",
      "Validation: Epoch [23], Batch [518/938], Loss: 0.4070759117603302\n",
      "Validation: Epoch [23], Batch [519/938], Loss: 0.3902336657047272\n",
      "Validation: Epoch [23], Batch [520/938], Loss: 0.42443525791168213\n",
      "Validation: Epoch [23], Batch [521/938], Loss: 0.3100133240222931\n",
      "Validation: Epoch [23], Batch [522/938], Loss: 0.46776190400123596\n",
      "Validation: Epoch [23], Batch [523/938], Loss: 0.31097638607025146\n",
      "Validation: Epoch [23], Batch [524/938], Loss: 0.33266985416412354\n",
      "Validation: Epoch [23], Batch [525/938], Loss: 0.48751944303512573\n",
      "Validation: Epoch [23], Batch [526/938], Loss: 0.47401776909828186\n",
      "Validation: Epoch [23], Batch [527/938], Loss: 0.3580915629863739\n",
      "Validation: Epoch [23], Batch [528/938], Loss: 0.37489232420921326\n",
      "Validation: Epoch [23], Batch [529/938], Loss: 0.457730770111084\n",
      "Validation: Epoch [23], Batch [530/938], Loss: 0.4858422875404358\n",
      "Validation: Epoch [23], Batch [531/938], Loss: 0.5493887662887573\n",
      "Validation: Epoch [23], Batch [532/938], Loss: 0.3203301727771759\n",
      "Validation: Epoch [23], Batch [533/938], Loss: 0.6471266746520996\n",
      "Validation: Epoch [23], Batch [534/938], Loss: 0.34030818939208984\n",
      "Validation: Epoch [23], Batch [535/938], Loss: 0.46780988574028015\n",
      "Validation: Epoch [23], Batch [536/938], Loss: 0.3237413763999939\n",
      "Validation: Epoch [23], Batch [537/938], Loss: 0.5173465013504028\n",
      "Validation: Epoch [23], Batch [538/938], Loss: 0.4754810631275177\n",
      "Validation: Epoch [23], Batch [539/938], Loss: 0.47878462076187134\n",
      "Validation: Epoch [23], Batch [540/938], Loss: 0.48353537917137146\n",
      "Validation: Epoch [23], Batch [541/938], Loss: 0.43067288398742676\n",
      "Validation: Epoch [23], Batch [542/938], Loss: 0.5284255743026733\n",
      "Validation: Epoch [23], Batch [543/938], Loss: 0.5479955077171326\n",
      "Validation: Epoch [23], Batch [544/938], Loss: 0.5458405017852783\n",
      "Validation: Epoch [23], Batch [545/938], Loss: 0.3350704312324524\n",
      "Validation: Epoch [23], Batch [546/938], Loss: 0.2610170543193817\n",
      "Validation: Epoch [23], Batch [547/938], Loss: 0.6303418874740601\n",
      "Validation: Epoch [23], Batch [548/938], Loss: 0.4635641574859619\n",
      "Validation: Epoch [23], Batch [549/938], Loss: 0.5682137608528137\n",
      "Validation: Epoch [23], Batch [550/938], Loss: 0.3561081290245056\n",
      "Validation: Epoch [23], Batch [551/938], Loss: 0.4858686327934265\n",
      "Validation: Epoch [23], Batch [552/938], Loss: 0.4808700680732727\n",
      "Validation: Epoch [23], Batch [553/938], Loss: 0.32083046436309814\n",
      "Validation: Epoch [23], Batch [554/938], Loss: 0.4534720480442047\n",
      "Validation: Epoch [23], Batch [555/938], Loss: 0.46766918897628784\n",
      "Validation: Epoch [23], Batch [556/938], Loss: 0.6880130767822266\n",
      "Validation: Epoch [23], Batch [557/938], Loss: 0.28388717770576477\n",
      "Validation: Epoch [23], Batch [558/938], Loss: 0.430293470621109\n",
      "Validation: Epoch [23], Batch [559/938], Loss: 0.5508464574813843\n",
      "Validation: Epoch [23], Batch [560/938], Loss: 0.45374205708503723\n",
      "Validation: Epoch [23], Batch [561/938], Loss: 0.5534034967422485\n",
      "Validation: Epoch [23], Batch [562/938], Loss: 0.6037700772285461\n",
      "Validation: Epoch [23], Batch [563/938], Loss: 0.5368989109992981\n",
      "Validation: Epoch [23], Batch [564/938], Loss: 0.48509907722473145\n",
      "Validation: Epoch [23], Batch [565/938], Loss: 0.7082587480545044\n",
      "Validation: Epoch [23], Batch [566/938], Loss: 0.5068173408508301\n",
      "Validation: Epoch [23], Batch [567/938], Loss: 0.43747445940971375\n",
      "Validation: Epoch [23], Batch [568/938], Loss: 0.3574438691139221\n",
      "Validation: Epoch [23], Batch [569/938], Loss: 0.378658264875412\n",
      "Validation: Epoch [23], Batch [570/938], Loss: 0.4849805235862732\n",
      "Validation: Epoch [23], Batch [571/938], Loss: 0.33645695447921753\n",
      "Validation: Epoch [23], Batch [572/938], Loss: 0.39093828201293945\n",
      "Validation: Epoch [23], Batch [573/938], Loss: 0.4335672855377197\n",
      "Validation: Epoch [23], Batch [574/938], Loss: 0.41861864924430847\n",
      "Validation: Epoch [23], Batch [575/938], Loss: 0.34481891989707947\n",
      "Validation: Epoch [23], Batch [576/938], Loss: 0.3357027471065521\n",
      "Validation: Epoch [23], Batch [577/938], Loss: 0.6655701398849487\n",
      "Validation: Epoch [23], Batch [578/938], Loss: 0.30438029766082764\n",
      "Validation: Epoch [23], Batch [579/938], Loss: 0.5755118131637573\n",
      "Validation: Epoch [23], Batch [580/938], Loss: 0.5454075336456299\n",
      "Validation: Epoch [23], Batch [581/938], Loss: 0.42691370844841003\n",
      "Validation: Epoch [23], Batch [582/938], Loss: 0.34579548239707947\n",
      "Validation: Epoch [23], Batch [583/938], Loss: 0.5771216750144958\n",
      "Validation: Epoch [23], Batch [584/938], Loss: 0.29299044609069824\n",
      "Validation: Epoch [23], Batch [585/938], Loss: 0.4047241508960724\n",
      "Validation: Epoch [23], Batch [586/938], Loss: 0.5218991041183472\n",
      "Validation: Epoch [23], Batch [587/938], Loss: 0.2442939281463623\n",
      "Validation: Epoch [23], Batch [588/938], Loss: 0.3293561339378357\n",
      "Validation: Epoch [23], Batch [589/938], Loss: 0.4110985994338989\n",
      "Validation: Epoch [23], Batch [590/938], Loss: 0.49147018790245056\n",
      "Validation: Epoch [23], Batch [591/938], Loss: 0.2595374286174774\n",
      "Validation: Epoch [23], Batch [592/938], Loss: 0.35243940353393555\n",
      "Validation: Epoch [23], Batch [593/938], Loss: 0.4476873576641083\n",
      "Validation: Epoch [23], Batch [594/938], Loss: 0.2099759578704834\n",
      "Validation: Epoch [23], Batch [595/938], Loss: 0.40060892701148987\n",
      "Validation: Epoch [23], Batch [596/938], Loss: 0.6356436014175415\n",
      "Validation: Epoch [23], Batch [597/938], Loss: 0.7187305688858032\n",
      "Validation: Epoch [23], Batch [598/938], Loss: 0.40860819816589355\n",
      "Validation: Epoch [23], Batch [599/938], Loss: 0.46310997009277344\n",
      "Validation: Epoch [23], Batch [600/938], Loss: 0.3630804121494293\n",
      "Validation: Epoch [23], Batch [601/938], Loss: 0.41351956129074097\n",
      "Validation: Epoch [23], Batch [602/938], Loss: 0.4502502977848053\n",
      "Validation: Epoch [23], Batch [603/938], Loss: 0.4143104553222656\n",
      "Validation: Epoch [23], Batch [604/938], Loss: 0.49150845408439636\n",
      "Validation: Epoch [23], Batch [605/938], Loss: 0.4946354329586029\n",
      "Validation: Epoch [23], Batch [606/938], Loss: 0.48582667112350464\n",
      "Validation: Epoch [23], Batch [607/938], Loss: 0.523591160774231\n",
      "Validation: Epoch [23], Batch [608/938], Loss: 0.5291085243225098\n",
      "Validation: Epoch [23], Batch [609/938], Loss: 0.4684479832649231\n",
      "Validation: Epoch [23], Batch [610/938], Loss: 0.4262527823448181\n",
      "Validation: Epoch [23], Batch [611/938], Loss: 0.39207786321640015\n",
      "Validation: Epoch [23], Batch [612/938], Loss: 0.3816846013069153\n",
      "Validation: Epoch [23], Batch [613/938], Loss: 0.46321970224380493\n",
      "Validation: Epoch [23], Batch [614/938], Loss: 0.42869192361831665\n",
      "Validation: Epoch [23], Batch [615/938], Loss: 0.42453762888908386\n",
      "Validation: Epoch [23], Batch [616/938], Loss: 0.416764497756958\n",
      "Validation: Epoch [23], Batch [617/938], Loss: 0.7141982913017273\n",
      "Validation: Epoch [23], Batch [618/938], Loss: 0.4083445966243744\n",
      "Validation: Epoch [23], Batch [619/938], Loss: 0.4074738025665283\n",
      "Validation: Epoch [23], Batch [620/938], Loss: 0.3682023882865906\n",
      "Validation: Epoch [23], Batch [621/938], Loss: 0.2549874782562256\n",
      "Validation: Epoch [23], Batch [622/938], Loss: 0.3418648838996887\n",
      "Validation: Epoch [23], Batch [623/938], Loss: 0.4665634036064148\n",
      "Validation: Epoch [23], Batch [624/938], Loss: 0.4017805755138397\n",
      "Validation: Epoch [23], Batch [625/938], Loss: 0.38625386357307434\n",
      "Validation: Epoch [23], Batch [626/938], Loss: 0.6352834701538086\n",
      "Validation: Epoch [23], Batch [627/938], Loss: 0.3284898102283478\n",
      "Validation: Epoch [23], Batch [628/938], Loss: 0.5029858946800232\n",
      "Validation: Epoch [23], Batch [629/938], Loss: 0.604215681552887\n",
      "Validation: Epoch [23], Batch [630/938], Loss: 0.5446575284004211\n",
      "Validation: Epoch [23], Batch [631/938], Loss: 0.6484794616699219\n",
      "Validation: Epoch [23], Batch [632/938], Loss: 0.38010960817337036\n",
      "Validation: Epoch [23], Batch [633/938], Loss: 0.48805275559425354\n",
      "Validation: Epoch [23], Batch [634/938], Loss: 0.460172176361084\n",
      "Validation: Epoch [23], Batch [635/938], Loss: 0.486092746257782\n",
      "Validation: Epoch [23], Batch [636/938], Loss: 0.46707430481910706\n",
      "Validation: Epoch [23], Batch [637/938], Loss: 0.5734004378318787\n",
      "Validation: Epoch [23], Batch [638/938], Loss: 0.39247336983680725\n",
      "Validation: Epoch [23], Batch [639/938], Loss: 0.6283268332481384\n",
      "Validation: Epoch [23], Batch [640/938], Loss: 0.4307239055633545\n",
      "Validation: Epoch [23], Batch [641/938], Loss: 0.5826015472412109\n",
      "Validation: Epoch [23], Batch [642/938], Loss: 0.4872596561908722\n",
      "Validation: Epoch [23], Batch [643/938], Loss: 0.2669534683227539\n",
      "Validation: Epoch [23], Batch [644/938], Loss: 0.3390370309352875\n",
      "Validation: Epoch [23], Batch [645/938], Loss: 0.4524308443069458\n",
      "Validation: Epoch [23], Batch [646/938], Loss: 0.7258267402648926\n",
      "Validation: Epoch [23], Batch [647/938], Loss: 0.5059685111045837\n",
      "Validation: Epoch [23], Batch [648/938], Loss: 0.3828464150428772\n",
      "Validation: Epoch [23], Batch [649/938], Loss: 0.5739805698394775\n",
      "Validation: Epoch [23], Batch [650/938], Loss: 0.42616114020347595\n",
      "Validation: Epoch [23], Batch [651/938], Loss: 0.3538722097873688\n",
      "Validation: Epoch [23], Batch [652/938], Loss: 0.3282608985900879\n",
      "Validation: Epoch [23], Batch [653/938], Loss: 0.5567536354064941\n",
      "Validation: Epoch [23], Batch [654/938], Loss: 0.3742170035839081\n",
      "Validation: Epoch [23], Batch [655/938], Loss: 0.42542997002601624\n",
      "Validation: Epoch [23], Batch [656/938], Loss: 0.43309324979782104\n",
      "Validation: Epoch [23], Batch [657/938], Loss: 0.5074653625488281\n",
      "Validation: Epoch [23], Batch [658/938], Loss: 0.4171205461025238\n",
      "Validation: Epoch [23], Batch [659/938], Loss: 0.5635222792625427\n",
      "Validation: Epoch [23], Batch [660/938], Loss: 0.4599496126174927\n",
      "Validation: Epoch [23], Batch [661/938], Loss: 0.3556537926197052\n",
      "Validation: Epoch [23], Batch [662/938], Loss: 0.39254701137542725\n",
      "Validation: Epoch [23], Batch [663/938], Loss: 0.6268587112426758\n",
      "Validation: Epoch [23], Batch [664/938], Loss: 0.42509087920188904\n",
      "Validation: Epoch [23], Batch [665/938], Loss: 0.4060642719268799\n",
      "Validation: Epoch [23], Batch [666/938], Loss: 0.39895540475845337\n",
      "Validation: Epoch [23], Batch [667/938], Loss: 0.34351846575737\n",
      "Validation: Epoch [23], Batch [668/938], Loss: 0.23965570330619812\n",
      "Validation: Epoch [23], Batch [669/938], Loss: 0.3401995897293091\n",
      "Validation: Epoch [23], Batch [670/938], Loss: 0.7589480876922607\n",
      "Validation: Epoch [23], Batch [671/938], Loss: 0.6075655817985535\n",
      "Validation: Epoch [23], Batch [672/938], Loss: 0.5787156820297241\n",
      "Validation: Epoch [23], Batch [673/938], Loss: 0.43482470512390137\n",
      "Validation: Epoch [23], Batch [674/938], Loss: 0.30248966813087463\n",
      "Validation: Epoch [23], Batch [675/938], Loss: 0.4766271114349365\n",
      "Validation: Epoch [23], Batch [676/938], Loss: 0.44126877188682556\n",
      "Validation: Epoch [23], Batch [677/938], Loss: 0.33943384885787964\n",
      "Validation: Epoch [23], Batch [678/938], Loss: 0.44045570492744446\n",
      "Validation: Epoch [23], Batch [679/938], Loss: 0.2714789807796478\n",
      "Validation: Epoch [23], Batch [680/938], Loss: 0.29574498534202576\n",
      "Validation: Epoch [23], Batch [681/938], Loss: 0.43945789337158203\n",
      "Validation: Epoch [23], Batch [682/938], Loss: 0.4286133050918579\n",
      "Validation: Epoch [23], Batch [683/938], Loss: 0.35849082469940186\n",
      "Validation: Epoch [23], Batch [684/938], Loss: 0.4004088342189789\n",
      "Validation: Epoch [23], Batch [685/938], Loss: 0.42471927404403687\n",
      "Validation: Epoch [23], Batch [686/938], Loss: 0.4837811589241028\n",
      "Validation: Epoch [23], Batch [687/938], Loss: 0.49846839904785156\n",
      "Validation: Epoch [23], Batch [688/938], Loss: 0.5200484991073608\n",
      "Validation: Epoch [23], Batch [689/938], Loss: 0.6249278783798218\n",
      "Validation: Epoch [23], Batch [690/938], Loss: 0.3792007565498352\n",
      "Validation: Epoch [23], Batch [691/938], Loss: 0.4771617650985718\n",
      "Validation: Epoch [23], Batch [692/938], Loss: 0.5755192041397095\n",
      "Validation: Epoch [23], Batch [693/938], Loss: 0.38896676898002625\n",
      "Validation: Epoch [23], Batch [694/938], Loss: 0.3526573181152344\n",
      "Validation: Epoch [23], Batch [695/938], Loss: 0.4892159402370453\n",
      "Validation: Epoch [23], Batch [696/938], Loss: 0.531870424747467\n",
      "Validation: Epoch [23], Batch [697/938], Loss: 0.5915951132774353\n",
      "Validation: Epoch [23], Batch [698/938], Loss: 0.6327164769172668\n",
      "Validation: Epoch [23], Batch [699/938], Loss: 0.4571145474910736\n",
      "Validation: Epoch [23], Batch [700/938], Loss: 0.325575590133667\n",
      "Validation: Epoch [23], Batch [701/938], Loss: 0.6191248893737793\n",
      "Validation: Epoch [23], Batch [702/938], Loss: 0.3727986216545105\n",
      "Validation: Epoch [23], Batch [703/938], Loss: 0.48353955149650574\n",
      "Validation: Epoch [23], Batch [704/938], Loss: 0.3853086829185486\n",
      "Validation: Epoch [23], Batch [705/938], Loss: 0.5293126106262207\n",
      "Validation: Epoch [23], Batch [706/938], Loss: 0.3929319679737091\n",
      "Validation: Epoch [23], Batch [707/938], Loss: 0.4970114529132843\n",
      "Validation: Epoch [23], Batch [708/938], Loss: 0.5196051597595215\n",
      "Validation: Epoch [23], Batch [709/938], Loss: 0.4473278820514679\n",
      "Validation: Epoch [23], Batch [710/938], Loss: 0.3476705551147461\n",
      "Validation: Epoch [23], Batch [711/938], Loss: 0.5181755423545837\n",
      "Validation: Epoch [23], Batch [712/938], Loss: 0.479272723197937\n",
      "Validation: Epoch [23], Batch [713/938], Loss: 0.5833653211593628\n",
      "Validation: Epoch [23], Batch [714/938], Loss: 0.5406001210212708\n",
      "Validation: Epoch [23], Batch [715/938], Loss: 0.4958646893501282\n",
      "Validation: Epoch [23], Batch [716/938], Loss: 0.6145554780960083\n",
      "Validation: Epoch [23], Batch [717/938], Loss: 0.33865123987197876\n",
      "Validation: Epoch [23], Batch [718/938], Loss: 0.28315746784210205\n",
      "Validation: Epoch [23], Batch [719/938], Loss: 0.616512656211853\n",
      "Validation: Epoch [23], Batch [720/938], Loss: 0.4909857511520386\n",
      "Validation: Epoch [23], Batch [721/938], Loss: 0.4136272966861725\n",
      "Validation: Epoch [23], Batch [722/938], Loss: 0.4859829545021057\n",
      "Validation: Epoch [23], Batch [723/938], Loss: 0.311470091342926\n",
      "Validation: Epoch [23], Batch [724/938], Loss: 0.7975331544876099\n",
      "Validation: Epoch [23], Batch [725/938], Loss: 0.3971448540687561\n",
      "Validation: Epoch [23], Batch [726/938], Loss: 0.5391342639923096\n",
      "Validation: Epoch [23], Batch [727/938], Loss: 0.6089305877685547\n",
      "Validation: Epoch [23], Batch [728/938], Loss: 0.47853463888168335\n",
      "Validation: Epoch [23], Batch [729/938], Loss: 0.3790592849254608\n",
      "Validation: Epoch [23], Batch [730/938], Loss: 0.2375032901763916\n",
      "Validation: Epoch [23], Batch [731/938], Loss: 0.5434491634368896\n",
      "Validation: Epoch [23], Batch [732/938], Loss: 0.5068631768226624\n",
      "Validation: Epoch [23], Batch [733/938], Loss: 0.6347338557243347\n",
      "Validation: Epoch [23], Batch [734/938], Loss: 0.37397897243499756\n",
      "Validation: Epoch [23], Batch [735/938], Loss: 0.3649722635746002\n",
      "Validation: Epoch [23], Batch [736/938], Loss: 0.5082871317863464\n",
      "Validation: Epoch [23], Batch [737/938], Loss: 0.44506925344467163\n",
      "Validation: Epoch [23], Batch [738/938], Loss: 0.3740300238132477\n",
      "Validation: Epoch [23], Batch [739/938], Loss: 0.29411637783050537\n",
      "Validation: Epoch [23], Batch [740/938], Loss: 0.6185979843139648\n",
      "Validation: Epoch [23], Batch [741/938], Loss: 0.6479447484016418\n",
      "Validation: Epoch [23], Batch [742/938], Loss: 0.43553632497787476\n",
      "Validation: Epoch [23], Batch [743/938], Loss: 0.49491772055625916\n",
      "Validation: Epoch [23], Batch [744/938], Loss: 0.43928587436676025\n",
      "Validation: Epoch [23], Batch [745/938], Loss: 0.4769901633262634\n",
      "Validation: Epoch [23], Batch [746/938], Loss: 0.4565507471561432\n",
      "Validation: Epoch [23], Batch [747/938], Loss: 0.35655611753463745\n",
      "Validation: Epoch [23], Batch [748/938], Loss: 0.4755174517631531\n",
      "Validation: Epoch [23], Batch [749/938], Loss: 0.27496835589408875\n",
      "Validation: Epoch [23], Batch [750/938], Loss: 0.29267555475234985\n",
      "Validation: Epoch [23], Batch [751/938], Loss: 0.5850557088851929\n",
      "Validation: Epoch [23], Batch [752/938], Loss: 0.6076042652130127\n",
      "Validation: Epoch [23], Batch [753/938], Loss: 0.5184321999549866\n",
      "Validation: Epoch [23], Batch [754/938], Loss: 0.471418172121048\n",
      "Validation: Epoch [23], Batch [755/938], Loss: 0.5777614116668701\n",
      "Validation: Epoch [23], Batch [756/938], Loss: 0.43530309200286865\n",
      "Validation: Epoch [23], Batch [757/938], Loss: 0.5253351926803589\n",
      "Validation: Epoch [23], Batch [758/938], Loss: 0.4109930694103241\n",
      "Validation: Epoch [23], Batch [759/938], Loss: 0.5848730802536011\n",
      "Validation: Epoch [23], Batch [760/938], Loss: 0.28107988834381104\n",
      "Validation: Epoch [23], Batch [761/938], Loss: 0.5613495707511902\n",
      "Validation: Epoch [23], Batch [762/938], Loss: 0.460662841796875\n",
      "Validation: Epoch [23], Batch [763/938], Loss: 0.42591992020606995\n",
      "Validation: Epoch [23], Batch [764/938], Loss: 0.4828592538833618\n",
      "Validation: Epoch [23], Batch [765/938], Loss: 0.457457035779953\n",
      "Validation: Epoch [23], Batch [766/938], Loss: 0.5643134713172913\n",
      "Validation: Epoch [23], Batch [767/938], Loss: 0.5011793375015259\n",
      "Validation: Epoch [23], Batch [768/938], Loss: 0.30379775166511536\n",
      "Validation: Epoch [23], Batch [769/938], Loss: 0.6226571798324585\n",
      "Validation: Epoch [23], Batch [770/938], Loss: 0.32063308358192444\n",
      "Validation: Epoch [23], Batch [771/938], Loss: 0.4589441418647766\n",
      "Validation: Epoch [23], Batch [772/938], Loss: 0.21312406659126282\n",
      "Validation: Epoch [23], Batch [773/938], Loss: 0.6003732681274414\n",
      "Validation: Epoch [23], Batch [774/938], Loss: 0.3289761245250702\n",
      "Validation: Epoch [23], Batch [775/938], Loss: 0.4040120244026184\n",
      "Validation: Epoch [23], Batch [776/938], Loss: 0.39449045062065125\n",
      "Validation: Epoch [23], Batch [777/938], Loss: 0.34786900877952576\n",
      "Validation: Epoch [23], Batch [778/938], Loss: 0.38174349069595337\n",
      "Validation: Epoch [23], Batch [779/938], Loss: 0.49367785453796387\n",
      "Validation: Epoch [23], Batch [780/938], Loss: 0.4543065130710602\n",
      "Validation: Epoch [23], Batch [781/938], Loss: 0.5069334506988525\n",
      "Validation: Epoch [23], Batch [782/938], Loss: 0.32114142179489136\n",
      "Validation: Epoch [23], Batch [783/938], Loss: 0.6443333029747009\n",
      "Validation: Epoch [23], Batch [784/938], Loss: 0.3386048972606659\n",
      "Validation: Epoch [23], Batch [785/938], Loss: 0.40678930282592773\n",
      "Validation: Epoch [23], Batch [786/938], Loss: 0.477102667093277\n",
      "Validation: Epoch [23], Batch [787/938], Loss: 0.4360067844390869\n",
      "Validation: Epoch [23], Batch [788/938], Loss: 0.40545526146888733\n",
      "Validation: Epoch [23], Batch [789/938], Loss: 0.44355812668800354\n",
      "Validation: Epoch [23], Batch [790/938], Loss: 0.4306880831718445\n",
      "Validation: Epoch [23], Batch [791/938], Loss: 0.5876424312591553\n",
      "Validation: Epoch [23], Batch [792/938], Loss: 0.6483578085899353\n",
      "Validation: Epoch [23], Batch [793/938], Loss: 0.42066675424575806\n",
      "Validation: Epoch [23], Batch [794/938], Loss: 0.4802648425102234\n",
      "Validation: Epoch [23], Batch [795/938], Loss: 0.3401485085487366\n",
      "Validation: Epoch [23], Batch [796/938], Loss: 0.3873692452907562\n",
      "Validation: Epoch [23], Batch [797/938], Loss: 0.3050321042537689\n",
      "Validation: Epoch [23], Batch [798/938], Loss: 0.4697726368904114\n",
      "Validation: Epoch [23], Batch [799/938], Loss: 0.3823200762271881\n",
      "Validation: Epoch [23], Batch [800/938], Loss: 0.4086470901966095\n",
      "Validation: Epoch [23], Batch [801/938], Loss: 0.38238829374313354\n",
      "Validation: Epoch [23], Batch [802/938], Loss: 0.36875829100608826\n",
      "Validation: Epoch [23], Batch [803/938], Loss: 0.5147964358329773\n",
      "Validation: Epoch [23], Batch [804/938], Loss: 0.3844295144081116\n",
      "Validation: Epoch [23], Batch [805/938], Loss: 0.4544442296028137\n",
      "Validation: Epoch [23], Batch [806/938], Loss: 0.7335444688796997\n",
      "Validation: Epoch [23], Batch [807/938], Loss: 0.5379256010055542\n",
      "Validation: Epoch [23], Batch [808/938], Loss: 0.4075107276439667\n",
      "Validation: Epoch [23], Batch [809/938], Loss: 0.3269079923629761\n",
      "Validation: Epoch [23], Batch [810/938], Loss: 0.46060875058174133\n",
      "Validation: Epoch [23], Batch [811/938], Loss: 0.3776421546936035\n",
      "Validation: Epoch [23], Batch [812/938], Loss: 0.35411152243614197\n",
      "Validation: Epoch [23], Batch [813/938], Loss: 0.47983527183532715\n",
      "Validation: Epoch [23], Batch [814/938], Loss: 0.44909024238586426\n",
      "Validation: Epoch [23], Batch [815/938], Loss: 0.4659060835838318\n",
      "Validation: Epoch [23], Batch [816/938], Loss: 0.46089959144592285\n",
      "Validation: Epoch [23], Batch [817/938], Loss: 0.7505113482475281\n",
      "Validation: Epoch [23], Batch [818/938], Loss: 0.3230072557926178\n",
      "Validation: Epoch [23], Batch [819/938], Loss: 0.35542532801628113\n",
      "Validation: Epoch [23], Batch [820/938], Loss: 0.36680540442466736\n",
      "Validation: Epoch [23], Batch [821/938], Loss: 0.34184592962265015\n",
      "Validation: Epoch [23], Batch [822/938], Loss: 0.4475095272064209\n",
      "Validation: Epoch [23], Batch [823/938], Loss: 0.3566328287124634\n",
      "Validation: Epoch [23], Batch [824/938], Loss: 0.6074283123016357\n",
      "Validation: Epoch [23], Batch [825/938], Loss: 0.5243085026741028\n",
      "Validation: Epoch [23], Batch [826/938], Loss: 0.4717380404472351\n",
      "Validation: Epoch [23], Batch [827/938], Loss: 0.2874827980995178\n",
      "Validation: Epoch [23], Batch [828/938], Loss: 0.837550163269043\n",
      "Validation: Epoch [23], Batch [829/938], Loss: 0.3183891773223877\n",
      "Validation: Epoch [23], Batch [830/938], Loss: 0.352594256401062\n",
      "Validation: Epoch [23], Batch [831/938], Loss: 0.24277721345424652\n",
      "Validation: Epoch [23], Batch [832/938], Loss: 0.34006625413894653\n",
      "Validation: Epoch [23], Batch [833/938], Loss: 0.32918494939804077\n",
      "Validation: Epoch [23], Batch [834/938], Loss: 0.19614386558532715\n",
      "Validation: Epoch [23], Batch [835/938], Loss: 0.3209068775177002\n",
      "Validation: Epoch [23], Batch [836/938], Loss: 0.3693745732307434\n",
      "Validation: Epoch [23], Batch [837/938], Loss: 0.3953777253627777\n",
      "Validation: Epoch [23], Batch [838/938], Loss: 0.45167768001556396\n",
      "Validation: Epoch [23], Batch [839/938], Loss: 0.6099832057952881\n",
      "Validation: Epoch [23], Batch [840/938], Loss: 0.3231619894504547\n",
      "Validation: Epoch [23], Batch [841/938], Loss: 0.4836251735687256\n",
      "Validation: Epoch [23], Batch [842/938], Loss: 0.4719761908054352\n",
      "Validation: Epoch [23], Batch [843/938], Loss: 0.4926481544971466\n",
      "Validation: Epoch [23], Batch [844/938], Loss: 0.3994593024253845\n",
      "Validation: Epoch [23], Batch [845/938], Loss: 0.4603429436683655\n",
      "Validation: Epoch [23], Batch [846/938], Loss: 0.43412262201309204\n",
      "Validation: Epoch [23], Batch [847/938], Loss: 0.3787825107574463\n",
      "Validation: Epoch [23], Batch [848/938], Loss: 0.37921974062919617\n",
      "Validation: Epoch [23], Batch [849/938], Loss: 0.41131871938705444\n",
      "Validation: Epoch [23], Batch [850/938], Loss: 0.3796234130859375\n",
      "Validation: Epoch [23], Batch [851/938], Loss: 0.4413721561431885\n",
      "Validation: Epoch [23], Batch [852/938], Loss: 0.5300150513648987\n",
      "Validation: Epoch [23], Batch [853/938], Loss: 0.4097701907157898\n",
      "Validation: Epoch [23], Batch [854/938], Loss: 0.6204644441604614\n",
      "Validation: Epoch [23], Batch [855/938], Loss: 0.42087703943252563\n",
      "Validation: Epoch [23], Batch [856/938], Loss: 0.7936297059059143\n",
      "Validation: Epoch [23], Batch [857/938], Loss: 0.315982848405838\n",
      "Validation: Epoch [23], Batch [858/938], Loss: 0.5879012942314148\n",
      "Validation: Epoch [23], Batch [859/938], Loss: 0.4081069827079773\n",
      "Validation: Epoch [23], Batch [860/938], Loss: 0.5371171832084656\n",
      "Validation: Epoch [23], Batch [861/938], Loss: 0.42596420645713806\n",
      "Validation: Epoch [23], Batch [862/938], Loss: 0.5642517805099487\n",
      "Validation: Epoch [23], Batch [863/938], Loss: 0.6690788269042969\n",
      "Validation: Epoch [23], Batch [864/938], Loss: 0.5571026802062988\n",
      "Validation: Epoch [23], Batch [865/938], Loss: 0.510183572769165\n",
      "Validation: Epoch [23], Batch [866/938], Loss: 0.439900279045105\n",
      "Validation: Epoch [23], Batch [867/938], Loss: 0.48329806327819824\n",
      "Validation: Epoch [23], Batch [868/938], Loss: 0.366791695356369\n",
      "Validation: Epoch [23], Batch [869/938], Loss: 0.6635825037956238\n",
      "Validation: Epoch [23], Batch [870/938], Loss: 0.35542500019073486\n",
      "Validation: Epoch [23], Batch [871/938], Loss: 0.2726667523384094\n",
      "Validation: Epoch [23], Batch [872/938], Loss: 0.3400370180606842\n",
      "Validation: Epoch [23], Batch [873/938], Loss: 0.302298903465271\n",
      "Validation: Epoch [23], Batch [874/938], Loss: 0.5654231905937195\n",
      "Validation: Epoch [23], Batch [875/938], Loss: 0.5477225184440613\n",
      "Validation: Epoch [23], Batch [876/938], Loss: 0.4604232907295227\n",
      "Validation: Epoch [23], Batch [877/938], Loss: 0.38457101583480835\n",
      "Validation: Epoch [23], Batch [878/938], Loss: 0.6474462747573853\n",
      "Validation: Epoch [23], Batch [879/938], Loss: 0.3732759952545166\n",
      "Validation: Epoch [23], Batch [880/938], Loss: 0.6606082916259766\n",
      "Validation: Epoch [23], Batch [881/938], Loss: 0.6100813150405884\n",
      "Validation: Epoch [23], Batch [882/938], Loss: 0.5252982378005981\n",
      "Validation: Epoch [23], Batch [883/938], Loss: 0.622728705406189\n",
      "Validation: Epoch [23], Batch [884/938], Loss: 0.3628000020980835\n",
      "Validation: Epoch [23], Batch [885/938], Loss: 0.5651184916496277\n",
      "Validation: Epoch [23], Batch [886/938], Loss: 0.33469194173812866\n",
      "Validation: Epoch [23], Batch [887/938], Loss: 0.49207669496536255\n",
      "Validation: Epoch [23], Batch [888/938], Loss: 0.7623018622398376\n",
      "Validation: Epoch [23], Batch [889/938], Loss: 0.4905543923377991\n",
      "Validation: Epoch [23], Batch [890/938], Loss: 0.5180708169937134\n",
      "Validation: Epoch [23], Batch [891/938], Loss: 0.3617931008338928\n",
      "Validation: Epoch [23], Batch [892/938], Loss: 0.27000975608825684\n",
      "Validation: Epoch [23], Batch [893/938], Loss: 0.3847636282444\n",
      "Validation: Epoch [23], Batch [894/938], Loss: 0.36744239926338196\n",
      "Validation: Epoch [23], Batch [895/938], Loss: 0.4148485064506531\n",
      "Validation: Epoch [23], Batch [896/938], Loss: 0.2587852478027344\n",
      "Validation: Epoch [23], Batch [897/938], Loss: 0.6221853494644165\n",
      "Validation: Epoch [23], Batch [898/938], Loss: 0.5518733859062195\n",
      "Validation: Epoch [23], Batch [899/938], Loss: 0.45050448179244995\n",
      "Validation: Epoch [23], Batch [900/938], Loss: 0.3846868872642517\n",
      "Validation: Epoch [23], Batch [901/938], Loss: 0.27040308713912964\n",
      "Validation: Epoch [23], Batch [902/938], Loss: 0.27335089445114136\n",
      "Validation: Epoch [23], Batch [903/938], Loss: 0.4646875262260437\n",
      "Validation: Epoch [23], Batch [904/938], Loss: 0.6254755258560181\n",
      "Validation: Epoch [23], Batch [905/938], Loss: 0.46858230233192444\n",
      "Validation: Epoch [23], Batch [906/938], Loss: 0.5615268349647522\n",
      "Validation: Epoch [23], Batch [907/938], Loss: 0.40939679741859436\n",
      "Validation: Epoch [23], Batch [908/938], Loss: 0.7232309579849243\n",
      "Validation: Epoch [23], Batch [909/938], Loss: 0.31711772084236145\n",
      "Validation: Epoch [23], Batch [910/938], Loss: 0.35708850622177124\n",
      "Validation: Epoch [23], Batch [911/938], Loss: 0.48260125517845154\n",
      "Validation: Epoch [23], Batch [912/938], Loss: 0.3845268189907074\n",
      "Validation: Epoch [23], Batch [913/938], Loss: 0.5251659154891968\n",
      "Validation: Epoch [23], Batch [914/938], Loss: 0.34886711835861206\n",
      "Validation: Epoch [23], Batch [915/938], Loss: 0.45084136724472046\n",
      "Validation: Epoch [23], Batch [916/938], Loss: 0.5191541910171509\n",
      "Validation: Epoch [23], Batch [917/938], Loss: 0.3698146343231201\n",
      "Validation: Epoch [23], Batch [918/938], Loss: 0.6014704704284668\n",
      "Validation: Epoch [23], Batch [919/938], Loss: 0.3641982674598694\n",
      "Validation: Epoch [23], Batch [920/938], Loss: 0.31669753789901733\n",
      "Validation: Epoch [23], Batch [921/938], Loss: 0.6654192805290222\n",
      "Validation: Epoch [23], Batch [922/938], Loss: 0.22239941358566284\n",
      "Validation: Epoch [23], Batch [923/938], Loss: 0.43736281991004944\n",
      "Validation: Epoch [23], Batch [924/938], Loss: 0.579304039478302\n",
      "Validation: Epoch [23], Batch [925/938], Loss: 0.4014524221420288\n",
      "Validation: Epoch [23], Batch [926/938], Loss: 0.5790543556213379\n",
      "Validation: Epoch [23], Batch [927/938], Loss: 0.4081672430038452\n",
      "Validation: Epoch [23], Batch [928/938], Loss: 0.49248185753822327\n",
      "Validation: Epoch [23], Batch [929/938], Loss: 0.28277483582496643\n",
      "Validation: Epoch [23], Batch [930/938], Loss: 0.3128555417060852\n",
      "Validation: Epoch [23], Batch [931/938], Loss: 0.3548143804073334\n",
      "Validation: Epoch [23], Batch [932/938], Loss: 0.5704545974731445\n",
      "Validation: Epoch [23], Batch [933/938], Loss: 0.4351431131362915\n",
      "Validation: Epoch [23], Batch [934/938], Loss: 0.3790845572948456\n",
      "Validation: Epoch [23], Batch [935/938], Loss: 0.4925391674041748\n",
      "Validation: Epoch [23], Batch [936/938], Loss: 0.43215498328208923\n",
      "Validation: Epoch [23], Batch [937/938], Loss: 0.7144413590431213\n",
      "Validation: Epoch [23], Batch [938/938], Loss: 0.2631036341190338\n",
      "Accuracy of test set: 0.8380166666666666\n",
      "Train: Epoch [24], Batch [1/938], Loss: 0.695023775100708\n",
      "Train: Epoch [24], Batch [2/938], Loss: 0.4204767048358917\n",
      "Train: Epoch [24], Batch [3/938], Loss: 0.32058975100517273\n",
      "Train: Epoch [24], Batch [4/938], Loss: 0.376969575881958\n",
      "Train: Epoch [24], Batch [5/938], Loss: 0.43559369444847107\n",
      "Train: Epoch [24], Batch [6/938], Loss: 0.3682767450809479\n",
      "Train: Epoch [24], Batch [7/938], Loss: 0.3840322494506836\n",
      "Train: Epoch [24], Batch [8/938], Loss: 0.5894108414649963\n",
      "Train: Epoch [24], Batch [9/938], Loss: 0.2895996570587158\n",
      "Train: Epoch [24], Batch [10/938], Loss: 0.4861685633659363\n",
      "Train: Epoch [24], Batch [11/938], Loss: 0.6356273889541626\n",
      "Train: Epoch [24], Batch [12/938], Loss: 0.40013226866722107\n",
      "Train: Epoch [24], Batch [13/938], Loss: 0.47880423069000244\n",
      "Train: Epoch [24], Batch [14/938], Loss: 0.3190052807331085\n",
      "Train: Epoch [24], Batch [15/938], Loss: 0.27791696786880493\n",
      "Train: Epoch [24], Batch [16/938], Loss: 0.3612629175186157\n",
      "Train: Epoch [24], Batch [17/938], Loss: 0.4526602029800415\n",
      "Train: Epoch [24], Batch [18/938], Loss: 0.4471033811569214\n",
      "Train: Epoch [24], Batch [19/938], Loss: 0.34701699018478394\n",
      "Train: Epoch [24], Batch [20/938], Loss: 0.3583681583404541\n",
      "Train: Epoch [24], Batch [21/938], Loss: 0.613806962966919\n",
      "Train: Epoch [24], Batch [22/938], Loss: 0.38034242391586304\n",
      "Train: Epoch [24], Batch [23/938], Loss: 0.48395058512687683\n",
      "Train: Epoch [24], Batch [24/938], Loss: 0.48983055353164673\n",
      "Train: Epoch [24], Batch [25/938], Loss: 0.4071345925331116\n",
      "Train: Epoch [24], Batch [26/938], Loss: 0.5405659675598145\n",
      "Train: Epoch [24], Batch [27/938], Loss: 0.40669727325439453\n",
      "Train: Epoch [24], Batch [28/938], Loss: 0.4476708471775055\n",
      "Train: Epoch [24], Batch [29/938], Loss: 0.4325793981552124\n",
      "Train: Epoch [24], Batch [30/938], Loss: 0.3413855731487274\n",
      "Train: Epoch [24], Batch [31/938], Loss: 0.461811363697052\n",
      "Train: Epoch [24], Batch [32/938], Loss: 0.29359179735183716\n",
      "Train: Epoch [24], Batch [33/938], Loss: 0.3226964473724365\n",
      "Train: Epoch [24], Batch [34/938], Loss: 0.47104817628860474\n",
      "Train: Epoch [24], Batch [35/938], Loss: 0.45022356510162354\n",
      "Train: Epoch [24], Batch [36/938], Loss: 0.3661114275455475\n",
      "Train: Epoch [24], Batch [37/938], Loss: 0.45238935947418213\n",
      "Train: Epoch [24], Batch [38/938], Loss: 0.4994986951351166\n",
      "Train: Epoch [24], Batch [39/938], Loss: 0.6985213756561279\n",
      "Train: Epoch [24], Batch [40/938], Loss: 0.5404142737388611\n",
      "Train: Epoch [24], Batch [41/938], Loss: 0.6246363520622253\n",
      "Train: Epoch [24], Batch [42/938], Loss: 0.28794074058532715\n",
      "Train: Epoch [24], Batch [43/938], Loss: 0.6121526956558228\n",
      "Train: Epoch [24], Batch [44/938], Loss: 0.4366344213485718\n",
      "Train: Epoch [24], Batch [45/938], Loss: 0.49947410821914673\n",
      "Train: Epoch [24], Batch [46/938], Loss: 0.46214181184768677\n",
      "Train: Epoch [24], Batch [47/938], Loss: 0.4497830867767334\n",
      "Train: Epoch [24], Batch [48/938], Loss: 0.5073121786117554\n",
      "Train: Epoch [24], Batch [49/938], Loss: 0.3128634989261627\n",
      "Train: Epoch [24], Batch [50/938], Loss: 0.43213409185409546\n",
      "Train: Epoch [24], Batch [51/938], Loss: 0.6566985845565796\n",
      "Train: Epoch [24], Batch [52/938], Loss: 0.4357289671897888\n",
      "Train: Epoch [24], Batch [53/938], Loss: 0.4150024652481079\n",
      "Train: Epoch [24], Batch [54/938], Loss: 0.3825346827507019\n",
      "Train: Epoch [24], Batch [55/938], Loss: 0.4970712959766388\n",
      "Train: Epoch [24], Batch [56/938], Loss: 0.4384133517742157\n",
      "Train: Epoch [24], Batch [57/938], Loss: 0.4188021421432495\n",
      "Train: Epoch [24], Batch [58/938], Loss: 0.35477137565612793\n",
      "Train: Epoch [24], Batch [59/938], Loss: 0.5060819387435913\n",
      "Train: Epoch [24], Batch [60/938], Loss: 0.2961372435092926\n",
      "Train: Epoch [24], Batch [61/938], Loss: 0.4619525074958801\n",
      "Train: Epoch [24], Batch [62/938], Loss: 0.3351885676383972\n",
      "Train: Epoch [24], Batch [63/938], Loss: 0.5204448699951172\n",
      "Train: Epoch [24], Batch [64/938], Loss: 0.38619574904441833\n",
      "Train: Epoch [24], Batch [65/938], Loss: 0.45535609126091003\n",
      "Train: Epoch [24], Batch [66/938], Loss: 0.523675799369812\n",
      "Train: Epoch [24], Batch [67/938], Loss: 0.2708621621131897\n",
      "Train: Epoch [24], Batch [68/938], Loss: 0.5731480121612549\n",
      "Train: Epoch [24], Batch [69/938], Loss: 0.48474228382110596\n",
      "Train: Epoch [24], Batch [70/938], Loss: 0.36633411049842834\n",
      "Train: Epoch [24], Batch [71/938], Loss: 0.43921902775764465\n",
      "Train: Epoch [24], Batch [72/938], Loss: 0.621810793876648\n",
      "Train: Epoch [24], Batch [73/938], Loss: 0.46500036120414734\n",
      "Train: Epoch [24], Batch [74/938], Loss: 0.24569782614707947\n",
      "Train: Epoch [24], Batch [75/938], Loss: 0.5053369402885437\n",
      "Train: Epoch [24], Batch [76/938], Loss: 0.3871263861656189\n",
      "Train: Epoch [24], Batch [77/938], Loss: 0.33514559268951416\n",
      "Train: Epoch [24], Batch [78/938], Loss: 0.4995793104171753\n",
      "Train: Epoch [24], Batch [79/938], Loss: 0.4948313236236572\n",
      "Train: Epoch [24], Batch [80/938], Loss: 0.28252485394477844\n",
      "Train: Epoch [24], Batch [81/938], Loss: 0.41614994406700134\n",
      "Train: Epoch [24], Batch [82/938], Loss: 0.24357137084007263\n",
      "Train: Epoch [24], Batch [83/938], Loss: 0.5889550447463989\n",
      "Train: Epoch [24], Batch [84/938], Loss: 0.6415918469429016\n",
      "Train: Epoch [24], Batch [85/938], Loss: 0.45238250494003296\n",
      "Train: Epoch [24], Batch [86/938], Loss: 0.333402544260025\n",
      "Train: Epoch [24], Batch [87/938], Loss: 0.34001225233078003\n",
      "Train: Epoch [24], Batch [88/938], Loss: 0.397790789604187\n",
      "Train: Epoch [24], Batch [89/938], Loss: 0.40583667159080505\n",
      "Train: Epoch [24], Batch [90/938], Loss: 0.3282169699668884\n",
      "Train: Epoch [24], Batch [91/938], Loss: 0.35257577896118164\n",
      "Train: Epoch [24], Batch [92/938], Loss: 0.4292067885398865\n",
      "Train: Epoch [24], Batch [93/938], Loss: 0.5167029500007629\n",
      "Train: Epoch [24], Batch [94/938], Loss: 0.5609014630317688\n",
      "Train: Epoch [24], Batch [95/938], Loss: 0.49028870463371277\n",
      "Train: Epoch [24], Batch [96/938], Loss: 0.4235617518424988\n",
      "Train: Epoch [24], Batch [97/938], Loss: 0.37817564606666565\n",
      "Train: Epoch [24], Batch [98/938], Loss: 0.4394713044166565\n",
      "Train: Epoch [24], Batch [99/938], Loss: 0.4826970398426056\n",
      "Train: Epoch [24], Batch [100/938], Loss: 0.36858639121055603\n",
      "Train: Epoch [24], Batch [101/938], Loss: 0.6051881313323975\n",
      "Train: Epoch [24], Batch [102/938], Loss: 0.37829461693763733\n",
      "Train: Epoch [24], Batch [103/938], Loss: 0.3890170753002167\n",
      "Train: Epoch [24], Batch [104/938], Loss: 0.4249057471752167\n",
      "Train: Epoch [24], Batch [105/938], Loss: 0.5868493914604187\n",
      "Train: Epoch [24], Batch [106/938], Loss: 0.5082241296768188\n",
      "Train: Epoch [24], Batch [107/938], Loss: 0.2862873077392578\n",
      "Train: Epoch [24], Batch [108/938], Loss: 0.4432280659675598\n",
      "Train: Epoch [24], Batch [109/938], Loss: 0.626152515411377\n",
      "Train: Epoch [24], Batch [110/938], Loss: 0.5113092660903931\n",
      "Train: Epoch [24], Batch [111/938], Loss: 0.4362601637840271\n",
      "Train: Epoch [24], Batch [112/938], Loss: 0.5760893225669861\n",
      "Train: Epoch [24], Batch [113/938], Loss: 0.5486199855804443\n",
      "Train: Epoch [24], Batch [114/938], Loss: 0.35734838247299194\n",
      "Train: Epoch [24], Batch [115/938], Loss: 0.468819797039032\n",
      "Train: Epoch [24], Batch [116/938], Loss: 0.25187182426452637\n",
      "Train: Epoch [24], Batch [117/938], Loss: 0.6193876266479492\n",
      "Train: Epoch [24], Batch [118/938], Loss: 0.30677586793899536\n",
      "Train: Epoch [24], Batch [119/938], Loss: 0.3457139730453491\n",
      "Train: Epoch [24], Batch [120/938], Loss: 0.5187956690788269\n",
      "Train: Epoch [24], Batch [121/938], Loss: 0.43213504552841187\n",
      "Train: Epoch [24], Batch [122/938], Loss: 0.6989518404006958\n",
      "Train: Epoch [24], Batch [123/938], Loss: 0.3790486752986908\n",
      "Train: Epoch [24], Batch [124/938], Loss: 0.42704588174819946\n",
      "Train: Epoch [24], Batch [125/938], Loss: 0.4898560643196106\n",
      "Train: Epoch [24], Batch [126/938], Loss: 0.47517624497413635\n",
      "Train: Epoch [24], Batch [127/938], Loss: 0.6273178458213806\n",
      "Train: Epoch [24], Batch [128/938], Loss: 0.6130630970001221\n",
      "Train: Epoch [24], Batch [129/938], Loss: 0.5185432434082031\n",
      "Train: Epoch [24], Batch [130/938], Loss: 0.4550313353538513\n",
      "Train: Epoch [24], Batch [131/938], Loss: 0.5584354400634766\n",
      "Train: Epoch [24], Batch [132/938], Loss: 0.5268600583076477\n",
      "Train: Epoch [24], Batch [133/938], Loss: 0.5683194398880005\n",
      "Train: Epoch [24], Batch [134/938], Loss: 0.45516133308410645\n",
      "Train: Epoch [24], Batch [135/938], Loss: 0.4385862648487091\n",
      "Train: Epoch [24], Batch [136/938], Loss: 0.5327913761138916\n",
      "Train: Epoch [24], Batch [137/938], Loss: 0.298650324344635\n",
      "Train: Epoch [24], Batch [138/938], Loss: 0.49838122725486755\n",
      "Train: Epoch [24], Batch [139/938], Loss: 0.4181108772754669\n",
      "Train: Epoch [24], Batch [140/938], Loss: 0.4161456525325775\n",
      "Train: Epoch [24], Batch [141/938], Loss: 0.33093374967575073\n",
      "Train: Epoch [24], Batch [142/938], Loss: 0.2990509867668152\n",
      "Train: Epoch [24], Batch [143/938], Loss: 0.40289974212646484\n",
      "Train: Epoch [24], Batch [144/938], Loss: 0.5579685568809509\n",
      "Train: Epoch [24], Batch [145/938], Loss: 0.3497878909111023\n",
      "Train: Epoch [24], Batch [146/938], Loss: 0.3178948163986206\n",
      "Train: Epoch [24], Batch [147/938], Loss: 0.4030437469482422\n",
      "Train: Epoch [24], Batch [148/938], Loss: 0.3466877043247223\n",
      "Train: Epoch [24], Batch [149/938], Loss: 0.4595719575881958\n",
      "Train: Epoch [24], Batch [150/938], Loss: 0.38346824049949646\n",
      "Train: Epoch [24], Batch [151/938], Loss: 0.3701227009296417\n",
      "Train: Epoch [24], Batch [152/938], Loss: 0.4380472004413605\n",
      "Train: Epoch [24], Batch [153/938], Loss: 0.6991240978240967\n",
      "Train: Epoch [24], Batch [154/938], Loss: 0.3568287193775177\n",
      "Train: Epoch [24], Batch [155/938], Loss: 0.2935395836830139\n",
      "Train: Epoch [24], Batch [156/938], Loss: 0.42204993963241577\n",
      "Train: Epoch [24], Batch [157/938], Loss: 0.41201162338256836\n",
      "Train: Epoch [24], Batch [158/938], Loss: 0.5239506363868713\n",
      "Train: Epoch [24], Batch [159/938], Loss: 0.41706380248069763\n",
      "Train: Epoch [24], Batch [160/938], Loss: 0.6981498003005981\n",
      "Train: Epoch [24], Batch [161/938], Loss: 0.6573476791381836\n",
      "Train: Epoch [24], Batch [162/938], Loss: 0.4160923361778259\n",
      "Train: Epoch [24], Batch [163/938], Loss: 0.29408326745033264\n",
      "Train: Epoch [24], Batch [164/938], Loss: 0.5043749213218689\n",
      "Train: Epoch [24], Batch [165/938], Loss: 0.47698310017585754\n",
      "Train: Epoch [24], Batch [166/938], Loss: 0.616509735584259\n",
      "Train: Epoch [24], Batch [167/938], Loss: 0.48141375184059143\n",
      "Train: Epoch [24], Batch [168/938], Loss: 0.37865129113197327\n",
      "Train: Epoch [24], Batch [169/938], Loss: 0.5381143689155579\n",
      "Train: Epoch [24], Batch [170/938], Loss: 0.5752948522567749\n",
      "Train: Epoch [24], Batch [171/938], Loss: 0.4008345603942871\n",
      "Train: Epoch [24], Batch [172/938], Loss: 0.4430992305278778\n",
      "Train: Epoch [24], Batch [173/938], Loss: 0.3993886113166809\n",
      "Train: Epoch [24], Batch [174/938], Loss: 0.49714747071266174\n",
      "Train: Epoch [24], Batch [175/938], Loss: 0.49704140424728394\n",
      "Train: Epoch [24], Batch [176/938], Loss: 0.3927498757839203\n",
      "Train: Epoch [24], Batch [177/938], Loss: 0.5283653140068054\n",
      "Train: Epoch [24], Batch [178/938], Loss: 0.34295353293418884\n",
      "Train: Epoch [24], Batch [179/938], Loss: 0.3882586658000946\n",
      "Train: Epoch [24], Batch [180/938], Loss: 0.3910897672176361\n",
      "Train: Epoch [24], Batch [181/938], Loss: 0.4251302480697632\n",
      "Train: Epoch [24], Batch [182/938], Loss: 0.5838785171508789\n",
      "Train: Epoch [24], Batch [183/938], Loss: 0.4940114915370941\n",
      "Train: Epoch [24], Batch [184/938], Loss: 0.35861775279045105\n",
      "Train: Epoch [24], Batch [185/938], Loss: 0.48350706696510315\n",
      "Train: Epoch [24], Batch [186/938], Loss: 0.30423757433891296\n",
      "Train: Epoch [24], Batch [187/938], Loss: 0.20856192708015442\n",
      "Train: Epoch [24], Batch [188/938], Loss: 0.5459204912185669\n",
      "Train: Epoch [24], Batch [189/938], Loss: 0.38593974709510803\n",
      "Train: Epoch [24], Batch [190/938], Loss: 0.4716901183128357\n",
      "Train: Epoch [24], Batch [191/938], Loss: 0.4224957823753357\n",
      "Train: Epoch [24], Batch [192/938], Loss: 0.5635412931442261\n",
      "Train: Epoch [24], Batch [193/938], Loss: 0.31291845440864563\n",
      "Train: Epoch [24], Batch [194/938], Loss: 0.5668558478355408\n",
      "Train: Epoch [24], Batch [195/938], Loss: 0.6089665293693542\n",
      "Train: Epoch [24], Batch [196/938], Loss: 0.6445963978767395\n",
      "Train: Epoch [24], Batch [197/938], Loss: 0.46017950773239136\n",
      "Train: Epoch [24], Batch [198/938], Loss: 0.37026476860046387\n",
      "Train: Epoch [24], Batch [199/938], Loss: 0.39142656326293945\n",
      "Train: Epoch [24], Batch [200/938], Loss: 0.5028229355812073\n",
      "Train: Epoch [24], Batch [201/938], Loss: 0.23415879905223846\n",
      "Train: Epoch [24], Batch [202/938], Loss: 0.36546754837036133\n",
      "Train: Epoch [24], Batch [203/938], Loss: 0.47503718733787537\n",
      "Train: Epoch [24], Batch [204/938], Loss: 0.3428274989128113\n",
      "Train: Epoch [24], Batch [205/938], Loss: 0.35253381729125977\n",
      "Train: Epoch [24], Batch [206/938], Loss: 0.6384738087654114\n",
      "Train: Epoch [24], Batch [207/938], Loss: 0.4023759365081787\n",
      "Train: Epoch [24], Batch [208/938], Loss: 0.38120901584625244\n",
      "Train: Epoch [24], Batch [209/938], Loss: 0.36280301213264465\n",
      "Train: Epoch [24], Batch [210/938], Loss: 0.4721335470676422\n",
      "Train: Epoch [24], Batch [211/938], Loss: 0.5013300776481628\n",
      "Train: Epoch [24], Batch [212/938], Loss: 0.39955011010169983\n",
      "Train: Epoch [24], Batch [213/938], Loss: 0.4131064713001251\n",
      "Train: Epoch [24], Batch [214/938], Loss: 0.30680787563323975\n",
      "Train: Epoch [24], Batch [215/938], Loss: 0.39970478415489197\n",
      "Train: Epoch [24], Batch [216/938], Loss: 0.3152991533279419\n",
      "Train: Epoch [24], Batch [217/938], Loss: 0.32796046137809753\n",
      "Train: Epoch [24], Batch [218/938], Loss: 0.7305921912193298\n",
      "Train: Epoch [24], Batch [219/938], Loss: 0.27728116512298584\n",
      "Train: Epoch [24], Batch [220/938], Loss: 0.4048379063606262\n",
      "Train: Epoch [24], Batch [221/938], Loss: 0.6348347663879395\n",
      "Train: Epoch [24], Batch [222/938], Loss: 0.4569479525089264\n",
      "Train: Epoch [24], Batch [223/938], Loss: 0.3886258006095886\n",
      "Train: Epoch [24], Batch [224/938], Loss: 0.41104528307914734\n",
      "Train: Epoch [24], Batch [225/938], Loss: 0.37921833992004395\n",
      "Train: Epoch [24], Batch [226/938], Loss: 0.32132092118263245\n",
      "Train: Epoch [24], Batch [227/938], Loss: 0.5118275284767151\n",
      "Train: Epoch [24], Batch [228/938], Loss: 0.6182965040206909\n",
      "Train: Epoch [24], Batch [229/938], Loss: 0.5693973898887634\n",
      "Train: Epoch [24], Batch [230/938], Loss: 0.30243030190467834\n",
      "Train: Epoch [24], Batch [231/938], Loss: 0.3258247971534729\n",
      "Train: Epoch [24], Batch [232/938], Loss: 0.3554721176624298\n",
      "Train: Epoch [24], Batch [233/938], Loss: 0.41118505597114563\n",
      "Train: Epoch [24], Batch [234/938], Loss: 0.5189031958580017\n",
      "Train: Epoch [24], Batch [235/938], Loss: 0.3850114941596985\n",
      "Train: Epoch [24], Batch [236/938], Loss: 0.23669445514678955\n",
      "Train: Epoch [24], Batch [237/938], Loss: 0.48206213116645813\n",
      "Train: Epoch [24], Batch [238/938], Loss: 0.5542474389076233\n",
      "Train: Epoch [24], Batch [239/938], Loss: 0.5429205894470215\n",
      "Train: Epoch [24], Batch [240/938], Loss: 0.43503886461257935\n",
      "Train: Epoch [24], Batch [241/938], Loss: 0.5038537979125977\n",
      "Train: Epoch [24], Batch [242/938], Loss: 0.39648303389549255\n",
      "Train: Epoch [24], Batch [243/938], Loss: 0.6285927295684814\n",
      "Train: Epoch [24], Batch [244/938], Loss: 0.6069779992103577\n",
      "Train: Epoch [24], Batch [245/938], Loss: 0.3095777630805969\n",
      "Train: Epoch [24], Batch [246/938], Loss: 0.19807039201259613\n",
      "Train: Epoch [24], Batch [247/938], Loss: 0.34782499074935913\n",
      "Train: Epoch [24], Batch [248/938], Loss: 0.43839213252067566\n",
      "Train: Epoch [24], Batch [249/938], Loss: 0.45011842250823975\n",
      "Train: Epoch [24], Batch [250/938], Loss: 0.36511677503585815\n",
      "Train: Epoch [24], Batch [251/938], Loss: 0.3509907126426697\n",
      "Train: Epoch [24], Batch [252/938], Loss: 0.3701593577861786\n",
      "Train: Epoch [24], Batch [253/938], Loss: 0.37640225887298584\n",
      "Train: Epoch [24], Batch [254/938], Loss: 0.3200448453426361\n",
      "Train: Epoch [24], Batch [255/938], Loss: 0.37902411818504333\n",
      "Train: Epoch [24], Batch [256/938], Loss: 0.3645332157611847\n",
      "Train: Epoch [24], Batch [257/938], Loss: 0.3972128629684448\n",
      "Train: Epoch [24], Batch [258/938], Loss: 0.440393328666687\n",
      "Train: Epoch [24], Batch [259/938], Loss: 0.6080006957054138\n",
      "Train: Epoch [24], Batch [260/938], Loss: 0.46511954069137573\n",
      "Train: Epoch [24], Batch [261/938], Loss: 0.5436511039733887\n",
      "Train: Epoch [24], Batch [262/938], Loss: 0.2674999535083771\n",
      "Train: Epoch [24], Batch [263/938], Loss: 0.5817179679870605\n",
      "Train: Epoch [24], Batch [264/938], Loss: 0.4173901379108429\n",
      "Train: Epoch [24], Batch [265/938], Loss: 0.4697338938713074\n",
      "Train: Epoch [24], Batch [266/938], Loss: 0.3543586730957031\n",
      "Train: Epoch [24], Batch [267/938], Loss: 0.33754250407218933\n",
      "Train: Epoch [24], Batch [268/938], Loss: 0.5154980421066284\n",
      "Train: Epoch [24], Batch [269/938], Loss: 0.4107975661754608\n",
      "Train: Epoch [24], Batch [270/938], Loss: 0.3810560703277588\n",
      "Train: Epoch [24], Batch [271/938], Loss: 0.47953736782073975\n",
      "Train: Epoch [24], Batch [272/938], Loss: 0.38140976428985596\n",
      "Train: Epoch [24], Batch [273/938], Loss: 0.563719630241394\n",
      "Train: Epoch [24], Batch [274/938], Loss: 0.29432108998298645\n",
      "Train: Epoch [24], Batch [275/938], Loss: 0.32865869998931885\n",
      "Train: Epoch [24], Batch [276/938], Loss: 0.3775290846824646\n",
      "Train: Epoch [24], Batch [277/938], Loss: 0.5642379522323608\n",
      "Train: Epoch [24], Batch [278/938], Loss: 0.3155173361301422\n",
      "Train: Epoch [24], Batch [279/938], Loss: 0.26015380024909973\n",
      "Train: Epoch [24], Batch [280/938], Loss: 0.4899539649486542\n",
      "Train: Epoch [24], Batch [281/938], Loss: 0.49733036756515503\n",
      "Train: Epoch [24], Batch [282/938], Loss: 0.6226473450660706\n",
      "Train: Epoch [24], Batch [283/938], Loss: 0.26092082262039185\n",
      "Train: Epoch [24], Batch [284/938], Loss: 0.3506210446357727\n",
      "Train: Epoch [24], Batch [285/938], Loss: 0.4171891212463379\n",
      "Train: Epoch [24], Batch [286/938], Loss: 0.3167533874511719\n",
      "Train: Epoch [24], Batch [287/938], Loss: 0.392111599445343\n",
      "Train: Epoch [24], Batch [288/938], Loss: 0.31215041875839233\n",
      "Train: Epoch [24], Batch [289/938], Loss: 0.44295063614845276\n",
      "Train: Epoch [24], Batch [290/938], Loss: 0.4061982035636902\n",
      "Train: Epoch [24], Batch [291/938], Loss: 0.3751307725906372\n",
      "Train: Epoch [24], Batch [292/938], Loss: 0.40347805619239807\n",
      "Train: Epoch [24], Batch [293/938], Loss: 0.4145369827747345\n",
      "Train: Epoch [24], Batch [294/938], Loss: 0.49362653493881226\n",
      "Train: Epoch [24], Batch [295/938], Loss: 0.41728124022483826\n",
      "Train: Epoch [24], Batch [296/938], Loss: 0.3241853415966034\n",
      "Train: Epoch [24], Batch [297/938], Loss: 0.2666929066181183\n",
      "Train: Epoch [24], Batch [298/938], Loss: 0.3751376271247864\n",
      "Train: Epoch [24], Batch [299/938], Loss: 0.28005561232566833\n",
      "Train: Epoch [24], Batch [300/938], Loss: 0.5297127962112427\n",
      "Train: Epoch [24], Batch [301/938], Loss: 0.4896094501018524\n",
      "Train: Epoch [24], Batch [302/938], Loss: 0.2929367423057556\n",
      "Train: Epoch [24], Batch [303/938], Loss: 0.47204315662384033\n",
      "Train: Epoch [24], Batch [304/938], Loss: 0.3070501983165741\n",
      "Train: Epoch [24], Batch [305/938], Loss: 0.5325706005096436\n",
      "Train: Epoch [24], Batch [306/938], Loss: 0.32592639327049255\n",
      "Train: Epoch [24], Batch [307/938], Loss: 0.47263601422309875\n",
      "Train: Epoch [24], Batch [308/938], Loss: 0.3949308395385742\n",
      "Train: Epoch [24], Batch [309/938], Loss: 0.3399929702281952\n",
      "Train: Epoch [24], Batch [310/938], Loss: 0.22522804141044617\n",
      "Train: Epoch [24], Batch [311/938], Loss: 0.5203579664230347\n",
      "Train: Epoch [24], Batch [312/938], Loss: 0.38820430636405945\n",
      "Train: Epoch [24], Batch [313/938], Loss: 0.5160443186759949\n",
      "Train: Epoch [24], Batch [314/938], Loss: 0.4078693091869354\n",
      "Train: Epoch [24], Batch [315/938], Loss: 0.4761969745159149\n",
      "Train: Epoch [24], Batch [316/938], Loss: 0.47673407196998596\n",
      "Train: Epoch [24], Batch [317/938], Loss: 0.3946186900138855\n",
      "Train: Epoch [24], Batch [318/938], Loss: 0.6309554576873779\n",
      "Train: Epoch [24], Batch [319/938], Loss: 0.356642484664917\n",
      "Train: Epoch [24], Batch [320/938], Loss: 0.34916290640830994\n",
      "Train: Epoch [24], Batch [321/938], Loss: 0.34865304827690125\n",
      "Train: Epoch [24], Batch [322/938], Loss: 0.5286600589752197\n",
      "Train: Epoch [24], Batch [323/938], Loss: 0.7138004302978516\n",
      "Train: Epoch [24], Batch [324/938], Loss: 0.22562435269355774\n",
      "Train: Epoch [24], Batch [325/938], Loss: 0.419696569442749\n",
      "Train: Epoch [24], Batch [326/938], Loss: 0.49450403451919556\n",
      "Train: Epoch [24], Batch [327/938], Loss: 0.39740315079689026\n",
      "Train: Epoch [24], Batch [328/938], Loss: 0.3502315878868103\n",
      "Train: Epoch [24], Batch [329/938], Loss: 0.49161362648010254\n",
      "Train: Epoch [24], Batch [330/938], Loss: 0.5129531621932983\n",
      "Train: Epoch [24], Batch [331/938], Loss: 0.49619990587234497\n",
      "Train: Epoch [24], Batch [332/938], Loss: 0.33208656311035156\n",
      "Train: Epoch [24], Batch [333/938], Loss: 0.47597599029541016\n",
      "Train: Epoch [24], Batch [334/938], Loss: 0.33964046835899353\n",
      "Train: Epoch [24], Batch [335/938], Loss: 0.3684513568878174\n",
      "Train: Epoch [24], Batch [336/938], Loss: 0.3368760943412781\n",
      "Train: Epoch [24], Batch [337/938], Loss: 0.37590593099594116\n",
      "Train: Epoch [24], Batch [338/938], Loss: 0.2939261496067047\n",
      "Train: Epoch [24], Batch [339/938], Loss: 0.4594874978065491\n",
      "Train: Epoch [24], Batch [340/938], Loss: 0.6022113561630249\n",
      "Train: Epoch [24], Batch [341/938], Loss: 0.379721075296402\n",
      "Train: Epoch [24], Batch [342/938], Loss: 0.31083452701568604\n",
      "Train: Epoch [24], Batch [343/938], Loss: 0.358929306268692\n",
      "Train: Epoch [24], Batch [344/938], Loss: 0.3167107105255127\n",
      "Train: Epoch [24], Batch [345/938], Loss: 0.38015857338905334\n",
      "Train: Epoch [24], Batch [346/938], Loss: 0.363665372133255\n",
      "Train: Epoch [24], Batch [347/938], Loss: 0.3131570518016815\n",
      "Train: Epoch [24], Batch [348/938], Loss: 0.6077114343643188\n",
      "Train: Epoch [24], Batch [349/938], Loss: 0.26986801624298096\n",
      "Train: Epoch [24], Batch [350/938], Loss: 0.29254812002182007\n",
      "Train: Epoch [24], Batch [351/938], Loss: 0.24867728352546692\n",
      "Train: Epoch [24], Batch [352/938], Loss: 0.5152571797370911\n",
      "Train: Epoch [24], Batch [353/938], Loss: 0.4435153007507324\n",
      "Train: Epoch [24], Batch [354/938], Loss: 0.35258960723876953\n",
      "Train: Epoch [24], Batch [355/938], Loss: 0.42142510414123535\n",
      "Train: Epoch [24], Batch [356/938], Loss: 0.34079140424728394\n",
      "Train: Epoch [24], Batch [357/938], Loss: 0.47075894474983215\n",
      "Train: Epoch [24], Batch [358/938], Loss: 0.37604251503944397\n",
      "Train: Epoch [24], Batch [359/938], Loss: 0.3860313594341278\n",
      "Train: Epoch [24], Batch [360/938], Loss: 0.4155956506729126\n",
      "Train: Epoch [24], Batch [361/938], Loss: 0.33021578192710876\n",
      "Train: Epoch [24], Batch [362/938], Loss: 0.2616349160671234\n",
      "Train: Epoch [24], Batch [363/938], Loss: 0.4504356384277344\n",
      "Train: Epoch [24], Batch [364/938], Loss: 0.41296258568763733\n",
      "Train: Epoch [24], Batch [365/938], Loss: 0.497865229845047\n",
      "Train: Epoch [24], Batch [366/938], Loss: 0.42470818758010864\n",
      "Train: Epoch [24], Batch [367/938], Loss: 0.3003094494342804\n",
      "Train: Epoch [24], Batch [368/938], Loss: 0.37610530853271484\n",
      "Train: Epoch [24], Batch [369/938], Loss: 0.5881781578063965\n",
      "Train: Epoch [24], Batch [370/938], Loss: 0.2640969753265381\n",
      "Train: Epoch [24], Batch [371/938], Loss: 0.5678563117980957\n",
      "Train: Epoch [24], Batch [372/938], Loss: 0.3832767605781555\n",
      "Train: Epoch [24], Batch [373/938], Loss: 0.5017998218536377\n",
      "Train: Epoch [24], Batch [374/938], Loss: 0.4239816963672638\n",
      "Train: Epoch [24], Batch [375/938], Loss: 0.436867892742157\n",
      "Train: Epoch [24], Batch [376/938], Loss: 0.3869571089744568\n",
      "Train: Epoch [24], Batch [377/938], Loss: 0.38797956705093384\n",
      "Train: Epoch [24], Batch [378/938], Loss: 0.4385349154472351\n",
      "Train: Epoch [24], Batch [379/938], Loss: 0.3303419053554535\n",
      "Train: Epoch [24], Batch [380/938], Loss: 0.4828730523586273\n",
      "Train: Epoch [24], Batch [381/938], Loss: 0.42133596539497375\n",
      "Train: Epoch [24], Batch [382/938], Loss: 0.4667350649833679\n",
      "Train: Epoch [24], Batch [383/938], Loss: 0.6220457553863525\n",
      "Train: Epoch [24], Batch [384/938], Loss: 0.4132159948348999\n",
      "Train: Epoch [24], Batch [385/938], Loss: 0.3129750192165375\n",
      "Train: Epoch [24], Batch [386/938], Loss: 0.3577846884727478\n",
      "Train: Epoch [24], Batch [387/938], Loss: 0.496005117893219\n",
      "Train: Epoch [24], Batch [388/938], Loss: 0.4339321255683899\n",
      "Train: Epoch [24], Batch [389/938], Loss: 0.4191403388977051\n",
      "Train: Epoch [24], Batch [390/938], Loss: 0.32624387741088867\n",
      "Train: Epoch [24], Batch [391/938], Loss: 0.24884094297885895\n",
      "Train: Epoch [24], Batch [392/938], Loss: 0.3686400055885315\n",
      "Train: Epoch [24], Batch [393/938], Loss: 0.48905569314956665\n",
      "Train: Epoch [24], Batch [394/938], Loss: 0.46429890394210815\n",
      "Train: Epoch [24], Batch [395/938], Loss: 0.43625983595848083\n",
      "Train: Epoch [24], Batch [396/938], Loss: 0.30660495162010193\n",
      "Train: Epoch [24], Batch [397/938], Loss: 0.3085085153579712\n",
      "Train: Epoch [24], Batch [398/938], Loss: 0.3932746946811676\n",
      "Train: Epoch [24], Batch [399/938], Loss: 0.4736248552799225\n",
      "Train: Epoch [24], Batch [400/938], Loss: 0.36711394786834717\n",
      "Train: Epoch [24], Batch [401/938], Loss: 0.635680079460144\n",
      "Train: Epoch [24], Batch [402/938], Loss: 0.4525163769721985\n",
      "Train: Epoch [24], Batch [403/938], Loss: 0.5015196204185486\n",
      "Train: Epoch [24], Batch [404/938], Loss: 0.48150020837783813\n",
      "Train: Epoch [24], Batch [405/938], Loss: 0.38321784138679504\n",
      "Train: Epoch [24], Batch [406/938], Loss: 0.3581102192401886\n",
      "Train: Epoch [24], Batch [407/938], Loss: 0.31833305954933167\n",
      "Train: Epoch [24], Batch [408/938], Loss: 0.5952322483062744\n",
      "Train: Epoch [24], Batch [409/938], Loss: 0.6835078001022339\n",
      "Train: Epoch [24], Batch [410/938], Loss: 0.49102622270584106\n",
      "Train: Epoch [24], Batch [411/938], Loss: 0.41443195939064026\n",
      "Train: Epoch [24], Batch [412/938], Loss: 0.4947909116744995\n",
      "Train: Epoch [24], Batch [413/938], Loss: 0.5725648403167725\n",
      "Train: Epoch [24], Batch [414/938], Loss: 0.6033303737640381\n",
      "Train: Epoch [24], Batch [415/938], Loss: 0.4321090281009674\n",
      "Train: Epoch [24], Batch [416/938], Loss: 0.40828850865364075\n",
      "Train: Epoch [24], Batch [417/938], Loss: 0.41348785161972046\n",
      "Train: Epoch [24], Batch [418/938], Loss: 0.3974192142486572\n",
      "Train: Epoch [24], Batch [419/938], Loss: 0.30639398097991943\n",
      "Train: Epoch [24], Batch [420/938], Loss: 0.44642069935798645\n",
      "Train: Epoch [24], Batch [421/938], Loss: 0.3604124188423157\n",
      "Train: Epoch [24], Batch [422/938], Loss: 0.40959104895591736\n",
      "Train: Epoch [24], Batch [423/938], Loss: 0.4516565203666687\n",
      "Train: Epoch [24], Batch [424/938], Loss: 0.3724712133407593\n",
      "Train: Epoch [24], Batch [425/938], Loss: 0.47142383456230164\n",
      "Train: Epoch [24], Batch [426/938], Loss: 0.45118987560272217\n",
      "Train: Epoch [24], Batch [427/938], Loss: 0.37463414669036865\n",
      "Train: Epoch [24], Batch [428/938], Loss: 0.35882601141929626\n",
      "Train: Epoch [24], Batch [429/938], Loss: 0.5398968458175659\n",
      "Train: Epoch [24], Batch [430/938], Loss: 0.40294694900512695\n",
      "Train: Epoch [24], Batch [431/938], Loss: 0.27923157811164856\n",
      "Train: Epoch [24], Batch [432/938], Loss: 0.41563451290130615\n",
      "Train: Epoch [24], Batch [433/938], Loss: 0.571664571762085\n",
      "Train: Epoch [24], Batch [434/938], Loss: 0.25111138820648193\n",
      "Train: Epoch [24], Batch [435/938], Loss: 0.5650820732116699\n",
      "Train: Epoch [24], Batch [436/938], Loss: 0.4687543511390686\n",
      "Train: Epoch [24], Batch [437/938], Loss: 0.4952392280101776\n",
      "Train: Epoch [24], Batch [438/938], Loss: 0.2706839144229889\n",
      "Train: Epoch [24], Batch [439/938], Loss: 0.45195716619491577\n",
      "Train: Epoch [24], Batch [440/938], Loss: 0.4265140891075134\n",
      "Train: Epoch [24], Batch [441/938], Loss: 0.47700661420822144\n",
      "Train: Epoch [24], Batch [442/938], Loss: 0.34361886978149414\n",
      "Train: Epoch [24], Batch [443/938], Loss: 0.34220877289772034\n",
      "Train: Epoch [24], Batch [444/938], Loss: 0.36917299032211304\n",
      "Train: Epoch [24], Batch [445/938], Loss: 0.48038768768310547\n",
      "Train: Epoch [24], Batch [446/938], Loss: 0.561173677444458\n",
      "Train: Epoch [24], Batch [447/938], Loss: 0.4697743058204651\n",
      "Train: Epoch [24], Batch [448/938], Loss: 0.5732744932174683\n",
      "Train: Epoch [24], Batch [449/938], Loss: 0.298808753490448\n",
      "Train: Epoch [24], Batch [450/938], Loss: 0.5825423002243042\n",
      "Train: Epoch [24], Batch [451/938], Loss: 0.37810078263282776\n",
      "Train: Epoch [24], Batch [452/938], Loss: 0.3983822166919708\n",
      "Train: Epoch [24], Batch [453/938], Loss: 0.6344084143638611\n",
      "Train: Epoch [24], Batch [454/938], Loss: 0.4234764277935028\n",
      "Train: Epoch [24], Batch [455/938], Loss: 0.5436239242553711\n",
      "Train: Epoch [24], Batch [456/938], Loss: 0.4565536081790924\n",
      "Train: Epoch [24], Batch [457/938], Loss: 0.47136566042900085\n",
      "Train: Epoch [24], Batch [458/938], Loss: 0.42518141865730286\n",
      "Train: Epoch [24], Batch [459/938], Loss: 0.22142393887043\n",
      "Train: Epoch [24], Batch [460/938], Loss: 0.45732951164245605\n",
      "Train: Epoch [24], Batch [461/938], Loss: 0.40988630056381226\n",
      "Train: Epoch [24], Batch [462/938], Loss: 0.3369646668434143\n",
      "Train: Epoch [24], Batch [463/938], Loss: 0.30480456352233887\n",
      "Train: Epoch [24], Batch [464/938], Loss: 0.6644575595855713\n",
      "Train: Epoch [24], Batch [465/938], Loss: 0.4765676259994507\n",
      "Train: Epoch [24], Batch [466/938], Loss: 0.6895201206207275\n",
      "Train: Epoch [24], Batch [467/938], Loss: 0.4813304841518402\n",
      "Train: Epoch [24], Batch [468/938], Loss: 0.5114827752113342\n",
      "Train: Epoch [24], Batch [469/938], Loss: 0.32841530442237854\n",
      "Train: Epoch [24], Batch [470/938], Loss: 0.4511632025241852\n",
      "Train: Epoch [24], Batch [471/938], Loss: 0.36321282386779785\n",
      "Train: Epoch [24], Batch [472/938], Loss: 0.48001256585121155\n",
      "Train: Epoch [24], Batch [473/938], Loss: 0.35013896226882935\n",
      "Train: Epoch [24], Batch [474/938], Loss: 0.44405168294906616\n",
      "Train: Epoch [24], Batch [475/938], Loss: 0.4032825827598572\n",
      "Train: Epoch [24], Batch [476/938], Loss: 0.2745894491672516\n",
      "Train: Epoch [24], Batch [477/938], Loss: 0.36355090141296387\n",
      "Train: Epoch [24], Batch [478/938], Loss: 0.44630229473114014\n",
      "Train: Epoch [24], Batch [479/938], Loss: 0.33845898509025574\n",
      "Train: Epoch [24], Batch [480/938], Loss: 0.31646662950515747\n",
      "Train: Epoch [24], Batch [481/938], Loss: 0.32703402638435364\n",
      "Train: Epoch [24], Batch [482/938], Loss: 0.3271526098251343\n",
      "Train: Epoch [24], Batch [483/938], Loss: 0.542066216468811\n",
      "Train: Epoch [24], Batch [484/938], Loss: 0.40690580010414124\n",
      "Train: Epoch [24], Batch [485/938], Loss: 0.3168562352657318\n",
      "Train: Epoch [24], Batch [486/938], Loss: 0.3364417254924774\n",
      "Train: Epoch [24], Batch [487/938], Loss: 0.5781870484352112\n",
      "Train: Epoch [24], Batch [488/938], Loss: 0.4497483968734741\n",
      "Train: Epoch [24], Batch [489/938], Loss: 0.5686835050582886\n",
      "Train: Epoch [24], Batch [490/938], Loss: 0.36854076385498047\n",
      "Train: Epoch [24], Batch [491/938], Loss: 0.49648571014404297\n",
      "Train: Epoch [24], Batch [492/938], Loss: 0.4279896914958954\n",
      "Train: Epoch [24], Batch [493/938], Loss: 0.4380439221858978\n",
      "Train: Epoch [24], Batch [494/938], Loss: 0.30341464281082153\n",
      "Train: Epoch [24], Batch [495/938], Loss: 0.48788636922836304\n",
      "Train: Epoch [24], Batch [496/938], Loss: 0.46884381771087646\n",
      "Train: Epoch [24], Batch [497/938], Loss: 0.4460001587867737\n",
      "Train: Epoch [24], Batch [498/938], Loss: 0.3187451958656311\n",
      "Train: Epoch [24], Batch [499/938], Loss: 0.35607123374938965\n",
      "Train: Epoch [24], Batch [500/938], Loss: 0.53095543384552\n",
      "Train: Epoch [24], Batch [501/938], Loss: 0.2613377571105957\n",
      "Train: Epoch [24], Batch [502/938], Loss: 0.2389514148235321\n",
      "Train: Epoch [24], Batch [503/938], Loss: 0.49117156863212585\n",
      "Train: Epoch [24], Batch [504/938], Loss: 0.38444286584854126\n",
      "Train: Epoch [24], Batch [505/938], Loss: 0.31911131739616394\n",
      "Train: Epoch [24], Batch [506/938], Loss: 0.5056535601615906\n",
      "Train: Epoch [24], Batch [507/938], Loss: 0.5325750112533569\n",
      "Train: Epoch [24], Batch [508/938], Loss: 0.2807576656341553\n",
      "Train: Epoch [24], Batch [509/938], Loss: 0.5006132125854492\n",
      "Train: Epoch [24], Batch [510/938], Loss: 0.5480726957321167\n",
      "Train: Epoch [24], Batch [511/938], Loss: 0.33484455943107605\n",
      "Train: Epoch [24], Batch [512/938], Loss: 0.3145442008972168\n",
      "Train: Epoch [24], Batch [513/938], Loss: 0.5097630023956299\n",
      "Train: Epoch [24], Batch [514/938], Loss: 0.5674034357070923\n",
      "Train: Epoch [24], Batch [515/938], Loss: 0.2912307381629944\n",
      "Train: Epoch [24], Batch [516/938], Loss: 0.2743825912475586\n",
      "Train: Epoch [24], Batch [517/938], Loss: 0.34986475110054016\n",
      "Train: Epoch [24], Batch [518/938], Loss: 0.408039391040802\n",
      "Train: Epoch [24], Batch [519/938], Loss: 0.40582114458084106\n",
      "Train: Epoch [24], Batch [520/938], Loss: 0.39118248224258423\n",
      "Train: Epoch [24], Batch [521/938], Loss: 0.3350689709186554\n",
      "Train: Epoch [24], Batch [522/938], Loss: 0.3605245351791382\n",
      "Train: Epoch [24], Batch [523/938], Loss: 0.3960072994232178\n",
      "Train: Epoch [24], Batch [524/938], Loss: 0.4192864000797272\n",
      "Train: Epoch [24], Batch [525/938], Loss: 0.3333054184913635\n",
      "Train: Epoch [24], Batch [526/938], Loss: 0.34062403440475464\n",
      "Train: Epoch [24], Batch [527/938], Loss: 0.35706305503845215\n",
      "Train: Epoch [24], Batch [528/938], Loss: 0.4506452977657318\n",
      "Train: Epoch [24], Batch [529/938], Loss: 0.30798208713531494\n",
      "Train: Epoch [24], Batch [530/938], Loss: 0.372505784034729\n",
      "Train: Epoch [24], Batch [531/938], Loss: 0.5492770671844482\n",
      "Train: Epoch [24], Batch [532/938], Loss: 0.38981834053993225\n",
      "Train: Epoch [24], Batch [533/938], Loss: 0.37036073207855225\n",
      "Train: Epoch [24], Batch [534/938], Loss: 0.4096102714538574\n",
      "Train: Epoch [24], Batch [535/938], Loss: 0.5008887648582458\n",
      "Train: Epoch [24], Batch [536/938], Loss: 0.46236491203308105\n",
      "Train: Epoch [24], Batch [537/938], Loss: 0.46108293533325195\n",
      "Train: Epoch [24], Batch [538/938], Loss: 0.3030484616756439\n",
      "Train: Epoch [24], Batch [539/938], Loss: 0.43102043867111206\n",
      "Train: Epoch [24], Batch [540/938], Loss: 0.2773209512233734\n",
      "Train: Epoch [24], Batch [541/938], Loss: 0.42101478576660156\n",
      "Train: Epoch [24], Batch [542/938], Loss: 0.3839230537414551\n",
      "Train: Epoch [24], Batch [543/938], Loss: 0.5687822699546814\n",
      "Train: Epoch [24], Batch [544/938], Loss: 0.47471344470977783\n",
      "Train: Epoch [24], Batch [545/938], Loss: 0.3580271601676941\n",
      "Train: Epoch [24], Batch [546/938], Loss: 0.31526443362236023\n",
      "Train: Epoch [24], Batch [547/938], Loss: 0.3979185223579407\n",
      "Train: Epoch [24], Batch [548/938], Loss: 0.6173771619796753\n",
      "Train: Epoch [24], Batch [549/938], Loss: 0.35720983147621155\n",
      "Train: Epoch [24], Batch [550/938], Loss: 0.42116981744766235\n",
      "Train: Epoch [24], Batch [551/938], Loss: 0.3924452066421509\n",
      "Train: Epoch [24], Batch [552/938], Loss: 0.3833094835281372\n",
      "Train: Epoch [24], Batch [553/938], Loss: 0.40872445702552795\n",
      "Train: Epoch [24], Batch [554/938], Loss: 0.28167837858200073\n",
      "Train: Epoch [24], Batch [555/938], Loss: 0.35693079233169556\n",
      "Train: Epoch [24], Batch [556/938], Loss: 0.26969480514526367\n",
      "Train: Epoch [24], Batch [557/938], Loss: 0.5652013421058655\n",
      "Train: Epoch [24], Batch [558/938], Loss: 0.3333494961261749\n",
      "Train: Epoch [24], Batch [559/938], Loss: 0.40404173731803894\n",
      "Train: Epoch [24], Batch [560/938], Loss: 0.4702550768852234\n",
      "Train: Epoch [24], Batch [561/938], Loss: 0.2423119843006134\n",
      "Train: Epoch [24], Batch [562/938], Loss: 0.40517696738243103\n",
      "Train: Epoch [24], Batch [563/938], Loss: 0.38813239336013794\n",
      "Train: Epoch [24], Batch [564/938], Loss: 0.3035643696784973\n",
      "Train: Epoch [24], Batch [565/938], Loss: 0.6184092164039612\n",
      "Train: Epoch [24], Batch [566/938], Loss: 0.23823267221450806\n",
      "Train: Epoch [24], Batch [567/938], Loss: 0.24885334074497223\n",
      "Train: Epoch [24], Batch [568/938], Loss: 0.4497058391571045\n",
      "Train: Epoch [24], Batch [569/938], Loss: 0.5664522051811218\n",
      "Train: Epoch [24], Batch [570/938], Loss: 0.4718477427959442\n",
      "Train: Epoch [24], Batch [571/938], Loss: 0.6378259658813477\n",
      "Train: Epoch [24], Batch [572/938], Loss: 0.5909806489944458\n",
      "Train: Epoch [24], Batch [573/938], Loss: 0.31421488523483276\n",
      "Train: Epoch [24], Batch [574/938], Loss: 0.5355209112167358\n",
      "Train: Epoch [24], Batch [575/938], Loss: 0.4165003299713135\n",
      "Train: Epoch [24], Batch [576/938], Loss: 0.34571412205696106\n",
      "Train: Epoch [24], Batch [577/938], Loss: 0.49828192591667175\n",
      "Train: Epoch [24], Batch [578/938], Loss: 0.4323118031024933\n",
      "Train: Epoch [24], Batch [579/938], Loss: 0.5717538595199585\n",
      "Train: Epoch [24], Batch [580/938], Loss: 0.6843440532684326\n",
      "Train: Epoch [24], Batch [581/938], Loss: 0.4065544903278351\n",
      "Train: Epoch [24], Batch [582/938], Loss: 0.31423965096473694\n",
      "Train: Epoch [24], Batch [583/938], Loss: 0.5947535634040833\n",
      "Train: Epoch [24], Batch [584/938], Loss: 0.3712695837020874\n",
      "Train: Epoch [24], Batch [585/938], Loss: 0.3540118634700775\n",
      "Train: Epoch [24], Batch [586/938], Loss: 0.27025532722473145\n",
      "Train: Epoch [24], Batch [587/938], Loss: 0.5458616018295288\n",
      "Train: Epoch [24], Batch [588/938], Loss: 0.3286021649837494\n",
      "Train: Epoch [24], Batch [589/938], Loss: 0.4292030930519104\n",
      "Train: Epoch [24], Batch [590/938], Loss: 0.29573333263397217\n",
      "Train: Epoch [24], Batch [591/938], Loss: 0.5078158974647522\n",
      "Train: Epoch [24], Batch [592/938], Loss: 0.31482264399528503\n",
      "Train: Epoch [24], Batch [593/938], Loss: 0.5631732940673828\n",
      "Train: Epoch [24], Batch [594/938], Loss: 0.38748764991760254\n",
      "Train: Epoch [24], Batch [595/938], Loss: 0.2968119978904724\n",
      "Train: Epoch [24], Batch [596/938], Loss: 0.4708291292190552\n",
      "Train: Epoch [24], Batch [597/938], Loss: 0.22579920291900635\n",
      "Train: Epoch [24], Batch [598/938], Loss: 0.5375540852546692\n",
      "Train: Epoch [24], Batch [599/938], Loss: 0.3717891275882721\n",
      "Train: Epoch [24], Batch [600/938], Loss: 0.38050416111946106\n",
      "Train: Epoch [24], Batch [601/938], Loss: 0.47361814975738525\n",
      "Train: Epoch [24], Batch [602/938], Loss: 0.613115131855011\n",
      "Train: Epoch [24], Batch [603/938], Loss: 0.4771058261394501\n",
      "Train: Epoch [24], Batch [604/938], Loss: 0.502560555934906\n",
      "Train: Epoch [24], Batch [605/938], Loss: 0.4095270037651062\n",
      "Train: Epoch [24], Batch [606/938], Loss: 0.3596533536911011\n",
      "Train: Epoch [24], Batch [607/938], Loss: 0.30325019359588623\n",
      "Train: Epoch [24], Batch [608/938], Loss: 0.24087214469909668\n",
      "Train: Epoch [24], Batch [609/938], Loss: 0.37933364510536194\n",
      "Train: Epoch [24], Batch [610/938], Loss: 0.3275271952152252\n",
      "Train: Epoch [24], Batch [611/938], Loss: 0.6331660747528076\n",
      "Train: Epoch [24], Batch [612/938], Loss: 0.2745237350463867\n",
      "Train: Epoch [24], Batch [613/938], Loss: 0.42123061418533325\n",
      "Train: Epoch [24], Batch [614/938], Loss: 0.30725762248039246\n",
      "Train: Epoch [24], Batch [615/938], Loss: 0.6221423745155334\n",
      "Train: Epoch [24], Batch [616/938], Loss: 0.5818884968757629\n",
      "Train: Epoch [24], Batch [617/938], Loss: 0.6685665845870972\n",
      "Train: Epoch [24], Batch [618/938], Loss: 0.4758140444755554\n",
      "Train: Epoch [24], Batch [619/938], Loss: 0.5238063931465149\n",
      "Train: Epoch [24], Batch [620/938], Loss: 0.3395228683948517\n",
      "Train: Epoch [24], Batch [621/938], Loss: 0.5699109435081482\n",
      "Train: Epoch [24], Batch [622/938], Loss: 0.3297291100025177\n",
      "Train: Epoch [24], Batch [623/938], Loss: 0.3796238303184509\n",
      "Train: Epoch [24], Batch [624/938], Loss: 0.48430997133255005\n",
      "Train: Epoch [24], Batch [625/938], Loss: 0.5033910274505615\n",
      "Train: Epoch [24], Batch [626/938], Loss: 0.5903565287590027\n",
      "Train: Epoch [24], Batch [627/938], Loss: 0.6415566802024841\n",
      "Train: Epoch [24], Batch [628/938], Loss: 0.2627336382865906\n",
      "Train: Epoch [24], Batch [629/938], Loss: 0.34378713369369507\n",
      "Train: Epoch [24], Batch [630/938], Loss: 0.3842926621437073\n",
      "Train: Epoch [24], Batch [631/938], Loss: 0.45059657096862793\n",
      "Train: Epoch [24], Batch [632/938], Loss: 0.3841252326965332\n",
      "Train: Epoch [24], Batch [633/938], Loss: 0.34326672554016113\n",
      "Train: Epoch [24], Batch [634/938], Loss: 0.4324847459793091\n",
      "Train: Epoch [24], Batch [635/938], Loss: 0.4942473769187927\n",
      "Train: Epoch [24], Batch [636/938], Loss: 0.3799590468406677\n",
      "Train: Epoch [24], Batch [637/938], Loss: 0.4653324484825134\n",
      "Train: Epoch [24], Batch [638/938], Loss: 0.3590787649154663\n",
      "Train: Epoch [24], Batch [639/938], Loss: 0.6179214119911194\n",
      "Train: Epoch [24], Batch [640/938], Loss: 0.3045068681240082\n",
      "Train: Epoch [24], Batch [641/938], Loss: 0.21158091723918915\n",
      "Train: Epoch [24], Batch [642/938], Loss: 0.5427507162094116\n",
      "Train: Epoch [24], Batch [643/938], Loss: 0.3335636854171753\n",
      "Train: Epoch [24], Batch [644/938], Loss: 0.2967299520969391\n",
      "Train: Epoch [24], Batch [645/938], Loss: 0.29997819662094116\n",
      "Train: Epoch [24], Batch [646/938], Loss: 0.44382748007774353\n",
      "Train: Epoch [24], Batch [647/938], Loss: 0.2654056251049042\n",
      "Train: Epoch [24], Batch [648/938], Loss: 0.44927722215652466\n",
      "Train: Epoch [24], Batch [649/938], Loss: 0.5782217979431152\n",
      "Train: Epoch [24], Batch [650/938], Loss: 0.24892815947532654\n",
      "Train: Epoch [24], Batch [651/938], Loss: 0.40948840975761414\n",
      "Train: Epoch [24], Batch [652/938], Loss: 0.27014634013175964\n",
      "Train: Epoch [24], Batch [653/938], Loss: 0.34250420331954956\n",
      "Train: Epoch [24], Batch [654/938], Loss: 0.5417136549949646\n",
      "Train: Epoch [24], Batch [655/938], Loss: 0.21604064106941223\n",
      "Train: Epoch [24], Batch [656/938], Loss: 0.33272865414619446\n",
      "Train: Epoch [24], Batch [657/938], Loss: 0.3806697726249695\n",
      "Train: Epoch [24], Batch [658/938], Loss: 0.4069797992706299\n",
      "Train: Epoch [24], Batch [659/938], Loss: 0.38154229521751404\n",
      "Train: Epoch [24], Batch [660/938], Loss: 0.38373252749443054\n",
      "Train: Epoch [24], Batch [661/938], Loss: 0.3524647355079651\n",
      "Train: Epoch [24], Batch [662/938], Loss: 0.4242706298828125\n",
      "Train: Epoch [24], Batch [663/938], Loss: 0.40485942363739014\n",
      "Train: Epoch [24], Batch [664/938], Loss: 0.6938566565513611\n",
      "Train: Epoch [24], Batch [665/938], Loss: 0.5454989075660706\n",
      "Train: Epoch [24], Batch [666/938], Loss: 0.3617454469203949\n",
      "Train: Epoch [24], Batch [667/938], Loss: 0.45471513271331787\n",
      "Train: Epoch [24], Batch [668/938], Loss: 0.5921444892883301\n",
      "Train: Epoch [24], Batch [669/938], Loss: 0.7217182517051697\n",
      "Train: Epoch [24], Batch [670/938], Loss: 0.3224347233772278\n",
      "Train: Epoch [24], Batch [671/938], Loss: 0.5245103240013123\n",
      "Train: Epoch [24], Batch [672/938], Loss: 0.448080450296402\n",
      "Train: Epoch [24], Batch [673/938], Loss: 0.3385102450847626\n",
      "Train: Epoch [24], Batch [674/938], Loss: 0.34462031722068787\n",
      "Train: Epoch [24], Batch [675/938], Loss: 0.44066479802131653\n",
      "Train: Epoch [24], Batch [676/938], Loss: 0.5754653811454773\n",
      "Train: Epoch [24], Batch [677/938], Loss: 0.263840913772583\n",
      "Train: Epoch [24], Batch [678/938], Loss: 0.3291971683502197\n",
      "Train: Epoch [24], Batch [679/938], Loss: 0.5237000584602356\n",
      "Train: Epoch [24], Batch [680/938], Loss: 0.5285015106201172\n",
      "Train: Epoch [24], Batch [681/938], Loss: 0.47249385714530945\n",
      "Train: Epoch [24], Batch [682/938], Loss: 0.549221396446228\n",
      "Train: Epoch [24], Batch [683/938], Loss: 0.3571990728378296\n",
      "Train: Epoch [24], Batch [684/938], Loss: 0.478899210691452\n",
      "Train: Epoch [24], Batch [685/938], Loss: 0.4680350720882416\n",
      "Train: Epoch [24], Batch [686/938], Loss: 0.5057235956192017\n",
      "Train: Epoch [24], Batch [687/938], Loss: 0.5509337186813354\n",
      "Train: Epoch [24], Batch [688/938], Loss: 0.40163806080818176\n",
      "Train: Epoch [24], Batch [689/938], Loss: 0.4519432485103607\n",
      "Train: Epoch [24], Batch [690/938], Loss: 0.4430590867996216\n",
      "Train: Epoch [24], Batch [691/938], Loss: 0.4697338342666626\n",
      "Train: Epoch [24], Batch [692/938], Loss: 0.35138532519340515\n",
      "Train: Epoch [24], Batch [693/938], Loss: 0.452976256608963\n",
      "Train: Epoch [24], Batch [694/938], Loss: 0.48272159695625305\n",
      "Train: Epoch [24], Batch [695/938], Loss: 0.34276044368743896\n",
      "Train: Epoch [24], Batch [696/938], Loss: 0.417611300945282\n",
      "Train: Epoch [24], Batch [697/938], Loss: 0.504499614238739\n",
      "Train: Epoch [24], Batch [698/938], Loss: 0.6435629725456238\n",
      "Train: Epoch [24], Batch [699/938], Loss: 0.2849041521549225\n",
      "Train: Epoch [24], Batch [700/938], Loss: 0.3151068091392517\n",
      "Train: Epoch [24], Batch [701/938], Loss: 0.30292993783950806\n",
      "Train: Epoch [24], Batch [702/938], Loss: 0.37983155250549316\n",
      "Train: Epoch [24], Batch [703/938], Loss: 0.434084951877594\n",
      "Train: Epoch [24], Batch [704/938], Loss: 0.3168919086456299\n",
      "Train: Epoch [24], Batch [705/938], Loss: 0.4193532466888428\n",
      "Train: Epoch [24], Batch [706/938], Loss: 0.417236328125\n",
      "Train: Epoch [24], Batch [707/938], Loss: 0.4007992148399353\n",
      "Train: Epoch [24], Batch [708/938], Loss: 0.4563872516155243\n",
      "Train: Epoch [24], Batch [709/938], Loss: 0.2704649567604065\n",
      "Train: Epoch [24], Batch [710/938], Loss: 0.30786532163619995\n",
      "Train: Epoch [24], Batch [711/938], Loss: 0.4131661057472229\n",
      "Train: Epoch [24], Batch [712/938], Loss: 0.389461874961853\n",
      "Train: Epoch [24], Batch [713/938], Loss: 0.3040224611759186\n",
      "Train: Epoch [24], Batch [714/938], Loss: 0.42722806334495544\n",
      "Train: Epoch [24], Batch [715/938], Loss: 0.3866489827632904\n",
      "Train: Epoch [24], Batch [716/938], Loss: 0.6045846343040466\n",
      "Train: Epoch [24], Batch [717/938], Loss: 0.418395459651947\n",
      "Train: Epoch [24], Batch [718/938], Loss: 0.43099719285964966\n",
      "Train: Epoch [24], Batch [719/938], Loss: 0.6078107357025146\n",
      "Train: Epoch [24], Batch [720/938], Loss: 0.4321187734603882\n",
      "Train: Epoch [24], Batch [721/938], Loss: 0.3658701181411743\n",
      "Train: Epoch [24], Batch [722/938], Loss: 0.3356458842754364\n",
      "Train: Epoch [24], Batch [723/938], Loss: 0.30241650342941284\n",
      "Train: Epoch [24], Batch [724/938], Loss: 0.5703079700469971\n",
      "Train: Epoch [24], Batch [725/938], Loss: 0.35520362854003906\n",
      "Train: Epoch [24], Batch [726/938], Loss: 0.5103442668914795\n",
      "Train: Epoch [24], Batch [727/938], Loss: 0.34474503993988037\n",
      "Train: Epoch [24], Batch [728/938], Loss: 0.4294411540031433\n",
      "Train: Epoch [24], Batch [729/938], Loss: 0.4726451635360718\n",
      "Train: Epoch [24], Batch [730/938], Loss: 0.48788368701934814\n",
      "Train: Epoch [24], Batch [731/938], Loss: 0.4986787736415863\n",
      "Train: Epoch [24], Batch [732/938], Loss: 0.44165170192718506\n",
      "Train: Epoch [24], Batch [733/938], Loss: 0.37611278891563416\n",
      "Train: Epoch [24], Batch [734/938], Loss: 0.4217688739299774\n",
      "Train: Epoch [24], Batch [735/938], Loss: 0.434121310710907\n",
      "Train: Epoch [24], Batch [736/938], Loss: 0.4056527018547058\n",
      "Train: Epoch [24], Batch [737/938], Loss: 0.7132040858268738\n",
      "Train: Epoch [24], Batch [738/938], Loss: 0.37348833680152893\n",
      "Train: Epoch [24], Batch [739/938], Loss: 0.28383520245552063\n",
      "Train: Epoch [24], Batch [740/938], Loss: 0.22923070192337036\n",
      "Train: Epoch [24], Batch [741/938], Loss: 0.6547119617462158\n",
      "Train: Epoch [24], Batch [742/938], Loss: 0.33669185638427734\n",
      "Train: Epoch [24], Batch [743/938], Loss: 0.4090382754802704\n",
      "Train: Epoch [24], Batch [744/938], Loss: 0.5332014560699463\n",
      "Train: Epoch [24], Batch [745/938], Loss: 0.45136338472366333\n",
      "Train: Epoch [24], Batch [746/938], Loss: 0.31369608640670776\n",
      "Train: Epoch [24], Batch [747/938], Loss: 0.5910659432411194\n",
      "Train: Epoch [24], Batch [748/938], Loss: 0.34731313586235046\n",
      "Train: Epoch [24], Batch [749/938], Loss: 0.42619597911834717\n",
      "Train: Epoch [24], Batch [750/938], Loss: 0.48531070351600647\n",
      "Train: Epoch [24], Batch [751/938], Loss: 0.39094626903533936\n",
      "Train: Epoch [24], Batch [752/938], Loss: 0.5232580900192261\n",
      "Train: Epoch [24], Batch [753/938], Loss: 0.3861823081970215\n",
      "Train: Epoch [24], Batch [754/938], Loss: 0.49462658166885376\n",
      "Train: Epoch [24], Batch [755/938], Loss: 0.48766466975212097\n",
      "Train: Epoch [24], Batch [756/938], Loss: 0.3515877425670624\n",
      "Train: Epoch [24], Batch [757/938], Loss: 0.2773302495479584\n",
      "Train: Epoch [24], Batch [758/938], Loss: 0.29715272784233093\n",
      "Train: Epoch [24], Batch [759/938], Loss: 0.4293787181377411\n",
      "Train: Epoch [24], Batch [760/938], Loss: 0.32871466875076294\n",
      "Train: Epoch [24], Batch [761/938], Loss: 0.26074469089508057\n",
      "Train: Epoch [24], Batch [762/938], Loss: 0.2764729857444763\n",
      "Train: Epoch [24], Batch [763/938], Loss: 0.4829955995082855\n",
      "Train: Epoch [24], Batch [764/938], Loss: 0.5555522441864014\n",
      "Train: Epoch [24], Batch [765/938], Loss: 0.38496559858322144\n",
      "Train: Epoch [24], Batch [766/938], Loss: 0.3609861433506012\n",
      "Train: Epoch [24], Batch [767/938], Loss: 0.34676629304885864\n",
      "Train: Epoch [24], Batch [768/938], Loss: 0.3651250898838043\n",
      "Train: Epoch [24], Batch [769/938], Loss: 0.3045468330383301\n",
      "Train: Epoch [24], Batch [770/938], Loss: 0.3766961097717285\n",
      "Train: Epoch [24], Batch [771/938], Loss: 0.3414117097854614\n",
      "Train: Epoch [24], Batch [772/938], Loss: 0.4833666682243347\n",
      "Train: Epoch [24], Batch [773/938], Loss: 0.5188003778457642\n",
      "Train: Epoch [24], Batch [774/938], Loss: 0.3001404106616974\n",
      "Train: Epoch [24], Batch [775/938], Loss: 0.3247421979904175\n",
      "Train: Epoch [24], Batch [776/938], Loss: 0.3950875401496887\n",
      "Train: Epoch [24], Batch [777/938], Loss: 0.28254252672195435\n",
      "Train: Epoch [24], Batch [778/938], Loss: 0.3096450865268707\n",
      "Train: Epoch [24], Batch [779/938], Loss: 0.47415879368782043\n",
      "Train: Epoch [24], Batch [780/938], Loss: 0.23584648966789246\n",
      "Train: Epoch [24], Batch [781/938], Loss: 0.5096977949142456\n",
      "Train: Epoch [24], Batch [782/938], Loss: 0.45247867703437805\n",
      "Train: Epoch [24], Batch [783/938], Loss: 0.2211199849843979\n",
      "Train: Epoch [24], Batch [784/938], Loss: 0.29471299052238464\n",
      "Train: Epoch [24], Batch [785/938], Loss: 0.2829030156135559\n",
      "Train: Epoch [24], Batch [786/938], Loss: 0.4679938852787018\n",
      "Train: Epoch [24], Batch [787/938], Loss: 0.3380376696586609\n",
      "Train: Epoch [24], Batch [788/938], Loss: 0.4758477210998535\n",
      "Train: Epoch [24], Batch [789/938], Loss: 0.637458086013794\n",
      "Train: Epoch [24], Batch [790/938], Loss: 0.3446115255355835\n",
      "Train: Epoch [24], Batch [791/938], Loss: 0.47699183225631714\n",
      "Train: Epoch [24], Batch [792/938], Loss: 0.41940274834632874\n",
      "Train: Epoch [24], Batch [793/938], Loss: 0.5003394484519958\n",
      "Train: Epoch [24], Batch [794/938], Loss: 0.49925923347473145\n",
      "Train: Epoch [24], Batch [795/938], Loss: 0.6301432847976685\n",
      "Train: Epoch [24], Batch [796/938], Loss: 0.5363221168518066\n",
      "Train: Epoch [24], Batch [797/938], Loss: 0.4412454664707184\n",
      "Train: Epoch [24], Batch [798/938], Loss: 0.30781999230384827\n",
      "Train: Epoch [24], Batch [799/938], Loss: 0.28148654103279114\n",
      "Train: Epoch [24], Batch [800/938], Loss: 0.28670206665992737\n",
      "Train: Epoch [24], Batch [801/938], Loss: 0.4780411124229431\n",
      "Train: Epoch [24], Batch [802/938], Loss: 0.26718536019325256\n",
      "Train: Epoch [24], Batch [803/938], Loss: 0.3541242480278015\n",
      "Train: Epoch [24], Batch [804/938], Loss: 0.4393209218978882\n",
      "Train: Epoch [24], Batch [805/938], Loss: 0.5109480023384094\n",
      "Train: Epoch [24], Batch [806/938], Loss: 0.20420384407043457\n",
      "Train: Epoch [24], Batch [807/938], Loss: 0.5210865139961243\n",
      "Train: Epoch [24], Batch [808/938], Loss: 0.40207579731941223\n",
      "Train: Epoch [24], Batch [809/938], Loss: 0.22964219748973846\n",
      "Train: Epoch [24], Batch [810/938], Loss: 0.3807150721549988\n",
      "Train: Epoch [24], Batch [811/938], Loss: 0.4789092242717743\n",
      "Train: Epoch [24], Batch [812/938], Loss: 0.22615568339824677\n",
      "Train: Epoch [24], Batch [813/938], Loss: 0.3026334345340729\n",
      "Train: Epoch [24], Batch [814/938], Loss: 0.388735294342041\n",
      "Train: Epoch [24], Batch [815/938], Loss: 0.4160262942314148\n",
      "Train: Epoch [24], Batch [816/938], Loss: 0.5881562829017639\n",
      "Train: Epoch [24], Batch [817/938], Loss: 0.31447625160217285\n",
      "Train: Epoch [24], Batch [818/938], Loss: 0.32794302701950073\n",
      "Train: Epoch [24], Batch [819/938], Loss: 0.4620751440525055\n",
      "Train: Epoch [24], Batch [820/938], Loss: 0.33587950468063354\n",
      "Train: Epoch [24], Batch [821/938], Loss: 0.41980504989624023\n",
      "Train: Epoch [24], Batch [822/938], Loss: 0.24486935138702393\n",
      "Train: Epoch [24], Batch [823/938], Loss: 0.36975130438804626\n",
      "Train: Epoch [24], Batch [824/938], Loss: 0.31000298261642456\n",
      "Train: Epoch [24], Batch [825/938], Loss: 0.4216211140155792\n",
      "Train: Epoch [24], Batch [826/938], Loss: 0.41497087478637695\n",
      "Train: Epoch [24], Batch [827/938], Loss: 0.2565305829048157\n",
      "Train: Epoch [24], Batch [828/938], Loss: 0.47879618406295776\n",
      "Train: Epoch [24], Batch [829/938], Loss: 0.39772889018058777\n",
      "Train: Epoch [24], Batch [830/938], Loss: 0.41286447644233704\n",
      "Train: Epoch [24], Batch [831/938], Loss: 0.398986279964447\n",
      "Train: Epoch [24], Batch [832/938], Loss: 0.5521870255470276\n",
      "Train: Epoch [24], Batch [833/938], Loss: 0.3329922556877136\n",
      "Train: Epoch [24], Batch [834/938], Loss: 0.5509742498397827\n",
      "Train: Epoch [24], Batch [835/938], Loss: 0.46168795228004456\n",
      "Train: Epoch [24], Batch [836/938], Loss: 0.279662162065506\n",
      "Train: Epoch [24], Batch [837/938], Loss: 0.3376358449459076\n",
      "Train: Epoch [24], Batch [838/938], Loss: 0.36744558811187744\n",
      "Train: Epoch [24], Batch [839/938], Loss: 0.4487646818161011\n",
      "Train: Epoch [24], Batch [840/938], Loss: 0.3332301080226898\n",
      "Train: Epoch [24], Batch [841/938], Loss: 0.39920470118522644\n",
      "Train: Epoch [24], Batch [842/938], Loss: 0.2845965325832367\n",
      "Train: Epoch [24], Batch [843/938], Loss: 0.39338138699531555\n",
      "Train: Epoch [24], Batch [844/938], Loss: 0.41204309463500977\n",
      "Train: Epoch [24], Batch [845/938], Loss: 0.37374043464660645\n",
      "Train: Epoch [24], Batch [846/938], Loss: 0.4239919185638428\n",
      "Train: Epoch [24], Batch [847/938], Loss: 0.3480525314807892\n",
      "Train: Epoch [24], Batch [848/938], Loss: 0.4293628931045532\n",
      "Train: Epoch [24], Batch [849/938], Loss: 0.48387759923934937\n",
      "Train: Epoch [24], Batch [850/938], Loss: 0.3652532398700714\n",
      "Train: Epoch [24], Batch [851/938], Loss: 0.5566449165344238\n",
      "Train: Epoch [24], Batch [852/938], Loss: 0.4677070379257202\n",
      "Train: Epoch [24], Batch [853/938], Loss: 0.40292662382125854\n",
      "Train: Epoch [24], Batch [854/938], Loss: 0.305401474237442\n",
      "Train: Epoch [24], Batch [855/938], Loss: 0.577635645866394\n",
      "Train: Epoch [24], Batch [856/938], Loss: 0.29831963777542114\n",
      "Train: Epoch [24], Batch [857/938], Loss: 0.2874259054660797\n",
      "Train: Epoch [24], Batch [858/938], Loss: 0.3731001913547516\n",
      "Train: Epoch [24], Batch [859/938], Loss: 0.4545952081680298\n",
      "Train: Epoch [24], Batch [860/938], Loss: 0.4545114040374756\n",
      "Train: Epoch [24], Batch [861/938], Loss: 0.4675118625164032\n",
      "Train: Epoch [24], Batch [862/938], Loss: 0.5181008577346802\n",
      "Train: Epoch [24], Batch [863/938], Loss: 0.5381325483322144\n",
      "Train: Epoch [24], Batch [864/938], Loss: 0.5996707081794739\n",
      "Train: Epoch [24], Batch [865/938], Loss: 0.609107255935669\n",
      "Train: Epoch [24], Batch [866/938], Loss: 0.44079551100730896\n",
      "Train: Epoch [24], Batch [867/938], Loss: 0.39785781502723694\n",
      "Train: Epoch [24], Batch [868/938], Loss: 0.32142505049705505\n",
      "Train: Epoch [24], Batch [869/938], Loss: 0.46526259183883667\n",
      "Train: Epoch [24], Batch [870/938], Loss: 0.46238866448402405\n",
      "Train: Epoch [24], Batch [871/938], Loss: 0.3857417404651642\n",
      "Train: Epoch [24], Batch [872/938], Loss: 0.3516705334186554\n",
      "Train: Epoch [24], Batch [873/938], Loss: 0.40020379424095154\n",
      "Train: Epoch [24], Batch [874/938], Loss: 0.33279314637184143\n",
      "Train: Epoch [24], Batch [875/938], Loss: 0.29052430391311646\n",
      "Train: Epoch [24], Batch [876/938], Loss: 0.46392178535461426\n",
      "Train: Epoch [24], Batch [877/938], Loss: 0.6351211071014404\n",
      "Train: Epoch [24], Batch [878/938], Loss: 0.18737466633319855\n",
      "Train: Epoch [24], Batch [879/938], Loss: 0.494348406791687\n",
      "Train: Epoch [24], Batch [880/938], Loss: 0.39802247285842896\n",
      "Train: Epoch [24], Batch [881/938], Loss: 0.3660942316055298\n",
      "Train: Epoch [24], Batch [882/938], Loss: 0.32474854588508606\n",
      "Train: Epoch [24], Batch [883/938], Loss: 0.3265358805656433\n",
      "Train: Epoch [24], Batch [884/938], Loss: 0.2661585807800293\n",
      "Train: Epoch [24], Batch [885/938], Loss: 0.5298640131950378\n",
      "Train: Epoch [24], Batch [886/938], Loss: 0.42896798253059387\n",
      "Train: Epoch [24], Batch [887/938], Loss: 0.3238722085952759\n",
      "Train: Epoch [24], Batch [888/938], Loss: 0.3635918200016022\n",
      "Train: Epoch [24], Batch [889/938], Loss: 0.42006176710128784\n",
      "Train: Epoch [24], Batch [890/938], Loss: 0.41619449853897095\n",
      "Train: Epoch [24], Batch [891/938], Loss: 0.5300493836402893\n",
      "Train: Epoch [24], Batch [892/938], Loss: 0.44798702001571655\n",
      "Train: Epoch [24], Batch [893/938], Loss: 0.32508447766304016\n",
      "Train: Epoch [24], Batch [894/938], Loss: 0.24441055953502655\n",
      "Train: Epoch [24], Batch [895/938], Loss: 0.43005871772766113\n",
      "Train: Epoch [24], Batch [896/938], Loss: 0.3128235638141632\n",
      "Train: Epoch [24], Batch [897/938], Loss: 0.47588416934013367\n",
      "Train: Epoch [24], Batch [898/938], Loss: 0.45824387669563293\n",
      "Train: Epoch [24], Batch [899/938], Loss: 0.496404230594635\n",
      "Train: Epoch [24], Batch [900/938], Loss: 0.3734472990036011\n",
      "Train: Epoch [24], Batch [901/938], Loss: 0.3658360242843628\n",
      "Train: Epoch [24], Batch [902/938], Loss: 0.3238385319709778\n",
      "Train: Epoch [24], Batch [903/938], Loss: 0.5638054609298706\n",
      "Train: Epoch [24], Batch [904/938], Loss: 0.26465579867362976\n",
      "Train: Epoch [24], Batch [905/938], Loss: 0.3986721634864807\n",
      "Train: Epoch [24], Batch [906/938], Loss: 0.18038202822208405\n",
      "Train: Epoch [24], Batch [907/938], Loss: 0.3158656656742096\n",
      "Train: Epoch [24], Batch [908/938], Loss: 0.5791378021240234\n",
      "Train: Epoch [24], Batch [909/938], Loss: 0.3767775893211365\n",
      "Train: Epoch [24], Batch [910/938], Loss: 0.2596929669380188\n",
      "Train: Epoch [24], Batch [911/938], Loss: 0.3425420820713043\n",
      "Train: Epoch [24], Batch [912/938], Loss: 0.35658684372901917\n",
      "Train: Epoch [24], Batch [913/938], Loss: 0.41419410705566406\n",
      "Train: Epoch [24], Batch [914/938], Loss: 0.6169638633728027\n",
      "Train: Epoch [24], Batch [915/938], Loss: 0.5678324103355408\n",
      "Train: Epoch [24], Batch [916/938], Loss: 0.7615747451782227\n",
      "Train: Epoch [24], Batch [917/938], Loss: 0.5785155892372131\n",
      "Train: Epoch [24], Batch [918/938], Loss: 0.5515029430389404\n",
      "Train: Epoch [24], Batch [919/938], Loss: 0.4153227210044861\n",
      "Train: Epoch [24], Batch [920/938], Loss: 0.3888426721096039\n",
      "Train: Epoch [24], Batch [921/938], Loss: 0.42675936222076416\n",
      "Train: Epoch [24], Batch [922/938], Loss: 0.46676105260849\n",
      "Train: Epoch [24], Batch [923/938], Loss: 0.467977374792099\n",
      "Train: Epoch [24], Batch [924/938], Loss: 0.4177808463573456\n",
      "Train: Epoch [24], Batch [925/938], Loss: 0.5429194569587708\n",
      "Train: Epoch [24], Batch [926/938], Loss: 0.371987521648407\n",
      "Train: Epoch [24], Batch [927/938], Loss: 0.32618698477745056\n",
      "Train: Epoch [24], Batch [928/938], Loss: 0.4275496304035187\n",
      "Train: Epoch [24], Batch [929/938], Loss: 0.4746125340461731\n",
      "Train: Epoch [24], Batch [930/938], Loss: 0.3881694972515106\n",
      "Train: Epoch [24], Batch [931/938], Loss: 0.5115618705749512\n",
      "Train: Epoch [24], Batch [932/938], Loss: 0.45827966928482056\n",
      "Train: Epoch [24], Batch [933/938], Loss: 0.44392216205596924\n",
      "Train: Epoch [24], Batch [934/938], Loss: 0.43226951360702515\n",
      "Train: Epoch [24], Batch [935/938], Loss: 0.27893534302711487\n",
      "Train: Epoch [24], Batch [936/938], Loss: 0.4431658387184143\n",
      "Train: Epoch [24], Batch [937/938], Loss: 0.6033133864402771\n",
      "Train: Epoch [24], Batch [938/938], Loss: 0.2297966033220291\n",
      "Accuracy of train set: 0.8506\n",
      "Validation: Epoch [24], Batch [1/938], Loss: 0.28533899784088135\n",
      "Validation: Epoch [24], Batch [2/938], Loss: 0.4001270532608032\n",
      "Validation: Epoch [24], Batch [3/938], Loss: 0.34203946590423584\n",
      "Validation: Epoch [24], Batch [4/938], Loss: 0.3361968994140625\n",
      "Validation: Epoch [24], Batch [5/938], Loss: 0.3753686249256134\n",
      "Validation: Epoch [24], Batch [6/938], Loss: 0.3552117645740509\n",
      "Validation: Epoch [24], Batch [7/938], Loss: 0.49276280403137207\n",
      "Validation: Epoch [24], Batch [8/938], Loss: 0.47020670771598816\n",
      "Validation: Epoch [24], Batch [9/938], Loss: 0.3240984082221985\n",
      "Validation: Epoch [24], Batch [10/938], Loss: 0.2810564637184143\n",
      "Validation: Epoch [24], Batch [11/938], Loss: 0.5623162984848022\n",
      "Validation: Epoch [24], Batch [12/938], Loss: 0.3747565746307373\n",
      "Validation: Epoch [24], Batch [13/938], Loss: 0.3243841528892517\n",
      "Validation: Epoch [24], Batch [14/938], Loss: 0.411617249250412\n",
      "Validation: Epoch [24], Batch [15/938], Loss: 0.4086171090602875\n",
      "Validation: Epoch [24], Batch [16/938], Loss: 0.45981255173683167\n",
      "Validation: Epoch [24], Batch [17/938], Loss: 0.46787598729133606\n",
      "Validation: Epoch [24], Batch [18/938], Loss: 0.44427722692489624\n",
      "Validation: Epoch [24], Batch [19/938], Loss: 0.33720043301582336\n",
      "Validation: Epoch [24], Batch [20/938], Loss: 0.3625458776950836\n",
      "Validation: Epoch [24], Batch [21/938], Loss: 0.3892197608947754\n",
      "Validation: Epoch [24], Batch [22/938], Loss: 0.5182803869247437\n",
      "Validation: Epoch [24], Batch [23/938], Loss: 0.3028358221054077\n",
      "Validation: Epoch [24], Batch [24/938], Loss: 0.29442280530929565\n",
      "Validation: Epoch [24], Batch [25/938], Loss: 0.4037754535675049\n",
      "Validation: Epoch [24], Batch [26/938], Loss: 0.4840134382247925\n",
      "Validation: Epoch [24], Batch [27/938], Loss: 0.22579170763492584\n",
      "Validation: Epoch [24], Batch [28/938], Loss: 0.4870899021625519\n",
      "Validation: Epoch [24], Batch [29/938], Loss: 0.43172577023506165\n",
      "Validation: Epoch [24], Batch [30/938], Loss: 0.3063214123249054\n",
      "Validation: Epoch [24], Batch [31/938], Loss: 0.41192036867141724\n",
      "Validation: Epoch [24], Batch [32/938], Loss: 0.4411836862564087\n",
      "Validation: Epoch [24], Batch [33/938], Loss: 0.4098952114582062\n",
      "Validation: Epoch [24], Batch [34/938], Loss: 0.34948471188545227\n",
      "Validation: Epoch [24], Batch [35/938], Loss: 0.3524021804332733\n",
      "Validation: Epoch [24], Batch [36/938], Loss: 0.6072735786437988\n",
      "Validation: Epoch [24], Batch [37/938], Loss: 0.27874961495399475\n",
      "Validation: Epoch [24], Batch [38/938], Loss: 0.6135666370391846\n",
      "Validation: Epoch [24], Batch [39/938], Loss: 0.3070541024208069\n",
      "Validation: Epoch [24], Batch [40/938], Loss: 0.3034570813179016\n",
      "Validation: Epoch [24], Batch [41/938], Loss: 0.5356625914573669\n",
      "Validation: Epoch [24], Batch [42/938], Loss: 0.419214129447937\n",
      "Validation: Epoch [24], Batch [43/938], Loss: 0.474019318819046\n",
      "Validation: Epoch [24], Batch [44/938], Loss: 0.5448333621025085\n",
      "Validation: Epoch [24], Batch [45/938], Loss: 0.31241652369499207\n",
      "Validation: Epoch [24], Batch [46/938], Loss: 0.4516468942165375\n",
      "Validation: Epoch [24], Batch [47/938], Loss: 0.3853892385959625\n",
      "Validation: Epoch [24], Batch [48/938], Loss: 0.6210402846336365\n",
      "Validation: Epoch [24], Batch [49/938], Loss: 0.5496619343757629\n",
      "Validation: Epoch [24], Batch [50/938], Loss: 0.5735820531845093\n",
      "Validation: Epoch [24], Batch [51/938], Loss: 0.2929537892341614\n",
      "Validation: Epoch [24], Batch [52/938], Loss: 0.2902959883213043\n",
      "Validation: Epoch [24], Batch [53/938], Loss: 0.4417199194431305\n",
      "Validation: Epoch [24], Batch [54/938], Loss: 0.4520983099937439\n",
      "Validation: Epoch [24], Batch [55/938], Loss: 0.3444094657897949\n",
      "Validation: Epoch [24], Batch [56/938], Loss: 0.43615081906318665\n",
      "Validation: Epoch [24], Batch [57/938], Loss: 0.3000596761703491\n",
      "Validation: Epoch [24], Batch [58/938], Loss: 0.5517280101776123\n",
      "Validation: Epoch [24], Batch [59/938], Loss: 0.23993784189224243\n",
      "Validation: Epoch [24], Batch [60/938], Loss: 0.42512059211730957\n",
      "Validation: Epoch [24], Batch [61/938], Loss: 0.5312165021896362\n",
      "Validation: Epoch [24], Batch [62/938], Loss: 0.3429640829563141\n",
      "Validation: Epoch [24], Batch [63/938], Loss: 0.3013005554676056\n",
      "Validation: Epoch [24], Batch [64/938], Loss: 0.40537574887275696\n",
      "Validation: Epoch [24], Batch [65/938], Loss: 0.3944714069366455\n",
      "Validation: Epoch [24], Batch [66/938], Loss: 0.5693222284317017\n",
      "Validation: Epoch [24], Batch [67/938], Loss: 0.2909550666809082\n",
      "Validation: Epoch [24], Batch [68/938], Loss: 0.46736276149749756\n",
      "Validation: Epoch [24], Batch [69/938], Loss: 0.5554400682449341\n",
      "Validation: Epoch [24], Batch [70/938], Loss: 0.2565770149230957\n",
      "Validation: Epoch [24], Batch [71/938], Loss: 0.6612076759338379\n",
      "Validation: Epoch [24], Batch [72/938], Loss: 0.28935056924819946\n",
      "Validation: Epoch [24], Batch [73/938], Loss: 0.31969305872917175\n",
      "Validation: Epoch [24], Batch [74/938], Loss: 0.4134276509284973\n",
      "Validation: Epoch [24], Batch [75/938], Loss: 0.53279048204422\n",
      "Validation: Epoch [24], Batch [76/938], Loss: 0.35256022214889526\n",
      "Validation: Epoch [24], Batch [77/938], Loss: 0.5223524570465088\n",
      "Validation: Epoch [24], Batch [78/938], Loss: 0.42208772897720337\n",
      "Validation: Epoch [24], Batch [79/938], Loss: 0.5042386651039124\n",
      "Validation: Epoch [24], Batch [80/938], Loss: 0.3948724865913391\n",
      "Validation: Epoch [24], Batch [81/938], Loss: 0.4492455720901489\n",
      "Validation: Epoch [24], Batch [82/938], Loss: 0.5014984011650085\n",
      "Validation: Epoch [24], Batch [83/938], Loss: 0.4221998453140259\n",
      "Validation: Epoch [24], Batch [84/938], Loss: 0.430780291557312\n",
      "Validation: Epoch [24], Batch [85/938], Loss: 0.48297056555747986\n",
      "Validation: Epoch [24], Batch [86/938], Loss: 0.3268211781978607\n",
      "Validation: Epoch [24], Batch [87/938], Loss: 0.5406234860420227\n",
      "Validation: Epoch [24], Batch [88/938], Loss: 0.19576221704483032\n",
      "Validation: Epoch [24], Batch [89/938], Loss: 0.4782751500606537\n",
      "Validation: Epoch [24], Batch [90/938], Loss: 0.304538369178772\n",
      "Validation: Epoch [24], Batch [91/938], Loss: 0.7152705788612366\n",
      "Validation: Epoch [24], Batch [92/938], Loss: 0.27200427651405334\n",
      "Validation: Epoch [24], Batch [93/938], Loss: 0.34499531984329224\n",
      "Validation: Epoch [24], Batch [94/938], Loss: 0.46938279271125793\n",
      "Validation: Epoch [24], Batch [95/938], Loss: 0.4931745231151581\n",
      "Validation: Epoch [24], Batch [96/938], Loss: 0.43794649839401245\n",
      "Validation: Epoch [24], Batch [97/938], Loss: 0.6373050212860107\n",
      "Validation: Epoch [24], Batch [98/938], Loss: 0.29630550742149353\n",
      "Validation: Epoch [24], Batch [99/938], Loss: 0.4378317892551422\n",
      "Validation: Epoch [24], Batch [100/938], Loss: 0.37653684616088867\n",
      "Validation: Epoch [24], Batch [101/938], Loss: 0.26676687598228455\n",
      "Validation: Epoch [24], Batch [102/938], Loss: 0.598764181137085\n",
      "Validation: Epoch [24], Batch [103/938], Loss: 0.4408438801765442\n",
      "Validation: Epoch [24], Batch [104/938], Loss: 0.45038318634033203\n",
      "Validation: Epoch [24], Batch [105/938], Loss: 0.3029319643974304\n",
      "Validation: Epoch [24], Batch [106/938], Loss: 0.48473989963531494\n",
      "Validation: Epoch [24], Batch [107/938], Loss: 0.33248600363731384\n",
      "Validation: Epoch [24], Batch [108/938], Loss: 0.558373749256134\n",
      "Validation: Epoch [24], Batch [109/938], Loss: 0.49657416343688965\n",
      "Validation: Epoch [24], Batch [110/938], Loss: 0.4646890163421631\n",
      "Validation: Epoch [24], Batch [111/938], Loss: 0.43755507469177246\n",
      "Validation: Epoch [24], Batch [112/938], Loss: 0.5891973972320557\n",
      "Validation: Epoch [24], Batch [113/938], Loss: 0.23499524593353271\n",
      "Validation: Epoch [24], Batch [114/938], Loss: 0.45047950744628906\n",
      "Validation: Epoch [24], Batch [115/938], Loss: 0.4102424681186676\n",
      "Validation: Epoch [24], Batch [116/938], Loss: 0.37609434127807617\n",
      "Validation: Epoch [24], Batch [117/938], Loss: 0.6186546087265015\n",
      "Validation: Epoch [24], Batch [118/938], Loss: 0.41287052631378174\n",
      "Validation: Epoch [24], Batch [119/938], Loss: 0.46073824167251587\n",
      "Validation: Epoch [24], Batch [120/938], Loss: 0.35878443717956543\n",
      "Validation: Epoch [24], Batch [121/938], Loss: 0.4510885179042816\n",
      "Validation: Epoch [24], Batch [122/938], Loss: 0.27970090508461\n",
      "Validation: Epoch [24], Batch [123/938], Loss: 0.2868211269378662\n",
      "Validation: Epoch [24], Batch [124/938], Loss: 0.37740659713745117\n",
      "Validation: Epoch [24], Batch [125/938], Loss: 0.44345709681510925\n",
      "Validation: Epoch [24], Batch [126/938], Loss: 0.4619232714176178\n",
      "Validation: Epoch [24], Batch [127/938], Loss: 0.43003830313682556\n",
      "Validation: Epoch [24], Batch [128/938], Loss: 0.4696097671985626\n",
      "Validation: Epoch [24], Batch [129/938], Loss: 0.33498111367225647\n",
      "Validation: Epoch [24], Batch [130/938], Loss: 0.42734602093696594\n",
      "Validation: Epoch [24], Batch [131/938], Loss: 0.5398919582366943\n",
      "Validation: Epoch [24], Batch [132/938], Loss: 0.437469482421875\n",
      "Validation: Epoch [24], Batch [133/938], Loss: 0.599461019039154\n",
      "Validation: Epoch [24], Batch [134/938], Loss: 0.42390719056129456\n",
      "Validation: Epoch [24], Batch [135/938], Loss: 0.24418199062347412\n",
      "Validation: Epoch [24], Batch [136/938], Loss: 0.5273050665855408\n",
      "Validation: Epoch [24], Batch [137/938], Loss: 0.37303832173347473\n",
      "Validation: Epoch [24], Batch [138/938], Loss: 0.2780139446258545\n",
      "Validation: Epoch [24], Batch [139/938], Loss: 0.3971572518348694\n",
      "Validation: Epoch [24], Batch [140/938], Loss: 0.4024454355239868\n",
      "Validation: Epoch [24], Batch [141/938], Loss: 0.38635557889938354\n",
      "Validation: Epoch [24], Batch [142/938], Loss: 0.4682461619377136\n",
      "Validation: Epoch [24], Batch [143/938], Loss: 0.4636458158493042\n",
      "Validation: Epoch [24], Batch [144/938], Loss: 0.49207040667533875\n",
      "Validation: Epoch [24], Batch [145/938], Loss: 0.5729029774665833\n",
      "Validation: Epoch [24], Batch [146/938], Loss: 0.45426446199417114\n",
      "Validation: Epoch [24], Batch [147/938], Loss: 0.32387709617614746\n",
      "Validation: Epoch [24], Batch [148/938], Loss: 0.26210200786590576\n",
      "Validation: Epoch [24], Batch [149/938], Loss: 0.33961814641952515\n",
      "Validation: Epoch [24], Batch [150/938], Loss: 0.5345699787139893\n",
      "Validation: Epoch [24], Batch [151/938], Loss: 0.5185785889625549\n",
      "Validation: Epoch [24], Batch [152/938], Loss: 0.28689393401145935\n",
      "Validation: Epoch [24], Batch [153/938], Loss: 0.4826821982860565\n",
      "Validation: Epoch [24], Batch [154/938], Loss: 0.29422399401664734\n",
      "Validation: Epoch [24], Batch [155/938], Loss: 0.3489029109477997\n",
      "Validation: Epoch [24], Batch [156/938], Loss: 0.3912753462791443\n",
      "Validation: Epoch [24], Batch [157/938], Loss: 0.39035099744796753\n",
      "Validation: Epoch [24], Batch [158/938], Loss: 0.4283140003681183\n",
      "Validation: Epoch [24], Batch [159/938], Loss: 0.49337854981422424\n",
      "Validation: Epoch [24], Batch [160/938], Loss: 0.45869606733322144\n",
      "Validation: Epoch [24], Batch [161/938], Loss: 0.2766677439212799\n",
      "Validation: Epoch [24], Batch [162/938], Loss: 0.4826384484767914\n",
      "Validation: Epoch [24], Batch [163/938], Loss: 0.785130500793457\n",
      "Validation: Epoch [24], Batch [164/938], Loss: 0.4468928575515747\n",
      "Validation: Epoch [24], Batch [165/938], Loss: 0.3501759171485901\n",
      "Validation: Epoch [24], Batch [166/938], Loss: 0.35516801476478577\n",
      "Validation: Epoch [24], Batch [167/938], Loss: 0.5308105945587158\n",
      "Validation: Epoch [24], Batch [168/938], Loss: 0.3135682940483093\n",
      "Validation: Epoch [24], Batch [169/938], Loss: 0.31193241477012634\n",
      "Validation: Epoch [24], Batch [170/938], Loss: 0.39303144812583923\n",
      "Validation: Epoch [24], Batch [171/938], Loss: 0.4598466157913208\n",
      "Validation: Epoch [24], Batch [172/938], Loss: 0.4645959138870239\n",
      "Validation: Epoch [24], Batch [173/938], Loss: 0.3281714916229248\n",
      "Validation: Epoch [24], Batch [174/938], Loss: 0.40276068449020386\n",
      "Validation: Epoch [24], Batch [175/938], Loss: 0.4337191581726074\n",
      "Validation: Epoch [24], Batch [176/938], Loss: 0.4356857240200043\n",
      "Validation: Epoch [24], Batch [177/938], Loss: 0.31226471066474915\n",
      "Validation: Epoch [24], Batch [178/938], Loss: 0.3762342929840088\n",
      "Validation: Epoch [24], Batch [179/938], Loss: 0.42229223251342773\n",
      "Validation: Epoch [24], Batch [180/938], Loss: 0.37974312901496887\n",
      "Validation: Epoch [24], Batch [181/938], Loss: 0.5869892239570618\n",
      "Validation: Epoch [24], Batch [182/938], Loss: 0.3924352526664734\n",
      "Validation: Epoch [24], Batch [183/938], Loss: 0.49993160367012024\n",
      "Validation: Epoch [24], Batch [184/938], Loss: 0.4375077784061432\n",
      "Validation: Epoch [24], Batch [185/938], Loss: 0.3178394138813019\n",
      "Validation: Epoch [24], Batch [186/938], Loss: 0.2346293330192566\n",
      "Validation: Epoch [24], Batch [187/938], Loss: 0.5592084527015686\n",
      "Validation: Epoch [24], Batch [188/938], Loss: 0.5235050916671753\n",
      "Validation: Epoch [24], Batch [189/938], Loss: 0.3273043930530548\n",
      "Validation: Epoch [24], Batch [190/938], Loss: 0.3409132957458496\n",
      "Validation: Epoch [24], Batch [191/938], Loss: 0.33540821075439453\n",
      "Validation: Epoch [24], Batch [192/938], Loss: 0.42872926592826843\n",
      "Validation: Epoch [24], Batch [193/938], Loss: 0.35469186305999756\n",
      "Validation: Epoch [24], Batch [194/938], Loss: 0.43874943256378174\n",
      "Validation: Epoch [24], Batch [195/938], Loss: 0.40065106749534607\n",
      "Validation: Epoch [24], Batch [196/938], Loss: 0.41906625032424927\n",
      "Validation: Epoch [24], Batch [197/938], Loss: 0.4329802691936493\n",
      "Validation: Epoch [24], Batch [198/938], Loss: 0.39772531390190125\n",
      "Validation: Epoch [24], Batch [199/938], Loss: 0.5216549634933472\n",
      "Validation: Epoch [24], Batch [200/938], Loss: 0.4743352234363556\n",
      "Validation: Epoch [24], Batch [201/938], Loss: 0.5517286658287048\n",
      "Validation: Epoch [24], Batch [202/938], Loss: 0.18781578540802002\n",
      "Validation: Epoch [24], Batch [203/938], Loss: 0.2127567082643509\n",
      "Validation: Epoch [24], Batch [204/938], Loss: 0.4533732235431671\n",
      "Validation: Epoch [24], Batch [205/938], Loss: 0.2120545208454132\n",
      "Validation: Epoch [24], Batch [206/938], Loss: 0.3357962965965271\n",
      "Validation: Epoch [24], Batch [207/938], Loss: 0.47316455841064453\n",
      "Validation: Epoch [24], Batch [208/938], Loss: 0.3234233856201172\n",
      "Validation: Epoch [24], Batch [209/938], Loss: 0.409349262714386\n",
      "Validation: Epoch [24], Batch [210/938], Loss: 0.5545597076416016\n",
      "Validation: Epoch [24], Batch [211/938], Loss: 0.46235865354537964\n",
      "Validation: Epoch [24], Batch [212/938], Loss: 0.5704239010810852\n",
      "Validation: Epoch [24], Batch [213/938], Loss: 0.41596540808677673\n",
      "Validation: Epoch [24], Batch [214/938], Loss: 0.5541683435440063\n",
      "Validation: Epoch [24], Batch [215/938], Loss: 0.5086333751678467\n",
      "Validation: Epoch [24], Batch [216/938], Loss: 0.3146083950996399\n",
      "Validation: Epoch [24], Batch [217/938], Loss: 0.4113454222679138\n",
      "Validation: Epoch [24], Batch [218/938], Loss: 0.5275663137435913\n",
      "Validation: Epoch [24], Batch [219/938], Loss: 0.5745691657066345\n",
      "Validation: Epoch [24], Batch [220/938], Loss: 0.5224848985671997\n",
      "Validation: Epoch [24], Batch [221/938], Loss: 0.5425155758857727\n",
      "Validation: Epoch [24], Batch [222/938], Loss: 0.47177308797836304\n",
      "Validation: Epoch [24], Batch [223/938], Loss: 0.486822247505188\n",
      "Validation: Epoch [24], Batch [224/938], Loss: 0.3087501525878906\n",
      "Validation: Epoch [24], Batch [225/938], Loss: 0.46553751826286316\n",
      "Validation: Epoch [24], Batch [226/938], Loss: 0.36336398124694824\n",
      "Validation: Epoch [24], Batch [227/938], Loss: 0.4008871018886566\n",
      "Validation: Epoch [24], Batch [228/938], Loss: 0.3274679183959961\n",
      "Validation: Epoch [24], Batch [229/938], Loss: 0.6682360172271729\n",
      "Validation: Epoch [24], Batch [230/938], Loss: 0.291726291179657\n",
      "Validation: Epoch [24], Batch [231/938], Loss: 0.486439973115921\n",
      "Validation: Epoch [24], Batch [232/938], Loss: 0.3519792854785919\n",
      "Validation: Epoch [24], Batch [233/938], Loss: 0.6827293038368225\n",
      "Validation: Epoch [24], Batch [234/938], Loss: 0.34722140431404114\n",
      "Validation: Epoch [24], Batch [235/938], Loss: 0.3537745177745819\n",
      "Validation: Epoch [24], Batch [236/938], Loss: 0.3399524986743927\n",
      "Validation: Epoch [24], Batch [237/938], Loss: 0.6560890674591064\n",
      "Validation: Epoch [24], Batch [238/938], Loss: 0.26289865374565125\n",
      "Validation: Epoch [24], Batch [239/938], Loss: 0.24564911425113678\n",
      "Validation: Epoch [24], Batch [240/938], Loss: 0.3170483112335205\n",
      "Validation: Epoch [24], Batch [241/938], Loss: 0.3019363284111023\n",
      "Validation: Epoch [24], Batch [242/938], Loss: 0.3016766607761383\n",
      "Validation: Epoch [24], Batch [243/938], Loss: 0.34255439043045044\n",
      "Validation: Epoch [24], Batch [244/938], Loss: 0.3371198773384094\n",
      "Validation: Epoch [24], Batch [245/938], Loss: 0.41766345500946045\n",
      "Validation: Epoch [24], Batch [246/938], Loss: 0.2913006544113159\n",
      "Validation: Epoch [24], Batch [247/938], Loss: 0.3516884744167328\n",
      "Validation: Epoch [24], Batch [248/938], Loss: 0.3065294623374939\n",
      "Validation: Epoch [24], Batch [249/938], Loss: 0.452260285615921\n",
      "Validation: Epoch [24], Batch [250/938], Loss: 0.30529549717903137\n",
      "Validation: Epoch [24], Batch [251/938], Loss: 0.5736878514289856\n",
      "Validation: Epoch [24], Batch [252/938], Loss: 0.48709771037101746\n",
      "Validation: Epoch [24], Batch [253/938], Loss: 0.3975048363208771\n",
      "Validation: Epoch [24], Batch [254/938], Loss: 0.47876477241516113\n",
      "Validation: Epoch [24], Batch [255/938], Loss: 0.46410882472991943\n",
      "Validation: Epoch [24], Batch [256/938], Loss: 0.48584461212158203\n",
      "Validation: Epoch [24], Batch [257/938], Loss: 0.37008416652679443\n",
      "Validation: Epoch [24], Batch [258/938], Loss: 0.4100286364555359\n",
      "Validation: Epoch [24], Batch [259/938], Loss: 0.48290306329727173\n",
      "Validation: Epoch [24], Batch [260/938], Loss: 0.6666301488876343\n",
      "Validation: Epoch [24], Batch [261/938], Loss: 0.4539933204650879\n",
      "Validation: Epoch [24], Batch [262/938], Loss: 0.37232452630996704\n",
      "Validation: Epoch [24], Batch [263/938], Loss: 0.402305543422699\n",
      "Validation: Epoch [24], Batch [264/938], Loss: 0.5232012271881104\n",
      "Validation: Epoch [24], Batch [265/938], Loss: 0.5959355235099792\n",
      "Validation: Epoch [24], Batch [266/938], Loss: 0.4044928550720215\n",
      "Validation: Epoch [24], Batch [267/938], Loss: 0.4959564507007599\n",
      "Validation: Epoch [24], Batch [268/938], Loss: 0.4134887456893921\n",
      "Validation: Epoch [24], Batch [269/938], Loss: 0.5703157186508179\n",
      "Validation: Epoch [24], Batch [270/938], Loss: 0.502655029296875\n",
      "Validation: Epoch [24], Batch [271/938], Loss: 0.35977113246917725\n",
      "Validation: Epoch [24], Batch [272/938], Loss: 0.36677035689353943\n",
      "Validation: Epoch [24], Batch [273/938], Loss: 0.37237757444381714\n",
      "Validation: Epoch [24], Batch [274/938], Loss: 0.1998698115348816\n",
      "Validation: Epoch [24], Batch [275/938], Loss: 0.3857392370700836\n",
      "Validation: Epoch [24], Batch [276/938], Loss: 0.2194528877735138\n",
      "Validation: Epoch [24], Batch [277/938], Loss: 0.4680444598197937\n",
      "Validation: Epoch [24], Batch [278/938], Loss: 0.3719880282878876\n",
      "Validation: Epoch [24], Batch [279/938], Loss: 0.40013042092323303\n",
      "Validation: Epoch [24], Batch [280/938], Loss: 0.3597540259361267\n",
      "Validation: Epoch [24], Batch [281/938], Loss: 0.2754177451133728\n",
      "Validation: Epoch [24], Batch [282/938], Loss: 0.5272587537765503\n",
      "Validation: Epoch [24], Batch [283/938], Loss: 0.48386305570602417\n",
      "Validation: Epoch [24], Batch [284/938], Loss: 0.5179823040962219\n",
      "Validation: Epoch [24], Batch [285/938], Loss: 0.49214935302734375\n",
      "Validation: Epoch [24], Batch [286/938], Loss: 0.3691120147705078\n",
      "Validation: Epoch [24], Batch [287/938], Loss: 0.3518538773059845\n",
      "Validation: Epoch [24], Batch [288/938], Loss: 0.38174423575401306\n",
      "Validation: Epoch [24], Batch [289/938], Loss: 0.25309649109840393\n",
      "Validation: Epoch [24], Batch [290/938], Loss: 0.39100563526153564\n",
      "Validation: Epoch [24], Batch [291/938], Loss: 0.4695279896259308\n",
      "Validation: Epoch [24], Batch [292/938], Loss: 0.30814385414123535\n",
      "Validation: Epoch [24], Batch [293/938], Loss: 0.7336785793304443\n",
      "Validation: Epoch [24], Batch [294/938], Loss: 0.315652459859848\n",
      "Validation: Epoch [24], Batch [295/938], Loss: 0.5954316854476929\n",
      "Validation: Epoch [24], Batch [296/938], Loss: 0.5115160942077637\n",
      "Validation: Epoch [24], Batch [297/938], Loss: 0.4267565608024597\n",
      "Validation: Epoch [24], Batch [298/938], Loss: 0.3478192985057831\n",
      "Validation: Epoch [24], Batch [299/938], Loss: 0.527612030506134\n",
      "Validation: Epoch [24], Batch [300/938], Loss: 0.3634423017501831\n",
      "Validation: Epoch [24], Batch [301/938], Loss: 0.302545428276062\n",
      "Validation: Epoch [24], Batch [302/938], Loss: 0.5918031930923462\n",
      "Validation: Epoch [24], Batch [303/938], Loss: 0.4733869731426239\n",
      "Validation: Epoch [24], Batch [304/938], Loss: 0.5642820000648499\n",
      "Validation: Epoch [24], Batch [305/938], Loss: 0.38785097002983093\n",
      "Validation: Epoch [24], Batch [306/938], Loss: 0.4246232211589813\n",
      "Validation: Epoch [24], Batch [307/938], Loss: 0.23831312358379364\n",
      "Validation: Epoch [24], Batch [308/938], Loss: 0.4216636121273041\n",
      "Validation: Epoch [24], Batch [309/938], Loss: 0.40768954157829285\n",
      "Validation: Epoch [24], Batch [310/938], Loss: 0.5913733243942261\n",
      "Validation: Epoch [24], Batch [311/938], Loss: 0.38105979561805725\n",
      "Validation: Epoch [24], Batch [312/938], Loss: 0.31781309843063354\n",
      "Validation: Epoch [24], Batch [313/938], Loss: 0.3718931972980499\n",
      "Validation: Epoch [24], Batch [314/938], Loss: 0.4876596927642822\n",
      "Validation: Epoch [24], Batch [315/938], Loss: 0.40959301590919495\n",
      "Validation: Epoch [24], Batch [316/938], Loss: 0.3287776708602905\n",
      "Validation: Epoch [24], Batch [317/938], Loss: 0.5840681791305542\n",
      "Validation: Epoch [24], Batch [318/938], Loss: 0.3219517767429352\n",
      "Validation: Epoch [24], Batch [319/938], Loss: 0.5283801555633545\n",
      "Validation: Epoch [24], Batch [320/938], Loss: 0.5068398714065552\n",
      "Validation: Epoch [24], Batch [321/938], Loss: 0.2434341013431549\n",
      "Validation: Epoch [24], Batch [322/938], Loss: 0.5700458884239197\n",
      "Validation: Epoch [24], Batch [323/938], Loss: 0.36004364490509033\n",
      "Validation: Epoch [24], Batch [324/938], Loss: 0.45134031772613525\n",
      "Validation: Epoch [24], Batch [325/938], Loss: 0.43447601795196533\n",
      "Validation: Epoch [24], Batch [326/938], Loss: 0.3662375509738922\n",
      "Validation: Epoch [24], Batch [327/938], Loss: 0.3958103656768799\n",
      "Validation: Epoch [24], Batch [328/938], Loss: 0.3494885265827179\n",
      "Validation: Epoch [24], Batch [329/938], Loss: 0.367275595664978\n",
      "Validation: Epoch [24], Batch [330/938], Loss: 0.4436397850513458\n",
      "Validation: Epoch [24], Batch [331/938], Loss: 0.2831318974494934\n",
      "Validation: Epoch [24], Batch [332/938], Loss: 0.5120531320571899\n",
      "Validation: Epoch [24], Batch [333/938], Loss: 0.3593398928642273\n",
      "Validation: Epoch [24], Batch [334/938], Loss: 0.41821086406707764\n",
      "Validation: Epoch [24], Batch [335/938], Loss: 0.5608812570571899\n",
      "Validation: Epoch [24], Batch [336/938], Loss: 0.35539427399635315\n",
      "Validation: Epoch [24], Batch [337/938], Loss: 0.3964693248271942\n",
      "Validation: Epoch [24], Batch [338/938], Loss: 0.3666765093803406\n",
      "Validation: Epoch [24], Batch [339/938], Loss: 0.2919391691684723\n",
      "Validation: Epoch [24], Batch [340/938], Loss: 0.3584141731262207\n",
      "Validation: Epoch [24], Batch [341/938], Loss: 0.44336044788360596\n",
      "Validation: Epoch [24], Batch [342/938], Loss: 0.37999457120895386\n",
      "Validation: Epoch [24], Batch [343/938], Loss: 0.4337904453277588\n",
      "Validation: Epoch [24], Batch [344/938], Loss: 0.41472601890563965\n",
      "Validation: Epoch [24], Batch [345/938], Loss: 0.5872728824615479\n",
      "Validation: Epoch [24], Batch [346/938], Loss: 0.6368148922920227\n",
      "Validation: Epoch [24], Batch [347/938], Loss: 0.4026779532432556\n",
      "Validation: Epoch [24], Batch [348/938], Loss: 0.49008139967918396\n",
      "Validation: Epoch [24], Batch [349/938], Loss: 0.41431906819343567\n",
      "Validation: Epoch [24], Batch [350/938], Loss: 0.4123668670654297\n",
      "Validation: Epoch [24], Batch [351/938], Loss: 0.5600873827934265\n",
      "Validation: Epoch [24], Batch [352/938], Loss: 0.4269079864025116\n",
      "Validation: Epoch [24], Batch [353/938], Loss: 0.3668696880340576\n",
      "Validation: Epoch [24], Batch [354/938], Loss: 0.5852776169776917\n",
      "Validation: Epoch [24], Batch [355/938], Loss: 0.5045264363288879\n",
      "Validation: Epoch [24], Batch [356/938], Loss: 0.4751695394515991\n",
      "Validation: Epoch [24], Batch [357/938], Loss: 0.49444952607154846\n",
      "Validation: Epoch [24], Batch [358/938], Loss: 0.37422263622283936\n",
      "Validation: Epoch [24], Batch [359/938], Loss: 0.33153021335601807\n",
      "Validation: Epoch [24], Batch [360/938], Loss: 0.5132696628570557\n",
      "Validation: Epoch [24], Batch [361/938], Loss: 0.7364322543144226\n",
      "Validation: Epoch [24], Batch [362/938], Loss: 0.3656049370765686\n",
      "Validation: Epoch [24], Batch [363/938], Loss: 0.4662880599498749\n",
      "Validation: Epoch [24], Batch [364/938], Loss: 0.5094783306121826\n",
      "Validation: Epoch [24], Batch [365/938], Loss: 0.21929997205734253\n",
      "Validation: Epoch [24], Batch [366/938], Loss: 0.4655643403530121\n",
      "Validation: Epoch [24], Batch [367/938], Loss: 0.376066654920578\n",
      "Validation: Epoch [24], Batch [368/938], Loss: 0.29937490820884705\n",
      "Validation: Epoch [24], Batch [369/938], Loss: 0.33211883902549744\n",
      "Validation: Epoch [24], Batch [370/938], Loss: 0.33009764552116394\n",
      "Validation: Epoch [24], Batch [371/938], Loss: 0.3620299994945526\n",
      "Validation: Epoch [24], Batch [372/938], Loss: 0.4974905550479889\n",
      "Validation: Epoch [24], Batch [373/938], Loss: 0.5407366752624512\n",
      "Validation: Epoch [24], Batch [374/938], Loss: 0.4976693391799927\n",
      "Validation: Epoch [24], Batch [375/938], Loss: 0.395553320646286\n",
      "Validation: Epoch [24], Batch [376/938], Loss: 0.35985422134399414\n",
      "Validation: Epoch [24], Batch [377/938], Loss: 0.44745954871177673\n",
      "Validation: Epoch [24], Batch [378/938], Loss: 0.36932143568992615\n",
      "Validation: Epoch [24], Batch [379/938], Loss: 0.32364675402641296\n",
      "Validation: Epoch [24], Batch [380/938], Loss: 0.38126033544540405\n",
      "Validation: Epoch [24], Batch [381/938], Loss: 0.40383386611938477\n",
      "Validation: Epoch [24], Batch [382/938], Loss: 0.4448102116584778\n",
      "Validation: Epoch [24], Batch [383/938], Loss: 0.5315024852752686\n",
      "Validation: Epoch [24], Batch [384/938], Loss: 0.6192260980606079\n",
      "Validation: Epoch [24], Batch [385/938], Loss: 0.40390220284461975\n",
      "Validation: Epoch [24], Batch [386/938], Loss: 0.7306206822395325\n",
      "Validation: Epoch [24], Batch [387/938], Loss: 0.45886027812957764\n",
      "Validation: Epoch [24], Batch [388/938], Loss: 0.38427624106407166\n",
      "Validation: Epoch [24], Batch [389/938], Loss: 0.37667280435562134\n",
      "Validation: Epoch [24], Batch [390/938], Loss: 0.3579787015914917\n",
      "Validation: Epoch [24], Batch [391/938], Loss: 0.22500288486480713\n",
      "Validation: Epoch [24], Batch [392/938], Loss: 0.41315409541130066\n",
      "Validation: Epoch [24], Batch [393/938], Loss: 0.2859206795692444\n",
      "Validation: Epoch [24], Batch [394/938], Loss: 0.48679548501968384\n",
      "Validation: Epoch [24], Batch [395/938], Loss: 0.25778913497924805\n",
      "Validation: Epoch [24], Batch [396/938], Loss: 0.47007718682289124\n",
      "Validation: Epoch [24], Batch [397/938], Loss: 0.44420933723449707\n",
      "Validation: Epoch [24], Batch [398/938], Loss: 0.3069806694984436\n",
      "Validation: Epoch [24], Batch [399/938], Loss: 0.36623692512512207\n",
      "Validation: Epoch [24], Batch [400/938], Loss: 0.393472284078598\n",
      "Validation: Epoch [24], Batch [401/938], Loss: 0.6743716597557068\n",
      "Validation: Epoch [24], Batch [402/938], Loss: 0.5195542573928833\n",
      "Validation: Epoch [24], Batch [403/938], Loss: 0.49247220158576965\n",
      "Validation: Epoch [24], Batch [404/938], Loss: 0.3589538335800171\n",
      "Validation: Epoch [24], Batch [405/938], Loss: 0.4454978406429291\n",
      "Validation: Epoch [24], Batch [406/938], Loss: 0.4935886263847351\n",
      "Validation: Epoch [24], Batch [407/938], Loss: 0.3784112334251404\n",
      "Validation: Epoch [24], Batch [408/938], Loss: 0.32987073063850403\n",
      "Validation: Epoch [24], Batch [409/938], Loss: 0.4468969702720642\n",
      "Validation: Epoch [24], Batch [410/938], Loss: 0.4938887655735016\n",
      "Validation: Epoch [24], Batch [411/938], Loss: 0.367312490940094\n",
      "Validation: Epoch [24], Batch [412/938], Loss: 0.4227454662322998\n",
      "Validation: Epoch [24], Batch [413/938], Loss: 0.459663450717926\n",
      "Validation: Epoch [24], Batch [414/938], Loss: 0.4695691466331482\n",
      "Validation: Epoch [24], Batch [415/938], Loss: 0.36098939180374146\n",
      "Validation: Epoch [24], Batch [416/938], Loss: 0.33120930194854736\n",
      "Validation: Epoch [24], Batch [417/938], Loss: 0.3137666583061218\n",
      "Validation: Epoch [24], Batch [418/938], Loss: 0.3572057783603668\n",
      "Validation: Epoch [24], Batch [419/938], Loss: 0.4547243118286133\n",
      "Validation: Epoch [24], Batch [420/938], Loss: 0.37031736969947815\n",
      "Validation: Epoch [24], Batch [421/938], Loss: 0.3210133910179138\n",
      "Validation: Epoch [24], Batch [422/938], Loss: 0.5260458588600159\n",
      "Validation: Epoch [24], Batch [423/938], Loss: 0.4046143591403961\n",
      "Validation: Epoch [24], Batch [424/938], Loss: 0.4172550439834595\n",
      "Validation: Epoch [24], Batch [425/938], Loss: 0.2724553346633911\n",
      "Validation: Epoch [24], Batch [426/938], Loss: 0.46272197365760803\n",
      "Validation: Epoch [24], Batch [427/938], Loss: 0.30023688077926636\n",
      "Validation: Epoch [24], Batch [428/938], Loss: 0.47766587138175964\n",
      "Validation: Epoch [24], Batch [429/938], Loss: 0.3149620592594147\n",
      "Validation: Epoch [24], Batch [430/938], Loss: 0.29004332423210144\n",
      "Validation: Epoch [24], Batch [431/938], Loss: 0.35990914702415466\n",
      "Validation: Epoch [24], Batch [432/938], Loss: 0.5857152938842773\n",
      "Validation: Epoch [24], Batch [433/938], Loss: 0.3993280529975891\n",
      "Validation: Epoch [24], Batch [434/938], Loss: 0.7366476058959961\n",
      "Validation: Epoch [24], Batch [435/938], Loss: 0.37495189905166626\n",
      "Validation: Epoch [24], Batch [436/938], Loss: 0.43951117992401123\n",
      "Validation: Epoch [24], Batch [437/938], Loss: 0.3848937451839447\n",
      "Validation: Epoch [24], Batch [438/938], Loss: 0.4513566792011261\n",
      "Validation: Epoch [24], Batch [439/938], Loss: 0.5194553732872009\n",
      "Validation: Epoch [24], Batch [440/938], Loss: 0.3708993196487427\n",
      "Validation: Epoch [24], Batch [441/938], Loss: 0.4400103688240051\n",
      "Validation: Epoch [24], Batch [442/938], Loss: 0.5924824476242065\n",
      "Validation: Epoch [24], Batch [443/938], Loss: 0.30393344163894653\n",
      "Validation: Epoch [24], Batch [444/938], Loss: 0.4780831038951874\n",
      "Validation: Epoch [24], Batch [445/938], Loss: 0.36874327063560486\n",
      "Validation: Epoch [24], Batch [446/938], Loss: 0.42104440927505493\n",
      "Validation: Epoch [24], Batch [447/938], Loss: 0.5104905366897583\n",
      "Validation: Epoch [24], Batch [448/938], Loss: 0.47961729764938354\n",
      "Validation: Epoch [24], Batch [449/938], Loss: 0.30751684308052063\n",
      "Validation: Epoch [24], Batch [450/938], Loss: 0.47551265358924866\n",
      "Validation: Epoch [24], Batch [451/938], Loss: 0.3661944568157196\n",
      "Validation: Epoch [24], Batch [452/938], Loss: 0.3058113753795624\n",
      "Validation: Epoch [24], Batch [453/938], Loss: 0.45212674140930176\n",
      "Validation: Epoch [24], Batch [454/938], Loss: 0.3849116265773773\n",
      "Validation: Epoch [24], Batch [455/938], Loss: 0.5462707281112671\n",
      "Validation: Epoch [24], Batch [456/938], Loss: 0.35922569036483765\n",
      "Validation: Epoch [24], Batch [457/938], Loss: 0.5834181308746338\n",
      "Validation: Epoch [24], Batch [458/938], Loss: 0.1920386254787445\n",
      "Validation: Epoch [24], Batch [459/938], Loss: 0.4455195367336273\n",
      "Validation: Epoch [24], Batch [460/938], Loss: 0.3439697325229645\n",
      "Validation: Epoch [24], Batch [461/938], Loss: 0.41799890995025635\n",
      "Validation: Epoch [24], Batch [462/938], Loss: 0.3010297119617462\n",
      "Validation: Epoch [24], Batch [463/938], Loss: 0.5355284810066223\n",
      "Validation: Epoch [24], Batch [464/938], Loss: 0.18236510455608368\n",
      "Validation: Epoch [24], Batch [465/938], Loss: 0.31713029742240906\n",
      "Validation: Epoch [24], Batch [466/938], Loss: 0.39618000388145447\n",
      "Validation: Epoch [24], Batch [467/938], Loss: 0.39276188611984253\n",
      "Validation: Epoch [24], Batch [468/938], Loss: 0.19937506318092346\n",
      "Validation: Epoch [24], Batch [469/938], Loss: 0.5318259000778198\n",
      "Validation: Epoch [24], Batch [470/938], Loss: 0.3899090886116028\n",
      "Validation: Epoch [24], Batch [471/938], Loss: 0.5626755952835083\n",
      "Validation: Epoch [24], Batch [472/938], Loss: 0.42641544342041016\n",
      "Validation: Epoch [24], Batch [473/938], Loss: 0.4632786512374878\n",
      "Validation: Epoch [24], Batch [474/938], Loss: 0.610295832157135\n",
      "Validation: Epoch [24], Batch [475/938], Loss: 0.6718322038650513\n",
      "Validation: Epoch [24], Batch [476/938], Loss: 0.41378673911094666\n",
      "Validation: Epoch [24], Batch [477/938], Loss: 0.5743481516838074\n",
      "Validation: Epoch [24], Batch [478/938], Loss: 0.38057222962379456\n",
      "Validation: Epoch [24], Batch [479/938], Loss: 0.6396487951278687\n",
      "Validation: Epoch [24], Batch [480/938], Loss: 0.5801171064376831\n",
      "Validation: Epoch [24], Batch [481/938], Loss: 0.2647113800048828\n",
      "Validation: Epoch [24], Batch [482/938], Loss: 0.26634079217910767\n",
      "Validation: Epoch [24], Batch [483/938], Loss: 0.4250548481941223\n",
      "Validation: Epoch [24], Batch [484/938], Loss: 0.42864882946014404\n",
      "Validation: Epoch [24], Batch [485/938], Loss: 0.28360435366630554\n",
      "Validation: Epoch [24], Batch [486/938], Loss: 0.3047961890697479\n",
      "Validation: Epoch [24], Batch [487/938], Loss: 0.5036386251449585\n",
      "Validation: Epoch [24], Batch [488/938], Loss: 0.20854327082633972\n",
      "Validation: Epoch [24], Batch [489/938], Loss: 0.1910274624824524\n",
      "Validation: Epoch [24], Batch [490/938], Loss: 0.42791998386383057\n",
      "Validation: Epoch [24], Batch [491/938], Loss: 0.3187192678451538\n",
      "Validation: Epoch [24], Batch [492/938], Loss: 0.39552009105682373\n",
      "Validation: Epoch [24], Batch [493/938], Loss: 0.6133019328117371\n",
      "Validation: Epoch [24], Batch [494/938], Loss: 0.27107250690460205\n",
      "Validation: Epoch [24], Batch [495/938], Loss: 0.5488664507865906\n",
      "Validation: Epoch [24], Batch [496/938], Loss: 0.5646653175354004\n",
      "Validation: Epoch [24], Batch [497/938], Loss: 0.6531214714050293\n",
      "Validation: Epoch [24], Batch [498/938], Loss: 0.3858518898487091\n",
      "Validation: Epoch [24], Batch [499/938], Loss: 0.5187494158744812\n",
      "Validation: Epoch [24], Batch [500/938], Loss: 0.34618791937828064\n",
      "Validation: Epoch [24], Batch [501/938], Loss: 0.4499332308769226\n",
      "Validation: Epoch [24], Batch [502/938], Loss: 0.49445199966430664\n",
      "Validation: Epoch [24], Batch [503/938], Loss: 0.48136216402053833\n",
      "Validation: Epoch [24], Batch [504/938], Loss: 0.46632614731788635\n",
      "Validation: Epoch [24], Batch [505/938], Loss: 0.44756701588630676\n",
      "Validation: Epoch [24], Batch [506/938], Loss: 0.4382031261920929\n",
      "Validation: Epoch [24], Batch [507/938], Loss: 0.44166550040245056\n",
      "Validation: Epoch [24], Batch [508/938], Loss: 0.42565983533859253\n",
      "Validation: Epoch [24], Batch [509/938], Loss: 0.30968207120895386\n",
      "Validation: Epoch [24], Batch [510/938], Loss: 0.4347133934497833\n",
      "Validation: Epoch [24], Batch [511/938], Loss: 0.4953254759311676\n",
      "Validation: Epoch [24], Batch [512/938], Loss: 0.27867189049720764\n",
      "Validation: Epoch [24], Batch [513/938], Loss: 0.49548453092575073\n",
      "Validation: Epoch [24], Batch [514/938], Loss: 0.4036920368671417\n",
      "Validation: Epoch [24], Batch [515/938], Loss: 0.4777025580406189\n",
      "Validation: Epoch [24], Batch [516/938], Loss: 0.4163101017475128\n",
      "Validation: Epoch [24], Batch [517/938], Loss: 0.48781952261924744\n",
      "Validation: Epoch [24], Batch [518/938], Loss: 0.2821972370147705\n",
      "Validation: Epoch [24], Batch [519/938], Loss: 0.48922935128211975\n",
      "Validation: Epoch [24], Batch [520/938], Loss: 0.32602107524871826\n",
      "Validation: Epoch [24], Batch [521/938], Loss: 0.28612786531448364\n",
      "Validation: Epoch [24], Batch [522/938], Loss: 0.5659821629524231\n",
      "Validation: Epoch [24], Batch [523/938], Loss: 0.4331991672515869\n",
      "Validation: Epoch [24], Batch [524/938], Loss: 0.5930246114730835\n",
      "Validation: Epoch [24], Batch [525/938], Loss: 0.23347370326519012\n",
      "Validation: Epoch [24], Batch [526/938], Loss: 0.3623751997947693\n",
      "Validation: Epoch [24], Batch [527/938], Loss: 0.48035091161727905\n",
      "Validation: Epoch [24], Batch [528/938], Loss: 0.7551144361495972\n",
      "Validation: Epoch [24], Batch [529/938], Loss: 0.4082363247871399\n",
      "Validation: Epoch [24], Batch [530/938], Loss: 0.3084765076637268\n",
      "Validation: Epoch [24], Batch [531/938], Loss: 0.31688547134399414\n",
      "Validation: Epoch [24], Batch [532/938], Loss: 0.4670711159706116\n",
      "Validation: Epoch [24], Batch [533/938], Loss: 0.4491877853870392\n",
      "Validation: Epoch [24], Batch [534/938], Loss: 0.24395965039730072\n",
      "Validation: Epoch [24], Batch [535/938], Loss: 0.40165701508522034\n",
      "Validation: Epoch [24], Batch [536/938], Loss: 0.5823198556900024\n",
      "Validation: Epoch [24], Batch [537/938], Loss: 0.3779788613319397\n",
      "Validation: Epoch [24], Batch [538/938], Loss: 0.3274841010570526\n",
      "Validation: Epoch [24], Batch [539/938], Loss: 0.30552804470062256\n",
      "Validation: Epoch [24], Batch [540/938], Loss: 0.49238547682762146\n",
      "Validation: Epoch [24], Batch [541/938], Loss: 0.44786375761032104\n",
      "Validation: Epoch [24], Batch [542/938], Loss: 0.4545519948005676\n",
      "Validation: Epoch [24], Batch [543/938], Loss: 0.5747838020324707\n",
      "Validation: Epoch [24], Batch [544/938], Loss: 0.39813628792762756\n",
      "Validation: Epoch [24], Batch [545/938], Loss: 0.31632062792778015\n",
      "Validation: Epoch [24], Batch [546/938], Loss: 0.42783868312835693\n",
      "Validation: Epoch [24], Batch [547/938], Loss: 0.41591158509254456\n",
      "Validation: Epoch [24], Batch [548/938], Loss: 0.5250764489173889\n",
      "Validation: Epoch [24], Batch [549/938], Loss: 0.45261427760124207\n",
      "Validation: Epoch [24], Batch [550/938], Loss: 0.5073289275169373\n",
      "Validation: Epoch [24], Batch [551/938], Loss: 0.30447250604629517\n",
      "Validation: Epoch [24], Batch [552/938], Loss: 0.5278738141059875\n",
      "Validation: Epoch [24], Batch [553/938], Loss: 0.3865371346473694\n",
      "Validation: Epoch [24], Batch [554/938], Loss: 0.45515283942222595\n",
      "Validation: Epoch [24], Batch [555/938], Loss: 0.32970061898231506\n",
      "Validation: Epoch [24], Batch [556/938], Loss: 0.3441735506057739\n",
      "Validation: Epoch [24], Batch [557/938], Loss: 0.39618292450904846\n",
      "Validation: Epoch [24], Batch [558/938], Loss: 0.2839145362377167\n",
      "Validation: Epoch [24], Batch [559/938], Loss: 0.6490665674209595\n",
      "Validation: Epoch [24], Batch [560/938], Loss: 0.41460829973220825\n",
      "Validation: Epoch [24], Batch [561/938], Loss: 0.3513234555721283\n",
      "Validation: Epoch [24], Batch [562/938], Loss: 0.697848916053772\n",
      "Validation: Epoch [24], Batch [563/938], Loss: 0.5333935022354126\n",
      "Validation: Epoch [24], Batch [564/938], Loss: 0.3162647783756256\n",
      "Validation: Epoch [24], Batch [565/938], Loss: 0.6284109950065613\n",
      "Validation: Epoch [24], Batch [566/938], Loss: 0.47225481271743774\n",
      "Validation: Epoch [24], Batch [567/938], Loss: 0.4194554388523102\n",
      "Validation: Epoch [24], Batch [568/938], Loss: 0.41816216707229614\n",
      "Validation: Epoch [24], Batch [569/938], Loss: 0.5169943571090698\n",
      "Validation: Epoch [24], Batch [570/938], Loss: 0.5920502543449402\n",
      "Validation: Epoch [24], Batch [571/938], Loss: 0.3734157979488373\n",
      "Validation: Epoch [24], Batch [572/938], Loss: 0.31700384616851807\n",
      "Validation: Epoch [24], Batch [573/938], Loss: 0.31502780318260193\n",
      "Validation: Epoch [24], Batch [574/938], Loss: 0.5086073875427246\n",
      "Validation: Epoch [24], Batch [575/938], Loss: 0.344710111618042\n",
      "Validation: Epoch [24], Batch [576/938], Loss: 0.2632618844509125\n",
      "Validation: Epoch [24], Batch [577/938], Loss: 0.46177810430526733\n",
      "Validation: Epoch [24], Batch [578/938], Loss: 0.345936119556427\n",
      "Validation: Epoch [24], Batch [579/938], Loss: 0.3916471004486084\n",
      "Validation: Epoch [24], Batch [580/938], Loss: 0.31658416986465454\n",
      "Validation: Epoch [24], Batch [581/938], Loss: 0.3400975167751312\n",
      "Validation: Epoch [24], Batch [582/938], Loss: 0.4341658353805542\n",
      "Validation: Epoch [24], Batch [583/938], Loss: 0.5745511054992676\n",
      "Validation: Epoch [24], Batch [584/938], Loss: 0.4396207928657532\n",
      "Validation: Epoch [24], Batch [585/938], Loss: 0.4591723084449768\n",
      "Validation: Epoch [24], Batch [586/938], Loss: 0.7474008202552795\n",
      "Validation: Epoch [24], Batch [587/938], Loss: 0.3184208571910858\n",
      "Validation: Epoch [24], Batch [588/938], Loss: 0.39657312631607056\n",
      "Validation: Epoch [24], Batch [589/938], Loss: 0.3359658718109131\n",
      "Validation: Epoch [24], Batch [590/938], Loss: 0.5133311748504639\n",
      "Validation: Epoch [24], Batch [591/938], Loss: 0.5431658625602722\n",
      "Validation: Epoch [24], Batch [592/938], Loss: 0.5307359099388123\n",
      "Validation: Epoch [24], Batch [593/938], Loss: 0.5002655386924744\n",
      "Validation: Epoch [24], Batch [594/938], Loss: 0.4517878592014313\n",
      "Validation: Epoch [24], Batch [595/938], Loss: 0.2605248689651489\n",
      "Validation: Epoch [24], Batch [596/938], Loss: 0.4812704920768738\n",
      "Validation: Epoch [24], Batch [597/938], Loss: 0.5393224954605103\n",
      "Validation: Epoch [24], Batch [598/938], Loss: 0.2629742920398712\n",
      "Validation: Epoch [24], Batch [599/938], Loss: 0.49773934483528137\n",
      "Validation: Epoch [24], Batch [600/938], Loss: 0.4186684489250183\n",
      "Validation: Epoch [24], Batch [601/938], Loss: 0.4772588312625885\n",
      "Validation: Epoch [24], Batch [602/938], Loss: 0.39036858081817627\n",
      "Validation: Epoch [24], Batch [603/938], Loss: 0.3868185877799988\n",
      "Validation: Epoch [24], Batch [604/938], Loss: 0.33786115050315857\n",
      "Validation: Epoch [24], Batch [605/938], Loss: 0.4764830470085144\n",
      "Validation: Epoch [24], Batch [606/938], Loss: 0.4975144863128662\n",
      "Validation: Epoch [24], Batch [607/938], Loss: 0.38317927718162537\n",
      "Validation: Epoch [24], Batch [608/938], Loss: 0.3931550085544586\n",
      "Validation: Epoch [24], Batch [609/938], Loss: 0.5062007904052734\n",
      "Validation: Epoch [24], Batch [610/938], Loss: 0.4115252196788788\n",
      "Validation: Epoch [24], Batch [611/938], Loss: 0.38150754570961\n",
      "Validation: Epoch [24], Batch [612/938], Loss: 0.4446007013320923\n",
      "Validation: Epoch [24], Batch [613/938], Loss: 0.32725903391838074\n",
      "Validation: Epoch [24], Batch [614/938], Loss: 0.5277664065361023\n",
      "Validation: Epoch [24], Batch [615/938], Loss: 0.4642738103866577\n",
      "Validation: Epoch [24], Batch [616/938], Loss: 0.5170553922653198\n",
      "Validation: Epoch [24], Batch [617/938], Loss: 0.30602213740348816\n",
      "Validation: Epoch [24], Batch [618/938], Loss: 0.5326582193374634\n",
      "Validation: Epoch [24], Batch [619/938], Loss: 0.31688886880874634\n",
      "Validation: Epoch [24], Batch [620/938], Loss: 0.5939717292785645\n",
      "Validation: Epoch [24], Batch [621/938], Loss: 0.4501086473464966\n",
      "Validation: Epoch [24], Batch [622/938], Loss: 0.4576370418071747\n",
      "Validation: Epoch [24], Batch [623/938], Loss: 0.40956443548202515\n",
      "Validation: Epoch [24], Batch [624/938], Loss: 0.4372935891151428\n",
      "Validation: Epoch [24], Batch [625/938], Loss: 0.3375588357448578\n",
      "Validation: Epoch [24], Batch [626/938], Loss: 0.4634478986263275\n",
      "Validation: Epoch [24], Batch [627/938], Loss: 0.3788275122642517\n",
      "Validation: Epoch [24], Batch [628/938], Loss: 0.33563661575317383\n",
      "Validation: Epoch [24], Batch [629/938], Loss: 0.4050716459751129\n",
      "Validation: Epoch [24], Batch [630/938], Loss: 0.38510236144065857\n",
      "Validation: Epoch [24], Batch [631/938], Loss: 0.4962686002254486\n",
      "Validation: Epoch [24], Batch [632/938], Loss: 0.4510074853897095\n",
      "Validation: Epoch [24], Batch [633/938], Loss: 0.34467750787734985\n",
      "Validation: Epoch [24], Batch [634/938], Loss: 0.4401777386665344\n",
      "Validation: Epoch [24], Batch [635/938], Loss: 0.664527416229248\n",
      "Validation: Epoch [24], Batch [636/938], Loss: 0.5116851329803467\n",
      "Validation: Epoch [24], Batch [637/938], Loss: 0.5868223309516907\n",
      "Validation: Epoch [24], Batch [638/938], Loss: 0.3179462254047394\n",
      "Validation: Epoch [24], Batch [639/938], Loss: 0.3913877308368683\n",
      "Validation: Epoch [24], Batch [640/938], Loss: 0.2768719792366028\n",
      "Validation: Epoch [24], Batch [641/938], Loss: 0.275129497051239\n",
      "Validation: Epoch [24], Batch [642/938], Loss: 0.33476758003234863\n",
      "Validation: Epoch [24], Batch [643/938], Loss: 0.4860895872116089\n",
      "Validation: Epoch [24], Batch [644/938], Loss: 0.18963006138801575\n",
      "Validation: Epoch [24], Batch [645/938], Loss: 0.3623763620853424\n",
      "Validation: Epoch [24], Batch [646/938], Loss: 0.584266185760498\n",
      "Validation: Epoch [24], Batch [647/938], Loss: 0.31482306122779846\n",
      "Validation: Epoch [24], Batch [648/938], Loss: 0.4400237798690796\n",
      "Validation: Epoch [24], Batch [649/938], Loss: 0.32232242822647095\n",
      "Validation: Epoch [24], Batch [650/938], Loss: 0.4415487051010132\n",
      "Validation: Epoch [24], Batch [651/938], Loss: 0.48685377836227417\n",
      "Validation: Epoch [24], Batch [652/938], Loss: 0.5547031760215759\n",
      "Validation: Epoch [24], Batch [653/938], Loss: 0.40054842829704285\n",
      "Validation: Epoch [24], Batch [654/938], Loss: 0.3877573609352112\n",
      "Validation: Epoch [24], Batch [655/938], Loss: 0.1888342797756195\n",
      "Validation: Epoch [24], Batch [656/938], Loss: 0.42578062415122986\n",
      "Validation: Epoch [24], Batch [657/938], Loss: 0.38010814785957336\n",
      "Validation: Epoch [24], Batch [658/938], Loss: 0.38552480936050415\n",
      "Validation: Epoch [24], Batch [659/938], Loss: 0.4724593758583069\n",
      "Validation: Epoch [24], Batch [660/938], Loss: 0.4479219317436218\n",
      "Validation: Epoch [24], Batch [661/938], Loss: 0.4220609664916992\n",
      "Validation: Epoch [24], Batch [662/938], Loss: 0.4332433342933655\n",
      "Validation: Epoch [24], Batch [663/938], Loss: 0.3348208963871002\n",
      "Validation: Epoch [24], Batch [664/938], Loss: 0.32704979181289673\n",
      "Validation: Epoch [24], Batch [665/938], Loss: 0.3149953782558441\n",
      "Validation: Epoch [24], Batch [666/938], Loss: 0.40061724185943604\n",
      "Validation: Epoch [24], Batch [667/938], Loss: 0.45633119344711304\n",
      "Validation: Epoch [24], Batch [668/938], Loss: 0.5674917697906494\n",
      "Validation: Epoch [24], Batch [669/938], Loss: 0.5426979660987854\n",
      "Validation: Epoch [24], Batch [670/938], Loss: 0.26999133825302124\n",
      "Validation: Epoch [24], Batch [671/938], Loss: 0.4618302583694458\n",
      "Validation: Epoch [24], Batch [672/938], Loss: 0.6624350547790527\n",
      "Validation: Epoch [24], Batch [673/938], Loss: 0.5278022289276123\n",
      "Validation: Epoch [24], Batch [674/938], Loss: 0.4310508668422699\n",
      "Validation: Epoch [24], Batch [675/938], Loss: 0.2595507502555847\n",
      "Validation: Epoch [24], Batch [676/938], Loss: 0.420885294675827\n",
      "Validation: Epoch [24], Batch [677/938], Loss: 0.5019197463989258\n",
      "Validation: Epoch [24], Batch [678/938], Loss: 0.42949968576431274\n",
      "Validation: Epoch [24], Batch [679/938], Loss: 0.41540348529815674\n",
      "Validation: Epoch [24], Batch [680/938], Loss: 0.6538018584251404\n",
      "Validation: Epoch [24], Batch [681/938], Loss: 0.3206889033317566\n",
      "Validation: Epoch [24], Batch [682/938], Loss: 0.42711710929870605\n",
      "Validation: Epoch [24], Batch [683/938], Loss: 0.5637855529785156\n",
      "Validation: Epoch [24], Batch [684/938], Loss: 0.3335148096084595\n",
      "Validation: Epoch [24], Batch [685/938], Loss: 0.5118637084960938\n",
      "Validation: Epoch [24], Batch [686/938], Loss: 0.6981073617935181\n",
      "Validation: Epoch [24], Batch [687/938], Loss: 0.5901920199394226\n",
      "Validation: Epoch [24], Batch [688/938], Loss: 0.5486472845077515\n",
      "Validation: Epoch [24], Batch [689/938], Loss: 0.4925965666770935\n",
      "Validation: Epoch [24], Batch [690/938], Loss: 0.7201305031776428\n",
      "Validation: Epoch [24], Batch [691/938], Loss: 0.4535611867904663\n",
      "Validation: Epoch [24], Batch [692/938], Loss: 0.470196932554245\n",
      "Validation: Epoch [24], Batch [693/938], Loss: 0.4480375647544861\n",
      "Validation: Epoch [24], Batch [694/938], Loss: 0.7150461673736572\n",
      "Validation: Epoch [24], Batch [695/938], Loss: 0.42873314023017883\n",
      "Validation: Epoch [24], Batch [696/938], Loss: 0.3247844874858856\n",
      "Validation: Epoch [24], Batch [697/938], Loss: 0.6503186225891113\n",
      "Validation: Epoch [24], Batch [698/938], Loss: 0.42341357469558716\n",
      "Validation: Epoch [24], Batch [699/938], Loss: 0.36706218123435974\n",
      "Validation: Epoch [24], Batch [700/938], Loss: 0.5767306685447693\n",
      "Validation: Epoch [24], Batch [701/938], Loss: 0.39508745074272156\n",
      "Validation: Epoch [24], Batch [702/938], Loss: 0.35853689908981323\n",
      "Validation: Epoch [24], Batch [703/938], Loss: 0.3567173480987549\n",
      "Validation: Epoch [24], Batch [704/938], Loss: 0.29525041580200195\n",
      "Validation: Epoch [24], Batch [705/938], Loss: 0.34827426075935364\n",
      "Validation: Epoch [24], Batch [706/938], Loss: 0.2567252218723297\n",
      "Validation: Epoch [24], Batch [707/938], Loss: 0.4449898600578308\n",
      "Validation: Epoch [24], Batch [708/938], Loss: 0.47699490189552307\n",
      "Validation: Epoch [24], Batch [709/938], Loss: 0.35871148109436035\n",
      "Validation: Epoch [24], Batch [710/938], Loss: 0.29437851905822754\n",
      "Validation: Epoch [24], Batch [711/938], Loss: 0.5042139291763306\n",
      "Validation: Epoch [24], Batch [712/938], Loss: 0.5956453084945679\n",
      "Validation: Epoch [24], Batch [713/938], Loss: 0.3626807928085327\n",
      "Validation: Epoch [24], Batch [714/938], Loss: 0.49694347381591797\n",
      "Validation: Epoch [24], Batch [715/938], Loss: 0.3734230697154999\n",
      "Validation: Epoch [24], Batch [716/938], Loss: 0.32387614250183105\n",
      "Validation: Epoch [24], Batch [717/938], Loss: 0.29581505060195923\n",
      "Validation: Epoch [24], Batch [718/938], Loss: 0.4466839134693146\n",
      "Validation: Epoch [24], Batch [719/938], Loss: 0.385329931974411\n",
      "Validation: Epoch [24], Batch [720/938], Loss: 0.32513368129730225\n",
      "Validation: Epoch [24], Batch [721/938], Loss: 0.4915858805179596\n",
      "Validation: Epoch [24], Batch [722/938], Loss: 0.43190932273864746\n",
      "Validation: Epoch [24], Batch [723/938], Loss: 0.2914891242980957\n",
      "Validation: Epoch [24], Batch [724/938], Loss: 0.6432146430015564\n",
      "Validation: Epoch [24], Batch [725/938], Loss: 0.3854699432849884\n",
      "Validation: Epoch [24], Batch [726/938], Loss: 0.2947404980659485\n",
      "Validation: Epoch [24], Batch [727/938], Loss: 0.2761918902397156\n",
      "Validation: Epoch [24], Batch [728/938], Loss: 0.3718065023422241\n",
      "Validation: Epoch [24], Batch [729/938], Loss: 0.6250798106193542\n",
      "Validation: Epoch [24], Batch [730/938], Loss: 0.4816707372665405\n",
      "Validation: Epoch [24], Batch [731/938], Loss: 0.34065788984298706\n",
      "Validation: Epoch [24], Batch [732/938], Loss: 0.3698996305465698\n",
      "Validation: Epoch [24], Batch [733/938], Loss: 0.4130154252052307\n",
      "Validation: Epoch [24], Batch [734/938], Loss: 0.5701496005058289\n",
      "Validation: Epoch [24], Batch [735/938], Loss: 0.42796963453292847\n",
      "Validation: Epoch [24], Batch [736/938], Loss: 0.5510408878326416\n",
      "Validation: Epoch [24], Batch [737/938], Loss: 0.4285016357898712\n",
      "Validation: Epoch [24], Batch [738/938], Loss: 0.29900068044662476\n",
      "Validation: Epoch [24], Batch [739/938], Loss: 0.47485658526420593\n",
      "Validation: Epoch [24], Batch [740/938], Loss: 0.46827232837677\n",
      "Validation: Epoch [24], Batch [741/938], Loss: 0.3859128952026367\n",
      "Validation: Epoch [24], Batch [742/938], Loss: 0.2227422446012497\n",
      "Validation: Epoch [24], Batch [743/938], Loss: 0.24242235720157623\n",
      "Validation: Epoch [24], Batch [744/938], Loss: 0.30733174085617065\n",
      "Validation: Epoch [24], Batch [745/938], Loss: 0.49858272075653076\n",
      "Validation: Epoch [24], Batch [746/938], Loss: 0.34081700444221497\n",
      "Validation: Epoch [24], Batch [747/938], Loss: 0.26909124851226807\n",
      "Validation: Epoch [24], Batch [748/938], Loss: 0.4713157117366791\n",
      "Validation: Epoch [24], Batch [749/938], Loss: 0.2725996673107147\n",
      "Validation: Epoch [24], Batch [750/938], Loss: 0.34724557399749756\n",
      "Validation: Epoch [24], Batch [751/938], Loss: 0.2766966223716736\n",
      "Validation: Epoch [24], Batch [752/938], Loss: 0.4076051712036133\n",
      "Validation: Epoch [24], Batch [753/938], Loss: 0.5740516185760498\n",
      "Validation: Epoch [24], Batch [754/938], Loss: 0.35410499572753906\n",
      "Validation: Epoch [24], Batch [755/938], Loss: 0.4298257827758789\n",
      "Validation: Epoch [24], Batch [756/938], Loss: 0.40972262620925903\n",
      "Validation: Epoch [24], Batch [757/938], Loss: 0.5458861589431763\n",
      "Validation: Epoch [24], Batch [758/938], Loss: 0.3716113269329071\n",
      "Validation: Epoch [24], Batch [759/938], Loss: 0.3072328567504883\n",
      "Validation: Epoch [24], Batch [760/938], Loss: 0.560671865940094\n",
      "Validation: Epoch [24], Batch [761/938], Loss: 0.46735864877700806\n",
      "Validation: Epoch [24], Batch [762/938], Loss: 0.27662307024002075\n",
      "Validation: Epoch [24], Batch [763/938], Loss: 0.3647894859313965\n",
      "Validation: Epoch [24], Batch [764/938], Loss: 0.3134232461452484\n",
      "Validation: Epoch [24], Batch [765/938], Loss: 0.4563113749027252\n",
      "Validation: Epoch [24], Batch [766/938], Loss: 0.3353824019432068\n",
      "Validation: Epoch [24], Batch [767/938], Loss: 0.3930743932723999\n",
      "Validation: Epoch [24], Batch [768/938], Loss: 0.5957833528518677\n",
      "Validation: Epoch [24], Batch [769/938], Loss: 0.6034114956855774\n",
      "Validation: Epoch [24], Batch [770/938], Loss: 0.24719101190567017\n",
      "Validation: Epoch [24], Batch [771/938], Loss: 0.40623074769973755\n",
      "Validation: Epoch [24], Batch [772/938], Loss: 0.3350257873535156\n",
      "Validation: Epoch [24], Batch [773/938], Loss: 0.6004751920700073\n",
      "Validation: Epoch [24], Batch [774/938], Loss: 0.32019421458244324\n",
      "Validation: Epoch [24], Batch [775/938], Loss: 0.568678081035614\n",
      "Validation: Epoch [24], Batch [776/938], Loss: 0.33262184262275696\n",
      "Validation: Epoch [24], Batch [777/938], Loss: 0.3769923448562622\n",
      "Validation: Epoch [24], Batch [778/938], Loss: 0.5109511017799377\n",
      "Validation: Epoch [24], Batch [779/938], Loss: 0.345562219619751\n",
      "Validation: Epoch [24], Batch [780/938], Loss: 0.5491124391555786\n",
      "Validation: Epoch [24], Batch [781/938], Loss: 0.497545063495636\n",
      "Validation: Epoch [24], Batch [782/938], Loss: 0.5114330649375916\n",
      "Validation: Epoch [24], Batch [783/938], Loss: 0.406882643699646\n",
      "Validation: Epoch [24], Batch [784/938], Loss: 0.32130950689315796\n",
      "Validation: Epoch [24], Batch [785/938], Loss: 0.587759256362915\n",
      "Validation: Epoch [24], Batch [786/938], Loss: 0.4569193422794342\n",
      "Validation: Epoch [24], Batch [787/938], Loss: 0.43866127729415894\n",
      "Validation: Epoch [24], Batch [788/938], Loss: 0.21331840753555298\n",
      "Validation: Epoch [24], Batch [789/938], Loss: 0.471690833568573\n",
      "Validation: Epoch [24], Batch [790/938], Loss: 0.5093359351158142\n",
      "Validation: Epoch [24], Batch [791/938], Loss: 0.3375290036201477\n",
      "Validation: Epoch [24], Batch [792/938], Loss: 0.2819776237010956\n",
      "Validation: Epoch [24], Batch [793/938], Loss: 0.37689119577407837\n",
      "Validation: Epoch [24], Batch [794/938], Loss: 0.45052456855773926\n",
      "Validation: Epoch [24], Batch [795/938], Loss: 0.5299871563911438\n",
      "Validation: Epoch [24], Batch [796/938], Loss: 0.38376951217651367\n",
      "Validation: Epoch [24], Batch [797/938], Loss: 0.4178828299045563\n",
      "Validation: Epoch [24], Batch [798/938], Loss: 0.34214213490486145\n",
      "Validation: Epoch [24], Batch [799/938], Loss: 0.4711405038833618\n",
      "Validation: Epoch [24], Batch [800/938], Loss: 0.38308411836624146\n",
      "Validation: Epoch [24], Batch [801/938], Loss: 0.3170108199119568\n",
      "Validation: Epoch [24], Batch [802/938], Loss: 0.4624345004558563\n",
      "Validation: Epoch [24], Batch [803/938], Loss: 0.3973945379257202\n",
      "Validation: Epoch [24], Batch [804/938], Loss: 0.39883387088775635\n",
      "Validation: Epoch [24], Batch [805/938], Loss: 0.5515059232711792\n",
      "Validation: Epoch [24], Batch [806/938], Loss: 0.4308914244174957\n",
      "Validation: Epoch [24], Batch [807/938], Loss: 0.452472448348999\n",
      "Validation: Epoch [24], Batch [808/938], Loss: 0.5871928930282593\n",
      "Validation: Epoch [24], Batch [809/938], Loss: 0.459937185049057\n",
      "Validation: Epoch [24], Batch [810/938], Loss: 0.5304269790649414\n",
      "Validation: Epoch [24], Batch [811/938], Loss: 0.2803473174571991\n",
      "Validation: Epoch [24], Batch [812/938], Loss: 0.46206602454185486\n",
      "Validation: Epoch [24], Batch [813/938], Loss: 0.39245927333831787\n",
      "Validation: Epoch [24], Batch [814/938], Loss: 0.5895870923995972\n",
      "Validation: Epoch [24], Batch [815/938], Loss: 0.3633122742176056\n",
      "Validation: Epoch [24], Batch [816/938], Loss: 0.3637411594390869\n",
      "Validation: Epoch [24], Batch [817/938], Loss: 0.2462397813796997\n",
      "Validation: Epoch [24], Batch [818/938], Loss: 0.41893595457077026\n",
      "Validation: Epoch [24], Batch [819/938], Loss: 0.4204801619052887\n",
      "Validation: Epoch [24], Batch [820/938], Loss: 0.5579535961151123\n",
      "Validation: Epoch [24], Batch [821/938], Loss: 0.45442062616348267\n",
      "Validation: Epoch [24], Batch [822/938], Loss: 0.46099555492401123\n",
      "Validation: Epoch [24], Batch [823/938], Loss: 0.3227149248123169\n",
      "Validation: Epoch [24], Batch [824/938], Loss: 0.28907179832458496\n",
      "Validation: Epoch [24], Batch [825/938], Loss: 0.40826094150543213\n",
      "Validation: Epoch [24], Batch [826/938], Loss: 0.23955261707305908\n",
      "Validation: Epoch [24], Batch [827/938], Loss: 0.338975727558136\n",
      "Validation: Epoch [24], Batch [828/938], Loss: 0.42348945140838623\n",
      "Validation: Epoch [24], Batch [829/938], Loss: 0.27505362033843994\n",
      "Validation: Epoch [24], Batch [830/938], Loss: 0.3726106882095337\n",
      "Validation: Epoch [24], Batch [831/938], Loss: 0.47737863659858704\n",
      "Validation: Epoch [24], Batch [832/938], Loss: 0.33468401432037354\n",
      "Validation: Epoch [24], Batch [833/938], Loss: 0.47575217485427856\n",
      "Validation: Epoch [24], Batch [834/938], Loss: 0.33520734310150146\n",
      "Validation: Epoch [24], Batch [835/938], Loss: 0.4702460467815399\n",
      "Validation: Epoch [24], Batch [836/938], Loss: 0.3391527235507965\n",
      "Validation: Epoch [24], Batch [837/938], Loss: 0.4482395648956299\n",
      "Validation: Epoch [24], Batch [838/938], Loss: 0.5524569153785706\n",
      "Validation: Epoch [24], Batch [839/938], Loss: 0.6093098521232605\n",
      "Validation: Epoch [24], Batch [840/938], Loss: 0.49062541127204895\n",
      "Validation: Epoch [24], Batch [841/938], Loss: 0.41036418080329895\n",
      "Validation: Epoch [24], Batch [842/938], Loss: 0.3049774765968323\n",
      "Validation: Epoch [24], Batch [843/938], Loss: 0.37533465027809143\n",
      "Validation: Epoch [24], Batch [844/938], Loss: 0.3070065677165985\n",
      "Validation: Epoch [24], Batch [845/938], Loss: 0.5544357299804688\n",
      "Validation: Epoch [24], Batch [846/938], Loss: 0.3482520878314972\n",
      "Validation: Epoch [24], Batch [847/938], Loss: 0.5242381691932678\n",
      "Validation: Epoch [24], Batch [848/938], Loss: 0.44731730222702026\n",
      "Validation: Epoch [24], Batch [849/938], Loss: 0.3477775752544403\n",
      "Validation: Epoch [24], Batch [850/938], Loss: 0.5237494707107544\n",
      "Validation: Epoch [24], Batch [851/938], Loss: 0.43970128893852234\n",
      "Validation: Epoch [24], Batch [852/938], Loss: 0.43777045607566833\n",
      "Validation: Epoch [24], Batch [853/938], Loss: 0.44993090629577637\n",
      "Validation: Epoch [24], Batch [854/938], Loss: 0.39558055996894836\n",
      "Validation: Epoch [24], Batch [855/938], Loss: 0.41496366262435913\n",
      "Validation: Epoch [24], Batch [856/938], Loss: 0.38713887333869934\n",
      "Validation: Epoch [24], Batch [857/938], Loss: 0.41331538558006287\n",
      "Validation: Epoch [24], Batch [858/938], Loss: 0.5458609461784363\n",
      "Validation: Epoch [24], Batch [859/938], Loss: 0.3294023275375366\n",
      "Validation: Epoch [24], Batch [860/938], Loss: 0.5051071643829346\n",
      "Validation: Epoch [24], Batch [861/938], Loss: 0.33860012888908386\n",
      "Validation: Epoch [24], Batch [862/938], Loss: 0.4276775121688843\n",
      "Validation: Epoch [24], Batch [863/938], Loss: 0.37870633602142334\n",
      "Validation: Epoch [24], Batch [864/938], Loss: 0.564762532711029\n",
      "Validation: Epoch [24], Batch [865/938], Loss: 0.4514366388320923\n",
      "Validation: Epoch [24], Batch [866/938], Loss: 0.3521689474582672\n",
      "Validation: Epoch [24], Batch [867/938], Loss: 0.49615278840065\n",
      "Validation: Epoch [24], Batch [868/938], Loss: 0.6608350276947021\n",
      "Validation: Epoch [24], Batch [869/938], Loss: 0.3840680718421936\n",
      "Validation: Epoch [24], Batch [870/938], Loss: 0.48575082421302795\n",
      "Validation: Epoch [24], Batch [871/938], Loss: 0.5037950873374939\n",
      "Validation: Epoch [24], Batch [872/938], Loss: 0.28632745146751404\n",
      "Validation: Epoch [24], Batch [873/938], Loss: 0.6197225451469421\n",
      "Validation: Epoch [24], Batch [874/938], Loss: 0.5295335054397583\n",
      "Validation: Epoch [24], Batch [875/938], Loss: 0.5045566558837891\n",
      "Validation: Epoch [24], Batch [876/938], Loss: 0.523360550403595\n",
      "Validation: Epoch [24], Batch [877/938], Loss: 0.27518483996391296\n",
      "Validation: Epoch [24], Batch [878/938], Loss: 0.6019816994667053\n",
      "Validation: Epoch [24], Batch [879/938], Loss: 0.3622515797615051\n",
      "Validation: Epoch [24], Batch [880/938], Loss: 0.30270177125930786\n",
      "Validation: Epoch [24], Batch [881/938], Loss: 0.5214753746986389\n",
      "Validation: Epoch [24], Batch [882/938], Loss: 0.4906860291957855\n",
      "Validation: Epoch [24], Batch [883/938], Loss: 0.2565702497959137\n",
      "Validation: Epoch [24], Batch [884/938], Loss: 0.30719825625419617\n",
      "Validation: Epoch [24], Batch [885/938], Loss: 0.38671377301216125\n",
      "Validation: Epoch [24], Batch [886/938], Loss: 0.3410702049732208\n",
      "Validation: Epoch [24], Batch [887/938], Loss: 0.2349008321762085\n",
      "Validation: Epoch [24], Batch [888/938], Loss: 0.6248032450675964\n",
      "Validation: Epoch [24], Batch [889/938], Loss: 0.33776718378067017\n",
      "Validation: Epoch [24], Batch [890/938], Loss: 0.25779518485069275\n",
      "Validation: Epoch [24], Batch [891/938], Loss: 0.5283927917480469\n",
      "Validation: Epoch [24], Batch [892/938], Loss: 0.4483488202095032\n",
      "Validation: Epoch [24], Batch [893/938], Loss: 0.4835483133792877\n",
      "Validation: Epoch [24], Batch [894/938], Loss: 0.3516325056552887\n",
      "Validation: Epoch [24], Batch [895/938], Loss: 0.5134655833244324\n",
      "Validation: Epoch [24], Batch [896/938], Loss: 0.5247292518615723\n",
      "Validation: Epoch [24], Batch [897/938], Loss: 0.42812448740005493\n",
      "Validation: Epoch [24], Batch [898/938], Loss: 0.5163695812225342\n",
      "Validation: Epoch [24], Batch [899/938], Loss: 0.2736790180206299\n",
      "Validation: Epoch [24], Batch [900/938], Loss: 0.414721816778183\n",
      "Validation: Epoch [24], Batch [901/938], Loss: 0.3423773944377899\n",
      "Validation: Epoch [24], Batch [902/938], Loss: 0.44319844245910645\n",
      "Validation: Epoch [24], Batch [903/938], Loss: 0.3271407186985016\n",
      "Validation: Epoch [24], Batch [904/938], Loss: 0.41190099716186523\n",
      "Validation: Epoch [24], Batch [905/938], Loss: 0.3506420850753784\n",
      "Validation: Epoch [24], Batch [906/938], Loss: 0.46477600932121277\n",
      "Validation: Epoch [24], Batch [907/938], Loss: 0.26377296447753906\n",
      "Validation: Epoch [24], Batch [908/938], Loss: 0.46763572096824646\n",
      "Validation: Epoch [24], Batch [909/938], Loss: 0.4160993993282318\n",
      "Validation: Epoch [24], Batch [910/938], Loss: 0.36099672317504883\n",
      "Validation: Epoch [24], Batch [911/938], Loss: 0.4368962049484253\n",
      "Validation: Epoch [24], Batch [912/938], Loss: 0.41207802295684814\n",
      "Validation: Epoch [24], Batch [913/938], Loss: 0.3396792709827423\n",
      "Validation: Epoch [24], Batch [914/938], Loss: 0.5574305653572083\n",
      "Validation: Epoch [24], Batch [915/938], Loss: 0.38348421454429626\n",
      "Validation: Epoch [24], Batch [916/938], Loss: 0.37643808126449585\n",
      "Validation: Epoch [24], Batch [917/938], Loss: 0.48116815090179443\n",
      "Validation: Epoch [24], Batch [918/938], Loss: 0.4105958938598633\n",
      "Validation: Epoch [24], Batch [919/938], Loss: 0.35608452558517456\n",
      "Validation: Epoch [24], Batch [920/938], Loss: 0.5127343535423279\n",
      "Validation: Epoch [24], Batch [921/938], Loss: 0.33609509468078613\n",
      "Validation: Epoch [24], Batch [922/938], Loss: 0.2563619911670685\n",
      "Validation: Epoch [24], Batch [923/938], Loss: 0.38044726848602295\n",
      "Validation: Epoch [24], Batch [924/938], Loss: 0.2630629539489746\n",
      "Validation: Epoch [24], Batch [925/938], Loss: 0.44875380396842957\n",
      "Validation: Epoch [24], Batch [926/938], Loss: 0.35580724477767944\n",
      "Validation: Epoch [24], Batch [927/938], Loss: 0.384184867143631\n",
      "Validation: Epoch [24], Batch [928/938], Loss: 0.6408621072769165\n",
      "Validation: Epoch [24], Batch [929/938], Loss: 0.39024776220321655\n",
      "Validation: Epoch [24], Batch [930/938], Loss: 0.35714176297187805\n",
      "Validation: Epoch [24], Batch [931/938], Loss: 0.5353942513465881\n",
      "Validation: Epoch [24], Batch [932/938], Loss: 0.530218780040741\n",
      "Validation: Epoch [24], Batch [933/938], Loss: 0.24527209997177124\n",
      "Validation: Epoch [24], Batch [934/938], Loss: 0.3893090784549713\n",
      "Validation: Epoch [24], Batch [935/938], Loss: 0.2781538963317871\n",
      "Validation: Epoch [24], Batch [936/938], Loss: 0.40873026847839355\n",
      "Validation: Epoch [24], Batch [937/938], Loss: 0.44110676646232605\n",
      "Validation: Epoch [24], Batch [938/938], Loss: 0.32624003291130066\n",
      "Accuracy of test set: 0.8486833333333333\n",
      "Train: Epoch [25], Batch [1/938], Loss: 0.34659695625305176\n",
      "Train: Epoch [25], Batch [2/938], Loss: 0.28280508518218994\n",
      "Train: Epoch [25], Batch [3/938], Loss: 0.33064979314804077\n",
      "Train: Epoch [25], Batch [4/938], Loss: 0.28560537099838257\n",
      "Train: Epoch [25], Batch [5/938], Loss: 0.26742276549339294\n",
      "Train: Epoch [25], Batch [6/938], Loss: 0.5065582394599915\n",
      "Train: Epoch [25], Batch [7/938], Loss: 0.321249395608902\n",
      "Train: Epoch [25], Batch [8/938], Loss: 0.43476077914237976\n",
      "Train: Epoch [25], Batch [9/938], Loss: 0.6816385388374329\n",
      "Train: Epoch [25], Batch [10/938], Loss: 0.4682680666446686\n",
      "Train: Epoch [25], Batch [11/938], Loss: 0.37024402618408203\n",
      "Train: Epoch [25], Batch [12/938], Loss: 0.5457943677902222\n",
      "Train: Epoch [25], Batch [13/938], Loss: 0.49237558245658875\n",
      "Train: Epoch [25], Batch [14/938], Loss: 0.33201828598976135\n",
      "Train: Epoch [25], Batch [15/938], Loss: 0.4233669340610504\n",
      "Train: Epoch [25], Batch [16/938], Loss: 0.44442418217658997\n",
      "Train: Epoch [25], Batch [17/938], Loss: 0.4286295771598816\n",
      "Train: Epoch [25], Batch [18/938], Loss: 0.589465856552124\n",
      "Train: Epoch [25], Batch [19/938], Loss: 0.5204586386680603\n",
      "Train: Epoch [25], Batch [20/938], Loss: 0.27564170956611633\n",
      "Train: Epoch [25], Batch [21/938], Loss: 0.46619725227355957\n",
      "Train: Epoch [25], Batch [22/938], Loss: 0.2970777750015259\n",
      "Train: Epoch [25], Batch [23/938], Loss: 0.39938342571258545\n",
      "Train: Epoch [25], Batch [24/938], Loss: 0.3930719494819641\n",
      "Train: Epoch [25], Batch [25/938], Loss: 0.49134358763694763\n",
      "Train: Epoch [25], Batch [26/938], Loss: 0.6021087169647217\n",
      "Train: Epoch [25], Batch [27/938], Loss: 0.3953210413455963\n",
      "Train: Epoch [25], Batch [28/938], Loss: 0.4347086250782013\n",
      "Train: Epoch [25], Batch [29/938], Loss: 0.35426801443099976\n",
      "Train: Epoch [25], Batch [30/938], Loss: 0.6024436950683594\n",
      "Train: Epoch [25], Batch [31/938], Loss: 0.2706051468849182\n",
      "Train: Epoch [25], Batch [32/938], Loss: 0.43203258514404297\n",
      "Train: Epoch [25], Batch [33/938], Loss: 0.32714760303497314\n",
      "Train: Epoch [25], Batch [34/938], Loss: 0.33764296770095825\n",
      "Train: Epoch [25], Batch [35/938], Loss: 0.4005531370639801\n",
      "Train: Epoch [25], Batch [36/938], Loss: 0.2792689800262451\n",
      "Train: Epoch [25], Batch [37/938], Loss: 0.30319449305534363\n",
      "Train: Epoch [25], Batch [38/938], Loss: 0.4936554431915283\n",
      "Train: Epoch [25], Batch [39/938], Loss: 0.33436399698257446\n",
      "Train: Epoch [25], Batch [40/938], Loss: 0.3880356252193451\n",
      "Train: Epoch [25], Batch [41/938], Loss: 0.44412919878959656\n",
      "Train: Epoch [25], Batch [42/938], Loss: 0.42540863156318665\n",
      "Train: Epoch [25], Batch [43/938], Loss: 0.30782100558280945\n",
      "Train: Epoch [25], Batch [44/938], Loss: 0.4310346841812134\n",
      "Train: Epoch [25], Batch [45/938], Loss: 0.4733555316925049\n",
      "Train: Epoch [25], Batch [46/938], Loss: 0.32044869661331177\n",
      "Train: Epoch [25], Batch [47/938], Loss: 0.31312352418899536\n",
      "Train: Epoch [25], Batch [48/938], Loss: 0.23320868611335754\n",
      "Train: Epoch [25], Batch [49/938], Loss: 0.38004058599472046\n",
      "Train: Epoch [25], Batch [50/938], Loss: 0.5377151370048523\n",
      "Train: Epoch [25], Batch [51/938], Loss: 0.4135799705982208\n",
      "Train: Epoch [25], Batch [52/938], Loss: 0.4746091961860657\n",
      "Train: Epoch [25], Batch [53/938], Loss: 0.6837450861930847\n",
      "Train: Epoch [25], Batch [54/938], Loss: 0.4108424186706543\n",
      "Train: Epoch [25], Batch [55/938], Loss: 0.3514220714569092\n",
      "Train: Epoch [25], Batch [56/938], Loss: 0.35539916157722473\n",
      "Train: Epoch [25], Batch [57/938], Loss: 0.44206157326698303\n",
      "Train: Epoch [25], Batch [58/938], Loss: 0.48632290959358215\n",
      "Train: Epoch [25], Batch [59/938], Loss: 0.5022804737091064\n",
      "Train: Epoch [25], Batch [60/938], Loss: 0.43964987993240356\n",
      "Train: Epoch [25], Batch [61/938], Loss: 0.4666784107685089\n",
      "Train: Epoch [25], Batch [62/938], Loss: 0.34935787320137024\n",
      "Train: Epoch [25], Batch [63/938], Loss: 0.3677614629268646\n",
      "Train: Epoch [25], Batch [64/938], Loss: 0.3199808597564697\n",
      "Train: Epoch [25], Batch [65/938], Loss: 0.5159257650375366\n",
      "Train: Epoch [25], Batch [66/938], Loss: 0.2667757272720337\n",
      "Train: Epoch [25], Batch [67/938], Loss: 0.4293537437915802\n",
      "Train: Epoch [25], Batch [68/938], Loss: 0.28176558017730713\n",
      "Train: Epoch [25], Batch [69/938], Loss: 0.31163036823272705\n",
      "Train: Epoch [25], Batch [70/938], Loss: 0.3799654245376587\n",
      "Train: Epoch [25], Batch [71/938], Loss: 0.29176196455955505\n",
      "Train: Epoch [25], Batch [72/938], Loss: 0.37139931321144104\n",
      "Train: Epoch [25], Batch [73/938], Loss: 0.48139792680740356\n",
      "Train: Epoch [25], Batch [74/938], Loss: 0.5123718976974487\n",
      "Train: Epoch [25], Batch [75/938], Loss: 0.4801822900772095\n",
      "Train: Epoch [25], Batch [76/938], Loss: 0.31172260642051697\n",
      "Train: Epoch [25], Batch [77/938], Loss: 0.46428143978118896\n",
      "Train: Epoch [25], Batch [78/938], Loss: 0.4513198137283325\n",
      "Train: Epoch [25], Batch [79/938], Loss: 0.5873632431030273\n",
      "Train: Epoch [25], Batch [80/938], Loss: 0.4810166656970978\n",
      "Train: Epoch [25], Batch [81/938], Loss: 0.4695143401622772\n",
      "Train: Epoch [25], Batch [82/938], Loss: 0.4633559286594391\n",
      "Train: Epoch [25], Batch [83/938], Loss: 0.47601357102394104\n",
      "Train: Epoch [25], Batch [84/938], Loss: 0.5333700180053711\n",
      "Train: Epoch [25], Batch [85/938], Loss: 0.2517564296722412\n",
      "Train: Epoch [25], Batch [86/938], Loss: 0.4413485527038574\n",
      "Train: Epoch [25], Batch [87/938], Loss: 0.3158547580242157\n",
      "Train: Epoch [25], Batch [88/938], Loss: 0.3612144887447357\n",
      "Train: Epoch [25], Batch [89/938], Loss: 0.6330902576446533\n",
      "Train: Epoch [25], Batch [90/938], Loss: 0.4040711522102356\n",
      "Train: Epoch [25], Batch [91/938], Loss: 0.4156848192214966\n",
      "Train: Epoch [25], Batch [92/938], Loss: 0.5388216972351074\n",
      "Train: Epoch [25], Batch [93/938], Loss: 0.36061662435531616\n",
      "Train: Epoch [25], Batch [94/938], Loss: 0.34591034054756165\n",
      "Train: Epoch [25], Batch [95/938], Loss: 0.6606025099754333\n",
      "Train: Epoch [25], Batch [96/938], Loss: 0.38726890087127686\n",
      "Train: Epoch [25], Batch [97/938], Loss: 0.4098411202430725\n",
      "Train: Epoch [25], Batch [98/938], Loss: 0.279821515083313\n",
      "Train: Epoch [25], Batch [99/938], Loss: 0.4043938219547272\n",
      "Train: Epoch [25], Batch [100/938], Loss: 0.5427789092063904\n",
      "Train: Epoch [25], Batch [101/938], Loss: 0.5667833685874939\n",
      "Train: Epoch [25], Batch [102/938], Loss: 0.5027357339859009\n",
      "Train: Epoch [25], Batch [103/938], Loss: 0.31875595450401306\n",
      "Train: Epoch [25], Batch [104/938], Loss: 0.5558241605758667\n",
      "Train: Epoch [25], Batch [105/938], Loss: 0.33779823780059814\n",
      "Train: Epoch [25], Batch [106/938], Loss: 0.4358779788017273\n",
      "Train: Epoch [25], Batch [107/938], Loss: 0.4840461015701294\n",
      "Train: Epoch [25], Batch [108/938], Loss: 0.40067893266677856\n",
      "Train: Epoch [25], Batch [109/938], Loss: 0.5198127627372742\n",
      "Train: Epoch [25], Batch [110/938], Loss: 0.5881574153900146\n",
      "Train: Epoch [25], Batch [111/938], Loss: 0.4853668212890625\n",
      "Train: Epoch [25], Batch [112/938], Loss: 0.43552064895629883\n",
      "Train: Epoch [25], Batch [113/938], Loss: 0.31426718831062317\n",
      "Train: Epoch [25], Batch [114/938], Loss: 0.47260546684265137\n",
      "Train: Epoch [25], Batch [115/938], Loss: 0.43183523416519165\n",
      "Train: Epoch [25], Batch [116/938], Loss: 0.36424338817596436\n",
      "Train: Epoch [25], Batch [117/938], Loss: 0.3719564378261566\n",
      "Train: Epoch [25], Batch [118/938], Loss: 0.4688476622104645\n",
      "Train: Epoch [25], Batch [119/938], Loss: 0.3721877634525299\n",
      "Train: Epoch [25], Batch [120/938], Loss: 0.23204921185970306\n",
      "Train: Epoch [25], Batch [121/938], Loss: 0.4721076786518097\n",
      "Train: Epoch [25], Batch [122/938], Loss: 0.5187377333641052\n",
      "Train: Epoch [25], Batch [123/938], Loss: 0.48031502962112427\n",
      "Train: Epoch [25], Batch [124/938], Loss: 0.4474184215068817\n",
      "Train: Epoch [25], Batch [125/938], Loss: 0.4983673691749573\n",
      "Train: Epoch [25], Batch [126/938], Loss: 0.5745508670806885\n",
      "Train: Epoch [25], Batch [127/938], Loss: 0.4148590564727783\n",
      "Train: Epoch [25], Batch [128/938], Loss: 0.38339173793792725\n",
      "Train: Epoch [25], Batch [129/938], Loss: 0.28338953852653503\n",
      "Train: Epoch [25], Batch [130/938], Loss: 0.7623741626739502\n",
      "Train: Epoch [25], Batch [131/938], Loss: 0.23890379071235657\n",
      "Train: Epoch [25], Batch [132/938], Loss: 0.39657124876976013\n",
      "Train: Epoch [25], Batch [133/938], Loss: 0.41072380542755127\n",
      "Train: Epoch [25], Batch [134/938], Loss: 0.6172536015510559\n",
      "Train: Epoch [25], Batch [135/938], Loss: 0.4403528571128845\n",
      "Train: Epoch [25], Batch [136/938], Loss: 0.6681430339813232\n",
      "Train: Epoch [25], Batch [137/938], Loss: 0.2975647449493408\n",
      "Train: Epoch [25], Batch [138/938], Loss: 0.4284440577030182\n",
      "Train: Epoch [25], Batch [139/938], Loss: 0.5536736249923706\n",
      "Train: Epoch [25], Batch [140/938], Loss: 0.3023456037044525\n",
      "Train: Epoch [25], Batch [141/938], Loss: 0.3726845979690552\n",
      "Train: Epoch [25], Batch [142/938], Loss: 0.32765570282936096\n",
      "Train: Epoch [25], Batch [143/938], Loss: 0.5700993537902832\n",
      "Train: Epoch [25], Batch [144/938], Loss: 0.4132435619831085\n",
      "Train: Epoch [25], Batch [145/938], Loss: 0.28061994910240173\n",
      "Train: Epoch [25], Batch [146/938], Loss: 0.2919372618198395\n",
      "Train: Epoch [25], Batch [147/938], Loss: 0.3703482747077942\n",
      "Train: Epoch [25], Batch [148/938], Loss: 0.3744763731956482\n",
      "Train: Epoch [25], Batch [149/938], Loss: 0.5772194862365723\n",
      "Train: Epoch [25], Batch [150/938], Loss: 0.611937403678894\n",
      "Train: Epoch [25], Batch [151/938], Loss: 0.3793734014034271\n",
      "Train: Epoch [25], Batch [152/938], Loss: 0.4190303683280945\n",
      "Train: Epoch [25], Batch [153/938], Loss: 0.21584875881671906\n",
      "Train: Epoch [25], Batch [154/938], Loss: 0.30821990966796875\n",
      "Train: Epoch [25], Batch [155/938], Loss: 0.361003041267395\n",
      "Train: Epoch [25], Batch [156/938], Loss: 0.36426234245300293\n",
      "Train: Epoch [25], Batch [157/938], Loss: 0.4780218303203583\n",
      "Train: Epoch [25], Batch [158/938], Loss: 0.559418797492981\n",
      "Train: Epoch [25], Batch [159/938], Loss: 0.3672031760215759\n",
      "Train: Epoch [25], Batch [160/938], Loss: 0.3919403851032257\n",
      "Train: Epoch [25], Batch [161/938], Loss: 0.4204432964324951\n",
      "Train: Epoch [25], Batch [162/938], Loss: 0.4620116949081421\n",
      "Train: Epoch [25], Batch [163/938], Loss: 0.2455892264842987\n",
      "Train: Epoch [25], Batch [164/938], Loss: 0.41502732038497925\n",
      "Train: Epoch [25], Batch [165/938], Loss: 0.3918824791908264\n",
      "Train: Epoch [25], Batch [166/938], Loss: 0.29127147793769836\n",
      "Train: Epoch [25], Batch [167/938], Loss: 0.25305476784706116\n",
      "Train: Epoch [25], Batch [168/938], Loss: 0.3234696090221405\n",
      "Train: Epoch [25], Batch [169/938], Loss: 0.4068274199962616\n",
      "Train: Epoch [25], Batch [170/938], Loss: 0.23039664328098297\n",
      "Train: Epoch [25], Batch [171/938], Loss: 0.43098312616348267\n",
      "Train: Epoch [25], Batch [172/938], Loss: 0.4612959623336792\n",
      "Train: Epoch [25], Batch [173/938], Loss: 0.2752416729927063\n",
      "Train: Epoch [25], Batch [174/938], Loss: 0.31222018599510193\n",
      "Train: Epoch [25], Batch [175/938], Loss: 0.5176248550415039\n",
      "Train: Epoch [25], Batch [176/938], Loss: 0.4502955973148346\n",
      "Train: Epoch [25], Batch [177/938], Loss: 0.7002868056297302\n",
      "Train: Epoch [25], Batch [178/938], Loss: 0.2859851121902466\n",
      "Train: Epoch [25], Batch [179/938], Loss: 0.42913591861724854\n",
      "Train: Epoch [25], Batch [180/938], Loss: 0.4651053547859192\n",
      "Train: Epoch [25], Batch [181/938], Loss: 0.494517058134079\n",
      "Train: Epoch [25], Batch [182/938], Loss: 0.32732510566711426\n",
      "Train: Epoch [25], Batch [183/938], Loss: 0.414120614528656\n",
      "Train: Epoch [25], Batch [184/938], Loss: 0.7350444793701172\n",
      "Train: Epoch [25], Batch [185/938], Loss: 0.38219940662384033\n",
      "Train: Epoch [25], Batch [186/938], Loss: 0.562618613243103\n",
      "Train: Epoch [25], Batch [187/938], Loss: 0.5838910341262817\n",
      "Train: Epoch [25], Batch [188/938], Loss: 0.4835436940193176\n",
      "Train: Epoch [25], Batch [189/938], Loss: 0.6939201951026917\n",
      "Train: Epoch [25], Batch [190/938], Loss: 0.6470491290092468\n",
      "Train: Epoch [25], Batch [191/938], Loss: 0.48905348777770996\n",
      "Train: Epoch [25], Batch [192/938], Loss: 0.27975407242774963\n",
      "Train: Epoch [25], Batch [193/938], Loss: 0.47056981921195984\n",
      "Train: Epoch [25], Batch [194/938], Loss: 0.4094459116458893\n",
      "Train: Epoch [25], Batch [195/938], Loss: 0.22189289331436157\n",
      "Train: Epoch [25], Batch [196/938], Loss: 0.46987631916999817\n",
      "Train: Epoch [25], Batch [197/938], Loss: 0.3232775330543518\n",
      "Train: Epoch [25], Batch [198/938], Loss: 0.541318416595459\n",
      "Train: Epoch [25], Batch [199/938], Loss: 0.533524751663208\n",
      "Train: Epoch [25], Batch [200/938], Loss: 0.44140225648880005\n",
      "Train: Epoch [25], Batch [201/938], Loss: 0.5417441129684448\n",
      "Train: Epoch [25], Batch [202/938], Loss: 0.27032211422920227\n",
      "Train: Epoch [25], Batch [203/938], Loss: 0.2906757593154907\n",
      "Train: Epoch [25], Batch [204/938], Loss: 0.325274258852005\n",
      "Train: Epoch [25], Batch [205/938], Loss: 0.42772629857063293\n",
      "Train: Epoch [25], Batch [206/938], Loss: 0.46216481924057007\n",
      "Train: Epoch [25], Batch [207/938], Loss: 0.5130258202552795\n",
      "Train: Epoch [25], Batch [208/938], Loss: 0.552367627620697\n",
      "Train: Epoch [25], Batch [209/938], Loss: 0.40981271862983704\n",
      "Train: Epoch [25], Batch [210/938], Loss: 0.5123992562294006\n",
      "Train: Epoch [25], Batch [211/938], Loss: 0.34225237369537354\n",
      "Train: Epoch [25], Batch [212/938], Loss: 0.530386745929718\n",
      "Train: Epoch [25], Batch [213/938], Loss: 0.26386401057243347\n",
      "Train: Epoch [25], Batch [214/938], Loss: 0.5327134728431702\n",
      "Train: Epoch [25], Batch [215/938], Loss: 0.3556872308254242\n",
      "Train: Epoch [25], Batch [216/938], Loss: 0.3277270793914795\n",
      "Train: Epoch [25], Batch [217/938], Loss: 0.35400140285491943\n",
      "Train: Epoch [25], Batch [218/938], Loss: 0.4025191068649292\n",
      "Train: Epoch [25], Batch [219/938], Loss: 0.46258094906806946\n",
      "Train: Epoch [25], Batch [220/938], Loss: 0.4881826937198639\n",
      "Train: Epoch [25], Batch [221/938], Loss: 0.33291006088256836\n",
      "Train: Epoch [25], Batch [222/938], Loss: 0.5097541213035583\n",
      "Train: Epoch [25], Batch [223/938], Loss: 0.38129252195358276\n",
      "Train: Epoch [25], Batch [224/938], Loss: 0.30091944336891174\n",
      "Train: Epoch [25], Batch [225/938], Loss: 0.42777949571609497\n",
      "Train: Epoch [25], Batch [226/938], Loss: 0.45014458894729614\n",
      "Train: Epoch [25], Batch [227/938], Loss: 0.3278202712535858\n",
      "Train: Epoch [25], Batch [228/938], Loss: 0.6759400963783264\n",
      "Train: Epoch [25], Batch [229/938], Loss: 0.3667934834957123\n",
      "Train: Epoch [25], Batch [230/938], Loss: 0.3887954354286194\n",
      "Train: Epoch [25], Batch [231/938], Loss: 0.48484477400779724\n",
      "Train: Epoch [25], Batch [232/938], Loss: 0.4174172580242157\n",
      "Train: Epoch [25], Batch [233/938], Loss: 0.630679190158844\n",
      "Train: Epoch [25], Batch [234/938], Loss: 0.3384475111961365\n",
      "Train: Epoch [25], Batch [235/938], Loss: 0.3327092230319977\n",
      "Train: Epoch [25], Batch [236/938], Loss: 0.384482741355896\n",
      "Train: Epoch [25], Batch [237/938], Loss: 0.520434558391571\n",
      "Train: Epoch [25], Batch [238/938], Loss: 0.34336960315704346\n",
      "Train: Epoch [25], Batch [239/938], Loss: 0.5204053521156311\n",
      "Train: Epoch [25], Batch [240/938], Loss: 0.36063188314437866\n",
      "Train: Epoch [25], Batch [241/938], Loss: 0.5717402696609497\n",
      "Train: Epoch [25], Batch [242/938], Loss: 0.28589630126953125\n",
      "Train: Epoch [25], Batch [243/938], Loss: 0.38521695137023926\n",
      "Train: Epoch [25], Batch [244/938], Loss: 0.42225340008735657\n",
      "Train: Epoch [25], Batch [245/938], Loss: 0.5520632266998291\n",
      "Train: Epoch [25], Batch [246/938], Loss: 0.3480095863342285\n",
      "Train: Epoch [25], Batch [247/938], Loss: 0.23121510446071625\n",
      "Train: Epoch [25], Batch [248/938], Loss: 0.4289962351322174\n",
      "Train: Epoch [25], Batch [249/938], Loss: 0.5051397681236267\n",
      "Train: Epoch [25], Batch [250/938], Loss: 0.23964154720306396\n",
      "Train: Epoch [25], Batch [251/938], Loss: 0.44883155822753906\n",
      "Train: Epoch [25], Batch [252/938], Loss: 0.22930046916007996\n",
      "Train: Epoch [25], Batch [253/938], Loss: 0.3216221332550049\n",
      "Train: Epoch [25], Batch [254/938], Loss: 0.44806694984436035\n",
      "Train: Epoch [25], Batch [255/938], Loss: 0.22676560282707214\n",
      "Train: Epoch [25], Batch [256/938], Loss: 0.37693101167678833\n",
      "Train: Epoch [25], Batch [257/938], Loss: 0.35143908858299255\n",
      "Train: Epoch [25], Batch [258/938], Loss: 0.37593433260917664\n",
      "Train: Epoch [25], Batch [259/938], Loss: 0.4466833472251892\n",
      "Train: Epoch [25], Batch [260/938], Loss: 0.36505186557769775\n",
      "Train: Epoch [25], Batch [261/938], Loss: 0.6273850202560425\n",
      "Train: Epoch [25], Batch [262/938], Loss: 0.5491903424263\n",
      "Train: Epoch [25], Batch [263/938], Loss: 0.3595908582210541\n",
      "Train: Epoch [25], Batch [264/938], Loss: 0.5251864194869995\n",
      "Train: Epoch [25], Batch [265/938], Loss: 0.3124639093875885\n",
      "Train: Epoch [25], Batch [266/938], Loss: 0.4193602502346039\n",
      "Train: Epoch [25], Batch [267/938], Loss: 0.2957063615322113\n",
      "Train: Epoch [25], Batch [268/938], Loss: 0.42616036534309387\n",
      "Train: Epoch [25], Batch [269/938], Loss: 0.41640710830688477\n",
      "Train: Epoch [25], Batch [270/938], Loss: 0.5176348686218262\n",
      "Train: Epoch [25], Batch [271/938], Loss: 0.3926765024662018\n",
      "Train: Epoch [25], Batch [272/938], Loss: 0.31965625286102295\n",
      "Train: Epoch [25], Batch [273/938], Loss: 0.6555014252662659\n",
      "Train: Epoch [25], Batch [274/938], Loss: 0.43706122040748596\n",
      "Train: Epoch [25], Batch [275/938], Loss: 0.33378469944000244\n",
      "Train: Epoch [25], Batch [276/938], Loss: 0.3814207911491394\n",
      "Train: Epoch [25], Batch [277/938], Loss: 0.4947037696838379\n",
      "Train: Epoch [25], Batch [278/938], Loss: 0.42315948009490967\n",
      "Train: Epoch [25], Batch [279/938], Loss: 0.40613433718681335\n",
      "Train: Epoch [25], Batch [280/938], Loss: 0.348922461271286\n",
      "Train: Epoch [25], Batch [281/938], Loss: 0.5435616374015808\n",
      "Train: Epoch [25], Batch [282/938], Loss: 0.47103753685951233\n",
      "Train: Epoch [25], Batch [283/938], Loss: 0.5067593455314636\n",
      "Train: Epoch [25], Batch [284/938], Loss: 0.5196491479873657\n",
      "Train: Epoch [25], Batch [285/938], Loss: 0.30829259753227234\n",
      "Train: Epoch [25], Batch [286/938], Loss: 0.3099501430988312\n",
      "Train: Epoch [25], Batch [287/938], Loss: 0.31993648409843445\n",
      "Train: Epoch [25], Batch [288/938], Loss: 0.39170700311660767\n",
      "Train: Epoch [25], Batch [289/938], Loss: 0.3090892434120178\n",
      "Train: Epoch [25], Batch [290/938], Loss: 0.46573176980018616\n",
      "Train: Epoch [25], Batch [291/938], Loss: 0.41101011633872986\n",
      "Train: Epoch [25], Batch [292/938], Loss: 0.394074410200119\n",
      "Train: Epoch [25], Batch [293/938], Loss: 0.43048566579818726\n",
      "Train: Epoch [25], Batch [294/938], Loss: 0.390386700630188\n",
      "Train: Epoch [25], Batch [295/938], Loss: 0.41411322355270386\n",
      "Train: Epoch [25], Batch [296/938], Loss: 0.4554232060909271\n",
      "Train: Epoch [25], Batch [297/938], Loss: 0.46900391578674316\n",
      "Train: Epoch [25], Batch [298/938], Loss: 0.2582380473613739\n",
      "Train: Epoch [25], Batch [299/938], Loss: 0.3585677444934845\n",
      "Train: Epoch [25], Batch [300/938], Loss: 0.3773006796836853\n",
      "Train: Epoch [25], Batch [301/938], Loss: 0.36438077688217163\n",
      "Train: Epoch [25], Batch [302/938], Loss: 0.43861836194992065\n",
      "Train: Epoch [25], Batch [303/938], Loss: 0.5949960947036743\n",
      "Train: Epoch [25], Batch [304/938], Loss: 0.4246070981025696\n",
      "Train: Epoch [25], Batch [305/938], Loss: 0.42808961868286133\n",
      "Train: Epoch [25], Batch [306/938], Loss: 0.5676822066307068\n",
      "Train: Epoch [25], Batch [307/938], Loss: 0.26997169852256775\n",
      "Train: Epoch [25], Batch [308/938], Loss: 0.28863266110420227\n",
      "Train: Epoch [25], Batch [309/938], Loss: 0.21127498149871826\n",
      "Train: Epoch [25], Batch [310/938], Loss: 0.3129190504550934\n",
      "Train: Epoch [25], Batch [311/938], Loss: 0.35389330983161926\n",
      "Train: Epoch [25], Batch [312/938], Loss: 0.3741094172000885\n",
      "Train: Epoch [25], Batch [313/938], Loss: 0.377605676651001\n",
      "Train: Epoch [25], Batch [314/938], Loss: 0.5091224312782288\n",
      "Train: Epoch [25], Batch [315/938], Loss: 0.29335471987724304\n",
      "Train: Epoch [25], Batch [316/938], Loss: 0.43745943903923035\n",
      "Train: Epoch [25], Batch [317/938], Loss: 0.2790629267692566\n",
      "Train: Epoch [25], Batch [318/938], Loss: 0.31740108132362366\n",
      "Train: Epoch [25], Batch [319/938], Loss: 0.6202120780944824\n",
      "Train: Epoch [25], Batch [320/938], Loss: 0.40235260128974915\n",
      "Train: Epoch [25], Batch [321/938], Loss: 0.32708728313446045\n",
      "Train: Epoch [25], Batch [322/938], Loss: 0.5904301404953003\n",
      "Train: Epoch [25], Batch [323/938], Loss: 0.47904694080352783\n",
      "Train: Epoch [25], Batch [324/938], Loss: 0.3599301278591156\n",
      "Train: Epoch [25], Batch [325/938], Loss: 0.39855509996414185\n",
      "Train: Epoch [25], Batch [326/938], Loss: 0.3726840615272522\n",
      "Train: Epoch [25], Batch [327/938], Loss: 0.2895205020904541\n",
      "Train: Epoch [25], Batch [328/938], Loss: 0.4201868772506714\n",
      "Train: Epoch [25], Batch [329/938], Loss: 0.4461514353752136\n",
      "Train: Epoch [25], Batch [330/938], Loss: 0.438332200050354\n",
      "Train: Epoch [25], Batch [331/938], Loss: 0.47418397665023804\n",
      "Train: Epoch [25], Batch [332/938], Loss: 0.36630499362945557\n",
      "Train: Epoch [25], Batch [333/938], Loss: 0.3761349320411682\n",
      "Train: Epoch [25], Batch [334/938], Loss: 0.5320346355438232\n",
      "Train: Epoch [25], Batch [335/938], Loss: 0.44602489471435547\n",
      "Train: Epoch [25], Batch [336/938], Loss: 0.5088940858840942\n",
      "Train: Epoch [25], Batch [337/938], Loss: 0.37001562118530273\n",
      "Train: Epoch [25], Batch [338/938], Loss: 0.2811712324619293\n",
      "Train: Epoch [25], Batch [339/938], Loss: 0.4874141216278076\n",
      "Train: Epoch [25], Batch [340/938], Loss: 0.3354024887084961\n",
      "Train: Epoch [25], Batch [341/938], Loss: 0.424618661403656\n",
      "Train: Epoch [25], Batch [342/938], Loss: 0.27398037910461426\n",
      "Train: Epoch [25], Batch [343/938], Loss: 0.3716944754123688\n",
      "Train: Epoch [25], Batch [344/938], Loss: 0.2979568541049957\n",
      "Train: Epoch [25], Batch [345/938], Loss: 0.39154255390167236\n",
      "Train: Epoch [25], Batch [346/938], Loss: 0.23277541995048523\n",
      "Train: Epoch [25], Batch [347/938], Loss: 0.5494012832641602\n",
      "Train: Epoch [25], Batch [348/938], Loss: 0.5388385653495789\n",
      "Train: Epoch [25], Batch [349/938], Loss: 0.3084976077079773\n",
      "Train: Epoch [25], Batch [350/938], Loss: 0.42941951751708984\n",
      "Train: Epoch [25], Batch [351/938], Loss: 0.34985873103141785\n",
      "Train: Epoch [25], Batch [352/938], Loss: 0.31132426857948303\n",
      "Train: Epoch [25], Batch [353/938], Loss: 0.24929691851139069\n",
      "Train: Epoch [25], Batch [354/938], Loss: 0.4597971737384796\n",
      "Train: Epoch [25], Batch [355/938], Loss: 0.26402023434638977\n",
      "Train: Epoch [25], Batch [356/938], Loss: 0.37377285957336426\n",
      "Train: Epoch [25], Batch [357/938], Loss: 0.48124900460243225\n",
      "Train: Epoch [25], Batch [358/938], Loss: 0.5490904450416565\n",
      "Train: Epoch [25], Batch [359/938], Loss: 0.2982385456562042\n",
      "Train: Epoch [25], Batch [360/938], Loss: 0.3505372107028961\n",
      "Train: Epoch [25], Batch [361/938], Loss: 0.5309709310531616\n",
      "Train: Epoch [25], Batch [362/938], Loss: 0.6476139426231384\n",
      "Train: Epoch [25], Batch [363/938], Loss: 0.47020071744918823\n",
      "Train: Epoch [25], Batch [364/938], Loss: 0.40310099720954895\n",
      "Train: Epoch [25], Batch [365/938], Loss: 0.18540586531162262\n",
      "Train: Epoch [25], Batch [366/938], Loss: 0.7514813542366028\n",
      "Train: Epoch [25], Batch [367/938], Loss: 0.43038344383239746\n",
      "Train: Epoch [25], Batch [368/938], Loss: 0.4154619574546814\n",
      "Train: Epoch [25], Batch [369/938], Loss: 0.5075680017471313\n",
      "Train: Epoch [25], Batch [370/938], Loss: 0.35835716128349304\n",
      "Train: Epoch [25], Batch [371/938], Loss: 0.5601818561553955\n",
      "Train: Epoch [25], Batch [372/938], Loss: 0.37793806195259094\n",
      "Train: Epoch [25], Batch [373/938], Loss: 0.3485885262489319\n",
      "Train: Epoch [25], Batch [374/938], Loss: 0.60474693775177\n",
      "Train: Epoch [25], Batch [375/938], Loss: 0.374909907579422\n",
      "Train: Epoch [25], Batch [376/938], Loss: 0.4736100435256958\n",
      "Train: Epoch [25], Batch [377/938], Loss: 0.28266844153404236\n",
      "Train: Epoch [25], Batch [378/938], Loss: 0.22145643830299377\n",
      "Train: Epoch [25], Batch [379/938], Loss: 0.4174235463142395\n",
      "Train: Epoch [25], Batch [380/938], Loss: 0.516890287399292\n",
      "Train: Epoch [25], Batch [381/938], Loss: 0.2325834184885025\n",
      "Train: Epoch [25], Batch [382/938], Loss: 0.2834199368953705\n",
      "Train: Epoch [25], Batch [383/938], Loss: 0.47972601652145386\n",
      "Train: Epoch [25], Batch [384/938], Loss: 0.4864560663700104\n",
      "Train: Epoch [25], Batch [385/938], Loss: 0.2945990562438965\n",
      "Train: Epoch [25], Batch [386/938], Loss: 0.30713897943496704\n",
      "Train: Epoch [25], Batch [387/938], Loss: 0.33819812536239624\n",
      "Train: Epoch [25], Batch [388/938], Loss: 0.4760177731513977\n",
      "Train: Epoch [25], Batch [389/938], Loss: 0.40548616647720337\n",
      "Train: Epoch [25], Batch [390/938], Loss: 0.546663224697113\n",
      "Train: Epoch [25], Batch [391/938], Loss: 0.5252036452293396\n",
      "Train: Epoch [25], Batch [392/938], Loss: 0.7104507684707642\n",
      "Train: Epoch [25], Batch [393/938], Loss: 0.3036629259586334\n",
      "Train: Epoch [25], Batch [394/938], Loss: 0.5879249572753906\n",
      "Train: Epoch [25], Batch [395/938], Loss: 0.3439193069934845\n",
      "Train: Epoch [25], Batch [396/938], Loss: 0.1590595841407776\n",
      "Train: Epoch [25], Batch [397/938], Loss: 0.4197153151035309\n",
      "Train: Epoch [25], Batch [398/938], Loss: 0.4587459862232208\n",
      "Train: Epoch [25], Batch [399/938], Loss: 0.6352221965789795\n",
      "Train: Epoch [25], Batch [400/938], Loss: 0.43011531233787537\n",
      "Train: Epoch [25], Batch [401/938], Loss: 0.33196961879730225\n",
      "Train: Epoch [25], Batch [402/938], Loss: 0.27230188250541687\n",
      "Train: Epoch [25], Batch [403/938], Loss: 0.34535881876945496\n",
      "Train: Epoch [25], Batch [404/938], Loss: 0.39661934971809387\n",
      "Train: Epoch [25], Batch [405/938], Loss: 0.35644999146461487\n",
      "Train: Epoch [25], Batch [406/938], Loss: 0.2938171327114105\n",
      "Train: Epoch [25], Batch [407/938], Loss: 0.3849829137325287\n",
      "Train: Epoch [25], Batch [408/938], Loss: 0.2844051718711853\n",
      "Train: Epoch [25], Batch [409/938], Loss: 0.6219683885574341\n",
      "Train: Epoch [25], Batch [410/938], Loss: 0.39197495579719543\n",
      "Train: Epoch [25], Batch [411/938], Loss: 0.3875639736652374\n",
      "Train: Epoch [25], Batch [412/938], Loss: 0.4093497097492218\n",
      "Train: Epoch [25], Batch [413/938], Loss: 0.44385454058647156\n",
      "Train: Epoch [25], Batch [414/938], Loss: 0.4410746097564697\n",
      "Train: Epoch [25], Batch [415/938], Loss: 0.47543948888778687\n",
      "Train: Epoch [25], Batch [416/938], Loss: 0.4408970773220062\n",
      "Train: Epoch [25], Batch [417/938], Loss: 0.36400583386421204\n",
      "Train: Epoch [25], Batch [418/938], Loss: 0.3709658980369568\n",
      "Train: Epoch [25], Batch [419/938], Loss: 0.37077876925468445\n",
      "Train: Epoch [25], Batch [420/938], Loss: 0.4302138388156891\n",
      "Train: Epoch [25], Batch [421/938], Loss: 0.45541346073150635\n",
      "Train: Epoch [25], Batch [422/938], Loss: 0.5535386800765991\n",
      "Train: Epoch [25], Batch [423/938], Loss: 0.44521448016166687\n",
      "Train: Epoch [25], Batch [424/938], Loss: 0.306246280670166\n",
      "Train: Epoch [25], Batch [425/938], Loss: 0.3483063876628876\n",
      "Train: Epoch [25], Batch [426/938], Loss: 0.3333737254142761\n",
      "Train: Epoch [25], Batch [427/938], Loss: 0.47677183151245117\n",
      "Train: Epoch [25], Batch [428/938], Loss: 0.36025285720825195\n",
      "Train: Epoch [25], Batch [429/938], Loss: 0.549144983291626\n",
      "Train: Epoch [25], Batch [430/938], Loss: 0.4623675048351288\n",
      "Train: Epoch [25], Batch [431/938], Loss: 0.40638870000839233\n",
      "Train: Epoch [25], Batch [432/938], Loss: 0.4874950647354126\n",
      "Train: Epoch [25], Batch [433/938], Loss: 0.36936867237091064\n",
      "Train: Epoch [25], Batch [434/938], Loss: 0.42464014887809753\n",
      "Train: Epoch [25], Batch [435/938], Loss: 0.4598197638988495\n",
      "Train: Epoch [25], Batch [436/938], Loss: 0.3133743405342102\n",
      "Train: Epoch [25], Batch [437/938], Loss: 0.4480183720588684\n",
      "Train: Epoch [25], Batch [438/938], Loss: 0.40621763467788696\n",
      "Train: Epoch [25], Batch [439/938], Loss: 0.318242609500885\n",
      "Train: Epoch [25], Batch [440/938], Loss: 0.3810131549835205\n",
      "Train: Epoch [25], Batch [441/938], Loss: 0.32718151807785034\n",
      "Train: Epoch [25], Batch [442/938], Loss: 0.41747549176216125\n",
      "Train: Epoch [25], Batch [443/938], Loss: 0.27414950728416443\n",
      "Train: Epoch [25], Batch [444/938], Loss: 0.5187063813209534\n",
      "Train: Epoch [25], Batch [445/938], Loss: 0.3539979159832001\n",
      "Train: Epoch [25], Batch [446/938], Loss: 0.3881552219390869\n",
      "Train: Epoch [25], Batch [447/938], Loss: 0.4708639085292816\n",
      "Train: Epoch [25], Batch [448/938], Loss: 0.2744747996330261\n",
      "Train: Epoch [25], Batch [449/938], Loss: 0.3416734039783478\n",
      "Train: Epoch [25], Batch [450/938], Loss: 0.4693622887134552\n",
      "Train: Epoch [25], Batch [451/938], Loss: 0.33897921442985535\n",
      "Train: Epoch [25], Batch [452/938], Loss: 0.360039085149765\n",
      "Train: Epoch [25], Batch [453/938], Loss: 0.5615594387054443\n",
      "Train: Epoch [25], Batch [454/938], Loss: 0.4694996476173401\n",
      "Train: Epoch [25], Batch [455/938], Loss: 0.5262910723686218\n",
      "Train: Epoch [25], Batch [456/938], Loss: 0.34488630294799805\n",
      "Train: Epoch [25], Batch [457/938], Loss: 0.4612867832183838\n",
      "Train: Epoch [25], Batch [458/938], Loss: 0.3335595726966858\n",
      "Train: Epoch [25], Batch [459/938], Loss: 0.32014089822769165\n",
      "Train: Epoch [25], Batch [460/938], Loss: 0.26212286949157715\n",
      "Train: Epoch [25], Batch [461/938], Loss: 0.551748514175415\n",
      "Train: Epoch [25], Batch [462/938], Loss: 0.40583890676498413\n",
      "Train: Epoch [25], Batch [463/938], Loss: 0.6631640195846558\n",
      "Train: Epoch [25], Batch [464/938], Loss: 0.5483893156051636\n",
      "Train: Epoch [25], Batch [465/938], Loss: 0.4633270800113678\n",
      "Train: Epoch [25], Batch [466/938], Loss: 0.5222190022468567\n",
      "Train: Epoch [25], Batch [467/938], Loss: 0.49744483828544617\n",
      "Train: Epoch [25], Batch [468/938], Loss: 0.4961261749267578\n",
      "Train: Epoch [25], Batch [469/938], Loss: 0.38517138361930847\n",
      "Train: Epoch [25], Batch [470/938], Loss: 0.31492623686790466\n",
      "Train: Epoch [25], Batch [471/938], Loss: 0.3539893925189972\n",
      "Train: Epoch [25], Batch [472/938], Loss: 0.3783637583255768\n",
      "Train: Epoch [25], Batch [473/938], Loss: 0.47793832421302795\n",
      "Train: Epoch [25], Batch [474/938], Loss: 0.33840951323509216\n",
      "Train: Epoch [25], Batch [475/938], Loss: 0.5069020390510559\n",
      "Train: Epoch [25], Batch [476/938], Loss: 0.31815600395202637\n",
      "Train: Epoch [25], Batch [477/938], Loss: 0.6429504156112671\n",
      "Train: Epoch [25], Batch [478/938], Loss: 0.37169134616851807\n",
      "Train: Epoch [25], Batch [479/938], Loss: 0.47916534543037415\n",
      "Train: Epoch [25], Batch [480/938], Loss: 0.40086954832077026\n",
      "Train: Epoch [25], Batch [481/938], Loss: 0.4741573929786682\n",
      "Train: Epoch [25], Batch [482/938], Loss: 0.5263751745223999\n",
      "Train: Epoch [25], Batch [483/938], Loss: 0.3198537826538086\n",
      "Train: Epoch [25], Batch [484/938], Loss: 0.456459641456604\n",
      "Train: Epoch [25], Batch [485/938], Loss: 0.5725226402282715\n",
      "Train: Epoch [25], Batch [486/938], Loss: 0.35408592224121094\n",
      "Train: Epoch [25], Batch [487/938], Loss: 0.3766670227050781\n",
      "Train: Epoch [25], Batch [488/938], Loss: 0.4194943606853485\n",
      "Train: Epoch [25], Batch [489/938], Loss: 0.4051222801208496\n",
      "Train: Epoch [25], Batch [490/938], Loss: 0.46168357133865356\n",
      "Train: Epoch [25], Batch [491/938], Loss: 0.36704134941101074\n",
      "Train: Epoch [25], Batch [492/938], Loss: 0.41107383370399475\n",
      "Train: Epoch [25], Batch [493/938], Loss: 0.4131931662559509\n",
      "Train: Epoch [25], Batch [494/938], Loss: 0.4730466902256012\n",
      "Train: Epoch [25], Batch [495/938], Loss: 0.5152726173400879\n",
      "Train: Epoch [25], Batch [496/938], Loss: 0.32821905612945557\n",
      "Train: Epoch [25], Batch [497/938], Loss: 0.474628746509552\n",
      "Train: Epoch [25], Batch [498/938], Loss: 0.39924684166908264\n",
      "Train: Epoch [25], Batch [499/938], Loss: 0.5202327966690063\n",
      "Train: Epoch [25], Batch [500/938], Loss: 0.4282127618789673\n",
      "Train: Epoch [25], Batch [501/938], Loss: 0.35464397072792053\n",
      "Train: Epoch [25], Batch [502/938], Loss: 0.3216334879398346\n",
      "Train: Epoch [25], Batch [503/938], Loss: 0.4262705147266388\n",
      "Train: Epoch [25], Batch [504/938], Loss: 0.3339213728904724\n",
      "Train: Epoch [25], Batch [505/938], Loss: 0.4279212951660156\n",
      "Train: Epoch [25], Batch [506/938], Loss: 0.413149356842041\n",
      "Train: Epoch [25], Batch [507/938], Loss: 0.35291510820388794\n",
      "Train: Epoch [25], Batch [508/938], Loss: 0.28888559341430664\n",
      "Train: Epoch [25], Batch [509/938], Loss: 0.39315661787986755\n",
      "Train: Epoch [25], Batch [510/938], Loss: 0.5077072381973267\n",
      "Train: Epoch [25], Batch [511/938], Loss: 0.30516988039016724\n",
      "Train: Epoch [25], Batch [512/938], Loss: 0.4878077507019043\n",
      "Train: Epoch [25], Batch [513/938], Loss: 0.412298321723938\n",
      "Train: Epoch [25], Batch [514/938], Loss: 0.5114722847938538\n",
      "Train: Epoch [25], Batch [515/938], Loss: 0.7392799854278564\n",
      "Train: Epoch [25], Batch [516/938], Loss: 0.687146008014679\n",
      "Train: Epoch [25], Batch [517/938], Loss: 0.2610665559768677\n",
      "Train: Epoch [25], Batch [518/938], Loss: 0.2991860508918762\n",
      "Train: Epoch [25], Batch [519/938], Loss: 0.43596434593200684\n",
      "Train: Epoch [25], Batch [520/938], Loss: 0.4322623610496521\n",
      "Train: Epoch [25], Batch [521/938], Loss: 0.3926940858364105\n",
      "Train: Epoch [25], Batch [522/938], Loss: 0.3548937141895294\n",
      "Train: Epoch [25], Batch [523/938], Loss: 0.36454641819000244\n",
      "Train: Epoch [25], Batch [524/938], Loss: 0.3757029175758362\n",
      "Train: Epoch [25], Batch [525/938], Loss: 0.4114007353782654\n",
      "Train: Epoch [25], Batch [526/938], Loss: 0.4254780411720276\n",
      "Train: Epoch [25], Batch [527/938], Loss: 0.4279943108558655\n",
      "Train: Epoch [25], Batch [528/938], Loss: 0.45996275544166565\n",
      "Train: Epoch [25], Batch [529/938], Loss: 0.48839691281318665\n",
      "Train: Epoch [25], Batch [530/938], Loss: 0.3576233983039856\n",
      "Train: Epoch [25], Batch [531/938], Loss: 0.3848342299461365\n",
      "Train: Epoch [25], Batch [532/938], Loss: 0.5599830150604248\n",
      "Train: Epoch [25], Batch [533/938], Loss: 0.6333780288696289\n",
      "Train: Epoch [25], Batch [534/938], Loss: 0.601178765296936\n",
      "Train: Epoch [25], Batch [535/938], Loss: 0.22855691611766815\n",
      "Train: Epoch [25], Batch [536/938], Loss: 0.3411654531955719\n",
      "Train: Epoch [25], Batch [537/938], Loss: 0.4338473081588745\n",
      "Train: Epoch [25], Batch [538/938], Loss: 0.38702622056007385\n",
      "Train: Epoch [25], Batch [539/938], Loss: 0.29694557189941406\n",
      "Train: Epoch [25], Batch [540/938], Loss: 0.3573535680770874\n",
      "Train: Epoch [25], Batch [541/938], Loss: 0.5062916874885559\n",
      "Train: Epoch [25], Batch [542/938], Loss: 0.5893439650535583\n",
      "Train: Epoch [25], Batch [543/938], Loss: 0.438783198595047\n",
      "Train: Epoch [25], Batch [544/938], Loss: 0.3346928060054779\n",
      "Train: Epoch [25], Batch [545/938], Loss: 0.5658963918685913\n",
      "Train: Epoch [25], Batch [546/938], Loss: 0.37851467728614807\n",
      "Train: Epoch [25], Batch [547/938], Loss: 0.5378212332725525\n",
      "Train: Epoch [25], Batch [548/938], Loss: 0.44412627816200256\n",
      "Train: Epoch [25], Batch [549/938], Loss: 0.3974536657333374\n",
      "Train: Epoch [25], Batch [550/938], Loss: 0.3730713129043579\n",
      "Train: Epoch [25], Batch [551/938], Loss: 0.5923296213150024\n",
      "Train: Epoch [25], Batch [552/938], Loss: 0.5642208456993103\n",
      "Train: Epoch [25], Batch [553/938], Loss: 0.4034273326396942\n",
      "Train: Epoch [25], Batch [554/938], Loss: 0.2315666675567627\n",
      "Train: Epoch [25], Batch [555/938], Loss: 0.3713962733745575\n",
      "Train: Epoch [25], Batch [556/938], Loss: 0.5457839965820312\n",
      "Train: Epoch [25], Batch [557/938], Loss: 0.32920145988464355\n",
      "Train: Epoch [25], Batch [558/938], Loss: 0.40597206354141235\n",
      "Train: Epoch [25], Batch [559/938], Loss: 0.3801279366016388\n",
      "Train: Epoch [25], Batch [560/938], Loss: 0.44331493973731995\n",
      "Train: Epoch [25], Batch [561/938], Loss: 0.24706289172172546\n",
      "Train: Epoch [25], Batch [562/938], Loss: 0.38566452264785767\n",
      "Train: Epoch [25], Batch [563/938], Loss: 0.35392871499061584\n",
      "Train: Epoch [25], Batch [564/938], Loss: 0.6449540257453918\n",
      "Train: Epoch [25], Batch [565/938], Loss: 0.49303901195526123\n",
      "Train: Epoch [25], Batch [566/938], Loss: 0.4947107434272766\n",
      "Train: Epoch [25], Batch [567/938], Loss: 0.39384809136390686\n",
      "Train: Epoch [25], Batch [568/938], Loss: 0.2949724495410919\n",
      "Train: Epoch [25], Batch [569/938], Loss: 0.3321431577205658\n",
      "Train: Epoch [25], Batch [570/938], Loss: 0.2833040952682495\n",
      "Train: Epoch [25], Batch [571/938], Loss: 0.3271799087524414\n",
      "Train: Epoch [25], Batch [572/938], Loss: 0.38778921961784363\n",
      "Train: Epoch [25], Batch [573/938], Loss: 0.42047208547592163\n",
      "Train: Epoch [25], Batch [574/938], Loss: 0.5926275849342346\n",
      "Train: Epoch [25], Batch [575/938], Loss: 0.3740406930446625\n",
      "Train: Epoch [25], Batch [576/938], Loss: 0.5102816820144653\n",
      "Train: Epoch [25], Batch [577/938], Loss: 0.543181836605072\n",
      "Train: Epoch [25], Batch [578/938], Loss: 0.36330947279930115\n",
      "Train: Epoch [25], Batch [579/938], Loss: 0.3724532127380371\n",
      "Train: Epoch [25], Batch [580/938], Loss: 0.30057621002197266\n",
      "Train: Epoch [25], Batch [581/938], Loss: 0.4179624617099762\n",
      "Train: Epoch [25], Batch [582/938], Loss: 0.42352166771888733\n",
      "Train: Epoch [25], Batch [583/938], Loss: 0.545902669429779\n",
      "Train: Epoch [25], Batch [584/938], Loss: 0.4643850326538086\n",
      "Train: Epoch [25], Batch [585/938], Loss: 0.3535339832305908\n",
      "Train: Epoch [25], Batch [586/938], Loss: 0.2671172022819519\n",
      "Train: Epoch [25], Batch [587/938], Loss: 0.3207264840602875\n",
      "Train: Epoch [25], Batch [588/938], Loss: 0.44818612933158875\n",
      "Train: Epoch [25], Batch [589/938], Loss: 0.5783673524856567\n",
      "Train: Epoch [25], Batch [590/938], Loss: 0.3414265513420105\n",
      "Train: Epoch [25], Batch [591/938], Loss: 0.35656943917274475\n",
      "Train: Epoch [25], Batch [592/938], Loss: 0.3114018738269806\n",
      "Train: Epoch [25], Batch [593/938], Loss: 0.47254687547683716\n",
      "Train: Epoch [25], Batch [594/938], Loss: 0.4302314519882202\n",
      "Train: Epoch [25], Batch [595/938], Loss: 0.49803489446640015\n",
      "Train: Epoch [25], Batch [596/938], Loss: 0.4115026891231537\n",
      "Train: Epoch [25], Batch [597/938], Loss: 0.3798591196537018\n",
      "Train: Epoch [25], Batch [598/938], Loss: 0.3904193639755249\n",
      "Train: Epoch [25], Batch [599/938], Loss: 0.3510306477546692\n",
      "Train: Epoch [25], Batch [600/938], Loss: 0.40556448698043823\n",
      "Train: Epoch [25], Batch [601/938], Loss: 0.36498844623565674\n",
      "Train: Epoch [25], Batch [602/938], Loss: 0.5501149296760559\n",
      "Train: Epoch [25], Batch [603/938], Loss: 0.44470852613449097\n",
      "Train: Epoch [25], Batch [604/938], Loss: 0.3644627630710602\n",
      "Train: Epoch [25], Batch [605/938], Loss: 0.429614782333374\n",
      "Train: Epoch [25], Batch [606/938], Loss: 0.2796425521373749\n",
      "Train: Epoch [25], Batch [607/938], Loss: 0.5898676514625549\n",
      "Train: Epoch [25], Batch [608/938], Loss: 0.5038970112800598\n",
      "Train: Epoch [25], Batch [609/938], Loss: 0.24416890740394592\n",
      "Train: Epoch [25], Batch [610/938], Loss: 0.5084066987037659\n",
      "Train: Epoch [25], Batch [611/938], Loss: 0.4938548803329468\n",
      "Train: Epoch [25], Batch [612/938], Loss: 0.346391886472702\n",
      "Train: Epoch [25], Batch [613/938], Loss: 0.42075735330581665\n",
      "Train: Epoch [25], Batch [614/938], Loss: 0.48620277643203735\n",
      "Train: Epoch [25], Batch [615/938], Loss: 0.5701634883880615\n",
      "Train: Epoch [25], Batch [616/938], Loss: 0.6036734580993652\n",
      "Train: Epoch [25], Batch [617/938], Loss: 0.2541222870349884\n",
      "Train: Epoch [25], Batch [618/938], Loss: 0.37780076265335083\n",
      "Train: Epoch [25], Batch [619/938], Loss: 0.4213988184928894\n",
      "Train: Epoch [25], Batch [620/938], Loss: 0.21010175347328186\n",
      "Train: Epoch [25], Batch [621/938], Loss: 0.3785175085067749\n",
      "Train: Epoch [25], Batch [622/938], Loss: 0.5122973322868347\n",
      "Train: Epoch [25], Batch [623/938], Loss: 0.3144412636756897\n",
      "Train: Epoch [25], Batch [624/938], Loss: 0.39643245935440063\n",
      "Train: Epoch [25], Batch [625/938], Loss: 0.44868898391723633\n",
      "Train: Epoch [25], Batch [626/938], Loss: 0.33299946784973145\n",
      "Train: Epoch [25], Batch [627/938], Loss: 0.38365742564201355\n",
      "Train: Epoch [25], Batch [628/938], Loss: 0.31902918219566345\n",
      "Train: Epoch [25], Batch [629/938], Loss: 0.58933424949646\n",
      "Train: Epoch [25], Batch [630/938], Loss: 0.3333432376384735\n",
      "Train: Epoch [25], Batch [631/938], Loss: 0.49008071422576904\n",
      "Train: Epoch [25], Batch [632/938], Loss: 0.5224481821060181\n",
      "Train: Epoch [25], Batch [633/938], Loss: 0.4471384584903717\n",
      "Train: Epoch [25], Batch [634/938], Loss: 0.2716062664985657\n",
      "Train: Epoch [25], Batch [635/938], Loss: 0.31662362813949585\n",
      "Train: Epoch [25], Batch [636/938], Loss: 0.3695068359375\n",
      "Train: Epoch [25], Batch [637/938], Loss: 0.4020712971687317\n",
      "Train: Epoch [25], Batch [638/938], Loss: 0.48564812541007996\n",
      "Train: Epoch [25], Batch [639/938], Loss: 0.3430788516998291\n",
      "Train: Epoch [25], Batch [640/938], Loss: 0.4564319849014282\n",
      "Train: Epoch [25], Batch [641/938], Loss: 0.3536713719367981\n",
      "Train: Epoch [25], Batch [642/938], Loss: 0.22069041430950165\n",
      "Train: Epoch [25], Batch [643/938], Loss: 0.34112316370010376\n",
      "Train: Epoch [25], Batch [644/938], Loss: 0.3526897728443146\n",
      "Train: Epoch [25], Batch [645/938], Loss: 0.5576980113983154\n",
      "Train: Epoch [25], Batch [646/938], Loss: 0.2829585671424866\n",
      "Train: Epoch [25], Batch [647/938], Loss: 0.5404160022735596\n",
      "Train: Epoch [25], Batch [648/938], Loss: 0.3280218541622162\n",
      "Train: Epoch [25], Batch [649/938], Loss: 0.5380984544754028\n",
      "Train: Epoch [25], Batch [650/938], Loss: 0.3258243203163147\n",
      "Train: Epoch [25], Batch [651/938], Loss: 0.5530580878257751\n",
      "Train: Epoch [25], Batch [652/938], Loss: 0.560248613357544\n",
      "Train: Epoch [25], Batch [653/938], Loss: 0.4663226008415222\n",
      "Train: Epoch [25], Batch [654/938], Loss: 0.5371222496032715\n",
      "Train: Epoch [25], Batch [655/938], Loss: 0.30883097648620605\n",
      "Train: Epoch [25], Batch [656/938], Loss: 0.3143579363822937\n",
      "Train: Epoch [25], Batch [657/938], Loss: 0.8149943947792053\n",
      "Train: Epoch [25], Batch [658/938], Loss: 0.3197551965713501\n",
      "Train: Epoch [25], Batch [659/938], Loss: 0.22141389548778534\n",
      "Train: Epoch [25], Batch [660/938], Loss: 0.3214651644229889\n",
      "Train: Epoch [25], Batch [661/938], Loss: 0.3937431275844574\n",
      "Train: Epoch [25], Batch [662/938], Loss: 0.28426462411880493\n",
      "Train: Epoch [25], Batch [663/938], Loss: 0.42047926783561707\n",
      "Train: Epoch [25], Batch [664/938], Loss: 0.4108228385448456\n",
      "Train: Epoch [25], Batch [665/938], Loss: 0.46355170011520386\n",
      "Train: Epoch [25], Batch [666/938], Loss: 0.4367947578430176\n",
      "Train: Epoch [25], Batch [667/938], Loss: 0.3589097857475281\n",
      "Train: Epoch [25], Batch [668/938], Loss: 0.5536308288574219\n",
      "Train: Epoch [25], Batch [669/938], Loss: 0.6498028039932251\n",
      "Train: Epoch [25], Batch [670/938], Loss: 0.46058857440948486\n",
      "Train: Epoch [25], Batch [671/938], Loss: 0.43301963806152344\n",
      "Train: Epoch [25], Batch [672/938], Loss: 0.6029187440872192\n",
      "Train: Epoch [25], Batch [673/938], Loss: 0.6771151423454285\n",
      "Train: Epoch [25], Batch [674/938], Loss: 0.5018970370292664\n",
      "Train: Epoch [25], Batch [675/938], Loss: 0.29015204310417175\n",
      "Train: Epoch [25], Batch [676/938], Loss: 0.2207939177751541\n",
      "Train: Epoch [25], Batch [677/938], Loss: 0.36915045976638794\n",
      "Train: Epoch [25], Batch [678/938], Loss: 0.3444070518016815\n",
      "Train: Epoch [25], Batch [679/938], Loss: 0.4181586503982544\n",
      "Train: Epoch [25], Batch [680/938], Loss: 0.5735188126564026\n",
      "Train: Epoch [25], Batch [681/938], Loss: 0.3214681148529053\n",
      "Train: Epoch [25], Batch [682/938], Loss: 0.5290209054946899\n",
      "Train: Epoch [25], Batch [683/938], Loss: 0.4426936209201813\n",
      "Train: Epoch [25], Batch [684/938], Loss: 0.3542759120464325\n",
      "Train: Epoch [25], Batch [685/938], Loss: 0.3157656788825989\n",
      "Train: Epoch [25], Batch [686/938], Loss: 0.48522481322288513\n",
      "Train: Epoch [25], Batch [687/938], Loss: 0.32809188961982727\n",
      "Train: Epoch [25], Batch [688/938], Loss: 0.43278220295906067\n",
      "Train: Epoch [25], Batch [689/938], Loss: 0.6223795413970947\n",
      "Train: Epoch [25], Batch [690/938], Loss: 0.432476669549942\n",
      "Train: Epoch [25], Batch [691/938], Loss: 0.3518312871456146\n",
      "Train: Epoch [25], Batch [692/938], Loss: 0.8150098323822021\n",
      "Train: Epoch [25], Batch [693/938], Loss: 0.2828088700771332\n",
      "Train: Epoch [25], Batch [694/938], Loss: 0.4725605845451355\n",
      "Train: Epoch [25], Batch [695/938], Loss: 0.3649294674396515\n",
      "Train: Epoch [25], Batch [696/938], Loss: 0.4617805480957031\n",
      "Train: Epoch [25], Batch [697/938], Loss: 0.4474955201148987\n",
      "Train: Epoch [25], Batch [698/938], Loss: 0.4040400981903076\n",
      "Train: Epoch [25], Batch [699/938], Loss: 0.41918042302131653\n",
      "Train: Epoch [25], Batch [700/938], Loss: 0.5514498949050903\n",
      "Train: Epoch [25], Batch [701/938], Loss: 0.618986964225769\n",
      "Train: Epoch [25], Batch [702/938], Loss: 0.4830903112888336\n",
      "Train: Epoch [25], Batch [703/938], Loss: 0.3998053967952728\n",
      "Train: Epoch [25], Batch [704/938], Loss: 0.3644542992115021\n",
      "Train: Epoch [25], Batch [705/938], Loss: 0.6275453567504883\n",
      "Train: Epoch [25], Batch [706/938], Loss: 0.5352129340171814\n",
      "Train: Epoch [25], Batch [707/938], Loss: 0.38258615136146545\n",
      "Train: Epoch [25], Batch [708/938], Loss: 0.32567012310028076\n",
      "Train: Epoch [25], Batch [709/938], Loss: 0.5476086735725403\n",
      "Train: Epoch [25], Batch [710/938], Loss: 0.31966421008110046\n",
      "Train: Epoch [25], Batch [711/938], Loss: 0.4619978666305542\n",
      "Train: Epoch [25], Batch [712/938], Loss: 0.4826836884021759\n",
      "Train: Epoch [25], Batch [713/938], Loss: 0.5328908562660217\n",
      "Train: Epoch [25], Batch [714/938], Loss: 0.3642160892486572\n",
      "Train: Epoch [25], Batch [715/938], Loss: 0.3998453915119171\n",
      "Train: Epoch [25], Batch [716/938], Loss: 0.5234684348106384\n",
      "Train: Epoch [25], Batch [717/938], Loss: 0.2725266218185425\n",
      "Train: Epoch [25], Batch [718/938], Loss: 0.3652656674385071\n",
      "Train: Epoch [25], Batch [719/938], Loss: 0.3054230213165283\n",
      "Train: Epoch [25], Batch [720/938], Loss: 0.4283575713634491\n",
      "Train: Epoch [25], Batch [721/938], Loss: 0.44837281107902527\n",
      "Train: Epoch [25], Batch [722/938], Loss: 0.49243634939193726\n",
      "Train: Epoch [25], Batch [723/938], Loss: 0.418437123298645\n",
      "Train: Epoch [25], Batch [724/938], Loss: 0.5229207277297974\n",
      "Train: Epoch [25], Batch [725/938], Loss: 0.3189332187175751\n",
      "Train: Epoch [25], Batch [726/938], Loss: 0.43583130836486816\n",
      "Train: Epoch [25], Batch [727/938], Loss: 0.5324242115020752\n",
      "Train: Epoch [25], Batch [728/938], Loss: 0.28580161929130554\n",
      "Train: Epoch [25], Batch [729/938], Loss: 0.389372318983078\n",
      "Train: Epoch [25], Batch [730/938], Loss: 0.30615150928497314\n",
      "Train: Epoch [25], Batch [731/938], Loss: 0.2570623457431793\n",
      "Train: Epoch [25], Batch [732/938], Loss: 0.256969690322876\n",
      "Train: Epoch [25], Batch [733/938], Loss: 0.23670366406440735\n",
      "Train: Epoch [25], Batch [734/938], Loss: 0.3375554084777832\n",
      "Train: Epoch [25], Batch [735/938], Loss: 0.3157097399234772\n",
      "Train: Epoch [25], Batch [736/938], Loss: 0.3345717787742615\n",
      "Train: Epoch [25], Batch [737/938], Loss: 0.3182501792907715\n",
      "Train: Epoch [25], Batch [738/938], Loss: 0.39493948221206665\n",
      "Train: Epoch [25], Batch [739/938], Loss: 0.2148665487766266\n",
      "Train: Epoch [25], Batch [740/938], Loss: 0.37418490648269653\n",
      "Train: Epoch [25], Batch [741/938], Loss: 0.412819504737854\n",
      "Train: Epoch [25], Batch [742/938], Loss: 0.6701816916465759\n",
      "Train: Epoch [25], Batch [743/938], Loss: 0.23878253996372223\n",
      "Train: Epoch [25], Batch [744/938], Loss: 0.39174342155456543\n",
      "Train: Epoch [25], Batch [745/938], Loss: 0.29235604405403137\n",
      "Train: Epoch [25], Batch [746/938], Loss: 0.6249077916145325\n",
      "Train: Epoch [25], Batch [747/938], Loss: 0.4283788204193115\n",
      "Train: Epoch [25], Batch [748/938], Loss: 0.26692622900009155\n",
      "Train: Epoch [25], Batch [749/938], Loss: 0.4015677273273468\n",
      "Train: Epoch [25], Batch [750/938], Loss: 0.2671893537044525\n",
      "Train: Epoch [25], Batch [751/938], Loss: 0.43447571992874146\n",
      "Train: Epoch [25], Batch [752/938], Loss: 0.5119991302490234\n",
      "Train: Epoch [25], Batch [753/938], Loss: 0.5349406003952026\n",
      "Train: Epoch [25], Batch [754/938], Loss: 0.5264714956283569\n",
      "Train: Epoch [25], Batch [755/938], Loss: 0.3938472867012024\n",
      "Train: Epoch [25], Batch [756/938], Loss: 0.6504303812980652\n",
      "Train: Epoch [25], Batch [757/938], Loss: 0.3904363512992859\n",
      "Train: Epoch [25], Batch [758/938], Loss: 0.31510505080223083\n",
      "Train: Epoch [25], Batch [759/938], Loss: 0.4157133996486664\n",
      "Train: Epoch [25], Batch [760/938], Loss: 0.4385366141796112\n",
      "Train: Epoch [25], Batch [761/938], Loss: 0.2803196609020233\n",
      "Train: Epoch [25], Batch [762/938], Loss: 0.21808303892612457\n",
      "Train: Epoch [25], Batch [763/938], Loss: 0.2385183870792389\n",
      "Train: Epoch [25], Batch [764/938], Loss: 0.4620348811149597\n",
      "Train: Epoch [25], Batch [765/938], Loss: 0.3793930411338806\n",
      "Train: Epoch [25], Batch [766/938], Loss: 0.45141369104385376\n",
      "Train: Epoch [25], Batch [767/938], Loss: 0.45316600799560547\n",
      "Train: Epoch [25], Batch [768/938], Loss: 0.5525241494178772\n",
      "Train: Epoch [25], Batch [769/938], Loss: 0.3610754609107971\n",
      "Train: Epoch [25], Batch [770/938], Loss: 0.41419944167137146\n",
      "Train: Epoch [25], Batch [771/938], Loss: 0.362842857837677\n",
      "Train: Epoch [25], Batch [772/938], Loss: 0.46115753054618835\n",
      "Train: Epoch [25], Batch [773/938], Loss: 0.5363026261329651\n",
      "Train: Epoch [25], Batch [774/938], Loss: 0.24797086417675018\n",
      "Train: Epoch [25], Batch [775/938], Loss: 0.5890890955924988\n",
      "Train: Epoch [25], Batch [776/938], Loss: 0.314579039812088\n",
      "Train: Epoch [25], Batch [777/938], Loss: 0.3769497871398926\n",
      "Train: Epoch [25], Batch [778/938], Loss: 0.48953020572662354\n",
      "Train: Epoch [25], Batch [779/938], Loss: 0.49014726281166077\n",
      "Train: Epoch [25], Batch [780/938], Loss: 0.3633342981338501\n",
      "Train: Epoch [25], Batch [781/938], Loss: 0.5896669030189514\n",
      "Train: Epoch [25], Batch [782/938], Loss: 0.32633259892463684\n",
      "Train: Epoch [25], Batch [783/938], Loss: 0.28301477432250977\n",
      "Train: Epoch [25], Batch [784/938], Loss: 0.3780728876590729\n",
      "Train: Epoch [25], Batch [785/938], Loss: 0.3504635691642761\n",
      "Train: Epoch [25], Batch [786/938], Loss: 0.35429638624191284\n",
      "Train: Epoch [25], Batch [787/938], Loss: 0.4064001739025116\n",
      "Train: Epoch [25], Batch [788/938], Loss: 0.41313183307647705\n",
      "Train: Epoch [25], Batch [789/938], Loss: 0.43305790424346924\n",
      "Train: Epoch [25], Batch [790/938], Loss: 0.4697776138782501\n",
      "Train: Epoch [25], Batch [791/938], Loss: 0.5634353756904602\n",
      "Train: Epoch [25], Batch [792/938], Loss: 0.3960399925708771\n",
      "Train: Epoch [25], Batch [793/938], Loss: 0.3750099241733551\n",
      "Train: Epoch [25], Batch [794/938], Loss: 0.4275495707988739\n",
      "Train: Epoch [25], Batch [795/938], Loss: 0.3484989404678345\n",
      "Train: Epoch [25], Batch [796/938], Loss: 0.3953244984149933\n",
      "Train: Epoch [25], Batch [797/938], Loss: 0.4478015601634979\n",
      "Train: Epoch [25], Batch [798/938], Loss: 0.34936949610710144\n",
      "Train: Epoch [25], Batch [799/938], Loss: 0.3793538510799408\n",
      "Train: Epoch [25], Batch [800/938], Loss: 0.2799668312072754\n",
      "Train: Epoch [25], Batch [801/938], Loss: 0.2610032856464386\n",
      "Train: Epoch [25], Batch [802/938], Loss: 0.37243905663490295\n",
      "Train: Epoch [25], Batch [803/938], Loss: 0.3715136647224426\n",
      "Train: Epoch [25], Batch [804/938], Loss: 0.43169963359832764\n",
      "Train: Epoch [25], Batch [805/938], Loss: 0.49296894669532776\n",
      "Train: Epoch [25], Batch [806/938], Loss: 0.3940204083919525\n",
      "Train: Epoch [25], Batch [807/938], Loss: 0.5331850647926331\n",
      "Train: Epoch [25], Batch [808/938], Loss: 0.49472862482070923\n",
      "Train: Epoch [25], Batch [809/938], Loss: 0.4203779101371765\n",
      "Train: Epoch [25], Batch [810/938], Loss: 0.27991482615470886\n",
      "Train: Epoch [25], Batch [811/938], Loss: 0.5201292634010315\n",
      "Train: Epoch [25], Batch [812/938], Loss: 0.38732513785362244\n",
      "Train: Epoch [25], Batch [813/938], Loss: 0.23923544585704803\n",
      "Train: Epoch [25], Batch [814/938], Loss: 0.39715462923049927\n",
      "Train: Epoch [25], Batch [815/938], Loss: 0.5303125381469727\n",
      "Train: Epoch [25], Batch [816/938], Loss: 0.4282037913799286\n",
      "Train: Epoch [25], Batch [817/938], Loss: 0.2774442732334137\n",
      "Train: Epoch [25], Batch [818/938], Loss: 0.46119681000709534\n",
      "Train: Epoch [25], Batch [819/938], Loss: 0.2786174416542053\n",
      "Train: Epoch [25], Batch [820/938], Loss: 0.41789960861206055\n",
      "Train: Epoch [25], Batch [821/938], Loss: 0.3926771283149719\n",
      "Train: Epoch [25], Batch [822/938], Loss: 0.27682259678840637\n",
      "Train: Epoch [25], Batch [823/938], Loss: 0.5188049077987671\n",
      "Train: Epoch [25], Batch [824/938], Loss: 0.4004584550857544\n",
      "Train: Epoch [25], Batch [825/938], Loss: 0.36036038398742676\n",
      "Train: Epoch [25], Batch [826/938], Loss: 0.33700406551361084\n",
      "Train: Epoch [25], Batch [827/938], Loss: 0.4888043999671936\n",
      "Train: Epoch [25], Batch [828/938], Loss: 0.28738510608673096\n",
      "Train: Epoch [25], Batch [829/938], Loss: 0.5112510919570923\n",
      "Train: Epoch [25], Batch [830/938], Loss: 0.589255690574646\n",
      "Train: Epoch [25], Batch [831/938], Loss: 0.26187029480934143\n",
      "Train: Epoch [25], Batch [832/938], Loss: 0.5861468315124512\n",
      "Train: Epoch [25], Batch [833/938], Loss: 0.3111095130443573\n",
      "Train: Epoch [25], Batch [834/938], Loss: 0.3404390811920166\n",
      "Train: Epoch [25], Batch [835/938], Loss: 0.4642663598060608\n",
      "Train: Epoch [25], Batch [836/938], Loss: 0.5356164574623108\n",
      "Train: Epoch [25], Batch [837/938], Loss: 0.45349395275115967\n",
      "Train: Epoch [25], Batch [838/938], Loss: 0.42596012353897095\n",
      "Train: Epoch [25], Batch [839/938], Loss: 0.46697333455085754\n",
      "Train: Epoch [25], Batch [840/938], Loss: 0.3794674873352051\n",
      "Train: Epoch [25], Batch [841/938], Loss: 0.301025390625\n",
      "Train: Epoch [25], Batch [842/938], Loss: 0.2993032932281494\n",
      "Train: Epoch [25], Batch [843/938], Loss: 0.5689511299133301\n",
      "Train: Epoch [25], Batch [844/938], Loss: 0.4051758646965027\n",
      "Train: Epoch [25], Batch [845/938], Loss: 0.25232255458831787\n",
      "Train: Epoch [25], Batch [846/938], Loss: 0.39418143033981323\n",
      "Train: Epoch [25], Batch [847/938], Loss: 0.6156182289123535\n",
      "Train: Epoch [25], Batch [848/938], Loss: 0.5605423450469971\n",
      "Train: Epoch [25], Batch [849/938], Loss: 0.4063074290752411\n",
      "Train: Epoch [25], Batch [850/938], Loss: 0.48225146532058716\n",
      "Train: Epoch [25], Batch [851/938], Loss: 0.3964344263076782\n",
      "Train: Epoch [25], Batch [852/938], Loss: 0.33492565155029297\n",
      "Train: Epoch [25], Batch [853/938], Loss: 0.29900214076042175\n",
      "Train: Epoch [25], Batch [854/938], Loss: 0.3688531816005707\n",
      "Train: Epoch [25], Batch [855/938], Loss: 0.28597497940063477\n",
      "Train: Epoch [25], Batch [856/938], Loss: 0.441876083612442\n",
      "Train: Epoch [25], Batch [857/938], Loss: 0.37751150131225586\n",
      "Train: Epoch [25], Batch [858/938], Loss: 0.47912588715553284\n",
      "Train: Epoch [25], Batch [859/938], Loss: 0.26214995980262756\n",
      "Train: Epoch [25], Batch [860/938], Loss: 0.5280574560165405\n",
      "Train: Epoch [25], Batch [861/938], Loss: 0.43166467547416687\n",
      "Train: Epoch [25], Batch [862/938], Loss: 0.5381014347076416\n",
      "Train: Epoch [25], Batch [863/938], Loss: 0.462530255317688\n",
      "Train: Epoch [25], Batch [864/938], Loss: 0.35484811663627625\n",
      "Train: Epoch [25], Batch [865/938], Loss: 0.3106718063354492\n",
      "Train: Epoch [25], Batch [866/938], Loss: 0.3511885702610016\n",
      "Train: Epoch [25], Batch [867/938], Loss: 0.4864087700843811\n",
      "Train: Epoch [25], Batch [868/938], Loss: 0.27398234605789185\n",
      "Train: Epoch [25], Batch [869/938], Loss: 0.4042641818523407\n",
      "Train: Epoch [25], Batch [870/938], Loss: 0.6948257684707642\n",
      "Train: Epoch [25], Batch [871/938], Loss: 0.2982015907764435\n",
      "Train: Epoch [25], Batch [872/938], Loss: 0.5430675745010376\n",
      "Train: Epoch [25], Batch [873/938], Loss: 0.6733843684196472\n",
      "Train: Epoch [25], Batch [874/938], Loss: 0.4592578411102295\n",
      "Train: Epoch [25], Batch [875/938], Loss: 0.3894078731536865\n",
      "Train: Epoch [25], Batch [876/938], Loss: 0.5743119716644287\n",
      "Train: Epoch [25], Batch [877/938], Loss: 0.4719353914260864\n",
      "Train: Epoch [25], Batch [878/938], Loss: 0.2983001470565796\n",
      "Train: Epoch [25], Batch [879/938], Loss: 0.3322323262691498\n",
      "Train: Epoch [25], Batch [880/938], Loss: 0.3598648011684418\n",
      "Train: Epoch [25], Batch [881/938], Loss: 0.2901034951210022\n",
      "Train: Epoch [25], Batch [882/938], Loss: 0.38965705037117004\n",
      "Train: Epoch [25], Batch [883/938], Loss: 0.2620391249656677\n",
      "Train: Epoch [25], Batch [884/938], Loss: 0.3357471227645874\n",
      "Train: Epoch [25], Batch [885/938], Loss: 0.22419579327106476\n",
      "Train: Epoch [25], Batch [886/938], Loss: 0.5792832374572754\n",
      "Train: Epoch [25], Batch [887/938], Loss: 0.42778798937797546\n",
      "Train: Epoch [25], Batch [888/938], Loss: 0.4722940921783447\n",
      "Train: Epoch [25], Batch [889/938], Loss: 0.31135016679763794\n",
      "Train: Epoch [25], Batch [890/938], Loss: 0.2989453673362732\n",
      "Train: Epoch [25], Batch [891/938], Loss: 0.32009345293045044\n",
      "Train: Epoch [25], Batch [892/938], Loss: 0.4151102602481842\n",
      "Train: Epoch [25], Batch [893/938], Loss: 0.31785741448402405\n",
      "Train: Epoch [25], Batch [894/938], Loss: 0.5448722243309021\n",
      "Train: Epoch [25], Batch [895/938], Loss: 0.6623877286911011\n",
      "Train: Epoch [25], Batch [896/938], Loss: 0.3181823194026947\n",
      "Train: Epoch [25], Batch [897/938], Loss: 0.40060192346572876\n",
      "Train: Epoch [25], Batch [898/938], Loss: 0.24409359693527222\n",
      "Train: Epoch [25], Batch [899/938], Loss: 0.3738986849784851\n",
      "Train: Epoch [25], Batch [900/938], Loss: 0.3479449450969696\n",
      "Train: Epoch [25], Batch [901/938], Loss: 0.3698747158050537\n",
      "Train: Epoch [25], Batch [902/938], Loss: 0.49653273820877075\n",
      "Train: Epoch [25], Batch [903/938], Loss: 0.45944035053253174\n",
      "Train: Epoch [25], Batch [904/938], Loss: 0.3912332057952881\n",
      "Train: Epoch [25], Batch [905/938], Loss: 0.5241149067878723\n",
      "Train: Epoch [25], Batch [906/938], Loss: 0.36859065294265747\n",
      "Train: Epoch [25], Batch [907/938], Loss: 0.3584401607513428\n",
      "Train: Epoch [25], Batch [908/938], Loss: 0.5712185502052307\n",
      "Train: Epoch [25], Batch [909/938], Loss: 0.275718629360199\n",
      "Train: Epoch [25], Batch [910/938], Loss: 0.3406963646411896\n",
      "Train: Epoch [25], Batch [911/938], Loss: 0.743517279624939\n",
      "Train: Epoch [25], Batch [912/938], Loss: 0.39481157064437866\n",
      "Train: Epoch [25], Batch [913/938], Loss: 0.5186406373977661\n",
      "Train: Epoch [25], Batch [914/938], Loss: 0.25229066610336304\n",
      "Train: Epoch [25], Batch [915/938], Loss: 0.3699088394641876\n",
      "Train: Epoch [25], Batch [916/938], Loss: 0.3632908761501312\n",
      "Train: Epoch [25], Batch [917/938], Loss: 0.4202830493450165\n",
      "Train: Epoch [25], Batch [918/938], Loss: 0.44444555044174194\n",
      "Train: Epoch [25], Batch [919/938], Loss: 0.6018633246421814\n",
      "Train: Epoch [25], Batch [920/938], Loss: 0.5013409852981567\n",
      "Train: Epoch [25], Batch [921/938], Loss: 0.39557385444641113\n",
      "Train: Epoch [25], Batch [922/938], Loss: 0.3849719166755676\n",
      "Train: Epoch [25], Batch [923/938], Loss: 0.3577737510204315\n",
      "Train: Epoch [25], Batch [924/938], Loss: 0.34763121604919434\n",
      "Train: Epoch [25], Batch [925/938], Loss: 0.4223851263523102\n",
      "Train: Epoch [25], Batch [926/938], Loss: 0.36399903893470764\n",
      "Train: Epoch [25], Batch [927/938], Loss: 0.2469259649515152\n",
      "Train: Epoch [25], Batch [928/938], Loss: 0.2863391041755676\n",
      "Train: Epoch [25], Batch [929/938], Loss: 0.41627249121665955\n",
      "Train: Epoch [25], Batch [930/938], Loss: 0.5042616128921509\n",
      "Train: Epoch [25], Batch [931/938], Loss: 0.39552515745162964\n",
      "Train: Epoch [25], Batch [932/938], Loss: 0.4784848093986511\n",
      "Train: Epoch [25], Batch [933/938], Loss: 0.40766841173171997\n",
      "Train: Epoch [25], Batch [934/938], Loss: 0.35365793108940125\n",
      "Train: Epoch [25], Batch [935/938], Loss: 0.42258134484291077\n",
      "Train: Epoch [25], Batch [936/938], Loss: 0.33802321553230286\n",
      "Train: Epoch [25], Batch [937/938], Loss: 0.40285724401474\n",
      "Train: Epoch [25], Batch [938/938], Loss: 0.42509907484054565\n",
      "Accuracy of train set: 0.8555\n",
      "Validation: Epoch [25], Batch [1/938], Loss: 0.42498886585235596\n",
      "Validation: Epoch [25], Batch [2/938], Loss: 0.33941036462783813\n",
      "Validation: Epoch [25], Batch [3/938], Loss: 0.5080387592315674\n",
      "Validation: Epoch [25], Batch [4/938], Loss: 0.287841796875\n",
      "Validation: Epoch [25], Batch [5/938], Loss: 0.3702583909034729\n",
      "Validation: Epoch [25], Batch [6/938], Loss: 0.5822482109069824\n",
      "Validation: Epoch [25], Batch [7/938], Loss: 0.3164045810699463\n",
      "Validation: Epoch [25], Batch [8/938], Loss: 0.4632750451564789\n",
      "Validation: Epoch [25], Batch [9/938], Loss: 0.4123639464378357\n",
      "Validation: Epoch [25], Batch [10/938], Loss: 0.527337372303009\n",
      "Validation: Epoch [25], Batch [11/938], Loss: 0.2853934168815613\n",
      "Validation: Epoch [25], Batch [12/938], Loss: 0.4672786593437195\n",
      "Validation: Epoch [25], Batch [13/938], Loss: 0.5864173769950867\n",
      "Validation: Epoch [25], Batch [14/938], Loss: 0.3448322117328644\n",
      "Validation: Epoch [25], Batch [15/938], Loss: 0.42947354912757874\n",
      "Validation: Epoch [25], Batch [16/938], Loss: 0.334904283285141\n",
      "Validation: Epoch [25], Batch [17/938], Loss: 0.3760451376438141\n",
      "Validation: Epoch [25], Batch [18/938], Loss: 0.5188654065132141\n",
      "Validation: Epoch [25], Batch [19/938], Loss: 0.528564989566803\n",
      "Validation: Epoch [25], Batch [20/938], Loss: 0.3451308012008667\n",
      "Validation: Epoch [25], Batch [21/938], Loss: 0.39790597558021545\n",
      "Validation: Epoch [25], Batch [22/938], Loss: 0.4395456612110138\n",
      "Validation: Epoch [25], Batch [23/938], Loss: 0.38872426748275757\n",
      "Validation: Epoch [25], Batch [24/938], Loss: 0.49541598558425903\n",
      "Validation: Epoch [25], Batch [25/938], Loss: 0.36608392000198364\n",
      "Validation: Epoch [25], Batch [26/938], Loss: 0.3453991711139679\n",
      "Validation: Epoch [25], Batch [27/938], Loss: 0.4139251112937927\n",
      "Validation: Epoch [25], Batch [28/938], Loss: 0.42916053533554077\n",
      "Validation: Epoch [25], Batch [29/938], Loss: 0.3296729624271393\n",
      "Validation: Epoch [25], Batch [30/938], Loss: 0.2579181492328644\n",
      "Validation: Epoch [25], Batch [31/938], Loss: 0.13733015954494476\n",
      "Validation: Epoch [25], Batch [32/938], Loss: 0.41240042448043823\n",
      "Validation: Epoch [25], Batch [33/938], Loss: 0.3667481243610382\n",
      "Validation: Epoch [25], Batch [34/938], Loss: 0.5072040557861328\n",
      "Validation: Epoch [25], Batch [35/938], Loss: 0.5243396759033203\n",
      "Validation: Epoch [25], Batch [36/938], Loss: 0.5921815037727356\n",
      "Validation: Epoch [25], Batch [37/938], Loss: 0.3381306231021881\n",
      "Validation: Epoch [25], Batch [38/938], Loss: 0.20854610204696655\n",
      "Validation: Epoch [25], Batch [39/938], Loss: 0.29472479224205017\n",
      "Validation: Epoch [25], Batch [40/938], Loss: 0.37430843710899353\n",
      "Validation: Epoch [25], Batch [41/938], Loss: 0.39584869146347046\n",
      "Validation: Epoch [25], Batch [42/938], Loss: 0.5563050508499146\n",
      "Validation: Epoch [25], Batch [43/938], Loss: 0.4707149565219879\n",
      "Validation: Epoch [25], Batch [44/938], Loss: 0.48682713508605957\n",
      "Validation: Epoch [25], Batch [45/938], Loss: 0.34608152508735657\n",
      "Validation: Epoch [25], Batch [46/938], Loss: 0.38469478487968445\n",
      "Validation: Epoch [25], Batch [47/938], Loss: 0.5190642476081848\n",
      "Validation: Epoch [25], Batch [48/938], Loss: 0.42098569869995117\n",
      "Validation: Epoch [25], Batch [49/938], Loss: 0.3434772193431854\n",
      "Validation: Epoch [25], Batch [50/938], Loss: 0.5010971426963806\n",
      "Validation: Epoch [25], Batch [51/938], Loss: 0.41850319504737854\n",
      "Validation: Epoch [25], Batch [52/938], Loss: 0.5889712572097778\n",
      "Validation: Epoch [25], Batch [53/938], Loss: 0.41435569524765015\n",
      "Validation: Epoch [25], Batch [54/938], Loss: 0.3597763180732727\n",
      "Validation: Epoch [25], Batch [55/938], Loss: 0.5546107292175293\n",
      "Validation: Epoch [25], Batch [56/938], Loss: 0.2996370494365692\n",
      "Validation: Epoch [25], Batch [57/938], Loss: 0.3039887845516205\n",
      "Validation: Epoch [25], Batch [58/938], Loss: 0.4609314501285553\n",
      "Validation: Epoch [25], Batch [59/938], Loss: 0.469853937625885\n",
      "Validation: Epoch [25], Batch [60/938], Loss: 0.39142122864723206\n",
      "Validation: Epoch [25], Batch [61/938], Loss: 0.6159058213233948\n",
      "Validation: Epoch [25], Batch [62/938], Loss: 0.26534634828567505\n",
      "Validation: Epoch [25], Batch [63/938], Loss: 0.27343493700027466\n",
      "Validation: Epoch [25], Batch [64/938], Loss: 0.30994629859924316\n",
      "Validation: Epoch [25], Batch [65/938], Loss: 0.41127994656562805\n",
      "Validation: Epoch [25], Batch [66/938], Loss: 0.44455984234809875\n",
      "Validation: Epoch [25], Batch [67/938], Loss: 0.5067607760429382\n",
      "Validation: Epoch [25], Batch [68/938], Loss: 0.3132765591144562\n",
      "Validation: Epoch [25], Batch [69/938], Loss: 0.4935269355773926\n",
      "Validation: Epoch [25], Batch [70/938], Loss: 0.39455312490463257\n",
      "Validation: Epoch [25], Batch [71/938], Loss: 0.5723837614059448\n",
      "Validation: Epoch [25], Batch [72/938], Loss: 0.38508740067481995\n",
      "Validation: Epoch [25], Batch [73/938], Loss: 0.28865373134613037\n",
      "Validation: Epoch [25], Batch [74/938], Loss: 0.45803093910217285\n",
      "Validation: Epoch [25], Batch [75/938], Loss: 0.5321317315101624\n",
      "Validation: Epoch [25], Batch [76/938], Loss: 0.6370187997817993\n",
      "Validation: Epoch [25], Batch [77/938], Loss: 0.3667251467704773\n",
      "Validation: Epoch [25], Batch [78/938], Loss: 0.4277735948562622\n",
      "Validation: Epoch [25], Batch [79/938], Loss: 0.5715656876564026\n",
      "Validation: Epoch [25], Batch [80/938], Loss: 0.21058061718940735\n",
      "Validation: Epoch [25], Batch [81/938], Loss: 0.2865443229675293\n",
      "Validation: Epoch [25], Batch [82/938], Loss: 0.468345046043396\n",
      "Validation: Epoch [25], Batch [83/938], Loss: 0.26056456565856934\n",
      "Validation: Epoch [25], Batch [84/938], Loss: 0.31474918127059937\n",
      "Validation: Epoch [25], Batch [85/938], Loss: 0.5728792548179626\n",
      "Validation: Epoch [25], Batch [86/938], Loss: 0.3927505910396576\n",
      "Validation: Epoch [25], Batch [87/938], Loss: 0.3181355595588684\n",
      "Validation: Epoch [25], Batch [88/938], Loss: 0.357585608959198\n",
      "Validation: Epoch [25], Batch [89/938], Loss: 0.5148029327392578\n",
      "Validation: Epoch [25], Batch [90/938], Loss: 0.45043516159057617\n",
      "Validation: Epoch [25], Batch [91/938], Loss: 0.30207446217536926\n",
      "Validation: Epoch [25], Batch [92/938], Loss: 0.43936270475387573\n",
      "Validation: Epoch [25], Batch [93/938], Loss: 0.3975721597671509\n",
      "Validation: Epoch [25], Batch [94/938], Loss: 0.37407633662223816\n",
      "Validation: Epoch [25], Batch [95/938], Loss: 0.3433125615119934\n",
      "Validation: Epoch [25], Batch [96/938], Loss: 0.5665919184684753\n",
      "Validation: Epoch [25], Batch [97/938], Loss: 0.3974127173423767\n",
      "Validation: Epoch [25], Batch [98/938], Loss: 0.3456873595714569\n",
      "Validation: Epoch [25], Batch [99/938], Loss: 0.48554542660713196\n",
      "Validation: Epoch [25], Batch [100/938], Loss: 0.34329846501350403\n",
      "Validation: Epoch [25], Batch [101/938], Loss: 0.3928309977054596\n",
      "Validation: Epoch [25], Batch [102/938], Loss: 0.31984761357307434\n",
      "Validation: Epoch [25], Batch [103/938], Loss: 0.5371030569076538\n",
      "Validation: Epoch [25], Batch [104/938], Loss: 0.5550004839897156\n",
      "Validation: Epoch [25], Batch [105/938], Loss: 0.3484918177127838\n",
      "Validation: Epoch [25], Batch [106/938], Loss: 0.47161194682121277\n",
      "Validation: Epoch [25], Batch [107/938], Loss: 0.40173807740211487\n",
      "Validation: Epoch [25], Batch [108/938], Loss: 0.48827114701271057\n",
      "Validation: Epoch [25], Batch [109/938], Loss: 0.7009328603744507\n",
      "Validation: Epoch [25], Batch [110/938], Loss: 0.44706928730010986\n",
      "Validation: Epoch [25], Batch [111/938], Loss: 0.32953017950057983\n",
      "Validation: Epoch [25], Batch [112/938], Loss: 0.436147540807724\n",
      "Validation: Epoch [25], Batch [113/938], Loss: 0.32838788628578186\n",
      "Validation: Epoch [25], Batch [114/938], Loss: 0.4595904052257538\n",
      "Validation: Epoch [25], Batch [115/938], Loss: 0.292170912027359\n",
      "Validation: Epoch [25], Batch [116/938], Loss: 0.5079008340835571\n",
      "Validation: Epoch [25], Batch [117/938], Loss: 0.444031685590744\n",
      "Validation: Epoch [25], Batch [118/938], Loss: 0.49043452739715576\n",
      "Validation: Epoch [25], Batch [119/938], Loss: 0.24426224827766418\n",
      "Validation: Epoch [25], Batch [120/938], Loss: 0.3804117441177368\n",
      "Validation: Epoch [25], Batch [121/938], Loss: 0.5097065567970276\n",
      "Validation: Epoch [25], Batch [122/938], Loss: 0.48408421874046326\n",
      "Validation: Epoch [25], Batch [123/938], Loss: 0.3665506839752197\n",
      "Validation: Epoch [25], Batch [124/938], Loss: 0.38761842250823975\n",
      "Validation: Epoch [25], Batch [125/938], Loss: 0.45833271741867065\n",
      "Validation: Epoch [25], Batch [126/938], Loss: 0.5263549089431763\n",
      "Validation: Epoch [25], Batch [127/938], Loss: 0.20300941169261932\n",
      "Validation: Epoch [25], Batch [128/938], Loss: 0.5497223734855652\n",
      "Validation: Epoch [25], Batch [129/938], Loss: 0.4986547529697418\n",
      "Validation: Epoch [25], Batch [130/938], Loss: 0.37055736780166626\n",
      "Validation: Epoch [25], Batch [131/938], Loss: 0.4479869604110718\n",
      "Validation: Epoch [25], Batch [132/938], Loss: 0.4895910322666168\n",
      "Validation: Epoch [25], Batch [133/938], Loss: 0.5430383682250977\n",
      "Validation: Epoch [25], Batch [134/938], Loss: 0.5810689926147461\n",
      "Validation: Epoch [25], Batch [135/938], Loss: 0.44493013620376587\n",
      "Validation: Epoch [25], Batch [136/938], Loss: 0.4518125057220459\n",
      "Validation: Epoch [25], Batch [137/938], Loss: 0.41166892647743225\n",
      "Validation: Epoch [25], Batch [138/938], Loss: 0.36881715059280396\n",
      "Validation: Epoch [25], Batch [139/938], Loss: 0.31006503105163574\n",
      "Validation: Epoch [25], Batch [140/938], Loss: 0.323739618062973\n",
      "Validation: Epoch [25], Batch [141/938], Loss: 0.30011942982673645\n",
      "Validation: Epoch [25], Batch [142/938], Loss: 0.5837332606315613\n",
      "Validation: Epoch [25], Batch [143/938], Loss: 0.45376497507095337\n",
      "Validation: Epoch [25], Batch [144/938], Loss: 0.4817284345626831\n",
      "Validation: Epoch [25], Batch [145/938], Loss: 0.46648553013801575\n",
      "Validation: Epoch [25], Batch [146/938], Loss: 0.42574527859687805\n",
      "Validation: Epoch [25], Batch [147/938], Loss: 0.4115227162837982\n",
      "Validation: Epoch [25], Batch [148/938], Loss: 0.6929695010185242\n",
      "Validation: Epoch [25], Batch [149/938], Loss: 0.6585506796836853\n",
      "Validation: Epoch [25], Batch [150/938], Loss: 0.3566282391548157\n",
      "Validation: Epoch [25], Batch [151/938], Loss: 0.2897304594516754\n",
      "Validation: Epoch [25], Batch [152/938], Loss: 0.5205135345458984\n",
      "Validation: Epoch [25], Batch [153/938], Loss: 0.39638203382492065\n",
      "Validation: Epoch [25], Batch [154/938], Loss: 0.31085699796676636\n",
      "Validation: Epoch [25], Batch [155/938], Loss: 0.5216198563575745\n",
      "Validation: Epoch [25], Batch [156/938], Loss: 0.3734321892261505\n",
      "Validation: Epoch [25], Batch [157/938], Loss: 0.3076546788215637\n",
      "Validation: Epoch [25], Batch [158/938], Loss: 0.3922143280506134\n",
      "Validation: Epoch [25], Batch [159/938], Loss: 0.3420575261116028\n",
      "Validation: Epoch [25], Batch [160/938], Loss: 0.4155479371547699\n",
      "Validation: Epoch [25], Batch [161/938], Loss: 0.512872040271759\n",
      "Validation: Epoch [25], Batch [162/938], Loss: 0.22998365759849548\n",
      "Validation: Epoch [25], Batch [163/938], Loss: 0.5500608682632446\n",
      "Validation: Epoch [25], Batch [164/938], Loss: 0.3995799422264099\n",
      "Validation: Epoch [25], Batch [165/938], Loss: 0.2522338628768921\n",
      "Validation: Epoch [25], Batch [166/938], Loss: 0.3341047763824463\n",
      "Validation: Epoch [25], Batch [167/938], Loss: 0.26333630084991455\n",
      "Validation: Epoch [25], Batch [168/938], Loss: 0.20494352281093597\n",
      "Validation: Epoch [25], Batch [169/938], Loss: 0.45208606123924255\n",
      "Validation: Epoch [25], Batch [170/938], Loss: 0.3362291753292084\n",
      "Validation: Epoch [25], Batch [171/938], Loss: 0.5118242502212524\n",
      "Validation: Epoch [25], Batch [172/938], Loss: 0.4786164164543152\n",
      "Validation: Epoch [25], Batch [173/938], Loss: 0.5184510946273804\n",
      "Validation: Epoch [25], Batch [174/938], Loss: 0.3141062259674072\n",
      "Validation: Epoch [25], Batch [175/938], Loss: 0.5152382254600525\n",
      "Validation: Epoch [25], Batch [176/938], Loss: 0.5359552502632141\n",
      "Validation: Epoch [25], Batch [177/938], Loss: 0.2547708749771118\n",
      "Validation: Epoch [25], Batch [178/938], Loss: 0.33954092860221863\n",
      "Validation: Epoch [25], Batch [179/938], Loss: 0.5759209394454956\n",
      "Validation: Epoch [25], Batch [180/938], Loss: 0.5572934150695801\n",
      "Validation: Epoch [25], Batch [181/938], Loss: 0.4614110291004181\n",
      "Validation: Epoch [25], Batch [182/938], Loss: 0.4206557869911194\n",
      "Validation: Epoch [25], Batch [183/938], Loss: 0.23613998293876648\n",
      "Validation: Epoch [25], Batch [184/938], Loss: 0.4543362855911255\n",
      "Validation: Epoch [25], Batch [185/938], Loss: 0.5632399320602417\n",
      "Validation: Epoch [25], Batch [186/938], Loss: 0.5121427774429321\n",
      "Validation: Epoch [25], Batch [187/938], Loss: 0.4082622230052948\n",
      "Validation: Epoch [25], Batch [188/938], Loss: 0.2507820427417755\n",
      "Validation: Epoch [25], Batch [189/938], Loss: 0.3806940019130707\n",
      "Validation: Epoch [25], Batch [190/938], Loss: 0.5846179127693176\n",
      "Validation: Epoch [25], Batch [191/938], Loss: 0.3725343644618988\n",
      "Validation: Epoch [25], Batch [192/938], Loss: 0.40584036707878113\n",
      "Validation: Epoch [25], Batch [193/938], Loss: 0.6011477708816528\n",
      "Validation: Epoch [25], Batch [194/938], Loss: 0.547412633895874\n",
      "Validation: Epoch [25], Batch [195/938], Loss: 0.36634936928749084\n",
      "Validation: Epoch [25], Batch [196/938], Loss: 0.3473280072212219\n",
      "Validation: Epoch [25], Batch [197/938], Loss: 0.2915147542953491\n",
      "Validation: Epoch [25], Batch [198/938], Loss: 0.33360975980758667\n",
      "Validation: Epoch [25], Batch [199/938], Loss: 0.4019072651863098\n",
      "Validation: Epoch [25], Batch [200/938], Loss: 0.31794029474258423\n",
      "Validation: Epoch [25], Batch [201/938], Loss: 0.4249323904514313\n",
      "Validation: Epoch [25], Batch [202/938], Loss: 0.41903746128082275\n",
      "Validation: Epoch [25], Batch [203/938], Loss: 0.7322311401367188\n",
      "Validation: Epoch [25], Batch [204/938], Loss: 0.42669638991355896\n",
      "Validation: Epoch [25], Batch [205/938], Loss: 0.4562009871006012\n",
      "Validation: Epoch [25], Batch [206/938], Loss: 0.4961027503013611\n",
      "Validation: Epoch [25], Batch [207/938], Loss: 0.5199781656265259\n",
      "Validation: Epoch [25], Batch [208/938], Loss: 0.38449791073799133\n",
      "Validation: Epoch [25], Batch [209/938], Loss: 0.34817761182785034\n",
      "Validation: Epoch [25], Batch [210/938], Loss: 0.378478467464447\n",
      "Validation: Epoch [25], Batch [211/938], Loss: 0.42638134956359863\n",
      "Validation: Epoch [25], Batch [212/938], Loss: 0.4216134250164032\n",
      "Validation: Epoch [25], Batch [213/938], Loss: 0.4393411874771118\n",
      "Validation: Epoch [25], Batch [214/938], Loss: 0.3892108201980591\n",
      "Validation: Epoch [25], Batch [215/938], Loss: 0.6139684915542603\n",
      "Validation: Epoch [25], Batch [216/938], Loss: 0.26897740364074707\n",
      "Validation: Epoch [25], Batch [217/938], Loss: 0.3376450836658478\n",
      "Validation: Epoch [25], Batch [218/938], Loss: 0.6229426264762878\n",
      "Validation: Epoch [25], Batch [219/938], Loss: 0.4393904209136963\n",
      "Validation: Epoch [25], Batch [220/938], Loss: 0.26340460777282715\n",
      "Validation: Epoch [25], Batch [221/938], Loss: 0.4762490689754486\n",
      "Validation: Epoch [25], Batch [222/938], Loss: 0.3706248998641968\n",
      "Validation: Epoch [25], Batch [223/938], Loss: 0.34368395805358887\n",
      "Validation: Epoch [25], Batch [224/938], Loss: 0.4412904977798462\n",
      "Validation: Epoch [25], Batch [225/938], Loss: 0.3802494704723358\n",
      "Validation: Epoch [25], Batch [226/938], Loss: 0.5056778192520142\n",
      "Validation: Epoch [25], Batch [227/938], Loss: 0.4483368992805481\n",
      "Validation: Epoch [25], Batch [228/938], Loss: 0.39532706141471863\n",
      "Validation: Epoch [25], Batch [229/938], Loss: 0.4062405824661255\n",
      "Validation: Epoch [25], Batch [230/938], Loss: 0.43645724654197693\n",
      "Validation: Epoch [25], Batch [231/938], Loss: 0.2534336447715759\n",
      "Validation: Epoch [25], Batch [232/938], Loss: 0.40121328830718994\n",
      "Validation: Epoch [25], Batch [233/938], Loss: 0.28909388184547424\n",
      "Validation: Epoch [25], Batch [234/938], Loss: 0.3181491494178772\n",
      "Validation: Epoch [25], Batch [235/938], Loss: 0.29034942388534546\n",
      "Validation: Epoch [25], Batch [236/938], Loss: 0.5798379182815552\n",
      "Validation: Epoch [25], Batch [237/938], Loss: 0.17250989377498627\n",
      "Validation: Epoch [25], Batch [238/938], Loss: 0.41853827238082886\n",
      "Validation: Epoch [25], Batch [239/938], Loss: 0.4365288019180298\n",
      "Validation: Epoch [25], Batch [240/938], Loss: 0.3432502746582031\n",
      "Validation: Epoch [25], Batch [241/938], Loss: 0.27872541546821594\n",
      "Validation: Epoch [25], Batch [242/938], Loss: 0.4136127233505249\n",
      "Validation: Epoch [25], Batch [243/938], Loss: 0.6996040344238281\n",
      "Validation: Epoch [25], Batch [244/938], Loss: 0.4627285301685333\n",
      "Validation: Epoch [25], Batch [245/938], Loss: 0.3807266354560852\n",
      "Validation: Epoch [25], Batch [246/938], Loss: 0.3974485993385315\n",
      "Validation: Epoch [25], Batch [247/938], Loss: 0.32906779646873474\n",
      "Validation: Epoch [25], Batch [248/938], Loss: 0.4297860860824585\n",
      "Validation: Epoch [25], Batch [249/938], Loss: 0.5168017148971558\n",
      "Validation: Epoch [25], Batch [250/938], Loss: 0.4253356158733368\n",
      "Validation: Epoch [25], Batch [251/938], Loss: 0.5581727027893066\n",
      "Validation: Epoch [25], Batch [252/938], Loss: 0.40727314352989197\n",
      "Validation: Epoch [25], Batch [253/938], Loss: 0.42350995540618896\n",
      "Validation: Epoch [25], Batch [254/938], Loss: 0.25186434388160706\n",
      "Validation: Epoch [25], Batch [255/938], Loss: 0.5069935917854309\n",
      "Validation: Epoch [25], Batch [256/938], Loss: 0.5748798847198486\n",
      "Validation: Epoch [25], Batch [257/938], Loss: 0.4362425208091736\n",
      "Validation: Epoch [25], Batch [258/938], Loss: 0.5385123491287231\n",
      "Validation: Epoch [25], Batch [259/938], Loss: 0.4487383961677551\n",
      "Validation: Epoch [25], Batch [260/938], Loss: 0.5685709714889526\n",
      "Validation: Epoch [25], Batch [261/938], Loss: 0.5761427283287048\n",
      "Validation: Epoch [25], Batch [262/938], Loss: 0.3598557710647583\n",
      "Validation: Epoch [25], Batch [263/938], Loss: 0.3416752219200134\n",
      "Validation: Epoch [25], Batch [264/938], Loss: 0.3472598195075989\n",
      "Validation: Epoch [25], Batch [265/938], Loss: 0.6775313019752502\n",
      "Validation: Epoch [25], Batch [266/938], Loss: 0.6607055068016052\n",
      "Validation: Epoch [25], Batch [267/938], Loss: 0.39685505628585815\n",
      "Validation: Epoch [25], Batch [268/938], Loss: 0.42464199662208557\n",
      "Validation: Epoch [25], Batch [269/938], Loss: 0.5499251484870911\n",
      "Validation: Epoch [25], Batch [270/938], Loss: 0.34074535965919495\n",
      "Validation: Epoch [25], Batch [271/938], Loss: 0.4158867299556732\n",
      "Validation: Epoch [25], Batch [272/938], Loss: 0.35423544049263\n",
      "Validation: Epoch [25], Batch [273/938], Loss: 0.24500228464603424\n",
      "Validation: Epoch [25], Batch [274/938], Loss: 0.5451241135597229\n",
      "Validation: Epoch [25], Batch [275/938], Loss: 0.551842987537384\n",
      "Validation: Epoch [25], Batch [276/938], Loss: 0.3526007831096649\n",
      "Validation: Epoch [25], Batch [277/938], Loss: 0.3891693949699402\n",
      "Validation: Epoch [25], Batch [278/938], Loss: 0.5743885040283203\n",
      "Validation: Epoch [25], Batch [279/938], Loss: 0.23977936804294586\n",
      "Validation: Epoch [25], Batch [280/938], Loss: 0.37587645649909973\n",
      "Validation: Epoch [25], Batch [281/938], Loss: 0.41181641817092896\n",
      "Validation: Epoch [25], Batch [282/938], Loss: 0.2752048075199127\n",
      "Validation: Epoch [25], Batch [283/938], Loss: 0.4625711143016815\n",
      "Validation: Epoch [25], Batch [284/938], Loss: 0.43418070673942566\n",
      "Validation: Epoch [25], Batch [285/938], Loss: 0.3623398542404175\n",
      "Validation: Epoch [25], Batch [286/938], Loss: 0.45277348160743713\n",
      "Validation: Epoch [25], Batch [287/938], Loss: 0.4510385990142822\n",
      "Validation: Epoch [25], Batch [288/938], Loss: 0.6692570447921753\n",
      "Validation: Epoch [25], Batch [289/938], Loss: 0.4180784523487091\n",
      "Validation: Epoch [25], Batch [290/938], Loss: 0.2618423402309418\n",
      "Validation: Epoch [25], Batch [291/938], Loss: 0.41822299361228943\n",
      "Validation: Epoch [25], Batch [292/938], Loss: 0.42203596234321594\n",
      "Validation: Epoch [25], Batch [293/938], Loss: 0.599359929561615\n",
      "Validation: Epoch [25], Batch [294/938], Loss: 0.47064775228500366\n",
      "Validation: Epoch [25], Batch [295/938], Loss: 0.5924850702285767\n",
      "Validation: Epoch [25], Batch [296/938], Loss: 0.3159632682800293\n",
      "Validation: Epoch [25], Batch [297/938], Loss: 0.3697071075439453\n",
      "Validation: Epoch [25], Batch [298/938], Loss: 0.38585329055786133\n",
      "Validation: Epoch [25], Batch [299/938], Loss: 0.3137906491756439\n",
      "Validation: Epoch [25], Batch [300/938], Loss: 0.5889396667480469\n",
      "Validation: Epoch [25], Batch [301/938], Loss: 0.3793964087963104\n",
      "Validation: Epoch [25], Batch [302/938], Loss: 0.39928704500198364\n",
      "Validation: Epoch [25], Batch [303/938], Loss: 0.38432133197784424\n",
      "Validation: Epoch [25], Batch [304/938], Loss: 0.27434080839157104\n",
      "Validation: Epoch [25], Batch [305/938], Loss: 0.4426302909851074\n",
      "Validation: Epoch [25], Batch [306/938], Loss: 0.39777716994285583\n",
      "Validation: Epoch [25], Batch [307/938], Loss: 0.44386881589889526\n",
      "Validation: Epoch [25], Batch [308/938], Loss: 0.4233209788799286\n",
      "Validation: Epoch [25], Batch [309/938], Loss: 0.32933011651039124\n",
      "Validation: Epoch [25], Batch [310/938], Loss: 0.45206159353256226\n",
      "Validation: Epoch [25], Batch [311/938], Loss: 0.4653751254081726\n",
      "Validation: Epoch [25], Batch [312/938], Loss: 0.5872939825057983\n",
      "Validation: Epoch [25], Batch [313/938], Loss: 0.4359542429447174\n",
      "Validation: Epoch [25], Batch [314/938], Loss: 0.3193076550960541\n",
      "Validation: Epoch [25], Batch [315/938], Loss: 0.4760705530643463\n",
      "Validation: Epoch [25], Batch [316/938], Loss: 0.37142422795295715\n",
      "Validation: Epoch [25], Batch [317/938], Loss: 0.37908685207366943\n",
      "Validation: Epoch [25], Batch [318/938], Loss: 0.5570644736289978\n",
      "Validation: Epoch [25], Batch [319/938], Loss: 0.4555240273475647\n",
      "Validation: Epoch [25], Batch [320/938], Loss: 0.20812590420246124\n",
      "Validation: Epoch [25], Batch [321/938], Loss: 0.5078229904174805\n",
      "Validation: Epoch [25], Batch [322/938], Loss: 0.3124285042285919\n",
      "Validation: Epoch [25], Batch [323/938], Loss: 0.4470658302307129\n",
      "Validation: Epoch [25], Batch [324/938], Loss: 0.24396701157093048\n",
      "Validation: Epoch [25], Batch [325/938], Loss: 0.38672882318496704\n",
      "Validation: Epoch [25], Batch [326/938], Loss: 0.3525811433792114\n",
      "Validation: Epoch [25], Batch [327/938], Loss: 0.42462337017059326\n",
      "Validation: Epoch [25], Batch [328/938], Loss: 0.5823884010314941\n",
      "Validation: Epoch [25], Batch [329/938], Loss: 0.31123608350753784\n",
      "Validation: Epoch [25], Batch [330/938], Loss: 0.4431910812854767\n",
      "Validation: Epoch [25], Batch [331/938], Loss: 0.6550514698028564\n",
      "Validation: Epoch [25], Batch [332/938], Loss: 0.48921266198158264\n",
      "Validation: Epoch [25], Batch [333/938], Loss: 0.3107333481311798\n",
      "Validation: Epoch [25], Batch [334/938], Loss: 0.48351794481277466\n",
      "Validation: Epoch [25], Batch [335/938], Loss: 0.37551653385162354\n",
      "Validation: Epoch [25], Batch [336/938], Loss: 0.6914169788360596\n",
      "Validation: Epoch [25], Batch [337/938], Loss: 0.3990733027458191\n",
      "Validation: Epoch [25], Batch [338/938], Loss: 0.5071993470191956\n",
      "Validation: Epoch [25], Batch [339/938], Loss: 0.5786059498786926\n",
      "Validation: Epoch [25], Batch [340/938], Loss: 0.4985222816467285\n",
      "Validation: Epoch [25], Batch [341/938], Loss: 0.3107512295246124\n",
      "Validation: Epoch [25], Batch [342/938], Loss: 0.6210333108901978\n",
      "Validation: Epoch [25], Batch [343/938], Loss: 0.38291385769844055\n",
      "Validation: Epoch [25], Batch [344/938], Loss: 0.44053635001182556\n",
      "Validation: Epoch [25], Batch [345/938], Loss: 0.7114740014076233\n",
      "Validation: Epoch [25], Batch [346/938], Loss: 0.29536300897598267\n",
      "Validation: Epoch [25], Batch [347/938], Loss: 0.6326934099197388\n",
      "Validation: Epoch [25], Batch [348/938], Loss: 0.5055525302886963\n",
      "Validation: Epoch [25], Batch [349/938], Loss: 0.3438850939273834\n",
      "Validation: Epoch [25], Batch [350/938], Loss: 0.5918729901313782\n",
      "Validation: Epoch [25], Batch [351/938], Loss: 0.37221699953079224\n",
      "Validation: Epoch [25], Batch [352/938], Loss: 0.4301782250404358\n",
      "Validation: Epoch [25], Batch [353/938], Loss: 0.37281614542007446\n",
      "Validation: Epoch [25], Batch [354/938], Loss: 0.4079921543598175\n",
      "Validation: Epoch [25], Batch [355/938], Loss: 0.39222872257232666\n",
      "Validation: Epoch [25], Batch [356/938], Loss: 0.4331813454627991\n",
      "Validation: Epoch [25], Batch [357/938], Loss: 0.41652095317840576\n",
      "Validation: Epoch [25], Batch [358/938], Loss: 0.4261104464530945\n",
      "Validation: Epoch [25], Batch [359/938], Loss: 0.46729007363319397\n",
      "Validation: Epoch [25], Batch [360/938], Loss: 0.5721998810768127\n",
      "Validation: Epoch [25], Batch [361/938], Loss: 0.518704891204834\n",
      "Validation: Epoch [25], Batch [362/938], Loss: 0.28384166955947876\n",
      "Validation: Epoch [25], Batch [363/938], Loss: 0.430655300617218\n",
      "Validation: Epoch [25], Batch [364/938], Loss: 0.35161054134368896\n",
      "Validation: Epoch [25], Batch [365/938], Loss: 0.4264999032020569\n",
      "Validation: Epoch [25], Batch [366/938], Loss: 0.45169854164123535\n",
      "Validation: Epoch [25], Batch [367/938], Loss: 0.3889144957065582\n",
      "Validation: Epoch [25], Batch [368/938], Loss: 0.2614611089229584\n",
      "Validation: Epoch [25], Batch [369/938], Loss: 0.3891645669937134\n",
      "Validation: Epoch [25], Batch [370/938], Loss: 0.5782017707824707\n",
      "Validation: Epoch [25], Batch [371/938], Loss: 0.6591819524765015\n",
      "Validation: Epoch [25], Batch [372/938], Loss: 0.44452929496765137\n",
      "Validation: Epoch [25], Batch [373/938], Loss: 0.44119247794151306\n",
      "Validation: Epoch [25], Batch [374/938], Loss: 0.6377210021018982\n",
      "Validation: Epoch [25], Batch [375/938], Loss: 0.2464391440153122\n",
      "Validation: Epoch [25], Batch [376/938], Loss: 0.43010303378105164\n",
      "Validation: Epoch [25], Batch [377/938], Loss: 0.4664396047592163\n",
      "Validation: Epoch [25], Batch [378/938], Loss: 0.308328241109848\n",
      "Validation: Epoch [25], Batch [379/938], Loss: 0.33859920501708984\n",
      "Validation: Epoch [25], Batch [380/938], Loss: 0.5022759437561035\n",
      "Validation: Epoch [25], Batch [381/938], Loss: 0.4682139456272125\n",
      "Validation: Epoch [25], Batch [382/938], Loss: 0.3109489679336548\n",
      "Validation: Epoch [25], Batch [383/938], Loss: 0.345462441444397\n",
      "Validation: Epoch [25], Batch [384/938], Loss: 0.6528855562210083\n",
      "Validation: Epoch [25], Batch [385/938], Loss: 0.4182566702365875\n",
      "Validation: Epoch [25], Batch [386/938], Loss: 0.32807114720344543\n",
      "Validation: Epoch [25], Batch [387/938], Loss: 0.4053858518600464\n",
      "Validation: Epoch [25], Batch [388/938], Loss: 0.8231097459793091\n",
      "Validation: Epoch [25], Batch [389/938], Loss: 0.3473620116710663\n",
      "Validation: Epoch [25], Batch [390/938], Loss: 0.471373975276947\n",
      "Validation: Epoch [25], Batch [391/938], Loss: 0.3167575001716614\n",
      "Validation: Epoch [25], Batch [392/938], Loss: 0.25147151947021484\n",
      "Validation: Epoch [25], Batch [393/938], Loss: 0.34353911876678467\n",
      "Validation: Epoch [25], Batch [394/938], Loss: 0.4142397940158844\n",
      "Validation: Epoch [25], Batch [395/938], Loss: 0.41582563519477844\n",
      "Validation: Epoch [25], Batch [396/938], Loss: 0.22508122026920319\n",
      "Validation: Epoch [25], Batch [397/938], Loss: 0.3661882281303406\n",
      "Validation: Epoch [25], Batch [398/938], Loss: 0.2557072639465332\n",
      "Validation: Epoch [25], Batch [399/938], Loss: 0.4442863166332245\n",
      "Validation: Epoch [25], Batch [400/938], Loss: 0.5802015066146851\n",
      "Validation: Epoch [25], Batch [401/938], Loss: 0.41804081201553345\n",
      "Validation: Epoch [25], Batch [402/938], Loss: 0.30692583322525024\n",
      "Validation: Epoch [25], Batch [403/938], Loss: 0.4122229814529419\n",
      "Validation: Epoch [25], Batch [404/938], Loss: 0.4583773910999298\n",
      "Validation: Epoch [25], Batch [405/938], Loss: 0.4772530794143677\n",
      "Validation: Epoch [25], Batch [406/938], Loss: 0.36590689420700073\n",
      "Validation: Epoch [25], Batch [407/938], Loss: 0.559433102607727\n",
      "Validation: Epoch [25], Batch [408/938], Loss: 0.4400048553943634\n",
      "Validation: Epoch [25], Batch [409/938], Loss: 0.4424576461315155\n",
      "Validation: Epoch [25], Batch [410/938], Loss: 0.37093284726142883\n",
      "Validation: Epoch [25], Batch [411/938], Loss: 0.3001623749732971\n",
      "Validation: Epoch [25], Batch [412/938], Loss: 0.5084646344184875\n",
      "Validation: Epoch [25], Batch [413/938], Loss: 0.6393143534660339\n",
      "Validation: Epoch [25], Batch [414/938], Loss: 0.41872912645339966\n",
      "Validation: Epoch [25], Batch [415/938], Loss: 0.3551190197467804\n",
      "Validation: Epoch [25], Batch [416/938], Loss: 0.40598371624946594\n",
      "Validation: Epoch [25], Batch [417/938], Loss: 0.3865084946155548\n",
      "Validation: Epoch [25], Batch [418/938], Loss: 0.4907063841819763\n",
      "Validation: Epoch [25], Batch [419/938], Loss: 0.43507617712020874\n",
      "Validation: Epoch [25], Batch [420/938], Loss: 0.3411434292793274\n",
      "Validation: Epoch [25], Batch [421/938], Loss: 0.37612485885620117\n",
      "Validation: Epoch [25], Batch [422/938], Loss: 0.8239602446556091\n",
      "Validation: Epoch [25], Batch [423/938], Loss: 0.30341583490371704\n",
      "Validation: Epoch [25], Batch [424/938], Loss: 0.3783565163612366\n",
      "Validation: Epoch [25], Batch [425/938], Loss: 0.37439247965812683\n",
      "Validation: Epoch [25], Batch [426/938], Loss: 0.376697838306427\n",
      "Validation: Epoch [25], Batch [427/938], Loss: 0.45597895979881287\n",
      "Validation: Epoch [25], Batch [428/938], Loss: 0.3295970559120178\n",
      "Validation: Epoch [25], Batch [429/938], Loss: 0.29865413904190063\n",
      "Validation: Epoch [25], Batch [430/938], Loss: 0.37006092071533203\n",
      "Validation: Epoch [25], Batch [431/938], Loss: 0.4370911121368408\n",
      "Validation: Epoch [25], Batch [432/938], Loss: 0.36978843808174133\n",
      "Validation: Epoch [25], Batch [433/938], Loss: 0.44633597135543823\n",
      "Validation: Epoch [25], Batch [434/938], Loss: 0.5671152472496033\n",
      "Validation: Epoch [25], Batch [435/938], Loss: 0.29442423582077026\n",
      "Validation: Epoch [25], Batch [436/938], Loss: 0.3881992995738983\n",
      "Validation: Epoch [25], Batch [437/938], Loss: 0.3628959059715271\n",
      "Validation: Epoch [25], Batch [438/938], Loss: 0.294494092464447\n",
      "Validation: Epoch [25], Batch [439/938], Loss: 0.39929452538490295\n",
      "Validation: Epoch [25], Batch [440/938], Loss: 0.4496007561683655\n",
      "Validation: Epoch [25], Batch [441/938], Loss: 0.29758188128471375\n",
      "Validation: Epoch [25], Batch [442/938], Loss: 0.3131565749645233\n",
      "Validation: Epoch [25], Batch [443/938], Loss: 0.34769853949546814\n",
      "Validation: Epoch [25], Batch [444/938], Loss: 0.270328164100647\n",
      "Validation: Epoch [25], Batch [445/938], Loss: 0.4498455226421356\n",
      "Validation: Epoch [25], Batch [446/938], Loss: 0.5065354108810425\n",
      "Validation: Epoch [25], Batch [447/938], Loss: 0.33392539620399475\n",
      "Validation: Epoch [25], Batch [448/938], Loss: 0.4274634122848511\n",
      "Validation: Epoch [25], Batch [449/938], Loss: 0.36510562896728516\n",
      "Validation: Epoch [25], Batch [450/938], Loss: 0.3269641101360321\n",
      "Validation: Epoch [25], Batch [451/938], Loss: 0.4182603657245636\n",
      "Validation: Epoch [25], Batch [452/938], Loss: 0.530659019947052\n",
      "Validation: Epoch [25], Batch [453/938], Loss: 0.5194886922836304\n",
      "Validation: Epoch [25], Batch [454/938], Loss: 0.5190660953521729\n",
      "Validation: Epoch [25], Batch [455/938], Loss: 0.6085655093193054\n",
      "Validation: Epoch [25], Batch [456/938], Loss: 0.5036988258361816\n",
      "Validation: Epoch [25], Batch [457/938], Loss: 0.35260170698165894\n",
      "Validation: Epoch [25], Batch [458/938], Loss: 0.2917027473449707\n",
      "Validation: Epoch [25], Batch [459/938], Loss: 0.31494465470314026\n",
      "Validation: Epoch [25], Batch [460/938], Loss: 0.406413733959198\n",
      "Validation: Epoch [25], Batch [461/938], Loss: 0.34881988167762756\n",
      "Validation: Epoch [25], Batch [462/938], Loss: 0.33846771717071533\n",
      "Validation: Epoch [25], Batch [463/938], Loss: 0.6419378519058228\n",
      "Validation: Epoch [25], Batch [464/938], Loss: 0.46909916400909424\n",
      "Validation: Epoch [25], Batch [465/938], Loss: 0.5834293961524963\n",
      "Validation: Epoch [25], Batch [466/938], Loss: 0.2817227840423584\n",
      "Validation: Epoch [25], Batch [467/938], Loss: 0.5720857977867126\n",
      "Validation: Epoch [25], Batch [468/938], Loss: 0.37748801708221436\n",
      "Validation: Epoch [25], Batch [469/938], Loss: 0.5129370093345642\n",
      "Validation: Epoch [25], Batch [470/938], Loss: 0.4639703631401062\n",
      "Validation: Epoch [25], Batch [471/938], Loss: 0.43025732040405273\n",
      "Validation: Epoch [25], Batch [472/938], Loss: 0.7180605530738831\n",
      "Validation: Epoch [25], Batch [473/938], Loss: 0.42668741941452026\n",
      "Validation: Epoch [25], Batch [474/938], Loss: 0.35119950771331787\n",
      "Validation: Epoch [25], Batch [475/938], Loss: 0.703922688961029\n",
      "Validation: Epoch [25], Batch [476/938], Loss: 0.6714075207710266\n",
      "Validation: Epoch [25], Batch [477/938], Loss: 0.45978856086730957\n",
      "Validation: Epoch [25], Batch [478/938], Loss: 0.3573989272117615\n",
      "Validation: Epoch [25], Batch [479/938], Loss: 0.6390836238861084\n",
      "Validation: Epoch [25], Batch [480/938], Loss: 0.2840815782546997\n",
      "Validation: Epoch [25], Batch [481/938], Loss: 0.3134557902812958\n",
      "Validation: Epoch [25], Batch [482/938], Loss: 0.3754829168319702\n",
      "Validation: Epoch [25], Batch [483/938], Loss: 0.34839531779289246\n",
      "Validation: Epoch [25], Batch [484/938], Loss: 0.6062455177307129\n",
      "Validation: Epoch [25], Batch [485/938], Loss: 0.6300532817840576\n",
      "Validation: Epoch [25], Batch [486/938], Loss: 0.45809847116470337\n",
      "Validation: Epoch [25], Batch [487/938], Loss: 0.3215760588645935\n",
      "Validation: Epoch [25], Batch [488/938], Loss: 0.3218868374824524\n",
      "Validation: Epoch [25], Batch [489/938], Loss: 0.5102742910385132\n",
      "Validation: Epoch [25], Batch [490/938], Loss: 0.4510731101036072\n",
      "Validation: Epoch [25], Batch [491/938], Loss: 0.46090394258499146\n",
      "Validation: Epoch [25], Batch [492/938], Loss: 0.2523123323917389\n",
      "Validation: Epoch [25], Batch [493/938], Loss: 0.4580104351043701\n",
      "Validation: Epoch [25], Batch [494/938], Loss: 0.5088045597076416\n",
      "Validation: Epoch [25], Batch [495/938], Loss: 0.4261362850666046\n",
      "Validation: Epoch [25], Batch [496/938], Loss: 0.38551414012908936\n",
      "Validation: Epoch [25], Batch [497/938], Loss: 0.49747931957244873\n",
      "Validation: Epoch [25], Batch [498/938], Loss: 0.3903385102748871\n",
      "Validation: Epoch [25], Batch [499/938], Loss: 0.37091925740242004\n",
      "Validation: Epoch [25], Batch [500/938], Loss: 0.2560889720916748\n",
      "Validation: Epoch [25], Batch [501/938], Loss: 0.3823143243789673\n",
      "Validation: Epoch [25], Batch [502/938], Loss: 0.3872274160385132\n",
      "Validation: Epoch [25], Batch [503/938], Loss: 0.617093563079834\n",
      "Validation: Epoch [25], Batch [504/938], Loss: 0.5025702714920044\n",
      "Validation: Epoch [25], Batch [505/938], Loss: 0.33798885345458984\n",
      "Validation: Epoch [25], Batch [506/938], Loss: 0.5234959125518799\n",
      "Validation: Epoch [25], Batch [507/938], Loss: 0.43116453289985657\n",
      "Validation: Epoch [25], Batch [508/938], Loss: 0.45873576402664185\n",
      "Validation: Epoch [25], Batch [509/938], Loss: 0.44247159361839294\n",
      "Validation: Epoch [25], Batch [510/938], Loss: 0.5179086327552795\n",
      "Validation: Epoch [25], Batch [511/938], Loss: 0.5078704357147217\n",
      "Validation: Epoch [25], Batch [512/938], Loss: 0.3523543179035187\n",
      "Validation: Epoch [25], Batch [513/938], Loss: 0.3017042279243469\n",
      "Validation: Epoch [25], Batch [514/938], Loss: 0.44002410769462585\n",
      "Validation: Epoch [25], Batch [515/938], Loss: 0.378878653049469\n",
      "Validation: Epoch [25], Batch [516/938], Loss: 0.26693856716156006\n",
      "Validation: Epoch [25], Batch [517/938], Loss: 0.43721285462379456\n",
      "Validation: Epoch [25], Batch [518/938], Loss: 0.6126542687416077\n",
      "Validation: Epoch [25], Batch [519/938], Loss: 0.41254669427871704\n",
      "Validation: Epoch [25], Batch [520/938], Loss: 0.5016402006149292\n",
      "Validation: Epoch [25], Batch [521/938], Loss: 0.33638066053390503\n",
      "Validation: Epoch [25], Batch [522/938], Loss: 0.39869290590286255\n",
      "Validation: Epoch [25], Batch [523/938], Loss: 0.49757713079452515\n",
      "Validation: Epoch [25], Batch [524/938], Loss: 0.5387585759162903\n",
      "Validation: Epoch [25], Batch [525/938], Loss: 0.5473195314407349\n",
      "Validation: Epoch [25], Batch [526/938], Loss: 0.35001832246780396\n",
      "Validation: Epoch [25], Batch [527/938], Loss: 0.3063001036643982\n",
      "Validation: Epoch [25], Batch [528/938], Loss: 0.5120155811309814\n",
      "Validation: Epoch [25], Batch [529/938], Loss: 0.35821017622947693\n",
      "Validation: Epoch [25], Batch [530/938], Loss: 0.48477715253829956\n",
      "Validation: Epoch [25], Batch [531/938], Loss: 0.31506437063217163\n",
      "Validation: Epoch [25], Batch [532/938], Loss: 0.30089762806892395\n",
      "Validation: Epoch [25], Batch [533/938], Loss: 0.38263022899627686\n",
      "Validation: Epoch [25], Batch [534/938], Loss: 0.579554557800293\n",
      "Validation: Epoch [25], Batch [535/938], Loss: 0.32306134700775146\n",
      "Validation: Epoch [25], Batch [536/938], Loss: 0.5187153220176697\n",
      "Validation: Epoch [25], Batch [537/938], Loss: 0.3492857813835144\n",
      "Validation: Epoch [25], Batch [538/938], Loss: 0.35443246364593506\n",
      "Validation: Epoch [25], Batch [539/938], Loss: 0.3850019872188568\n",
      "Validation: Epoch [25], Batch [540/938], Loss: 0.6916556358337402\n",
      "Validation: Epoch [25], Batch [541/938], Loss: 0.48009607195854187\n",
      "Validation: Epoch [25], Batch [542/938], Loss: 0.23689161241054535\n",
      "Validation: Epoch [25], Batch [543/938], Loss: 0.47157707810401917\n",
      "Validation: Epoch [25], Batch [544/938], Loss: 0.500987708568573\n",
      "Validation: Epoch [25], Batch [545/938], Loss: 0.4253249168395996\n",
      "Validation: Epoch [25], Batch [546/938], Loss: 0.416869580745697\n",
      "Validation: Epoch [25], Batch [547/938], Loss: 0.4152286648750305\n",
      "Validation: Epoch [25], Batch [548/938], Loss: 0.5044513940811157\n",
      "Validation: Epoch [25], Batch [549/938], Loss: 0.45302149653434753\n",
      "Validation: Epoch [25], Batch [550/938], Loss: 0.3636678457260132\n",
      "Validation: Epoch [25], Batch [551/938], Loss: 0.4644165635108948\n",
      "Validation: Epoch [25], Batch [552/938], Loss: 0.32507479190826416\n",
      "Validation: Epoch [25], Batch [553/938], Loss: 0.5230847001075745\n",
      "Validation: Epoch [25], Batch [554/938], Loss: 0.44735822081565857\n",
      "Validation: Epoch [25], Batch [555/938], Loss: 0.4612283706665039\n",
      "Validation: Epoch [25], Batch [556/938], Loss: 0.40327250957489014\n",
      "Validation: Epoch [25], Batch [557/938], Loss: 0.5270758271217346\n",
      "Validation: Epoch [25], Batch [558/938], Loss: 0.31103402376174927\n",
      "Validation: Epoch [25], Batch [559/938], Loss: 0.5434005260467529\n",
      "Validation: Epoch [25], Batch [560/938], Loss: 0.3389712870121002\n",
      "Validation: Epoch [25], Batch [561/938], Loss: 0.25272467732429504\n",
      "Validation: Epoch [25], Batch [562/938], Loss: 0.4182097315788269\n",
      "Validation: Epoch [25], Batch [563/938], Loss: 0.6660780310630798\n",
      "Validation: Epoch [25], Batch [564/938], Loss: 0.31577813625335693\n",
      "Validation: Epoch [25], Batch [565/938], Loss: 0.4327564239501953\n",
      "Validation: Epoch [25], Batch [566/938], Loss: 0.2407909482717514\n",
      "Validation: Epoch [25], Batch [567/938], Loss: 0.3088938295841217\n",
      "Validation: Epoch [25], Batch [568/938], Loss: 0.4216616749763489\n",
      "Validation: Epoch [25], Batch [569/938], Loss: 0.36458319425582886\n",
      "Validation: Epoch [25], Batch [570/938], Loss: 0.2790758013725281\n",
      "Validation: Epoch [25], Batch [571/938], Loss: 0.43167349696159363\n",
      "Validation: Epoch [25], Batch [572/938], Loss: 0.24883435666561127\n",
      "Validation: Epoch [25], Batch [573/938], Loss: 0.5206794738769531\n",
      "Validation: Epoch [25], Batch [574/938], Loss: 0.5866953134536743\n",
      "Validation: Epoch [25], Batch [575/938], Loss: 0.6036065816879272\n",
      "Validation: Epoch [25], Batch [576/938], Loss: 0.3956477642059326\n",
      "Validation: Epoch [25], Batch [577/938], Loss: 0.2943626642227173\n",
      "Validation: Epoch [25], Batch [578/938], Loss: 0.7221463322639465\n",
      "Validation: Epoch [25], Batch [579/938], Loss: 0.3293760418891907\n",
      "Validation: Epoch [25], Batch [580/938], Loss: 0.745730459690094\n",
      "Validation: Epoch [25], Batch [581/938], Loss: 0.3362770676612854\n",
      "Validation: Epoch [25], Batch [582/938], Loss: 0.43316733837127686\n",
      "Validation: Epoch [25], Batch [583/938], Loss: 0.4852367341518402\n",
      "Validation: Epoch [25], Batch [584/938], Loss: 0.424474835395813\n",
      "Validation: Epoch [25], Batch [585/938], Loss: 0.4365902543067932\n",
      "Validation: Epoch [25], Batch [586/938], Loss: 0.5018283128738403\n",
      "Validation: Epoch [25], Batch [587/938], Loss: 0.42145323753356934\n",
      "Validation: Epoch [25], Batch [588/938], Loss: 0.31236422061920166\n",
      "Validation: Epoch [25], Batch [589/938], Loss: 0.5818555951118469\n",
      "Validation: Epoch [25], Batch [590/938], Loss: 0.4807955324649811\n",
      "Validation: Epoch [25], Batch [591/938], Loss: 0.3385675549507141\n",
      "Validation: Epoch [25], Batch [592/938], Loss: 0.3364703357219696\n",
      "Validation: Epoch [25], Batch [593/938], Loss: 0.5116796493530273\n",
      "Validation: Epoch [25], Batch [594/938], Loss: 0.34664487838745117\n",
      "Validation: Epoch [25], Batch [595/938], Loss: 0.24487069249153137\n",
      "Validation: Epoch [25], Batch [596/938], Loss: 0.3622101843357086\n",
      "Validation: Epoch [25], Batch [597/938], Loss: 0.2752099633216858\n",
      "Validation: Epoch [25], Batch [598/938], Loss: 0.5876126885414124\n",
      "Validation: Epoch [25], Batch [599/938], Loss: 0.37389785051345825\n",
      "Validation: Epoch [25], Batch [600/938], Loss: 0.41840028762817383\n",
      "Validation: Epoch [25], Batch [601/938], Loss: 0.443173348903656\n",
      "Validation: Epoch [25], Batch [602/938], Loss: 0.5949940085411072\n",
      "Validation: Epoch [25], Batch [603/938], Loss: 0.38651129603385925\n",
      "Validation: Epoch [25], Batch [604/938], Loss: 0.2999248206615448\n",
      "Validation: Epoch [25], Batch [605/938], Loss: 0.2852318584918976\n",
      "Validation: Epoch [25], Batch [606/938], Loss: 0.53336501121521\n",
      "Validation: Epoch [25], Batch [607/938], Loss: 0.46541568636894226\n",
      "Validation: Epoch [25], Batch [608/938], Loss: 0.3999038636684418\n",
      "Validation: Epoch [25], Batch [609/938], Loss: 0.393530935049057\n",
      "Validation: Epoch [25], Batch [610/938], Loss: 0.255357563495636\n",
      "Validation: Epoch [25], Batch [611/938], Loss: 0.3408010005950928\n",
      "Validation: Epoch [25], Batch [612/938], Loss: 0.484575480222702\n",
      "Validation: Epoch [25], Batch [613/938], Loss: 0.2142869383096695\n",
      "Validation: Epoch [25], Batch [614/938], Loss: 0.5934839844703674\n",
      "Validation: Epoch [25], Batch [615/938], Loss: 0.42841553688049316\n",
      "Validation: Epoch [25], Batch [616/938], Loss: 0.2502242922782898\n",
      "Validation: Epoch [25], Batch [617/938], Loss: 0.3837765157222748\n",
      "Validation: Epoch [25], Batch [618/938], Loss: 0.31941813230514526\n",
      "Validation: Epoch [25], Batch [619/938], Loss: 0.37353628873825073\n",
      "Validation: Epoch [25], Batch [620/938], Loss: 0.2322038859128952\n",
      "Validation: Epoch [25], Batch [621/938], Loss: 0.29142624139785767\n",
      "Validation: Epoch [25], Batch [622/938], Loss: 0.5939611196517944\n",
      "Validation: Epoch [25], Batch [623/938], Loss: 0.41108742356300354\n",
      "Validation: Epoch [25], Batch [624/938], Loss: 0.47162923216819763\n",
      "Validation: Epoch [25], Batch [625/938], Loss: 0.5086431503295898\n",
      "Validation: Epoch [25], Batch [626/938], Loss: 0.26393720507621765\n",
      "Validation: Epoch [25], Batch [627/938], Loss: 0.43874022364616394\n",
      "Validation: Epoch [25], Batch [628/938], Loss: 0.4707401394844055\n",
      "Validation: Epoch [25], Batch [629/938], Loss: 0.40254485607147217\n",
      "Validation: Epoch [25], Batch [630/938], Loss: 0.3260416090488434\n",
      "Validation: Epoch [25], Batch [631/938], Loss: 0.5721012353897095\n",
      "Validation: Epoch [25], Batch [632/938], Loss: 0.42779475450515747\n",
      "Validation: Epoch [25], Batch [633/938], Loss: 0.21630807220935822\n",
      "Validation: Epoch [25], Batch [634/938], Loss: 0.2889854311943054\n",
      "Validation: Epoch [25], Batch [635/938], Loss: 0.3730984926223755\n",
      "Validation: Epoch [25], Batch [636/938], Loss: 0.4954867660999298\n",
      "Validation: Epoch [25], Batch [637/938], Loss: 0.28117480874061584\n",
      "Validation: Epoch [25], Batch [638/938], Loss: 0.3017788231372833\n",
      "Validation: Epoch [25], Batch [639/938], Loss: 0.5035481452941895\n",
      "Validation: Epoch [25], Batch [640/938], Loss: 0.5111473798751831\n",
      "Validation: Epoch [25], Batch [641/938], Loss: 0.447498083114624\n",
      "Validation: Epoch [25], Batch [642/938], Loss: 0.5060176253318787\n",
      "Validation: Epoch [25], Batch [643/938], Loss: 0.341747522354126\n",
      "Validation: Epoch [25], Batch [644/938], Loss: 0.3956735134124756\n",
      "Validation: Epoch [25], Batch [645/938], Loss: 0.4388795495033264\n",
      "Validation: Epoch [25], Batch [646/938], Loss: 0.5501348972320557\n",
      "Validation: Epoch [25], Batch [647/938], Loss: 0.36589086055755615\n",
      "Validation: Epoch [25], Batch [648/938], Loss: 0.5752206444740295\n",
      "Validation: Epoch [25], Batch [649/938], Loss: 0.4042050540447235\n",
      "Validation: Epoch [25], Batch [650/938], Loss: 0.37511199712753296\n",
      "Validation: Epoch [25], Batch [651/938], Loss: 0.5168684720993042\n",
      "Validation: Epoch [25], Batch [652/938], Loss: 0.45609286427497864\n",
      "Validation: Epoch [25], Batch [653/938], Loss: 0.43216562271118164\n",
      "Validation: Epoch [25], Batch [654/938], Loss: 0.37675905227661133\n",
      "Validation: Epoch [25], Batch [655/938], Loss: 0.31746402382850647\n",
      "Validation: Epoch [25], Batch [656/938], Loss: 0.4703596830368042\n",
      "Validation: Epoch [25], Batch [657/938], Loss: 0.4538590610027313\n",
      "Validation: Epoch [25], Batch [658/938], Loss: 0.2247161567211151\n",
      "Validation: Epoch [25], Batch [659/938], Loss: 0.3316206634044647\n",
      "Validation: Epoch [25], Batch [660/938], Loss: 0.42756521701812744\n",
      "Validation: Epoch [25], Batch [661/938], Loss: 0.4492272734642029\n",
      "Validation: Epoch [25], Batch [662/938], Loss: 0.2987576425075531\n",
      "Validation: Epoch [25], Batch [663/938], Loss: 0.6002041697502136\n",
      "Validation: Epoch [25], Batch [664/938], Loss: 0.6053563356399536\n",
      "Validation: Epoch [25], Batch [665/938], Loss: 0.5103520154953003\n",
      "Validation: Epoch [25], Batch [666/938], Loss: 0.47790032625198364\n",
      "Validation: Epoch [25], Batch [667/938], Loss: 0.385894238948822\n",
      "Validation: Epoch [25], Batch [668/938], Loss: 0.34073686599731445\n",
      "Validation: Epoch [25], Batch [669/938], Loss: 0.4585311710834503\n",
      "Validation: Epoch [25], Batch [670/938], Loss: 0.6124089956283569\n",
      "Validation: Epoch [25], Batch [671/938], Loss: 0.5726677775382996\n",
      "Validation: Epoch [25], Batch [672/938], Loss: 0.3922460675239563\n",
      "Validation: Epoch [25], Batch [673/938], Loss: 0.4261489808559418\n",
      "Validation: Epoch [25], Batch [674/938], Loss: 0.5261936783790588\n",
      "Validation: Epoch [25], Batch [675/938], Loss: 0.48360690474510193\n",
      "Validation: Epoch [25], Batch [676/938], Loss: 0.6393823623657227\n",
      "Validation: Epoch [25], Batch [677/938], Loss: 0.6622041463851929\n",
      "Validation: Epoch [25], Batch [678/938], Loss: 0.41360947489738464\n",
      "Validation: Epoch [25], Batch [679/938], Loss: 0.4957312047481537\n",
      "Validation: Epoch [25], Batch [680/938], Loss: 0.3076234757900238\n",
      "Validation: Epoch [25], Batch [681/938], Loss: 0.30763474106788635\n",
      "Validation: Epoch [25], Batch [682/938], Loss: 0.5489413738250732\n",
      "Validation: Epoch [25], Batch [683/938], Loss: 0.41425323486328125\n",
      "Validation: Epoch [25], Batch [684/938], Loss: 0.4072927236557007\n",
      "Validation: Epoch [25], Batch [685/938], Loss: 0.4400685727596283\n",
      "Validation: Epoch [25], Batch [686/938], Loss: 0.48667263984680176\n",
      "Validation: Epoch [25], Batch [687/938], Loss: 0.4309971332550049\n",
      "Validation: Epoch [25], Batch [688/938], Loss: 0.49920710921287537\n",
      "Validation: Epoch [25], Batch [689/938], Loss: 0.4682474136352539\n",
      "Validation: Epoch [25], Batch [690/938], Loss: 0.4995287358760834\n",
      "Validation: Epoch [25], Batch [691/938], Loss: 0.36635005474090576\n",
      "Validation: Epoch [25], Batch [692/938], Loss: 0.3659697473049164\n",
      "Validation: Epoch [25], Batch [693/938], Loss: 0.33241087198257446\n",
      "Validation: Epoch [25], Batch [694/938], Loss: 0.4272136092185974\n",
      "Validation: Epoch [25], Batch [695/938], Loss: 0.5442824959754944\n",
      "Validation: Epoch [25], Batch [696/938], Loss: 0.5701284408569336\n",
      "Validation: Epoch [25], Batch [697/938], Loss: 0.5200136303901672\n",
      "Validation: Epoch [25], Batch [698/938], Loss: 0.4020521342754364\n",
      "Validation: Epoch [25], Batch [699/938], Loss: 0.5562402606010437\n",
      "Validation: Epoch [25], Batch [700/938], Loss: 0.49642235040664673\n",
      "Validation: Epoch [25], Batch [701/938], Loss: 0.6699747443199158\n",
      "Validation: Epoch [25], Batch [702/938], Loss: 0.4539572596549988\n",
      "Validation: Epoch [25], Batch [703/938], Loss: 0.33124274015426636\n",
      "Validation: Epoch [25], Batch [704/938], Loss: 0.5645924210548401\n",
      "Validation: Epoch [25], Batch [705/938], Loss: 0.37148743867874146\n",
      "Validation: Epoch [25], Batch [706/938], Loss: 0.368792861700058\n",
      "Validation: Epoch [25], Batch [707/938], Loss: 0.6986899375915527\n",
      "Validation: Epoch [25], Batch [708/938], Loss: 0.4105520248413086\n",
      "Validation: Epoch [25], Batch [709/938], Loss: 0.5293580889701843\n",
      "Validation: Epoch [25], Batch [710/938], Loss: 0.5017911195755005\n",
      "Validation: Epoch [25], Batch [711/938], Loss: 0.4888814389705658\n",
      "Validation: Epoch [25], Batch [712/938], Loss: 0.28762078285217285\n",
      "Validation: Epoch [25], Batch [713/938], Loss: 0.40822967886924744\n",
      "Validation: Epoch [25], Batch [714/938], Loss: 0.3632938861846924\n",
      "Validation: Epoch [25], Batch [715/938], Loss: 0.5992198586463928\n",
      "Validation: Epoch [25], Batch [716/938], Loss: 0.30005648732185364\n",
      "Validation: Epoch [25], Batch [717/938], Loss: 0.3842981159687042\n",
      "Validation: Epoch [25], Batch [718/938], Loss: 0.4113728702068329\n",
      "Validation: Epoch [25], Batch [719/938], Loss: 0.3652876019477844\n",
      "Validation: Epoch [25], Batch [720/938], Loss: 0.6500378251075745\n",
      "Validation: Epoch [25], Batch [721/938], Loss: 0.25786858797073364\n",
      "Validation: Epoch [25], Batch [722/938], Loss: 0.30569928884506226\n",
      "Validation: Epoch [25], Batch [723/938], Loss: 0.31631189584732056\n",
      "Validation: Epoch [25], Batch [724/938], Loss: 0.491412878036499\n",
      "Validation: Epoch [25], Batch [725/938], Loss: 0.3760172724723816\n",
      "Validation: Epoch [25], Batch [726/938], Loss: 0.44939833879470825\n",
      "Validation: Epoch [25], Batch [727/938], Loss: 0.3339933156967163\n",
      "Validation: Epoch [25], Batch [728/938], Loss: 0.5317374467849731\n",
      "Validation: Epoch [25], Batch [729/938], Loss: 0.4071231484413147\n",
      "Validation: Epoch [25], Batch [730/938], Loss: 0.573192834854126\n",
      "Validation: Epoch [25], Batch [731/938], Loss: 0.4650145173072815\n",
      "Validation: Epoch [25], Batch [732/938], Loss: 0.3941463530063629\n",
      "Validation: Epoch [25], Batch [733/938], Loss: 0.32920318841934204\n",
      "Validation: Epoch [25], Batch [734/938], Loss: 0.29947182536125183\n",
      "Validation: Epoch [25], Batch [735/938], Loss: 0.37778085470199585\n",
      "Validation: Epoch [25], Batch [736/938], Loss: 0.4440021514892578\n",
      "Validation: Epoch [25], Batch [737/938], Loss: 0.32181447744369507\n",
      "Validation: Epoch [25], Batch [738/938], Loss: 0.5485204458236694\n",
      "Validation: Epoch [25], Batch [739/938], Loss: 0.6429865956306458\n",
      "Validation: Epoch [25], Batch [740/938], Loss: 0.3995859622955322\n",
      "Validation: Epoch [25], Batch [741/938], Loss: 0.5595793128013611\n",
      "Validation: Epoch [25], Batch [742/938], Loss: 0.37319979071617126\n",
      "Validation: Epoch [25], Batch [743/938], Loss: 0.6217334866523743\n",
      "Validation: Epoch [25], Batch [744/938], Loss: 0.34169840812683105\n",
      "Validation: Epoch [25], Batch [745/938], Loss: 0.3297509253025055\n",
      "Validation: Epoch [25], Batch [746/938], Loss: 0.2590731978416443\n",
      "Validation: Epoch [25], Batch [747/938], Loss: 0.4606471657752991\n",
      "Validation: Epoch [25], Batch [748/938], Loss: 0.29903876781463623\n",
      "Validation: Epoch [25], Batch [749/938], Loss: 0.569891631603241\n",
      "Validation: Epoch [25], Batch [750/938], Loss: 0.28844258189201355\n",
      "Validation: Epoch [25], Batch [751/938], Loss: 0.5355044007301331\n",
      "Validation: Epoch [25], Batch [752/938], Loss: 0.31566286087036133\n",
      "Validation: Epoch [25], Batch [753/938], Loss: 0.33576297760009766\n",
      "Validation: Epoch [25], Batch [754/938], Loss: 0.39582380652427673\n",
      "Validation: Epoch [25], Batch [755/938], Loss: 0.3774089217185974\n",
      "Validation: Epoch [25], Batch [756/938], Loss: 0.46630167961120605\n",
      "Validation: Epoch [25], Batch [757/938], Loss: 0.4896102845668793\n",
      "Validation: Epoch [25], Batch [758/938], Loss: 0.504921555519104\n",
      "Validation: Epoch [25], Batch [759/938], Loss: 0.25115111470222473\n",
      "Validation: Epoch [25], Batch [760/938], Loss: 0.49297860264778137\n",
      "Validation: Epoch [25], Batch [761/938], Loss: 0.5271786451339722\n",
      "Validation: Epoch [25], Batch [762/938], Loss: 0.31734132766723633\n",
      "Validation: Epoch [25], Batch [763/938], Loss: 0.5310988426208496\n",
      "Validation: Epoch [25], Batch [764/938], Loss: 0.492594450712204\n",
      "Validation: Epoch [25], Batch [765/938], Loss: 0.6343030333518982\n",
      "Validation: Epoch [25], Batch [766/938], Loss: 0.49814313650131226\n",
      "Validation: Epoch [25], Batch [767/938], Loss: 0.38171830773353577\n",
      "Validation: Epoch [25], Batch [768/938], Loss: 0.2674209177494049\n",
      "Validation: Epoch [25], Batch [769/938], Loss: 0.4372413158416748\n",
      "Validation: Epoch [25], Batch [770/938], Loss: 0.5574455261230469\n",
      "Validation: Epoch [25], Batch [771/938], Loss: 0.5728268623352051\n",
      "Validation: Epoch [25], Batch [772/938], Loss: 0.5277689695358276\n",
      "Validation: Epoch [25], Batch [773/938], Loss: 0.49968868494033813\n",
      "Validation: Epoch [25], Batch [774/938], Loss: 0.4201032519340515\n",
      "Validation: Epoch [25], Batch [775/938], Loss: 0.4475634694099426\n",
      "Validation: Epoch [25], Batch [776/938], Loss: 0.5435593724250793\n",
      "Validation: Epoch [25], Batch [777/938], Loss: 0.47672730684280396\n",
      "Validation: Epoch [25], Batch [778/938], Loss: 0.3685970604419708\n",
      "Validation: Epoch [25], Batch [779/938], Loss: 0.4078059196472168\n",
      "Validation: Epoch [25], Batch [780/938], Loss: 0.5209232568740845\n",
      "Validation: Epoch [25], Batch [781/938], Loss: 0.458749383687973\n",
      "Validation: Epoch [25], Batch [782/938], Loss: 0.44216015934944153\n",
      "Validation: Epoch [25], Batch [783/938], Loss: 0.45091503858566284\n",
      "Validation: Epoch [25], Batch [784/938], Loss: 0.43771815299987793\n",
      "Validation: Epoch [25], Batch [785/938], Loss: 0.559247612953186\n",
      "Validation: Epoch [25], Batch [786/938], Loss: 0.6520747542381287\n",
      "Validation: Epoch [25], Batch [787/938], Loss: 0.4578678011894226\n",
      "Validation: Epoch [25], Batch [788/938], Loss: 0.29095321893692017\n",
      "Validation: Epoch [25], Batch [789/938], Loss: 0.531338095664978\n",
      "Validation: Epoch [25], Batch [790/938], Loss: 0.41955244541168213\n",
      "Validation: Epoch [25], Batch [791/938], Loss: 0.30617934465408325\n",
      "Validation: Epoch [25], Batch [792/938], Loss: 0.4487701654434204\n",
      "Validation: Epoch [25], Batch [793/938], Loss: 0.3638657331466675\n",
      "Validation: Epoch [25], Batch [794/938], Loss: 0.3675556480884552\n",
      "Validation: Epoch [25], Batch [795/938], Loss: 0.45750030875205994\n",
      "Validation: Epoch [25], Batch [796/938], Loss: 0.5679140090942383\n",
      "Validation: Epoch [25], Batch [797/938], Loss: 0.7123905420303345\n",
      "Validation: Epoch [25], Batch [798/938], Loss: 0.3949149250984192\n",
      "Validation: Epoch [25], Batch [799/938], Loss: 0.34614574909210205\n",
      "Validation: Epoch [25], Batch [800/938], Loss: 0.3555540144443512\n",
      "Validation: Epoch [25], Batch [801/938], Loss: 0.3798903226852417\n",
      "Validation: Epoch [25], Batch [802/938], Loss: 0.5335543751716614\n",
      "Validation: Epoch [25], Batch [803/938], Loss: 0.34704333543777466\n",
      "Validation: Epoch [25], Batch [804/938], Loss: 0.38345298171043396\n",
      "Validation: Epoch [25], Batch [805/938], Loss: 0.2528547942638397\n",
      "Validation: Epoch [25], Batch [806/938], Loss: 0.44573771953582764\n",
      "Validation: Epoch [25], Batch [807/938], Loss: 0.5660200715065002\n",
      "Validation: Epoch [25], Batch [808/938], Loss: 0.2303418070077896\n",
      "Validation: Epoch [25], Batch [809/938], Loss: 0.3727126717567444\n",
      "Validation: Epoch [25], Batch [810/938], Loss: 0.282925546169281\n",
      "Validation: Epoch [25], Batch [811/938], Loss: 0.4774891138076782\n",
      "Validation: Epoch [25], Batch [812/938], Loss: 0.726304292678833\n",
      "Validation: Epoch [25], Batch [813/938], Loss: 0.48064103722572327\n",
      "Validation: Epoch [25], Batch [814/938], Loss: 0.4321010112762451\n",
      "Validation: Epoch [25], Batch [815/938], Loss: 0.3196282386779785\n",
      "Validation: Epoch [25], Batch [816/938], Loss: 0.6305150985717773\n",
      "Validation: Epoch [25], Batch [817/938], Loss: 0.4615807831287384\n",
      "Validation: Epoch [25], Batch [818/938], Loss: 0.4527876079082489\n",
      "Validation: Epoch [25], Batch [819/938], Loss: 0.5137284398078918\n",
      "Validation: Epoch [25], Batch [820/938], Loss: 0.4813461899757385\n",
      "Validation: Epoch [25], Batch [821/938], Loss: 0.5072340369224548\n",
      "Validation: Epoch [25], Batch [822/938], Loss: 0.5167176127433777\n",
      "Validation: Epoch [25], Batch [823/938], Loss: 0.35915204882621765\n",
      "Validation: Epoch [25], Batch [824/938], Loss: 0.5736156105995178\n",
      "Validation: Epoch [25], Batch [825/938], Loss: 0.38990524411201477\n",
      "Validation: Epoch [25], Batch [826/938], Loss: 0.4807482063770294\n",
      "Validation: Epoch [25], Batch [827/938], Loss: 0.312123566865921\n",
      "Validation: Epoch [25], Batch [828/938], Loss: 0.2876278758049011\n",
      "Validation: Epoch [25], Batch [829/938], Loss: 0.2801765203475952\n",
      "Validation: Epoch [25], Batch [830/938], Loss: 0.3114183843135834\n",
      "Validation: Epoch [25], Batch [831/938], Loss: 0.31096795201301575\n",
      "Validation: Epoch [25], Batch [832/938], Loss: 0.3561600148677826\n",
      "Validation: Epoch [25], Batch [833/938], Loss: 0.36206531524658203\n",
      "Validation: Epoch [25], Batch [834/938], Loss: 0.5928363800048828\n",
      "Validation: Epoch [25], Batch [835/938], Loss: 0.3205845057964325\n",
      "Validation: Epoch [25], Batch [836/938], Loss: 0.485892653465271\n",
      "Validation: Epoch [25], Batch [837/938], Loss: 0.39998725056648254\n",
      "Validation: Epoch [25], Batch [838/938], Loss: 0.7021931409835815\n",
      "Validation: Epoch [25], Batch [839/938], Loss: 0.33009085059165955\n",
      "Validation: Epoch [25], Batch [840/938], Loss: 0.26644372940063477\n",
      "Validation: Epoch [25], Batch [841/938], Loss: 0.31228896975517273\n",
      "Validation: Epoch [25], Batch [842/938], Loss: 0.29137179255485535\n",
      "Validation: Epoch [25], Batch [843/938], Loss: 0.5263222455978394\n",
      "Validation: Epoch [25], Batch [844/938], Loss: 0.5657547116279602\n",
      "Validation: Epoch [25], Batch [845/938], Loss: 0.4918949604034424\n",
      "Validation: Epoch [25], Batch [846/938], Loss: 0.47757047414779663\n",
      "Validation: Epoch [25], Batch [847/938], Loss: 0.34151601791381836\n",
      "Validation: Epoch [25], Batch [848/938], Loss: 0.3153798580169678\n",
      "Validation: Epoch [25], Batch [849/938], Loss: 0.3346005082130432\n",
      "Validation: Epoch [25], Batch [850/938], Loss: 0.4182860553264618\n",
      "Validation: Epoch [25], Batch [851/938], Loss: 0.40387722849845886\n",
      "Validation: Epoch [25], Batch [852/938], Loss: 0.548582911491394\n",
      "Validation: Epoch [25], Batch [853/938], Loss: 0.3089103102684021\n",
      "Validation: Epoch [25], Batch [854/938], Loss: 0.26589858531951904\n",
      "Validation: Epoch [25], Batch [855/938], Loss: 0.4036285877227783\n",
      "Validation: Epoch [25], Batch [856/938], Loss: 0.28046852350234985\n",
      "Validation: Epoch [25], Batch [857/938], Loss: 0.5604197978973389\n",
      "Validation: Epoch [25], Batch [858/938], Loss: 0.5259062051773071\n",
      "Validation: Epoch [25], Batch [859/938], Loss: 0.2947680950164795\n",
      "Validation: Epoch [25], Batch [860/938], Loss: 0.46471965312957764\n",
      "Validation: Epoch [25], Batch [861/938], Loss: 0.3730148375034332\n",
      "Validation: Epoch [25], Batch [862/938], Loss: 0.35198158025741577\n",
      "Validation: Epoch [25], Batch [863/938], Loss: 0.507416844367981\n",
      "Validation: Epoch [25], Batch [864/938], Loss: 0.33103030920028687\n",
      "Validation: Epoch [25], Batch [865/938], Loss: 0.33629339933395386\n",
      "Validation: Epoch [25], Batch [866/938], Loss: 0.5825013518333435\n",
      "Validation: Epoch [25], Batch [867/938], Loss: 0.627188503742218\n",
      "Validation: Epoch [25], Batch [868/938], Loss: 0.4673900306224823\n",
      "Validation: Epoch [25], Batch [869/938], Loss: 0.5046333074569702\n",
      "Validation: Epoch [25], Batch [870/938], Loss: 0.5697753429412842\n",
      "Validation: Epoch [25], Batch [871/938], Loss: 0.3816024661064148\n",
      "Validation: Epoch [25], Batch [872/938], Loss: 0.45164215564727783\n",
      "Validation: Epoch [25], Batch [873/938], Loss: 0.3936334550380707\n",
      "Validation: Epoch [25], Batch [874/938], Loss: 0.2713138163089752\n",
      "Validation: Epoch [25], Batch [875/938], Loss: 0.5255993008613586\n",
      "Validation: Epoch [25], Batch [876/938], Loss: 0.392293244600296\n",
      "Validation: Epoch [25], Batch [877/938], Loss: 0.3285753130912781\n",
      "Validation: Epoch [25], Batch [878/938], Loss: 0.5112305879592896\n",
      "Validation: Epoch [25], Batch [879/938], Loss: 0.5521378517150879\n",
      "Validation: Epoch [25], Batch [880/938], Loss: 0.26857197284698486\n",
      "Validation: Epoch [25], Batch [881/938], Loss: 0.29108163714408875\n",
      "Validation: Epoch [25], Batch [882/938], Loss: 0.3146921396255493\n",
      "Validation: Epoch [25], Batch [883/938], Loss: 0.449775755405426\n",
      "Validation: Epoch [25], Batch [884/938], Loss: 0.6163977980613708\n",
      "Validation: Epoch [25], Batch [885/938], Loss: 0.35138171911239624\n",
      "Validation: Epoch [25], Batch [886/938], Loss: 0.34747418761253357\n",
      "Validation: Epoch [25], Batch [887/938], Loss: 0.25878921151161194\n",
      "Validation: Epoch [25], Batch [888/938], Loss: 0.5241335034370422\n",
      "Validation: Epoch [25], Batch [889/938], Loss: 0.4099179208278656\n",
      "Validation: Epoch [25], Batch [890/938], Loss: 0.48059749603271484\n",
      "Validation: Epoch [25], Batch [891/938], Loss: 0.46633249521255493\n",
      "Validation: Epoch [25], Batch [892/938], Loss: 0.37757188081741333\n",
      "Validation: Epoch [25], Batch [893/938], Loss: 0.43555906414985657\n",
      "Validation: Epoch [25], Batch [894/938], Loss: 0.3916870355606079\n",
      "Validation: Epoch [25], Batch [895/938], Loss: 0.37727898359298706\n",
      "Validation: Epoch [25], Batch [896/938], Loss: 0.4554145932197571\n",
      "Validation: Epoch [25], Batch [897/938], Loss: 0.42978158593177795\n",
      "Validation: Epoch [25], Batch [898/938], Loss: 0.5450667142868042\n",
      "Validation: Epoch [25], Batch [899/938], Loss: 0.2939128577709198\n",
      "Validation: Epoch [25], Batch [900/938], Loss: 0.2598024308681488\n",
      "Validation: Epoch [25], Batch [901/938], Loss: 0.3134062588214874\n",
      "Validation: Epoch [25], Batch [902/938], Loss: 0.4894457459449768\n",
      "Validation: Epoch [25], Batch [903/938], Loss: 0.36585497856140137\n",
      "Validation: Epoch [25], Batch [904/938], Loss: 0.5312947630882263\n",
      "Validation: Epoch [25], Batch [905/938], Loss: 0.5746394395828247\n",
      "Validation: Epoch [25], Batch [906/938], Loss: 0.5087406635284424\n",
      "Validation: Epoch [25], Batch [907/938], Loss: 0.7557876110076904\n",
      "Validation: Epoch [25], Batch [908/938], Loss: 0.29355892539024353\n",
      "Validation: Epoch [25], Batch [909/938], Loss: 0.47452986240386963\n",
      "Validation: Epoch [25], Batch [910/938], Loss: 0.22490009665489197\n",
      "Validation: Epoch [25], Batch [911/938], Loss: 0.3744603991508484\n",
      "Validation: Epoch [25], Batch [912/938], Loss: 0.3859965205192566\n",
      "Validation: Epoch [25], Batch [913/938], Loss: 0.2339542806148529\n",
      "Validation: Epoch [25], Batch [914/938], Loss: 0.4759705066680908\n",
      "Validation: Epoch [25], Batch [915/938], Loss: 0.37326639890670776\n",
      "Validation: Epoch [25], Batch [916/938], Loss: 0.7082914113998413\n",
      "Validation: Epoch [25], Batch [917/938], Loss: 0.2577041983604431\n",
      "Validation: Epoch [25], Batch [918/938], Loss: 0.2416612207889557\n",
      "Validation: Epoch [25], Batch [919/938], Loss: 0.2467842847108841\n",
      "Validation: Epoch [25], Batch [920/938], Loss: 0.41412442922592163\n",
      "Validation: Epoch [25], Batch [921/938], Loss: 0.4720925986766815\n",
      "Validation: Epoch [25], Batch [922/938], Loss: 0.20096145570278168\n",
      "Validation: Epoch [25], Batch [923/938], Loss: 0.528427004814148\n",
      "Validation: Epoch [25], Batch [924/938], Loss: 0.4957082271575928\n",
      "Validation: Epoch [25], Batch [925/938], Loss: 0.28324660658836365\n",
      "Validation: Epoch [25], Batch [926/938], Loss: 0.4490789771080017\n",
      "Validation: Epoch [25], Batch [927/938], Loss: 0.3959842324256897\n",
      "Validation: Epoch [25], Batch [928/938], Loss: 0.31538522243499756\n",
      "Validation: Epoch [25], Batch [929/938], Loss: 0.541104257106781\n",
      "Validation: Epoch [25], Batch [930/938], Loss: 0.2220185250043869\n",
      "Validation: Epoch [25], Batch [931/938], Loss: 0.30341288447380066\n",
      "Validation: Epoch [25], Batch [932/938], Loss: 0.3774162232875824\n",
      "Validation: Epoch [25], Batch [933/938], Loss: 0.4120255410671234\n",
      "Validation: Epoch [25], Batch [934/938], Loss: 0.24952194094657898\n",
      "Validation: Epoch [25], Batch [935/938], Loss: 0.30443114042282104\n",
      "Validation: Epoch [25], Batch [936/938], Loss: 0.4331323206424713\n",
      "Validation: Epoch [25], Batch [937/938], Loss: 0.3374575674533844\n",
      "Validation: Epoch [25], Batch [938/938], Loss: 0.6330867409706116\n",
      "Accuracy of test set: 0.8530166666666666\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model([train_loader, test_loader], num_epochs=25, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceb5783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.8530166666666666\n"
     ]
    }
   ],
   "source": [
    "print(f'Final test accuracy: {test_accuracies[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e128ed",
   "metadata": {},
   "source": [
    "## Visualization of the labels and predictions\n",
    "\n",
    "In this section, you should visual one image from each class and show both the actual label and the predicted label for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c0b79fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_labels_predictions(data_loader):\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    \n",
    "    class_images = {class_name: None for class_name in class_names}\n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    for images, labels in data_loader:\n",
    "        # Predict labels using the trained model\n",
    "        outputs = model(images, parameters)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Iterate over the batch\n",
    "        for image, label, prediction in zip(images, labels, predicted):\n",
    "            class_name = class_names[label]\n",
    "            # If no image is stored for this class yet, store the current image\n",
    "            if class_images[class_name] is None:\n",
    "                class_images[class_name] = (image, label, prediction)\n",
    "                break\n",
    "    \n",
    "    # Visualize one image from each class along with actual and predicted labels\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "    for i, (class_name, (image, label, prediction)) in enumerate(class_images.items()):\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        ax.imshow(image.permute(1, 2, 0), cmap='gray')\n",
    "        ax.set_title(f'Actual: {class_name}\\nPredicted: {class_names[prediction]}')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_labels_predictions(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ece48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
